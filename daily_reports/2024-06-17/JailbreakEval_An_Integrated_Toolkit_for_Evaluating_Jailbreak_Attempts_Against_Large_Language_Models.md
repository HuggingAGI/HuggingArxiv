# JailbreakEval：大型语言模型越狱评估的集成工具箱
发布时间：2024年06月13日

`模型安全`
> JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models
>
> 越狱攻击试图让大型语言模型（LLMs）对禁令指令产生有害回应，严重威胁到LLMs的正当使用。尽管关于越狱攻击与防御的研究正在兴起，但如何判断一次越狱尝试是否成功，目前仍无定论。评估LLMs回应有害性的方法五花八门，包括人工标注或特定方式激活GPT-4等，各有千秋，也各有短板，影响着它们与人类价值观的契合度及成本效益。这种评估方法的多样性，使得研究人员在选择评估工具和比较不同越狱策略时面临挑战。本文深入分析了越狱评估的各种方法，参考了2023年5月至2024年4月间近九十项相关研究。我们提出了一套系统的越狱评估者分类，详细剖析了各类方法的利弊及其应用现状。为推动后续研究，我们开发了JailbreakEval，一个便捷的越狱评估工具包，内置多种评估工具，用户一键即可获取结果。JailbreakEval还支持用户在统一框架下定制评估流程，便于开发与比较。综上所述，JailbreakEval简化了越狱研究的评估流程，并推动了社区内越狱评估标准的统一。
>
> https://arxiv.org/abs/2406.09321

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.09321/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.09321/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.09321/x4.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2406.09321](https://arxiv.org/abs/2406.09321)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 最新论文订阅体验（3天）：公众号号菜单回复1
- 最新论文订阅新人优惠：公众号号菜单回复2