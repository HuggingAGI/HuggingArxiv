# 技巧大全：大型语言模型越狱攻击的基准评测
发布时间：2024年06月13日

`模型安全`
> Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs
>
> 大型语言模型（LLMs）虽在零-shot任务中表现出色，却易受越狱攻击，可能产生有害结果。近期研究将此类攻击细分为令牌级和提示级，但多聚焦于模型漏洞，忽视了防御策略。为此，我们分析了多种攻击场景对LLM的影响，并设定了越狱攻击的基准，推动标准化评估框架的发展。我们详细考察了LLMs上越狱攻击的八大关键因素，并针对六种防御技术，在两大常用数据集上进行了七项典型攻击，进行了约320次实验，耗时约50,000 GPU小时。实验表明，对防御增强型LLMs进行越狱攻击的标准化评估至关重要。相关代码已公开于https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking。
>
> https://arxiv.org/abs/2406.09324


<hr />

- 论文原文: [https://arxiv.org/abs/2406.09324](https://arxiv.org/abs/2406.09324)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 最新论文订阅体验（3天）：公众号号菜单回复1
- 最新论文订阅新人优惠：公众号号菜单回复2