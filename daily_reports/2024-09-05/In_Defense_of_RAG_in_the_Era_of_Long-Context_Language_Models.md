# 在长上下文语言模型盛行的当下，我们为 RAG 正名。
发布时间：2024年09月03日

`RAG`
> In Defense of RAG in the Era of Long-Context Language Models
>
> 尽管长上下文大型语言模型在处理长文本序列时表现出色，但我们认为其极长的上下文可能导致信息聚焦不足，从而影响答案质量。为此，我们重新探索了 RAG 在长上下文问答中的应用，并提出了一种创新的 OP-RAG 机制。该机制通过优化检索过程，确保在增加信息量的同时，答案质量呈现先升后降的倒 U 形曲线，找到最佳平衡点。实验结果显示，OP-RAG 在保持较低输入量的前提下，实现了比长上下文大型语言模型更高的答案质量，展现了其显著优势。
>
> https://arxiv.org/abs/2409.01666

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2409.01666](https://arxiv.org/abs/2409.01666)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)