# 探究与评估 LLM 在面对越狱攻击时的可靠性
发布时间：2024年08月17日

`模型安全`
> Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks
>
> 随着大型语言模型（LLM）在内容生成中的作用日益凸显，其社会影响力也愈发显著。然而，这些模型潜在的生成有害内容的能力不容忽视。为此，业界已采取措施，如加强社会伦理的遵守，但“越狱”现象——即通过精心设计的提示诱导模型产生有害反应——仍是一大挑战。面对这一持续威胁，本研究不仅构建了一个全面的评估框架，还开展了一项大规模实证实验，旨在深入分析LLM的抗攻击能力。我们选取了10种前沿的越狱策略、涵盖61个有害类别的1525个问题，以及13个主流LLM，运用攻击成功率、毒性评分、流畅度等多维度指标，全面评估模型在越狱情境下的表现。通过这些指标的综合分析，我们为各LLM提供了详尽的可靠性评分，并提出针对性建议，以降低其易受攻击性。此外，我们还深入探讨了模型、攻击策略与有害内容之间的关联，以及评估指标间的相互关系，从而验证了我们评估框架的多维有效性。实验结果显示，所有测试的LLM在某些策略面前均显脆弱，这强调了提升LLM可靠性的紧迫性。我们坚信，本研究能为LLM安全评估领域提供宝贵洞见，助力其抵御越狱威胁。
>
> https://arxiv.org/abs/2408.09326

**如遇无法添加，请+ vx: iamxxn886**
<hr />

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.09326/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.09326/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.09326/x4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.09326/x5.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.09326/x6.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.09326/x7.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2408.09326](https://arxiv.org/abs/2408.09326)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)