![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/imgs/follow2.gif)
# 负面提示：运用心理学原理，通过负面情感刺激来提升大型语言模型的性能
发布时间：2024年05月05日

`提示工程`
> NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli
>
> 大型语言模型（LLMs）在从传统计算到高级人工智能应用的广泛应用中扮演着核心角色。这种普及推动了跨学科对LLMs的深入研究，特别是在社会科学领域。研究发现，LLMs具备情感智能，且能通过正面情感刺激得到提升。这引出了一个问题：负面情绪是否也能以类似的方式影响LLMs，进而提升它们的性能？针对这一疑问，我们提出了NegativePrompt，这是一种基于心理学原理的新方法，包含十个精心设计的情感刺激。我们对五个主流的LLMs——Flan-T5-Large、Vicuna、Llama 2、ChatGPT和GPT-4——进行了45项任务的严格实验评估。结果显示，NegativePrompt显著提升了LLMs的性能，特别是在指令归纳任务中提升了12.89%，在BIG-Bench任务中提升了46.25%。此外，我们还进行了注意力可视化实验，以揭示NegativePrompt影响的内在机制。本研究不仅深化了我们对LLMs与情感互动的理解，还证实了NegativePrompt作为一种情感驱动方法的有效性，并为LLMs在现实世界应用中的性能提升提供了新的视角。相关代码可在 https://github.com/wangxu0820/NegativePrompt 查看。
>
> https://arxiv.org/abs/2405.02814

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2405.02814/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2405.02814/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2405.02814/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2405.02814/x4.png)


- 论文原文: [https://arxiv.org/abs/2405.02814](https://arxiv.org/abs/2405.02814)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886