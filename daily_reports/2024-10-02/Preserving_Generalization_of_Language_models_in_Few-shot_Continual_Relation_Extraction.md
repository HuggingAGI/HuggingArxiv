# 少样本持续关系提取中，如何保持语言模型的泛化能力？
发布时间：2024年09月30日

`提示工程`
> Preserving Generalization of Language models in Few-shot Continual Relation Extraction
>
> Few-shot Continual Relations Extraction (FCRE) 是一个充满活力的研究领域，模型能在有限标注数据下整合新知识，同时避免遗忘并保留预训练知识。我们提出了一种新方法，利用常被忽视的语言模型头部，通过互信息最大化策略，保持预训练知识的连续性，并优化分类性能。此外，我们探索了大型语言模型 (LLM) 在 FCRE 中的应用潜力。实验结果表明，该方法有效，并为未来研究提供了重要启示。
>
> https://arxiv.org/abs/2410.00334

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2410.00334](https://arxiv.org/abs/2410.00334)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)