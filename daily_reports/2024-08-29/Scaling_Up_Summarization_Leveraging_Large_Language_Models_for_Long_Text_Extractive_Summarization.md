# 扩展摘要领域：借助大型语言模型提升长文本抽取式摘要的效率
发布时间：2024年08月28日


> Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization
>
> 在数字文本爆炸性增长的时代，高效摘要工具变得至关重要。尽管大型语言模型 (LLM) 在 NLP 任务中表现出色，但其在抽取式摘要中的应用仍有待深入。本文推出的 EYEGLAXS 框架，专为长文本的抽取式摘要设计，采用 LLAMA2-7B 和 ChatGLM2-6B 模型，确保事实准确与语法正确，避免了生成式方法的常见问题。通过集成 Flash Attention 和参数高效微调 (PEFT) 技术，EYEGLAXS 有效应对了 LLM 的计算与资源挑战，并在 PubMed 和 ArXiv 等数据集上刷新了性能记录。此外，我们的研究还探讨了 LLM 在不同序列长度处理上的适应性及其在小型数据集上的训练效率，为抽取式文本摘要领域树立了新标杆，并启发了未来研究的新方向。
>
> https://arxiv.org/abs/2408.15801

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2408.15801](https://arxiv.org/abs/2408.15801)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)