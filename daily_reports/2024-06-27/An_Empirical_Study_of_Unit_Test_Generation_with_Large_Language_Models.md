# 大型语言模型在单元测试生成领域的实证探索
发布时间：2024年06月26日

`代码编写`
> An Empirical Study of Unit Test Generation with Large Language Models
>
> 单元测试在软件开发中至关重要，但手动编写既费时又具挑战性。大型语言模型（LLMs）的出现为自动化单元测试生成开辟了新途径。尽管现有研究多聚焦于闭源模型如ChatGPT和CodeX，但开源LLMs在数据隐私保护和某些任务上的表现已显示出其优势。有效的提示策略对发挥LLMs潜力至关重要。本研究首次基于17个Java项目，对比了五种不同结构和规模的开源LLMs，以及全面的评估指标，揭示了提示因素对性能的显著影响，并与商业GPT-4及传统工具Evosuite进行了比较，指出了基于LLM的单元测试生成中的局限。研究结果为未来基于LLM的单元测试生成提供了指导和启示。
>
> https://arxiv.org/abs/2406.18181


<hr />

- 论文原文: [https://arxiv.org/abs/2406.18181](https://arxiv.org/abs/2406.18181)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 加入社群，公众号回复LLM
- 最新论文订阅体验：公众号号菜单回复1