# 大型语言模型真的无偏见吗？通过“越狱”提示评估对抗性鲁健对偏见引发的挑战。
发布时间：2024年07月11日

`模型安全`
> Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation
>
> 大型语言模型（LLM）在人工智能领域展现了惊人的计算与语言能力，但它们也易受训练数据中的多种偏见影响，如选择、语言和确认偏见，以及性别、种族、性取向等刻板印象。本研究深入分析了最新LLM在回答中体现的这些偏见，及其对模型公平性与可靠性的影响。同时，我们探索了如何利用提示工程技术揭示LLM的隐性偏见，并测试其对抗越狱提示的鲁棒性。通过广泛实验，我们发现即便LLM具备高级功能与精密对齐流程，仍可能被诱导产生偏颇或不当回应。因此，强化偏见缓解技术，对于构建更可持续、更包容的人工智能至关重要。
>
> https://arxiv.org/abs/2407.08441

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.08441/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.08441/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.08441/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.08441/x4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.08441/x5.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.08441/x6.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2407.08441](https://arxiv.org/abs/2407.08441)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 加入社群，公众号回复LLM
- 最新论文订阅体验：公众号号菜单回复1