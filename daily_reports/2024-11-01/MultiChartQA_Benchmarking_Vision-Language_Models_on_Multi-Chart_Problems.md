# MultiChartQA：评估视觉-语言模型在多图表问题上的表现
发布时间：2024年10月18日


> MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems
>
> 多模态大型语言模型 (MLLM) 在视觉问答和图表理解等任务中表现出色，但现有图表任务基准难以应对现实世界中多图表场景的复杂性。当前基准多聚焦于单图表任务，忽视了实际应用中必需的多图表信息整合与多跳推理。为此，我们推出了 MultiChartQA 基准，涵盖直接问答、并行问答、比较推理和顺序推理四大领域，全面评估 MLLM 能力。评估结果显示，MLLM 与人类表现存在显著差距，凸显了多图表理解的挑战，并展示了 MultiChartQA 推动该领域发展的潜力。代码与数据已公开，详见 https://github.com/Zivenzhu/Multi-chart-QA。
>
> https://arxiv.org/abs/2410.14179

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2410.14179](https://arxiv.org/abs/2410.14179)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)