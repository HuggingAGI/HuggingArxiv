# 大型语言模型的提示混淆技术
发布时间：2024年09月17日


> Prompt Obfuscation for Large Language Models
>
> 系统提示包含详细指令，描述了底层大型语言模型（LLM）的任务，能轻松将基础模型转化为工具和服务，且开销极低。因其对实用性的关键影响，常被视为知识产权，如同软件代码。然而，通过提示注入，提取系统提示易如反掌。至今，尚无有效对策防止系统提示被盗，所有保护措施均可被精心设计的提示注入绕过。为此，我们提出替代传统系统提示的新方法——提示混淆。通过混淆，我们既能防止提示被提取，又仅以微小开销维持系统功能。核心在于找到一种表示，使混淆后的提示与原始提示功能相同，但无法从中推断出原始信息。我们采用基于优化的方法实现这一目标。为评估效果，我们对比了原始与混淆提示下的系统性能，结果显示混淆版本始终与原始版本相当。此外，我们进行了三种去混淆攻击，证明即使掌握混淆提示和LLM，也无法稳定提取有用信息。综上所述，提示混淆是一种有效保护知识产权、同时保持原始提示实用性的方法。
>
> https://arxiv.org/abs/2409.11026

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2409.11026](https://arxiv.org/abs/2409.11026)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)