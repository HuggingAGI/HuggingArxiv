# CogVLM2：专为图像与视频理解设计的视觉语言模型
发布时间：2024年08月29日

`多模态大模型`
> CogVLM2: Visual Language Models for Image and Video Understanding
>
> 摘要：我们始于 VisualGLM 和 CogVLM，持续探索 VLMs，旨在提升视觉与语言的融合、构建高效的高分辨率架构，并拓展更多模态与应用。本文介绍 CogVLM2 系列，新一代视觉语言模型，涵盖图像与视频理解，包括 CogVLM2、CogVLM2-Video 及 GLM-4V。CogVLM2 作为图像理解模型，沿用视觉专家架构，优化预训练与后训练流程，最高支持 $1344 \times 1344$ 像素输入。CogVLM2-Video 则整合时间戳多帧输入，并创新自动化时间定位数据构建。该系列在 MMBench、MM-Vet、TextVQA、MVBench 及 VCGBench 等评测中表现卓越。所有模型已开源，助力领域进步。
>
> https://arxiv.org/pdf/2408.16500

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />


<hr />

- 论文原文: [https://arxiv.org/pdf/2408.16500](https://arxiv.org/pdf/2408.16500)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)