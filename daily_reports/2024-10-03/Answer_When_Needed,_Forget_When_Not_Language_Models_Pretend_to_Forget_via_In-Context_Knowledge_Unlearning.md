# 语言模型能够“按需遗忘”：在需要时提供答案，在不需要时通过上下文知识遗忘来假装遗忘。
发布时间：2024年10月01日


> Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning
>
> 随着 LLM 在各领域的广泛应用，选择性遗忘特定信息的能力变得至关重要。例如，LLM 需向内部授权用户提供机密信息，同时对外部用户保密。为此，我们提出“上下文知识遗忘”方法，使模型在测试时根据查询上下文选择性遗忘信息。实验表明，该方法在遗忘准确率上高达 95%，同时保留 80% 不相关知识，显著优于基线。深入研究发现，微调后的 LLM 在最后一层做出遗忘决定，即“LLM 假装遗忘”。这一发现为提升 LLM 遗忘机制的鲁棒性提供了新思路，为未来研究奠定基础。
>
> https://arxiv.org/abs/2410.00382

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2410.00382](https://arxiv.org/abs/2410.00382)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)