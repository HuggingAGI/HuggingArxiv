# 在数据稀缺的情况下，借助大型语言模型和检索增强生成技术，提升紧凑型模型的性能。
发布时间：2024年10月01日

`RAG`
> Boosting the Capabilities of Compact Models in Low-Data Contexts with Large Language Models and Retrieval-Augmented Generation
>
> 当前语言建模技术对数据和计算的高需求，给低资源语言的处理带来了难题。声明性语言学知识通过提供特定语言规则，有望部分缓解这一数据短缺问题。本文提出了一种由大型语言模型（LLM）支持的检索增强生成（RAG）框架，用于改进小模型在形态学注释任务中的表现。我们利用语言信息弥补数据和参数的不足，同时引入LLM解析的描述语法。实验表明，结合语法输入、LLM的解析能力及小模型的可训练性，可大幅提升性能与效率。我们发现，RAG支持的紧凑模型在数据稀缺环境下表现卓越，刷新了该任务及目标语言的记录。此外，我们的方法为语言学家提供了更可靠的形态学注释工具，每个输出都附有详尽解释和置信度评分。
>
> https://arxiv.org/abs/2410.00387

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2410.00387](https://arxiv.org/abs/2410.00387)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)