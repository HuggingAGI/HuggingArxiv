# LLMSecCode：探究大型语言模型在安全编码领域的评估
发布时间：2024年08月28日

`代码编写`
> LLMSecCode: Evaluating Large Language Models for Secure Coding
>
> 在快速部署大型语言模型 (LLM) 时，必须慎重考虑其对网络安全的影响。我们的研究聚焦于优化适合安全编码 (SC) 的 LLM 选择流程。为此，我们提出了几个关键研究问题：如何简化 LLM 评估？评估应侧重哪些方面？如何确保评估的公正性？为解答这些问题，我们开发了开源框架 LLMSecCode，旨在客观评估 LLM 的 SC 能力。实验表明，参数和提示的变化分别导致 10% 和 9% 的性能差异。与外部权威结果对比，差异仅为 5%。我们致力于提升框架的易用性，并欢迎外部贡献以推动其发展。借助 LLMSecCode，我们期望推动安全编码领域 LLM 能力的标准化与基准测试。
>
> https://arxiv.org/abs/2408.16100

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2408.16100](https://arxiv.org/abs/2408.16100)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)