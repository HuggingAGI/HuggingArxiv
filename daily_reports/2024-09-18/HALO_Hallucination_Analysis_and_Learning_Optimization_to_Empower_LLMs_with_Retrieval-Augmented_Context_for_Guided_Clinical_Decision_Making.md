# HALO：通过幻觉分析与学习优化，为 LLM 注入检索增强的上下文，助力临床决策的精准引导。
发布时间：2024年09月16日

`RAG`
> HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making
>
> 大型语言模型（LLM）在自然语言处理方面取得了巨大进步，但它们容易产生不准确或不可靠的回答，这种现象被称为“幻觉”。在医疗等关键领域，这种幻觉可能带来严重风险。为此，我们推出了 HALO 框架，通过检测和缓解幻觉，显著提升医疗问答系统的准确性和可靠性。HALO 利用 LLM 生成查询的多种变体，并从外部知识库中检索相关信息，丰富上下文。通过最大边际相关性评分，优先处理检索到的信息，再由 LLM 生成答案，从而降低幻觉风险。LangChain 的集成进一步优化了这一流程，使得 Llama-3.1 和 ChatGPT 等模型的准确性大幅提升（Llama-3.1 从 44% 提升至 65%，ChatGPT 从 56% 提升至 70%）。HALO 框架凸显了在医疗问答系统中解决幻觉问题的重要性，最终有助于改善临床决策和患者护理。HALO 已开源，可在 https://github.com/ResponsibleAILab/HALO 获取。
>
> https://arxiv.org/abs/2409.10011

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2409.10011](https://arxiv.org/abs/2409.10011)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)