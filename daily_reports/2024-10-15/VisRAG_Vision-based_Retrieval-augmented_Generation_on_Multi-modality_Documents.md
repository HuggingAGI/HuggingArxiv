# VisRAG：多模态文档上的视觉检索增强生成
发布时间：2024年10月14日


> VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents
>
> Retrieval-augmented generation (RAG) 让大型语言模型 (LLM) 能够借助外部知识源进行生成，但现有系统仅依赖文本，无法利用视觉信息如布局和图像，这些在多模态文档中至关重要。本文提出的 VisRAG，通过构建基于视觉-语言模型 (VLM) 的 RAG 流程，直接将文档作为图像嵌入并检索，从而增强生成效果。相比传统文本 RAG，VisRAG 避免了解析过程中的信息损失，更全面地保留了原始文档的信息。实验显示，VisRAG 在检索和生成阶段均超越传统 RAG，端到端性能提升达 25-39%。VisRAG 不仅有效利用训练数据，还展现出强大的泛化能力，成为多模态文档 RAG 的理想选择。代码和数据已公开，详见 https://github.com/openbmb/visrag。
>
> https://arxiv.org/abs/2410.10594

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2410.10594](https://arxiv.org/abs/2410.10594)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)