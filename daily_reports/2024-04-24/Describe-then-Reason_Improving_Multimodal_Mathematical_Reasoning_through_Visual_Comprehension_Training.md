# 通过视觉理解训练提升多模态数学推理“描述然后推理”：这一方法通过增强视觉理解训练，显著提升了处理多模态数学问题推理的能力。
发布时间：2024年04月24日
`多模态大模型`
> 开源的多模态大型语言模型（MLLMs）在处理文本和视觉输入的任务上表现优异，但面对复杂的多模态数学推理挑战时，相较于 GPT-4V(ision) 和 Gemini-Pro 等专有模型，仍显不足。尽管通过细化中间步骤（理由）的微调能够提升一定的数学推理能力，但现有模型在视觉理解上仍有欠缺，主要是由于缺乏针对视觉的充分指导，这导致了对数学图表的解读不够精准。为应对这一挑战，我们设计了 VCAR，一个两阶段训练流程，特别强化了数学推理之外的视觉理解能力。该流程首先通过视觉描述生成任务增强 MLLMs 的视觉理解力，随后进行第二阶段训练，即在已有描述的基础上生成理由。在两个广泛认可的基准测试中，VCAR 的表现显著超过了仅依赖理由指导的基线方法，特别是在视觉需求较高的问题上。


[https://wx.zsxq.com/dweb2/index/topic_detail/1522451511111122](https://wx.zsxq.com/dweb2/index/topic_detail/1522451511111122)

[https://arxiv.org/abs/2404.14604](https://arxiv.org/abs/2404.14604)