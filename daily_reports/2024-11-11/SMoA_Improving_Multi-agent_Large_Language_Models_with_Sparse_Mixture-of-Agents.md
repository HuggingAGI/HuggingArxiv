# SMoA：使用稀疏的多智能体混合来改进多智能体大型语言模型
发布时间：2024年11月05日


> SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents
>
> 虽然多智能体系统已被证明能在各种任务和应用中显著提高大型语言模型（LLM）的性能，但智能体之间的密集交互可能会阻碍其效率和多样性。为了应对这些挑战，我们从稀疏智能体混合（SMoE）中获得灵感，并提出了一个稀疏智能体混合（SMoA）框架，以提高多智能体LLM的效率和多样性。与完全连接的结构不同，SMoA引入了新颖的响应选择和提前停止机制，以使各个LLM智能体之间的信息流稀疏化，在性能和效率之间取得平衡。此外，受SMoE框架中专家多样性原则的启发，以平衡专家之间的工作负载，我们为每个LLM智能体分配了不同的角色描述，促进了多样化和发散性思维。在推理、对齐和公平基准上的大量实验表明，SMoA实现了与传统智能体混合方法相当的性能，但计算成本显著降低。进一步的分析表明，SMoA更稳定，具有更大的扩展能力，并通过超参数优化具有相当大的潜力。代码和数据将在：https://github.com/David-Li0406/SMoA 提供。
>
> https://arxiv.org/abs/2411.03284

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2411.03284](https://arxiv.org/abs/2411.03284)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)