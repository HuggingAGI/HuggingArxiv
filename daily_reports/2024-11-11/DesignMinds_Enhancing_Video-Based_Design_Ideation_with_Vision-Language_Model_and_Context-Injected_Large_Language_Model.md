# DesignMinds：使用视觉语言模型和上下文注入的大型语言模型增强基于视频的设计构思
发布时间：2024年11月06日


> DesignMinds: Enhancing Video-Based Design Ideation with Vision-Language Model and Context-Injected Large Language Model
>
> 构思是基于视频的设计（VBD）的关键组成部分，其中视频是设计探索和灵感的主要媒介。生成式人工智能的出现为通过简化视频分析和促进创意生成来增强这一过程提供了相当大的潜力。在本文中，我们提出了 DesignMinds，这是一个将最先进的视觉语言模型（VLM）与上下文增强的大型语言模型（LLM）集成在一起的原型，以支持 VBD 中的构思。为了评估 DesignMinds，我们对 35 名设计从业者进行了一项受试者间研究，将其性能与基线条件进行比较。我们的结果表明，DesignMinds 显著提高了构思的灵活性和原创性，同时也增加了任务参与度。重要的是，这项技术的引入没有对用户体验、技术接受度或可用性产生负面影响。
>
> https://arxiv.org/abs/2411.03827

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2411.03827](https://arxiv.org/abs/2411.03827)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)