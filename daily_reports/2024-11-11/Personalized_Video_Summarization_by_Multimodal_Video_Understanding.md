# 通过多模态视频理解实现个性化视频摘要
发布时间：2024年11月05日

`多模态大模型`
> Personalized Video Summarization by Multimodal Video Understanding
>
> 视频摘要技术已被证明在访问和理解视频内容方面能够改善整体用户体验。如果已知用户的偏好，视频摘要可以从输入视频中识别出重要信息或相关内容，帮助他们获取必要的信息或确定他们对观看原始视频的兴趣。使视频摘要适应各种类型的视频和用户偏好需要大量的训练数据和昂贵的人工标注。为了促进此类研究，我们提出了一个用于视频摘要的新基准，它涵盖了各种用户偏好。此外，我们提出了一个名为“具有语言的视频摘要（VSL）”的管道，用于基于预训练的视觉语言模型（VLMs）的用户偏好视频摘要，以避免需要在大型训练数据集上训练视频摘要系统。该管道将视频和隐藏字幕作为输入，并通过将视频帧转换为文本来在场景级别进行语义分析。随后，用户的类型偏好被用作选择相关文本场景的基础。实验结果表明，我们提出的管道优于当前最先进的无监督视频摘要模型。我们表明，与基于监督查询的视频摘要模型相比，我们的方法在不同数据集上更具适应性。最后，运行时分析表明，当增加用户偏好和视频的数量时，我们的管道更适合实际使用。
>
> https://arxiv.org/abs/2411.03531

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2411.03531](https://arxiv.org/abs/2411.03531)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)