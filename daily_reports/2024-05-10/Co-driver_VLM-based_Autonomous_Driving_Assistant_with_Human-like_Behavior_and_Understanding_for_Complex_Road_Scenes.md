![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/imgs/follow2.gif)
# 智能副驾：一款基于视觉语言模型的自动驾驶辅助系统，它模仿人类驾驶员的行为，并能深刻理解复杂多变的道路环境。
发布时间：2024年05月09日

`应用案例`
> Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes
>
> 基于大型语言模型的自动驾驶技术在规划与控制领域展现出光明前景，但计算资源的沉重负担和模型幻觉问题仍制约着精准轨迹预测与控制信号的指导。为此，我们创新性地推出了Co-driver系统，它通过理解道路场景，赋予自动驾驶车辆灵活的驾驶行为。我们通过CARLA模拟器与ROS2系统验证了Co-driver的效能，并巧妙地利用了Nvidia 4090 24G GPU的强大性能。此外，我们还提供了一个包含图像与提示的数据集，用于优化系统的视觉语言模型。在实际驾驶测试中，Co-driver在夜间与阴暗环境下的成功率分别高达96.16%与89.7%，表现出色。我们将在https://github.com/ZionGo6/Co-driver公开Co-driver数据集，以飨业界。
>
> https://arxiv.org/abs/2405.05885


<hr />

- 论文原文: [https://arxiv.org/abs/2405.05885](https://arxiv.org/abs/2405.05885)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886