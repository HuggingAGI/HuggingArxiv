![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/imgs/follow2.gif)
# 微调 LLMs 以吸收新知，是否会催生幻觉？
发布时间：2024年05月09日

`动手训练`
> Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?
>
> 大型语言模型在监督式微调过程中可能会接触到预训练之外的新事实，这可能导致模型产生事实错误的幻觉。我们通过闭卷问答的受控实验，探究了微调中引入新知识对模型利用先前知识能力的影响。实验表明，大型语言模型在微调中学习新事实的速度较慢，但一旦学会，却会线性增加产生幻觉的倾向。我们的研究强调了微调中新知识引入的风险，并指出大型语言模型主要通过预训练获取事实知识，而微调则使其更高效地运用这些知识。
>
> https://arxiv.org/abs/2405.05904


<hr />

- 论文原文: [https://arxiv.org/abs/2405.05904](https://arxiv.org/abs/2405.05904)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886