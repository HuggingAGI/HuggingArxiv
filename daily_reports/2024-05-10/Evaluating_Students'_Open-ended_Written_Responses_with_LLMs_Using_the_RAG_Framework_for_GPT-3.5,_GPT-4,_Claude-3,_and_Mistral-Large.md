![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/imgs/follow2.gif)
# 借助RAG框架，我们评估了GPT-3.5、GPT-4、Claude-3和Mistral-Large在评价学生开放式书面回答方面的表现。本研究旨在探索这些大型语言模型在教育评估中的应用潜力。
发布时间：2024年05月08日

`RAG`
> Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large
>
> 教育工作者在评估学生的开放式书面考试答案时面临着既重要又耗时的挑战，这要求他们付出巨大的努力、保持一致性和精确性。大型语言模型（LLMs）的进步是解决这一难题的希望之光，它们能够在确保评估质量的同时节省教育工作者的时间。在我们的研究中，我们测试了ChatGPT-3.5、ChatGPT-4、Claude-3和Mistral-Large这四种LLMs在评估大学生对所学材料的开放式问题答案时的效果。每个模型在两种不同的温度设置下重复评估了54个答案，总共进行了4,320次评估。我们使用了RAG框架来指导LLMs进行评估。到2024年春季，我们的分析显示，这些LLMs在评分一致性和结果上存在显著差异。我们需要更深入地了解LLMs在教育评估中的优势和局限性。进一步的研究对于确定LLMs在教育评估中的准确性和经济性至关重要。
>
> https://arxiv.org/abs/2405.05444


<hr />

- 论文原文: [https://arxiv.org/abs/2405.05444](https://arxiv.org/abs/2405.05444)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886