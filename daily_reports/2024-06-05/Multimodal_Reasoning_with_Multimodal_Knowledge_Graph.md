# 融合多模态知识图谱的推理探索
发布时间：2024年06月04日

`多模态大模型`
> Multimodal Reasoning with Multimodal Knowledge Graph
>
> 大型语言模型（LLMs）在多模态推理中常受幻觉和知识陈旧之苦。尽管有方法尝试通过文本知识图谱来缓解，但单一模态的知识限制了跨模态理解的深度。为此，我们提出了多模态推理与多模态知识图谱（MR-MKG）方法，利用多模态知识图谱（MMKGs）深化跨模态知识学习，大幅提升LLMs的多模态推理能力。我们采用关系图注意力网络编码MMKGs，并设计跨模态对齐模块优化图像与文本的匹配。通过预训练基于MMKG的数据集，LLMs获得了多模态推理的初步能力。令人瞩目的是，MR-MKG在仅训练了极小部分参数（约2.25%）的情况下，便在多模态问答和类比推理任务中超越了现有最佳模型。
>
> https://arxiv.org/abs/2406.02030

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x5.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x6.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x7.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x8.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x9.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x10.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.02030/x11.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2406.02030](https://arxiv.org/abs/2406.02030)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886