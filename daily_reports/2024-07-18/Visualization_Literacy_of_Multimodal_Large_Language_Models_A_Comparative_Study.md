# 探究多模态大型语言模型的可视化能力：一项对比研究
发布时间：2024年06月24日

`多模态大模型`
> Visualization Literacy of Multimodal Large Language Models: A Comparative Study
>
> 多模态大型语言模型 (MLLMs) 的最新进展，不仅继承了大型语言模型的强大能力，还增强了多模态上下文的推理能力。这些模型的应用潜力远超仅处理文本的模型。近期在可视化领域的研究显示，MLLMs 不仅能理解并解释可视化结果，还能用自然语言向用户阐述其内容。尽管机器学习界已通过多项视觉理解基准测试了 MLLMs 的视觉能力，但其基于视觉感知执行特定可视化任务的能力，尤其是从可视化视角出发，仍未得到充分研究。本研究中，我们通过引入可视化素养概念，旨在评估 MLLMs 在这方面的表现。我们利用两个广泛认可的可视化素养评估数据集（VLAT 和 mini-VLAT）进行测试。在可视化素养框架内，我们设计了一套通用比较方案，不仅涵盖了多种多模态大型语言模型（如 GPT4-o、Claude 3 Opus、Gemini 1.5 Pro），还与人类基准进行了对比。研究结果表明，MLLMs 在可视化素养任务中表现卓越，尤其在识别相关性、集群和层次结构等特定任务上，其表现超越了人类。
>
> https://arxiv.org/abs/2407.10996

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.10996/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.10996/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.10996/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.10996/x4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.10996/5_stable_performance.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.10996/category_question_result_summary.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.10996/individual_question_result_summary.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2407.10996](https://arxiv.org/abs/2407.10996)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 加入社群，公众号回复LLM
- 最新论文订阅体验：公众号号菜单回复1