# 防御提示补丁：为 LLMs 提供对抗越狱攻击的鲁棒且透明的防护策略
发布时间：2024年05月30日

`模型安全`
> Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks
>
> 在调整大型语言模型时，安全性、安全性和合规性至关重要。然而，许多看似安全的模型却容易遭受越狱攻击，这些攻击通过恶意查询中的越狱提示绕过安全防护。为此，我们提出了防御性提示补丁（DPP），这是一种创新的防御机制，旨在保护LLMs免受复杂越狱策略的威胁。DPP不仅确保了最低的攻击成功率，还保持了模型的高效用，通过精心设计的可解释后缀提示，有效抵御了多种越狱技术。实证研究表明，DPP在多个模型上显著降低了攻击成功率，同时几乎不影响模型的效用。我们的方法在安全与功能之间取得了更好的平衡，并提供了一个适用于多种LLM平台的可扩展且可解释的解决方案。
>
> https://arxiv.org/abs/2405.20099

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2405.20099/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2405.20099/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2405.20099/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2405.20099/x4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2405.20099/x5.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2405.20099](https://arxiv.org/abs/2405.20099)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886