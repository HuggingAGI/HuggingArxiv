# SynopGround：一个大规模数据集，专为从电视剧及其剧情简介中进行多段视频定位而设计。
发布时间：2024年08月03日


> SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses
>
> 视频定位，作为多模态内容理解的核心问题，旨在从冗长的视频中精准定位自然语言查询。然而，现有数据集的局限性——仅涉及简单事件、视频或句子时长有限——阻碍了模型向更深层次的多模态理解进化。为此，我们推出了SynopGround数据集，汇集了超过2800小时的流行电视剧片段，并配以精心编写的时间精确的摘要。这些摘要段落，作为查询，不仅相互关联，还富含抽象表达，助力模型在更长上下文中掌握复杂概念。基于此，我们创新性地提出了多段落视频定位（MPVG）任务，以及对应的局部-全局多模态推理器（LGMR），以更有效地处理长期多模态输入。实验证明，我们的方法在多段落视频定位上超越了现有技术，为该领域提供了强有力的基线。数据集与代码已公开，详情请访问项目页面：https://synopground.github.io/。
>
> https://arxiv.org/abs/2408.01669

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.01669/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.01669/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.01669/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.01669/x4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.01669/x5.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.01669/x6.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.01669/x7.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2408.01669](https://arxiv.org/abs/2408.01669)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)