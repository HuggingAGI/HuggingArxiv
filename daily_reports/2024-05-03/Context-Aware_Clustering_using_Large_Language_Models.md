![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/imgs/follow2.gif)
# 本文探讨了利用大型语言模型进行上下文感知聚类的方法。
发布时间：2024年05月01日

`文本分类`
> Context-Aware Clustering using Large Language Models
>
> 大型语言模型（LLMs）在文本理解和生成方面取得了显著成就，但其在文本聚类任务中的应用尚未充分挖掘。我们发现，尽管闭源LLMs能够为实体集合提供优秀的聚类效果，但由于计算资源的庞大需求和成本问题，它们的应用并不具备可扩展性。为此，我们提出了CACTUS——一种创新的系统化方法，它利用开源LLMs来高效执行实体子集的监督聚类，尤其针对基于文本的实体。传统的文本聚类技术往往忽略了实体子集所提供的上下文信息。此外，尽管存在一些基于语言模型的聚类方法，但专为监督聚类任务设计的方法却寥寥无几。本文提出了一种新颖的聚类方法，通过LLMs捕获上下文，并通过一种可扩展的实体间注意力机制实现。我们设计了一种新的增强型三重损失函数，专为监督聚类量身定制，以应对直接应用三重损失函数于此问题的固有难题。同时，我们引入了一种基于文本增强技术的自监督聚类任务，旨在提升模型的泛化性能。在评估过程中，我们从闭源LLM中获取了基准聚类数据，并将这些信息转移到开源LLM中，实现了在监督聚类框架下，使用更快速、成本更低的开源模型来执行相同任务。我们在多个电子商务查询和产品聚类数据集上进行的实验证明，我们的方法在多个外部聚类评估指标上显著超越了现有的无监督和监督基线方法。
>
> https://arxiv.org/abs/2405.00988



- 论文原文: [https://arxiv.org/abs/2405.00988](https://arxiv.org/abs/2405.00988)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886