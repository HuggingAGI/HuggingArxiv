# MoME：多模态专家的融合，打造全能型多模态大型语言模型
发布时间：2024年07月17日

`多模态大模型`
> MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models
>
> 多模态大型语言模型 (MLLMs) 在视觉-语言任务中表现出色，但通用 MLLM 在多数任务上仍不及专业模型，主要因任务干扰所致。为此，我们提出多模态专家混合 (MoME)，包含视觉专家混合 (MoVE) 和语言专家混合 (MoLE)，旨在减轻干扰并提升通用 MLLM 性能。MoVE 能自适应调节视觉特征，而 MoLE 则通过稀疏门控专家实现低成本改进。MoME 专为视觉和语言模态设计，以适应任务差异。实验证明，MoME 大幅提升了通用 MLLMs 的性能。源代码已公开于 https://github.com/JiuTian-VL/MoME。
>
> https://arxiv.org/abs/2407.12709

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12709/Visualize.jpeg)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12709/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12709/MoVE_Comp.jpeg)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12709/MoVE_Visualization.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12709/MoLE_Visualizaion_sub1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12709/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12709/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12709/MoLE_Visualization_Full.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2407.12709](https://arxiv.org/abs/2407.12709)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 加入社群，公众号回复LLM
- 最新论文订阅体验：公众号号菜单回复1