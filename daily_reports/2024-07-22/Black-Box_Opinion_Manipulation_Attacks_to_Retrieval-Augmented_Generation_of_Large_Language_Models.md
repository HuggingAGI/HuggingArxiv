# 针对大型语言模型检索增强生成的黑盒意见操纵攻击
发布时间：2024年07月18日

`模型安全`
> Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models
>
> RAG 旨在解决大型语言模型的幻觉问题和实时约束，但也易受检索破坏攻击。本文聚焦于 RAG 在黑盒攻击下的脆弱性，特别是对意见操纵的影响。我们通过操纵检索排名并训练代理模型，实施对抗性检索攻击，实验显示这能显著改变 RAG 生成内容的意见极性。这不仅暴露了模型的脆弱性，更揭示了其对用户认知和决策的潜在危害，增加了误导用户的风险。
>
> https://arxiv.org/abs/2407.13757

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13757/method_2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13757/data_cons_2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13757/effective.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2407.13757](https://arxiv.org/abs/2407.13757)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 加入社群，公众号回复LLM
- 最新论文订阅体验：公众号号菜单回复1