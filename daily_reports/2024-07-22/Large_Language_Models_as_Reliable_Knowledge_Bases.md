# 大型语言模型能否成为可靠的知识库？
发布时间：2024年07月18日


> Large Language Models as Reliable Knowledge Bases?
>
> NLP 社区对利用 LLM 进行知识密集型任务的兴趣日益浓厚，将其视为潜在的知识库。然而，LLM 作为知识库的可靠性和适用范围仍待深入探讨。虽然有研究指出 LLM 能在参数中编码知识，但这并不足以全面评估其作为知识库的效能。本研究设定了 LLM 作为可靠知识库的标准，聚焦于事实性和一致性，并涵盖已知与未知知识。我们据此开发了评估指标，对 26 个流行 LLM 进行了全面评估，并分析了模型大小、指令调优及上下文学习等因素的影响。结果显示，即便如 GPT-3.5-turbo 这样的高性能模型，在事实性和一致性上也存在不足，而上下文学习和微调等策略也未能显著提升 LLM 作为知识库的表现。
>
> https://arxiv.org/abs/2407.13578

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x5.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x6.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x7.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x8.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x9.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x10.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x11.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x12.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.13578/x13.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2407.13578](https://arxiv.org/abs/2407.13578)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 加入社群，公众号回复LLM
- 最新论文订阅体验：公众号号菜单回复1