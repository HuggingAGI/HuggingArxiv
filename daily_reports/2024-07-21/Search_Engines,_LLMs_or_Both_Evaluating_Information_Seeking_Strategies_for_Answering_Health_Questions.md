# 在解答健康问题时，我们应选择搜索引擎、大型语言模型，还是两者结合？本文将评估这些信息寻求策略的有效性。
发布时间：2024年07月17日

`RAG`
> Search Engines, LLMs or Both? Evaluating Information Seeking Strategies for Answering Health Questions
>
> 传统搜索引擎一直是信息检索的核心工具，但新型大型语言模型（LLM）在多任务处理中表现出色，尤其在问答系统领域日益普及。未来，基于LLM的对话系统与传统网络引擎预计将并存，以不同方式服务用户。本研究聚焦于这两类系统在解答健康问题上的表现，通过对比不同搜索引擎、LLM及检索增强（RAG）方法，我们发现：尽管网页质量在排名靠后时未见下降，但LLM在准确回答健康问题上优于传统引擎。同时，LLM对输入提示极为敏感，而RAG则显著提升了信息检索的效率。
>
> https://arxiv.org/abs/2407.12468

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/search-engines.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/topic.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/proportion-answers-2020.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/proportion-answers-2021.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/proportion-answers-2022.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/answers-per-engine.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/lazy_user.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/diligent_user.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/zeroshot-2020.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/zeroshot-2021.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/zeroshot-2022.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/perc-errors.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/rag-2020.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/rag-2021.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/rag-2022.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/rag-expert-2020.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/rag-expert-2021.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2407.12468/rag-expert-2022.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2407.12468](https://arxiv.org/abs/2407.12468)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 加入社群，公众号回复LLM
- 最新论文订阅体验：公众号号菜单回复1