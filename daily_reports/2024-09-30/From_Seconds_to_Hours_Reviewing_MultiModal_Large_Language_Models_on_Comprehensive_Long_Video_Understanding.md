# 从秒到小时：多模态大型语言模型在长视频理解中的全面回顾
发布时间：2024年09月27日


> From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding
>
> 大型语言模型（LLM）与视觉编码器的结合，在视觉理解任务中展现了显著潜力。多模态大型语言模型（MM-LLM）因视觉数据的多样性，在处理图像、短视频和长视频时，设计和训练方式各异。本文聚焦于长视频理解相较于静态图像和短视频的显著差异与独特挑战。短视频包含空间与事件内时间信息，而长视频则涉及多事件间的长期时间信息。本文旨在梳理MM-LLM从图像到长视频理解的进展，分析各视觉任务的差异，并指出长视频理解面临的细粒度时空细节、动态事件及长期依赖等挑战。随后，详细总结了MM-LLM在长视频理解中的模型设计与训练方法的进步。最后，对比了现有MM-LLM在不同长度视频理解基准上的表现，并探讨了其在长视频理解领域的未来发展方向。
>
> https://arxiv.org/abs/2409.18938

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2409.18938](https://arxiv.org/abs/2409.18938)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)