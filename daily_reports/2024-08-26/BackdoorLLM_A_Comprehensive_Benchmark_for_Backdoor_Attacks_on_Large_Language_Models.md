# BackdoorLLM：大型语言模型后门攻击的综合评估基准
发布时间：2024年08月22日

`模型安全`
> BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models
>
> 生成型大型语言模型 (LLM) 虽在多任务上取得显著进展，但仍易受后门攻击，即特定触发词可诱导模型生成恶意响应。尽管后门研究多聚焦于视觉或文本分类，文本生成领域的后门攻击却鲜受关注。为此，我们推出了首个全面研究 LLM 后门攻击的基准 \textit{BackdoorLLM}，其特色包括：标准化训练流程的后门基准库、多元攻击策略（如数据中毒、权重中毒等）、涵盖 7 场景 6 架构的 200 余次实验评估，以及对 LLM 后门攻击有效性与局限的深入洞察。我们期望 \textit{BackdoorLLM} 能提升业界对后门威胁的警觉，并助力 AI 安全发展。相关代码已公开于 \url{https://github.com/bboylyg/BackdoorLLM}。
>
> https://arxiv.org/abs/2408.12798

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.12798/GPT-4-Detection.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.12798/Freeform.jpg)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.12798/Choice.jpg)

<hr />

- 论文原文: [https://arxiv.org/abs/2408.12798](https://arxiv.org/abs/2408.12798)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)