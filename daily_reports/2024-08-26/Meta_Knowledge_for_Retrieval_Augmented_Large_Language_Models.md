# 元知识在增强检索型大型语言模型中的应用
发布时间：2024年08月16日

`RAG`
> Meta Knowledge for Retrieval Augmented Large Language Models
>
> RAG 技术通过增强 LLM 的上下文相关信息，提升了其处理时效性和领域特定问题的能力，但构建高效综合多样化文档信息的 RAG 系统仍具挑战。我们创新性地将传统“检索-阅读”流程升级为“准备-重写-检索-阅读”框架，通过生成元数据和合成问答，以及引入元知识摘要，实现了对知识库的专家级理解。研究显示，增强查询在性能上显著超越传统 RAG 方法，且元知识查询进一步提升了检索和答案质量。该方法成本低廉，适应性强，为 RAG 流程的性能提升开辟了新路径。
>
> https://arxiv.org/abs/2408.09017

**如遇无法添加，请+ vx: iamxxn886**
<hr />

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.09017/KDD_main.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.09017/results_2.png)

<hr />

- 论文原文: [https://arxiv.org/abs/2408.09017](https://arxiv.org/abs/2408.09017)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)