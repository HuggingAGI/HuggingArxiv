# Cap2Sum：一种通过生成字幕来学习视频摘要的新方法
发布时间：2024年08月22日

`多模态大模型`
> Cap2Sum: Learning to Summarize Videos by Generating Captions
>
> 随着网络视频数据的激增，视频摘要技术显得尤为重要。但高昂的标注成本限制了研究规模，导致现有技术性能和泛化能力受限。为此，我们创新性地采用密集视频字幕作为训练信号，研发了Cap2Sum模型，该模型通过生成字幕来提炼视频精华。这种弱监督方法让我们得以在大规模数据集上训练，显著提升了性能和泛化能力。此外，我们结合CLIP模型，强化了对视频中易被忽视的关键对象的学习。Cap2Sum不仅支持零-shot视频摘要，还能通过目标数据集的实际摘要或字幕进行精准微调。为验证其性能，我们构建了TVSum-Caption和SumMe-Caption两个新数据集，并计划公开发布。实验结果显示，我们的方法在性能和泛化能力上均超越了现有技术。
>
> https://arxiv.org/abs/2408.12800

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2408.12800](https://arxiv.org/abs/2408.12800)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)