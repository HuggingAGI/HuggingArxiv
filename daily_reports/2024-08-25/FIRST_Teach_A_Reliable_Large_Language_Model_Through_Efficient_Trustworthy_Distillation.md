# 利用高效可信的蒸馏技术，培养一个可靠的大型语言模型
发布时间：2024年08月22日

`动手训练`
> FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation
>
> 随着大型语言模型（LLM）在日常生活中的普及，人们对其准确性和校准度的期望也随之提高。尽管微调已成为提升模型实用性的主流手段，但其导致的“调谐误校准”问题仍使其信任度不尽如人意。本文深入分析了微调模型中的误校准现象及其成因，并探讨了蒸馏技术对此问题的缓解作用。我们创新性地提出了高效可信蒸馏（FIRST）方法，该方法通过巧妙利用教师模型的一小部分知识，以经济高效的方式构建出可靠的语言模型。实验表明，该方法在提升模型准确性的同时，显著降低了误校准率，从而大幅增强了模型的信任度。
>
> https://arxiv.org/abs/2408.12168

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2408.12168](https://arxiv.org/abs/2408.12168)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)