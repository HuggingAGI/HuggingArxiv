# 大型语言模型：下一代密集检索的坚实基石——全面实证探究
发布时间：2024年08月22日

`RAG`
> Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment
>
> BERT和T5等预训练语言模型在密集检索中扮演着关键角色，但它们的泛化能力和领域准确性提升有限。近期研究采用大型语言模型（LLM）作为检索器，取得了显著成果。尽管如此，LLM相较于传统检索器的优势及其配置（如参数规模、预训练时长和对齐策略）对检索性能的影响尚不明确。本研究针对多种检索任务进行了深入分析，包括领域精度、数据效率、零样本泛化等，并评估了15种以上模型。研究显示，更大规模的模型和更长时间的预训练能持续提升领域精度和数据效率，同时在零样本泛化等方面展现出巨大潜力。这些发现突显了LLM在密集检索中的多面性和高效性，为该领域的未来发展提供了重要启示。
>
> https://arxiv.org/abs/2408.12194

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.12194/accuracy.jpg)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.12194/efficiency.jpg)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.12194/Lengthy.jpg)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.12194/generalization.jpg)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.12194/Instructions.jpg)

<hr />

- 论文原文: [https://arxiv.org/abs/2408.12194](https://arxiv.org/abs/2408.12194)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)