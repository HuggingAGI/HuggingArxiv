![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/imgs/follow2.gif)
# 在大型语言模型（LLM）支持的应用程序中，悄然兴起了一种难以为人所察觉的检索投毒攻击。
发布时间：2024年04月26日

`RAG`
> 当前，借助尖端的大型语言模型（LLM）应用开发框架，越来越多的LLM应用能够通过检索增强生成（RAG）技术轻松扩充其知识库。但这些框架在设计上对外部内容风险的考虑不足，容易让攻击者有机可乘。本文揭露了LLM应用面临的一个新风险——检索投毒，攻击者借此可以操纵应用在RAG过程中生成恶意回应。攻击者通过分析LLM应用框架，精心制作出外观上与正常文档无异的文档，这些文档虽提供正确信息，但一旦作为RAG的参考，却能误导应用产生错误结果。我们的初步实验显示，攻击者能够以88.33%的高成功率误导LLM，且在现实世界应用中的成功率也高达66.67%，这充分说明了检索投毒的严重性及其潜在的广泛影响。



- 论文原文 [https://arxiv.org/abs/2404.17196](https://arxiv.org/abs/2404.17196)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/imgs/qrcode.png)