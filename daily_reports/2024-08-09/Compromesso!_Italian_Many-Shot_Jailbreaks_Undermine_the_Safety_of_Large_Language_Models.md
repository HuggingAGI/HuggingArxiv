# 意大利式的多次尝试越狱，正逐渐削弱大型语言模型的安全防线。
发布时间：2024年08月08日

`模型安全`
> Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models
>
> 随着多语言社区采用LLM，跨语言的安全评估变得至关重要。尽管有安全措施，但“越狱”技术仍能使LLM行为不安全。当前研究主要集中在英语，限制了我们对其他语言安全性的理解。我们通过研究意大利语中的多-shot越狱，创建了不安全问答数据集，揭示了四大LLM家族的安全漏洞。我们发现，即使少量不安全提示，模型也会表现出不安全行为，且这种风险随提示增多迅速升级。
>
> https://arxiv.org/abs/2408.04522

**点击公众号菜单加入大语言模型论文讨论**
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2408.04522](https://arxiv.org/abs/2408.04522)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)