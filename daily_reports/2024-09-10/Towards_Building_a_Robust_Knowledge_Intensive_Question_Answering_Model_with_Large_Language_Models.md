# 构建鲁棒知识密集型问答模型：大型语言模型的探索之路
发布时间：2024年09月09日

`知识图谱`
> Towards Building a Robust Knowledge Intensive Question Answering Model with Large Language Models
>
> 随着LLM的发展，问答系统的智能和流畅性得到了显著提升，而检索增强技术的引入则让模型更有效地利用外部信息。然而，检索信息中的噪声和错误却对模型的鲁棒性构成了挑战。为此，我们首先构建了一个模拟多种干扰场景的数据集，包括关键信息缺失、噪声和冲突。为应对噪声导致的准确性下降，我们提出了一种基于数据增强的微调方法，以提升LLM的抗噪能力。同时，对比学习方法也被引入，以保持模型对外部信息的辨别力。实验结果显示，我们的方法不仅增强了模型的鲁棒性，还提升了其对外部信息的辨别能力。
>
> https://arxiv.org/abs/2409.05385

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2409.05385](https://arxiv.org/abs/2409.05385)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)