# Web2Code：一款大规模的网页转代码数据集与评估框架，专为多模态大型语言模型设计。
发布时间：2024年06月28日

`代码编写`
> Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs
>
> 摘要：多模态大型语言模型（MLLMs）在跨图像、视频和音频等多种任务中表现卓越。然而，它们在理解网页截图和生成HTML代码方面却表现不佳。为此，我们提出Web2Code，包含一个大规模网页到代码数据集和评估框架，旨在提升MLLMs在这两方面的能力。我们利用预训练LLMs增强现有数据集并生成多样网页图像，输入为网页图像和指令，输出为HTML代码，并加入网页内容的自然语言QA对以深化理解。我们还开发了评估框架来测试MLLMs在这两方面的能力。实验证明，我们的数据集不仅提升了我们提出的任务性能，也在一般视觉领域表现更佳。我们期待这项工作能推动适用于网页内容生成和自动化的通用MLLMs的发展。相关数据和代码将在指定链接提供。
>
> https://arxiv.org//pdf/2406.20098

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/cloud.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x5.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/ori.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/vicuna.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/crystalchat.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/ori.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/ori_code_img.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/ours.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x6.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x7.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x8.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x9.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x10.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x11.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x12.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/x13.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/junbo.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/pix2code.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/WebSight.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/QA.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/WebSRC.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/Modern_webpages.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2406.20098/Bootstrap_webpage.png)

<hr />

- 论文原文: [https://arxiv.org//pdf/2406.20098](https://arxiv.org//pdf/2406.20098)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 加入社群，公众号回复LLM
- 最新论文订阅体验：公众号号菜单回复1