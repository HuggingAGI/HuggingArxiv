# 探究大型语言模型问答系统中检索组件的效能评估
发布时间：2024年06月10日

`RAG`
> Evaluating the Retrieval Component in LLM-Based Question Answering Systems
>
> 基于LLMs的问答系统（QA）依赖检索组件获取特定领域信息，以避免生成错误或幻觉的回答。尽管检索器评估源自信息检索早期研究，但在LLM聊天机器人中的应用仍具挑战。本研究提出了一种简明的基线，用于评估基于RAG的聊天机器人中的检索器。研究显示，该评估框架更准确地描绘了检索器的表现，并与QA系统的整体性能更为契合。传统评估指标如精确度、召回率和F1分数可能不足以全面评估LLMs，因为即便检索不完美，LLMs仍能给出准确回答。我们的方法则考虑了LLMs的优势，能够排除无关上下文，并识别回答中的潜在错误和幻觉。
>
> https://arxiv.org/abs/2406.06458


<hr />

- 论文原文: [https://arxiv.org/abs/2406.06458](https://arxiv.org/abs/2406.06458)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 最新论文订阅体验（3天）：公众号号菜单回复1
- 最新论文订阅新人优惠：公众号号菜单回复2