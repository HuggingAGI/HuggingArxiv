# 本文探讨了运用语言模型辅助招聘决策的实践，并评估其有效性与潜在影响。
`应用案例`
> 随着大型语言模型（LLMs）技术的飞速发展，它们在众多领域的性能已经能与人类专家相媲美，因此，防止算法偏见的监管工作变得尤为迫切。这些努力主要集中在算法“审计”上，但现有的法规和科学文献对于如何执行这些审计却鲜有具体指导。本文提出了一种审计算法的方法——对应实验，它本是用于检测人类判断中的偏见的一种工具。在就业场景中，对应实验通过改变申请材料中的特定信息，如申请人姓名等，来衡量种族和性别对招聘决策的影响。我们利用这种方法，结合一份新的K-12教师职位申请语料库，对几个顶尖LLMs进行的候选人评估进行了审计。研究结果显示，模型在评估中存在一定程度的种族和性别差异，这一现象在不同类型申请材料和不同任务设定下均较为稳定。文章最后讨论了对应实验在审计算法时的一些局限性。
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.03086/x1.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.03086/x2.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.03086/x3.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.03086/x4.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.03086/x5.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.03086/x6.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.03086/x7.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2404.03086/x8.png)
