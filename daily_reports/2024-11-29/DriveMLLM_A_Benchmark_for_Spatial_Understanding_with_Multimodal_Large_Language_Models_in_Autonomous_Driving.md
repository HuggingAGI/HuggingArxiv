# DriveMLLM：自动驾驶中借助多模态大型语言模型实现空间理解的基准
发布时间：2024年11月20日

`多模态大模型`
> DriveMLLM: A Benchmark for Spatial Understanding with Multimodal Large Language Models in Autonomous Driving
>
> 自动驾驶需要全面理解 3D 环境，以助力运动预测、规划和地图绘制等高级任务。在本文中，我们推出了 DriveMLLM，这一专门用于评测自动驾驶中多模态大型语言模型（MLLMs）空间理解能力的基准。DriveMLLM 涵盖 2734 个前置摄像头图像，引入了绝对和相对空间推理任务，还伴有语言丰富多样的自然语言问题。为衡量 MLLMs 的表现，我们提出了侧重于空间理解的新型评估指标。我们在 DriveMLLM 上对几个前沿的 MLLMs 进行了评估，结果表明当前模型在理解驾驶场景中的复杂空间关系时存在局限。我们认为，这些发现凸显了对更先进的基于 MLLM 的空间推理方法的需求，也展现了 DriveMLLM 在推动自动驾驶深入研究方面的潜力。代码可在 url{https://github.com/XiandaGuo/Drive-MLLM}获取。
>
> https://arxiv.org/abs/2411.13112

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2411.13112](https://arxiv.org/abs/2411.13112)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)