# 从 52B 到 1T：Tele-FLM 系列中的经验启示
发布时间：2024年07月02日

`模型榜单`
> 52B to 1T: Lessons Learned via Tele-FLM Series
>
> 大型语言模型 (LLM) 是通往通用人工智能的关键进展。随着模型规模的扩大，学术界对超过 500 亿参数的 LLM 研究日益深入。本报告基于我们与 Tele-FLM（FLM-2）的合作，该模型拥有 520 亿参数。我们探讨了两个核心领域：首先，我们发现 Tele-FLM-52B 上的监督微调 (SFT) 支持“少即是多”的数据构建策略；其次，我们展示了如何从 520 亿逐步扩展到 1 万亿参数的最佳实践。为推动研究，我们将公开 Tele-FLM-1T 模型。
>
> https://arxiv.org/abs/2407.02783


<hr />

- 论文原文: [https://arxiv.org/abs/2407.02783](https://arxiv.org/abs/2407.02783)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 加入社群，公众号回复LLM
- 最新论文订阅体验：公众号号菜单回复1