# Reward-RAG：以奖励驱动的方式提升 RAG 的性能
发布时间：2024年10月03日

`RAG`
> Reward-RAG: Enhancing RAG with Reward Driven Supervision
>
> 本文介绍了一种名为 Reward-RAG 的创新方法，通过奖励驱动的监督机制提升 Retrieval-Augmented Generation (RAG) 模型。与以往依赖外部知识检索的 RAG 方法不同，我们利用 CriticGPT 训练特定领域的奖励模型，从而使检索信息更贴合实际需求。该模型生成的合成数据集进一步微调 RAG 编码器，使其输出更符合人类偏好。这种方法的灵活性使其能广泛应用于不同领域。我们在多领域的公开基准上测试了 Reward-RAG，并展示了其显著优于现有技术的性能提升。这些成果表明，结合奖励模型与 RAG 有望大幅提升自然语言生成任务的效果。
>
> https://arxiv.org/abs/2410.03780

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2410.03780](https://arxiv.org/abs/2410.03780)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)