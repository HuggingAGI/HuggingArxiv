# 大型语言模型的提示压缩：全面调查
发布时间：2024年10月17日


> Prompt Compression for Large Language Models: A Survey
>
> 在处理复杂自然语言任务时，大型语言模型 (LLM) 通常需要冗长的提示，这不仅增加了内存负担，还提高了推理成本。为应对这一挑战，提示压缩技术应运而生，并迅速成为研究热点。本文将提示压缩技术分为硬提示和软提示两大类，逐一比较其技术路径，并深入探讨了其背后的机制，如注意力优化、参数高效微调 (PEFT)、模态融合及新合成语言的应用。此外，我们还分析了这些技术在实际应用中的表现及局限，并展望了未来的发展方向，包括优化压缩编码器、融合硬软提示方法及借鉴多模态的智慧。
>
> https://arxiv.org/abs/2410.12388

**如遇无法添加，请+ vx: iamxxn886**
<hr />


<hr />

- 论文原文: [https://arxiv.org/abs/2410.12388](https://arxiv.org/abs/2410.12388)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)