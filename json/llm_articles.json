[
  {
    "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
    "submit_datetime": "2024年04月26日",
    "abstract": "Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.",
    "pdf_link": "https://arxiv.org/abs/2404.17546",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）的多项能力和安全技术，如RLHF、自动化红队对抗、提示设计和填充技术，本质上是从由奖励或潜能函数定义的未归一化目标分布中进行抽样。本研究中，我们采用了序列蒙特卡洛（SMC）方法来解决这些概率推断难题。我们特别利用学习到的扭曲函数来预测每个时间点潜能的期望未来值，从而将推断计算的重点放在有潜力的部分序列上。我们提出了一种创新的对比学习方法来训练这些扭曲函数，并与软强化学习领域的丰富文献建立了联系。此外，我们还展示了一种评估语言模型推断精度的新方法，即利用双向SMC界限来估计对数划分函数，进而计算推断分布与目标分布之间的KL散度。我们应用这些推断评估技术，证明了扭曲SMC在从预训练模型中抽取不良输出、生成情感多样的评论以及执行填充任务方面的有效性。",
    "title_cn": "本文介绍了一种新颖的方法，即通过扭曲的顺序蒙特卡洛（Twisted Sequential Monte Carlo）技术，来增强语言模型中的概率推断能力。",
    "tags": [
      "分类：LLM理论",
      "人工智能",
      ""
    ]
  },
  {
    "title": "Exploring the Distinctiveness and Fidelity of the Descriptions Generated by Large Vision-Language Models",
    "submit_datetime": "2024年04月26日",
    "abstract": "Large Vision-Language Models (LVLMs) are gaining traction for their remarkable ability to process and integrate visual and textual data. Despite their popularity, the capacity of LVLMs to generate precise, fine-grained textual descriptions has not been fully explored. This study addresses this gap by focusing on \\textit{distinctiveness} and \\textit{fidelity}, assessing how models like Open-Flamingo, IDEFICS, and MiniGPT-4 can distinguish between similar objects and accurately describe visual features. We proposed the Textual Retrieval-Augmented Classification (TRAC) framework, which, by leveraging its generative capabilities, allows us to delve deeper into analyzing fine-grained visual description generation. This research provides valuable insights into the generation quality of LVLMs, enhancing the understanding of multimodal language models. Notably, MiniGPT-4 stands out for its better ability to generate fine-grained descriptions, outperforming the other two models in this aspect. The code is provided at \\url{https://anonymous.4open.science/r/Explore_FGVDs-E277}.",
    "pdf_link": "https://arxiv.org/abs/2404.17534",
    "graphs": [],
    "abstract_cn": "大型视觉-语言模型（LVLMs）因其卓越的视觉与文本数据处理能力而备受瞩目。然而，这些模型在生成精细、详尽的文本描述方面的潜力尚未被充分挖掘。本研究聚焦于“独特性”与“忠实度”，探讨了Open-Flamingo、IDEFICS和MiniGPT-4等模型区分相似物体和精确表述视觉特征的能力。我们引入了文本检索增强分类（TRAC）框架，借助其生成特性，深入探讨了细粒度视觉描述的生成机制。此项研究深化了我们对LVLMs生成质量的认识，尤其是在多模态语言模型的理解上。特别值得一提的是，MiniGPT-4在生成精细描述方面表现更为出色，超越了其他两种模型。相关代码已在\\url{https://anonymous.4open.science/r/Explore_FGVDs-E277}提供。",
    "title_cn": "本文旨在探究大型视觉-语言模型所生成描述的独特性和准确性。",
    "tags": [
      "分类：LLM应用",
      "计算机视觉",
      ""
    ]
  },
  {
    "title": "Large Language Model Agent as a Mechanical Designer",
    "submit_datetime": "2024年04月26日",
    "abstract": "Conventional mechanical design paradigms rely on experts systematically refining concepts through experience-guided modification and FEA to meet specific requirements. However, this approach can be time-consuming and heavily dependent on prior knowledge and experience. While numerous machine learning models have been developed to streamline this intensive and expert-driven iterative process, these methods typically demand extensive training data and considerable computational resources. Furthermore, methods based on deep learning are usually restricted to the specific domains and tasks for which they were trained, limiting their applicability across different tasks. This creates a trade-off between the efficiency of automation and the demand for resources. In this study, we present a novel approach that integrates pre-trained LLMs with a FEM module. The FEM module evaluates each design and provides essential feedback, guiding the LLMs to continuously learn, plan, generate, and optimize designs without the need for domain-specific training. We demonstrate the effectiveness of our proposed framework in managing the iterative optimization of truss structures, showcasing its capability to reason about and refine designs according to structured feedback and criteria. Our results reveal that these LLM-based agents can successfully generate truss designs that comply with natural language specifications with a success rate of up to 90%, which varies according to the applied constraints. By employing prompt-based optimization techniques we show that LLM based agents exhibit optimization behavior when provided with solution-score pairs to iteratively refine designs to meet specifications. This ability of LLM agents to produce viable designs and optimize them based on their inherent reasoning capabilities highlights their potential to develop and implement effective design strategies autonomously.",
    "pdf_link": "https://arxiv.org/abs/2404.17525",
    "graphs": [],
    "abstract_cn": "传统机械设计依赖资深专家通过经验驱动的修改和有限元分析（FEA）来逐步完善设计，以满足特定需求。但这一过程不仅耗时，而且高度依赖于既有知识和经验。尽管众多机器学习模型被开发用以简化这一复杂且专家主导的迭代设计流程，它们往往需要庞大的训练数据集和高额的计算资源。特别是，基于深度学习的模型通常只适用于它们所训练的特定领域和任务，这限制了它们的通用性，从而在自动化效率和资源消耗之间形成了一种权衡。在本研究中，我们提出了一种创新的方法，将预训练的大型语言模型（LLM）与有限元模块（FEM）相结合。FEM模块对每个设计方案进行评估，并提供关键反馈，引导LLM进行持续的学习、规划、设计生成和优化，无需特定领域的训练。我们在桁架结构的迭代优化管理中证明了所提出框架的有效性，展示了其根据结构化反馈和标准进行设计推理和改进的能力。研究结果显示，基于LLM的智能体能够以高达90%的成功率生成符合自然语言规格的桁架设计，这一成功率会根据所施加的约束条件而变化。通过采用基于提示的优化技术，我们证明了基于LLM的智能体在提供解决方案与得分对时，能够展现出优化行为，以迭代方式精细化设计以满足特定规格。LLM智能体的这种能力，即基于其内在推理能力来生成可行的设计并进行优化，突显了它们在自主开发和实施有效设计策略方面的潜力。",
    "title_cn": "大型语言模型作为机械设计代理",
    "tags": [
      "分类：Agent\n\n这篇论文提出了一种将大型语言模型（LLM）与有限元模块（FEM）相结合的创新方法，用于自动化机械设计过程。该方法利用LLM的推理能力来生成和优化设计，而FEM模块则提供关键反馈以指导LLM的学习过程。这种方法展示了LLM在自主开发和实施有效设计策略方面的潜力，因此可以归类为Agent领域。",
      "机械设计",
      "人工智能"
    ]
  },
  {
    "title": "On the Use of Large Language Models to Generate Capability Ontologies",
    "submit_datetime": "2024年04月26日",
    "abstract": "Capability ontologies are increasingly used to model functionalities of systems or machines. The creation of such ontological models with all properties and constraints of capabilities is very complex and can only be done by ontology experts. However, Large Language Models (LLMs) have shown that they can generate machine-interpretable models from natural language text input and thus support engineers / ontology experts. Therefore, this paper investigates how LLMs can be used to create capability ontologies. We present a study with a series of experiments in which capabilities with varying complexities are generated using different prompting techniques and with different LLMs. Errors in the generated ontologies are recorded and compared. To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used. The results of this study are very promising because even for complex capabilities, the generated ontologies are almost free of errors.",
    "pdf_link": "https://arxiv.org/abs/2404.17524",
    "graphs": [],
    "abstract_cn": "能力本体论正日益广泛应用于系统或机器功能建模。构建这些包含所有属性和约束的本体论模型极为复杂，通常需要本体论专家的专业知识。幸运的是，大型语言模型（LLMs）已证明能够从自然语言文本中生成机器可理解的模型，为工程师和本体论专家提供助力。本篇论文探讨了如何利用LLMs来构建能力本体论。我们通过一系列实验研究，这些实验采用了不同的提示技巧和多种LLMs来生成不同复杂度的能力。实验中记录了生成本体论的错误，并进行了比较。为了评估生成本体论的质量，我们采用了一种结合RDF语法检查、OWL推理和SHACL约束的半自动化分析方法。研究结果显示，即便是处理复杂能力，生成的本体论也几乎无误，这一成果令人鼓舞。",
    "title_cn": "探讨如何利用大型语言模型来构建能力本体的实践",
    "tags": [
      "LLM应用",
      "系统建模",
      "人工智能"
    ]
  },
  {
    "title": "Enhancing Legal Compliance and Regulation Analysis with Large Language Models",
    "submit_datetime": "2024年04月26日",
    "abstract": "This research explores the application of Large Language Models (LLMs) for automating the extraction of requirement-related legal content in the food safety domain and checking legal compliance of regulatory artifacts. With Industry 4.0 revolutionizing the food industry and with the General Data Protection Regulation (GDPR) reshaping privacy policies and data processing agreements, there is a growing gap between regulatory analysis and recent technological advancements. This study aims to bridge this gap by leveraging LLMs, namely BERT and GPT models, to accurately classify legal provisions and automate compliance checks. Our findings demonstrate promising results, indicating LLMs' significant potential to enhance legal compliance and regulatory analysis efficiency, notably by reducing manual workload and improving accuracy within reasonable time and financial constraints.",
    "pdf_link": "https://arxiv.org/abs/2404.17522",
    "graphs": [],
    "abstract_cn": "本研究着眼于利用大型语言模型（LLMs）自动化提取食品安全领域的法律相关内容，并确保监管文件的合法性。在工业4.0浪潮推动食品行业转型，以及GDPR新规重塑隐私政策与数据处理协议的背景下，法规分析与技术进步之间出现了脱节。通过运用BERT和GPT等LLMs，研究旨在精确分类法律条文并实现合规性检查的自动化，以期填补这一空白。研究结果显示，LLMs在提升法律合规性和监管分析的效率方面展现出巨大潜力，显著降低了人工操作的负担，并在可控的时间和成本范围内提高了准确性。",
    "title_cn": "借助大型语言模型，提升法律合规与法规分析的效能",
    "tags": [
      "LLM应用",
      "食品安全",
      "法律合规"
    ]
  },
  {
    "title": "A Comprehensive Evaluation on Event Reasoning of Large Language Models",
    "submit_datetime": "2024年04月26日",
    "abstract": "Event reasoning is a fundamental ability that underlies many applications. It requires event schema knowledge to perform global reasoning and needs to deal with the diversity of the inter-event relations and the reasoning paradigms. How well LLMs accomplish event reasoning on various relations and reasoning paradigms remains unknown. To mitigate this disparity, we comprehensively evaluate the abilities of event reasoning of LLMs. We introduce a novel benchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of evaluation of schema and instance and is comprehensive in relations and reasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs have abilities to accomplish event reasoning but their performances are far from satisfactory. We also notice the imbalance of event reasoning abilities in LLMs. Besides, LLMs have event schema knowledge, however, they're not aligned with humans on how to utilize the knowledge. Based on these findings, we introduce two methods to guide the LLMs to utilize the event schema knowledge. Both methods achieve improvements.",
    "pdf_link": "https://arxiv.org/abs/2404.17513",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17513v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17513/x14.png"
      }
    ],
    "abstract_cn": "事件推理是支撑众多应用的基本技能，它依赖于事件模式知识以进行全局推理，并需应对事件间复杂关系及多样化的推理范式。目前，大型语言模型（LLMs）在不同关系和推理范式上的事件推理表现尚未明确。为了填补这一空白，我们对LLMs的事件推理能力进行了全面评估，并推出了EV2这一新的基准测试平台，旨在全面评估事件推理能力。EV2包含模式和实例两个层面的评估，覆盖了广泛的事件关系和推理范式。通过在EV2上的广泛实验，我们发现LLMs虽具备事件推理能力，但其表现尚有较大提升空间。我们还观察到LLMs在事件推理能力上存在不平衡现象。此外，尽管LLMs具备事件模式知识，但它们在如何应用这些知识上与人类存在差异。针对这些发现，我们提出了两种方法，引导LLMs更好地利用事件模式知识，两者均取得了显著提升。",
    "title_cn": "本文全面审视了大型语言模型在事件推理方面的表现。",
    "tags": [
      "分类：LLM应用",
      "人工智能",
      "事件推理"
    ]
  },
  {
    "title": "Ruffle&Riley: Insights from Designing and Evaluating a Large Language Model-Based Conversational Tutoring System",
    "submit_datetime": "2024年04月26日",
    "abstract": "Conversational tutoring systems (CTSs) offer learning experiences through interactions based on natural language. They are recognized for promoting cognitive engagement and improving learning outcomes, especially in reasoning tasks. Nonetheless, the cost associated with authoring CTS content is a major obstacle to widespread adoption and to research on effective instructional design. In this paper, we discuss and evaluate a novel type of CTS that leverages recent advances in large language models (LLMs) in two ways: First, the system enables AI-assisted content authoring by inducing an easily editable tutoring script automatically from a lesson text. Second, the system automates the script orchestration in a learning-by-teaching format via two LLM-based agents (Ruffle&Riley) acting as a student and a professor. The system allows for free-form conversations that follow the ITS-typical inner and outer loop structure. We evaluate Ruffle&Riley's ability to support biology lessons in two between-subject online user studies (N = 200) comparing the system to simpler QA chatbots and reading activity. Analyzing system usage patterns, pre/post-test scores and user experience surveys, we find that Ruffle&Riley users report high levels of engagement, understanding and perceive the offered support as helpful. Even though Ruffle&Riley users require more time to complete the activity, we did not find significant differences in short-term learning gains over the reading activity. Our system architecture and user study provide various insights for designers of future CTSs. We further open-source our system to support ongoing research on effective instructional design of LLM-based learning technologies.",
    "pdf_link": "https://arxiv.org/abs/2404.17460",
    "graphs": [],
    "abstract_cn": "会话式辅导系统（CTSs）利用自然语言互动提供学习体验，尤其在推理任务中，它们因其提升认知参与度和学习成效而备受推崇。然而，创作CTS内容的高昂成本成为了普及应用和深入教学设计研究的一大障碍。本文探讨并评估了一种新型CTS，它通过两种方式利用了大型语言模型（LLMs）的最新进展：首先，系统能够自动从课程文本生成易于编辑的辅导脚本，辅助AI内容创作；其次，通过两个基于LLM的代理（Ruffle&Riley）——分别模拟学生和教师的角色，系统自动化地在教学相长模式下编排对话脚本。该系统支持自由形式的对话，遵循智能辅导系统（ITS）的典型内外循环结构。通过两项在线用户研究（共200名参与者），我们评估了Ruffle&Riley在支持生物学课程方面的表现，并将其与简单的问答聊天机器人和阅读活动进行了对比。研究分析了系统使用模式、前后测试成绩以及用户体验调查，结果显示Ruffle&Riley的用户表现出高度的参与感、理解力，并认为所提供的帮助非常有用。尽管使用Ruffle&Riley的用户完成活动所需时间更长，但在短期学习成果上与阅读活动相比并无显著差异。我们的系统架构和用户研究为未来CTS设计者提供了宝贵的洞见，并且我们开源了系统，以支持对基于LLM的学习技术进行有效教学设计的持续研究。",
    "title_cn": "Ruffle&Riley：设计和评估一款基于大型语言模型的对话式教学系统的经验与洞见",
    "tags": [
      "Agent",
      "",
      "人工智能"
    ]
  },
  {
    "title": "\"ChatGPT Is Here to Help, Not to Replace Anybody\" -- An Evaluation of Students' Opinions On Integrating ChatGPT In CS Courses",
    "submit_datetime": "2024年04月26日",
    "abstract": "Large Language Models (LLMs) like GPT and Bard are capable of producing code based on textual descriptions, with remarkable efficacy. Such technology will have profound implications for computing education, raising concerns about cheating, excessive dependence, and a decline in computational thinking skills, among others. There has been extensive research on how teachers should handle this challenge but it is also important to understand how students feel about this paradigm shift. In this research, 52 first-year CS students were surveyed in order to assess their views on technologies with code-generation capabilities, both from academic and professional perspectives. Our findings indicate that while students generally favor the academic use of GPT, they don't over rely on it, only mildly asking for its help. Although most students benefit from GPT, some struggle to use it effectively, urging the need for specific GPT training. Opinions on GPT's impact on their professional lives vary, but there is a consensus on its importance in academic practice.",
    "pdf_link": "https://arxiv.org/abs/2404.17443",
    "graphs": [],
    "abstract_cn": "像 GPT 和 Bard 这样的大型语言模型能够根据文本描述高效生成代码，这在计算机教育领域可能引发一系列问题，包括对作弊行为、过度依赖以及计算思维能力下降的担忧。尽管已有大量研究探讨教师如何应对这一挑战，但了解学生对于这种技术变革的态度同样关键。本研究调查了52名大一计算机科学学生，旨在了解他们对具有代码生成功能技术的学术和职业看法。调查结果显示，学生们普遍认可 GPT 在学术上的用途，但并不会过度依赖它，而是适度地寻求其辅助。尽管 GPT 对大多数学生有所帮助，但也有学生在使用上存在困难，这凸显了对 GPT 特定培训的需求。关于 GPT 对他们未来职业生涯的影响，学生们的看法各异，但在学术实践中，他们普遍认为 GPT 的重要性不容忽视。",
    "title_cn": "\"ChatGPT 旨在助力，而非取代\"——探讨学生对于将 ChatGPT 整合进计算机科学课程的意见评估",
    "tags": [
      "分类：LLM应用",
      "计算机教育",
      ""
    ]
  },
  {
    "title": "InspectorRAGet: An Introspection Platform for RAG Evaluation",
    "submit_datetime": "2024年04月26日",
    "abstract": "Large Language Models (LLM) have become a popular approach for implementing Retrieval Augmented Generation (RAG) systems, and a significant amount of effort has been spent on building good models and metrics. In spite of increased recognition of the need for rigorous evaluation of RAG systems, few tools exist that go beyond the creation of model output and automatic calculation. We present InspectorRAGet, an introspection platform for RAG evaluation. InspectorRAGet allows the user to analyze aggregate and instance-level performance of RAG systems, using both human and algorithmic metrics as well as annotator quality. InspectorRAGet is suitable for multiple use cases and is available publicly to the community. The demo video is available at https://youtu.be/MJhe8QIXcEc",
    "pdf_link": "https://arxiv.org/abs/2404.17347",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLM）在构建检索增强生成（RAG）系统方面备受青睐，为此，人们投入了大量精力来开发优秀的模型和评价标准。尽管对RAG系统进行严谨评估的需求日益增长，但目前能够超越模型输出生成和自动计算的工具却相当稀缺。我们推出了InspectorRAGet，这是一个用于评估RAG系统的深度分析平台。该平台支持用户通过人工和算法评价指标，以及评估注释者的质量，来深入分析RAG系统的综合和个体表现。InspectorRAGet适用于多种应用场景，并且已经向公众开放。相关演示视频可以在 https://youtu.be/MJhe8QIXcEc 观看。",
    "title_cn": "InspectorRAGet：一个用于评估 RAG（Retrieval-Augmented Generation，检索增强生成）的内省式平台",
    "tags": [
      "RAG",
      "",
      "信息检索"
    ]
  },
  {
    "title": "When to Trust LLMs: Aligning Confidence with Response Quality",
    "submit_datetime": "2024年04月26日",
    "abstract": "Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text. This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains. Existing methods, which rely on verbalizing confidence to tell the reliability by inducing top-k responses and sampling-aggregating multiple responses, often fail, due to the lack of objective guidance of confidence. To address this, we propose CONfidence-Quality-ORDerpreserving alignment approach (CONQORD), leveraging reinforcement learning with a tailored dual-component reward function. This function encompasses quality reward and orderpreserving alignment reward functions. Specifically, the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality. Experiments demonstrate that our CONQORD significantly improves the alignment performance between confidence levels and response accuracy, without causing the model to become over-cautious. Furthermore, the aligned confidence provided by CONQORD informs when to trust LLMs, and acts as a determinant for initiating the retrieval process of external knowledge. Aligning confidence with response quality ensures more transparent and reliable responses, providing better trustworthiness.",
    "pdf_link": "https://arxiv.org/abs/2404.17287",
    "graphs": [],
    "abstract_cn": "尽管大型语言模型（LLMs）在自然语言生成领域取得了显著成就，但它们有时仍会产生错误或不合逻辑的文本。这一问题凸显了在安全至关重要的领域中，如何正确信任LLMs的重要性。目前的方法，通常依赖于通过生成置信度来评估模型的可靠性，但由于缺乏客观的置信度指导，这些方法常常失效。为了改善这一状况，我们提出了一种名为CONQORD的新型方法，该方法结合了强化学习和特别设计的双重奖励机制。这种机制不仅包含质量奖励，还包括顺序保持奖励，以激励模型对更高质量的输出表达更高的置信度，从而实现置信度与质量的一致性。实验结果证明，CONQORD显著提升了置信度与响应准确性的一致性，同时避免了模型过于保守。此外，CONQORD所提供的置信度对齐机制，不仅指导我们何时可以信赖LLMs，还作为触发外部知识检索过程的关键因素。通过确保置信度与响应质量的一致性，CONQORD确保了模型提供更透明、更可靠的输出，增强了其可信度。",
    "title_cn": "信赖大型语言模型的时机：确保信心与回答品质相匹配",
    "tags": [
      "LLM应用",
      "",
      "人工智能"
    ]
  },
  {
    "title": "Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM",
    "submit_datetime": "2024年04月26日",
    "abstract": "Retrieval-augmented language models have exhibited promising performance across various areas of natural language processing (NLP), including fact-critical tasks. However, due to the black-box nature of advanced large language models (LLMs) and the non-retrieval-oriented supervision signal of specific tasks, the training of retrieval model faces significant challenges under the setting of black-box LLM. We propose an approach leveraging Fine-grained Feedback with Reinforcement Retrieval (FFRR) to enhance fact-checking on news claims by using black-box LLM. FFRR adopts a two-level strategy to gather fine-grained feedback from the LLM, which serves as a reward for optimizing the retrieval policy, by rating the retrieved documents based on the non-retrieval ground truth of the task. We evaluate our model on two public datasets for real-world news claim verification, and the results demonstrate that FFRR achieves significant improvements over strong LLM-enabled and non-LLM baselines.",
    "pdf_link": "https://arxiv.org/abs/2404.17283",
    "graphs": [],
    "abstract_cn": "检索增强型语言模型在自然语言处理（NLP）的多个领域，尤其是在关键事实任务中，表现出了显著的潜力。但是，由于高级大型语言模型（LLM）的不可预测性以及特定任务的监督信号缺乏检索导向，使得在黑箱LLM环境下训练检索模型遭遇了不小的挑战。为了提升新闻声明的事实核查能力，我们提出了一种新颖的方法——细粒度反馈与强化检索（FFRR），它利用黑箱LLM来优化事实核查过程。FFRR通过两级策略，从LLM获取细粒度的反馈信息，这些信息随后作为优化检索策略的奖励信号，通过评估检索文档与任务非检索基准真相的一致性来进行。我们在两个公共数据集上对模型进行了评估，这些数据集专门用于验证现实世界中的新闻声明，评估结果显示FFRR在与强大的LLM支持的基准模型以及非LLM的基准模型相比，实现了显著的性能提升。",
    "title_cn": "本文介绍了一种强化检索方法，该方法通过细粒度反馈来优化黑盒大型语言模型在事实核查新闻声明方面的性能。",
    "tags": [
      "LLM应用",
      "",
      ""
    ]
  },
  {
    "title": "MovieChat+: Question-aware Sparse Memory for Long Video Question Answering",
    "submit_datetime": "2024年04月26日",
    "abstract": "Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific pre-defined vision tasks. Yet, existing methods either employ complex spatial-temporal modules or rely heavily on additional perception models to extract temporal features for video understanding, and they only perform well on short videos. For long videos, the computational complexity and memory costs associated with long-term temporal connections are significantly increased, posing additional challenges.Taking advantage of the Atkinson-Shiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination with our specially designed memory mechanism, we propose MovieChat to overcome these challenges. We lift pre-trained multi-modal large language models for understanding long videos without incorporating additional trainable temporal modules, employing a zero-shot approach. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video, 2K temporal grounding labels, and 14K manual annotations for validation of the effectiveness of our method. The code along with the dataset can be accessed via the following https://github.com/rese1f/MovieChat.",
    "pdf_link": "https://arxiv.org/abs/2404.17176",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17176v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17176/x18.png"
      }
    ],
    "abstract_cn": "近期，通过融合视频基础模型与大型语言模型，构建视频理解系统，有效突破了特定预设视觉任务的局限。不过，现有技术要么依赖复杂的时空处理模块，要么需要额外的感知模型来抽取时间特征，且主要适用于短视频。长视频的处理则面临计算复杂度和内存消耗随时间连接增长而显著上升的挑战。我们利用阿特金森-谢夫林记忆模型，并在变换器中使用令牌作为记忆载体，结合特别设计的记忆机制，提出了MovieChat解决方案。该模型通过零样本学习方式，提升了对长视频理解的预训练多模态大型语言模型性能，无需额外添加可训练的时间模块。MovieChat在长视频理解任务上达到了业界领先水平，并发布了包含1000段长视频、2000个时间定位标签和14000个手动注释的MovieChat-1K基准，用以验证方法的有效性。相关代码和数据集可在以下网址获取：https://github.com/rese1f/MovieChat。",
    "title_cn": "MovieChat+：为长篇视频问答量身定制的智能问答系统，具备问题感知的稀疏记忆功能。",
    "tags": [
      "LLM应用",
      "视频理解",
      "人工智能"
    ]
  },
  {
    "title": "A Unified Debugging Approach via LLM-Based Multi-Agent Synergy",
    "submit_datetime": "2024年04月26日",
    "abstract": "Tremendous efforts have been devoted to automating software debugging, a time-consuming process involving fault localization and repair generation. Recently, Large Language Models (LLMs) have shown great potential in automated debugging. However, we identified three challenges posed to traditional and LLM-based debugging tools: 1) the upstream imperfection of fault localization affects the downstream repair, 2) the deficiency in handling complex logic errors, and 3) the ignorance of program contexts. In this context, we propose the first automated, unified debugging framework, FixAgent, via LLM agent synergy. FixAgent can perform end-to-end localization, repair, and analysis of bugs. Our insight is that LLMs can benefit from general software engineering principles recognized by human developers in debugging, such as rubber duck debugging, enabling a better understanding of program functionality and logic bugs. Hence, we create three designs inspired by rubber ducking to address these challenges. They are agent specialization and synergy, key variable tracking, and program context comprehension, which request LLMs to provide explicit explanations and force them to focus on crucial program logic information. Experiments on the widely used dataset QuixBugs show that FixAgent correctly fixes 79 out of 80 bugs, 9 of which have never been fixed. It also plausibly patches 1.9X more defects than the best-performing repair tool on CodeFlaws, even with no bug location information and fewer than 0.6% sampling times. On average, FixAgent increases about 20% plausible and correct fixes compared to its base model using different LLMs, showing the effectiveness of our designs. Moreover, the correctness rate of FixAgent reaches remarkably 97.26%, indicating that FixAgent can potentially overcome the overfitting issue of the existing approaches.",
    "pdf_link": "https://arxiv.org/abs/2404.17153",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17153v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17153/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17153v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17153/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17153v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17153/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17153v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17153/x4.png"
      }
    ],
    "abstract_cn": "为了简化软件调试这一繁琐过程，包括故障定位和修复生成，人们投入了巨大的努力。近期，大型语言模型（LLMs）在自动化调试领域展现出巨大潜力。尽管如此，我们发现传统及基于LLM的调试工具面临三大挑战：故障定位的不精确性会波及后续的修复工作；在处理复杂逻辑错误方面的不足；以及对程序上下文的忽略。针对这些问题，我们首次提出了一个自动化、集成的调试框架——FixAgent，它通过LLM代理的协同作用实现。FixAgent能够一站式完成漏洞的定位、修复和分析。我们洞察到，LLMs可以借鉴人类开发者在调试过程中采用的通用软件工程原则，例如橡皮鸭调试法，以促进对程序功能和逻辑错误的深入理解。因此，我们设计了三种受橡皮鸭调试法启发的机制来应对这些挑战：代理的专业化与协同、关键变量追踪以及程序上下文理解，这些机制要求LLMs提供清晰的解释，并引导它们关注程序逻辑的关键信息。在广泛使用的QuixBugs数据集上的测试显示，FixAgent成功修复了80个漏洞中的79个，其中9个是之前未被修复的。即便在没有漏洞位置信息和低于0.6%的采样次数的条件下，它在CodeFlaws上修复的缺陷数量也比最佳修复工具多1.9倍。平均来说，与使用不同LLMs的基础模型相比，FixAgent在合理和正确修复方面提升了大约20%，证明了我们设计的成效。此外，FixAgent的正确率高达97.26%，这表明它可能有能力解决现有方法中存在的过拟合问题。",
    "title_cn": "本文提出了一种基于大型语言模型（LLM）的多代理协同统一调试方法，旨在优化调试流程，提高问题解决效率。",
    "tags": [
      "Agent",
      "软件工程",
      "自动化调试"
    ]
  },
  {
    "title": "Quantifying Memorization of Domain-Specific Pre-trained Language Models using Japanese Newspaper and Paywalls",
    "submit_datetime": "2024年04月26日",
    "abstract": "Dominant pre-trained language models (PLMs) have been successful in high-quality natural language generation. However, the analysis of their generation is not mature: do they acquire generalizable linguistic abstractions, or do they simply memorize and recover substrings of the training data? Especially, few studies focus on domain-specific PLM. In this study, we pre-trained domain-specific GPT-2 models using a limited corpus of Japanese newspaper articles and quantified memorization of training data by comparing them with general Japanese GPT-2 models. Our experiments revealed that domain-specific PLMs sometimes \"copy and paste\" on a large scale. Furthermore, we replicated the empirical finding that memorization is related to duplication, model size, and prompt length, in Japanese the same as in previous English studies. Our evaluations are relieved from data contamination concerns by focusing on newspaper paywalls, which prevent their use as training data. We hope that our paper encourages a sound discussion such as the security and copyright of PLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.17143",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17143v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17143/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17143v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17143/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17143v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17143/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17143v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17143/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17143v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17143/experimental_result.png"
      }
    ],
    "abstract_cn": "主流的预训练语言模型（PLMs）在自然语言生成的高品质输出上取得了显著成就。但对于它们的生成机制，我们的理解尚不成熟：这些模型究竟是掌握了通用的语言抽象能力，还是仅仅记住并复现了训练数据中的片段？特别是，针对特定领域的PLMs的研究更是寥寥无几。本研究中，我们利用有限的日本报纸文章集合，预训练了特定领域的GPT-2模型，并通过与通用的日本GPT-2模型的比较，量化了对训练数据的记忆力。实验结果显示，特定领域的PLMs有时会大规模地进行“复制粘贴”操作。此外，我们还复现了之前在英语研究中发现的现象：记忆力与复制行为、模型大小和提示长度有关，这一现象在日本语中同样存在。我们的评估通过专注于报纸的付费内容，避免了数据污染的问题，因为这些内容不会被用作训练材料。我们期望本文能够促进关于PLMs安全性和版权等问题的深入讨论。",
    "title_cn": "本文旨在通过日本报纸和付费墙的方式，探究特定领域预训练语言模型的记忆量化问题。",
    "tags": [
      "LLM理论",
      "",
      ""
    ]
  },
  {
    "title": "Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications",
    "submit_datetime": "2024年04月26日",
    "abstract": "Presently, with the assistance of advanced LLM application development frameworks, more and more LLM-powered applications can effortlessly augment the LLMs' knowledge with external content using the retrieval augmented generation (RAG) technique. However, these frameworks' designs do not have sufficient consideration of the risk of external content, thereby allowing attackers to undermine the applications developed with these frameworks. In this paper, we reveal a new threat to LLM-powered applications, termed retrieval poisoning, where attackers can guide the application to yield malicious responses during the RAG process. Specifically, through the analysis of LLM application frameworks, attackers can craft documents visually indistinguishable from benign ones. Despite the documents providing correct information, once they are used as reference sources for RAG, the application is misled into generating incorrect responses. Our preliminary experiments indicate that attackers can mislead LLMs with an 88.33\\% success rate, and achieve a 66.67\\% success rate in the real-world application, demonstrating the potential impact of retrieval poisoning.",
    "pdf_link": "https://arxiv.org/abs/2404.17196",
    "graphs": [],
    "abstract_cn": "当前，借助尖端的大型语言模型（LLM）应用开发框架，越来越多的LLM应用能够通过检索增强生成（RAG）技术轻松扩充其知识库。但这些框架在设计上对外部内容风险的考虑不足，容易让攻击者有机可乘。本文揭露了LLM应用面临的一个新风险——检索投毒，攻击者借此可以操纵应用在RAG过程中生成恶意回应。攻击者通过分析LLM应用框架，精心制作出外观上与正常文档无异的文档，这些文档虽提供正确信息，但一旦作为RAG的参考，却能误导应用产生错误结果。我们的初步实验显示，攻击者能够以88.33%的高成功率误导LLM，且在现实世界应用中的成功率也高达66.67%，这充分说明了检索投毒的严重性及其潜在的广泛影响。",
    "title_cn": "在大型语言模型（LLM）支持的应用程序中，悄然兴起了一种难以为人所察觉的检索投毒攻击。",
    "tags": [
      "分类：LLM应用",
      "网络安全",
      "人工智能"
    ]
  },
  {
    "title": "Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials",
    "submit_datetime": "2024年04月25日",
    "abstract": "Physically realistic materials are pivotal in augmenting the realism of 3D assets across various applications and lighting conditions. However, existing 3D assets and generative models often lack authentic material properties. Manual assignment of materials using graphic software is a tedious and time-consuming task. In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and describe materials, allowing the construction of a detailed material library. 2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects. 3) The correctly matched materials are then meticulously applied as reference for the new SVBRDF material generation according to the original diffuse map, significantly enhancing their visual authenticity. Make-it-Real offers a streamlined integration into the 3D content creation workflow, showcasing its utility as an essential tool for developers of 3D assets.",
    "pdf_link": "https://arxiv.org/abs/2404.16829",
    "graphs": [],
    "abstract_cn": "物理真实感材料对于提升3D资产在不同应用和光照条件下的真实度起着关键作用。但目前3D资产和生成模型往往缺少真实的材质属性。手动通过图形软件分配材质既繁琐又耗时。本文中，我们借助多模态大型语言模型（MLLMs），尤其是GPT-4V，提出了一种创新方法“Make-it-Real”：首先，我们展示了GPT-4V在识别和描述材质方面的高效能力，这有助于构建一个详尽的材质库；其次，通过结合视觉线索和层级化文本提示，GPT-4V能够精确地识别并匹配3D对象的相应部件的材质；最后，正确匹配的材质被精心应用于新的SVBRDF材质生成过程，以原始漫反射图为参考，显著提升了视觉真实感。“Make-it-Real”作为一个高效的工具，为3D内容创作流程提供了简化的集成方案，对3D资产开发者来说极具价值。",
    "title_cn": "实现真实：释放大型多模态模型的潜力，以绘制采用逼真材质的三维物体。",
    "tags": [
      "LLM应用",
      "3D建模",
      "人工智能"
    ]
  },
  {
    "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites",
    "submit_datetime": "2024年04月25日",
    "abstract": "In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448$\\times$448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.",
    "pdf_link": "https://arxiv.org/abs/2404.16821",
    "graphs": [],
    "abstract_cn": "本报告向您展示了InternVL 1.5，这是一款开源的多模态大型语言模型（MLLM），致力于缩小开源与商业专有模型在多模态理解领域的能力差距。我们带来了三项创新：（1）强化视觉编码器，通过持续学习策略优化了大规模视觉基础模型InternViT-6B，提升了视觉理解力，并便于在各类大型语言模型中迁移与复用。（2）动态高分辨率处理，根据输入图像的比例和分辨率，将图像分割成448×448像素的1至40块，支持最高4K分辨率的输入。（3）精心构建的高质量双语数据集，覆盖了日常生活场景和文档图像，并配有中英文问答对，显著提升了OCR及中文任务的处理能力。在一系列基准测试和对比研究中，InternVL 1.5与开源及商业模型相比，展现出了竞争力，在18个基准测试中的8个上达到了领先水平。相关代码已在GitHub上发布。",
    "title_cn": "我们距离 GPT-4V 还有多远？通过开源工具集，我们正逐步缩小与商业多模态模型之间的差距。",
    "tags": [
      "分类：LLM应用",
      "人工智能",
      "多模态理解"
    ]
  },
  {
    "title": "IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages",
    "submit_datetime": "2024年04月25日",
    "abstract": "As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench",
    "pdf_link": "https://arxiv.org/abs/2404.16816",
    "graphs": [],
    "abstract_cn": "随着大型语言模型（LLMs）在全球的广泛应用，确保它们能够体现全球语言多样性变得至关重要。印度，一个拥有14亿人口的多语言国家，为我们提供了丰富的研究素材。为了推动多语言LLM评估研究，我们推出了IndicGenBench——这是目前规模最大的基准测试平台，旨在评估LLM在29种印度语言上的用户生成任务表现，这些语言涵盖了13种不同的文字系统和4个语言系。IndicGenBench包含多种生成任务，包括跨语言摘要、机器翻译和跨语言问答等。该平台通过人工精选，首次为多种印度语言提供了多维度的并行评估数据，极大地扩展了现有基准测试的覆盖范围。我们在多种场景下对一系列商业和开源的LLMs进行了评估，包括GPT-3.5、GPT-4、PaLM-2、mT5、Gemma、BLOOM和LLaMA。其中，PaLM-2的最大模型在大多数任务上表现最为出色。然而，所有印度语言与英语相比仍有较大的性能差距，这表明我们仍需深化研究，以发展出更加全面和包容的多语言语言模型。IndicGenBench目前已在www.github.com/google-research-datasets/indic-gen-bench上线。",
    "title_cn": "IndicGenBench：一项多语言评估基准，旨在衡量大型语言模型在印度诸语言上的表现力。",
    "tags": [
      "LLM应用",
      "",
      "机器翻译"
    ]
  },
  {
    "title": "Make Your LLM Fully Utilize the Context",
    "submit_datetime": "2024年04月25日",
    "abstract": "While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU). Github Link: https://github.com/microsoft/FILM.",
    "pdf_link": "https://arxiv.org/abs/2404.16811",
    "graphs": [],
    "abstract_cn": "当前众多先进的大型语言模型虽能处理长篇输入，却常在充分利用长文本信息时力不从心，这一现象被称为“迷失于中间”的难题。我们假设这一问题源于在长文本训练时缺乏充分的显式指导，未能突出长文本中任意位置都可能蕴含重要信息。基于此洞察，本研究引入了信息密集型（IN2）训练方法，这是一种纯粹基于数据驱动的解决方案，用以攻克“迷失于中间”的挑战。IN2训练特别采用了一个合成的长文本问答数据集，该数据集中的问题答案需要对长文本中的一个短小片段（约128个令牌）进行细致的信息感知，并整合推理来自两个或更多短片段的信息。我们将这种信息密集型训练应用于Mistral-7B模型，进而推出了FILM-7B（意为填补中间的空白）。为全面测试FILM-7B处理长文本的能力，我们设计了三种探索性任务，覆盖了多样的文本风格和信息检索方式。测试结果证明，FILM-7B能从其32K的上下文窗口中稳健地提取信息。此外，FILM-7B在现实世界的长文本任务中显著提升了性能，例如在NarrativeQA任务上的F1分数从23.5提升至26.9，同时在短文本任务上也保持了相当的性能，如在MMLU任务上的准确率从59.3微降至59.2。项目代码已在GitHub上公开：https://github.com/microsoft/FILM。",
    "title_cn": "充分发挥您的大型语言模型的上下文理解能力",
    "tags": [
      "LLM理论",
      "",
      "信息检索"
    ]
  },
  {
    "title": "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning",
    "submit_datetime": "2024年04月25日",
    "abstract": "Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators.",
    "pdf_link": "https://arxiv.org/abs/2404.16807",
    "graphs": [],
    "abstract_cn": "生成常识推理（GCR）任务要求模型不仅要运用常识知识进行推理，还需产出连贯的语句。生成语句的品质固然关键，但多样性亦同样重要，因为它展现了模型调用不同常识知识的能力。大型语言模型（LLMs）通过上下文学习（ICL）在多项任务中提升了生成品质，且无需经过精细调整。尽管如此，对于LLMs输出的多样性，此前尚未有系统性的研究。为此，我们提出了一种新方法，旨在丰富LLMs的生成多样性，同时保持其品质。在三个GCR基准数据集上的实验结果显示，该方法在品质与多样性之间取得了完美的平衡。此外，通过该方法生成的语句，还可以作为训练数据，以增强现有常识生成器的多样性。",
    "title_cn": "通过上下文学习，提升大型语言模型在常识生成任务中的多样性表现。",
    "tags": [
      "LLM应用",
      "人工智能",
      ""
    ]
  },
  {
    "title": "AAPL: Adding Attributes to Prompt Learning for Vision-Language Models",
    "submit_datetime": "2024年04月25日",
    "abstract": "Recent advances in large pre-trained vision-language models have demonstrated remarkable performance on zero-shot downstream tasks. Building upon this, recent studies, such as CoOp and CoCoOp, have proposed the use of prompt learning, where context within a prompt is replaced with learnable vectors, leading to significant improvements over manually crafted prompts. However, the performance improvement for unseen classes is still marginal, and to tackle this problem, data augmentation has been frequently used in traditional zero-shot learning techniques. Through our experiments, we have identified important issues in CoOp and CoCoOp: the context learned through traditional image augmentation is biased toward seen classes, negatively impacting generalization to unseen classes. To address this problem, we propose adversarial token embedding to disentangle low-level visual augmentation features from high-level class information when inducing bias in learnable prompts. Through our novel mechanism called \"Adding Attributes to Prompt Learning\", AAPL, we guide the learnable context to effectively extract text features by focusing on high-level features for unseen classes. We have conducted experiments across 11 datasets, and overall, AAPL shows favorable performances compared to the existing methods in few-shot learning, zero-shot learning, cross-dataset, and domain generalization tasks.",
    "pdf_link": "https://arxiv.org/abs/2404.16804",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16804v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16804/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16804v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16804/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16804v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16804/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16804v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16804/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16804v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16804/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16804v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16804/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16804v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16804/x7.png"
      }
    ],
    "abstract_cn": "最新进展的大型预训练视觉-语言模型在零样本任务上取得了卓越成效。继此之后，如 CoOp 和 CoCoOp 等最新研究提出了提示学习的概念，即将提示中的上下文替换为可训练的向量，显著提升了性能。尽管如此，对于未见过的类别，性能提升依旧有限。为了克服这一难题，传统零样本学习中常采用数据增强技术。我们的实验揭示了 CoOp 和 CoCoOp 的关键问题：通过传统图像增强获得的上下文偏向于已知类别，这不利于对未知类别的泛化。为此，我们引入了对抗性标记嵌入技术，以在可学习提示中引入偏差时，将低层次的视觉增强特征与高层次的类别信息解耦。我们提出的“为提示学习添加属性”的新机制 AAPL，通过专注于未知类别的高层次特征，引导可学习上下文有效提取文本特征。我们在 11 个数据集上进行了广泛实验，结果显示 AAPL 在少样本学习、零样本学习、跨数据集和领域泛化任务中相比现有方法具有更佳的表现。",
    "title_cn": "AAPL：为视觉-语言模型引入属性，以增强提示学习的效果。",
    "tags": [
      "分类：LLM应用",
      "计算机视觉",
      ""
    ]
  },
  {
    "title": "Weak-to-Strong Extrapolation Expedites Alignment",
    "submit_datetime": "2024年04月25日",
    "abstract": "Although the capabilities of large language models (LLMs) ideally scale up with increasing data and compute, they are inevitably constrained by limited resources in reality. Suppose we have a moderately trained LLM (e.g., trained to align with human preference) in hand, can we further exploit its potential and cheaply acquire a stronger model? In this paper, we propose a simple method called ExPO to boost LLMs' alignment with human preference. ExPO assumes that a medium-aligned model can be interpolated between a less-aligned (weaker) model, e.g., the initial SFT model, and a better-aligned (stronger) one, thereby directly obtaining this stronger model by extrapolating from the weights of the former two relatively weaker models. On the AlpacaEval 2.0 benchmark, we show that ExPO pushes models trained with less preference data (e.g., 10% or 20%) to reach and even surpass the fully-trained one, without any additional training. Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and exhibits decent scalability across model sizes from 7B to 70B. Our work demonstrates the efficacy of model extrapolation in exploiting LLMs' capabilities, suggesting a promising direction that deserves future exploration.",
    "pdf_link": "https://arxiv.org/abs/2404.16792",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLM）的性能本应随着数据量和计算力的提升而增强，但实际上却受限于资源的有限性。如果我们已经拥有一个根据人类偏好适度训练的LLM，是否有可能进一步挖掘其潜力，以较低成本获得更强大的模型呢？本文提出了一种简洁的方法——ExPO，旨在提升LLM与人类偏好的契合度。ExPO基于这样的假设：在对齐程度较低（即较弱）的模型（如初始的SFT模型）和对齐程度较高（即较强）的模型之间，可以构建一个中等对齐度的模型，并通过从这两个较弱模型的权重进行外推，直接获得更强大的模型。在AlpacaEval 2.0基准测试中，我们证明了ExPO能够使使用较少偏好数据（如10%或20%）训练的模型达到甚至超过完全训练模型的性能，且无需额外训练。此外，ExPO还能显著提升现成的DPO/RLHF模型的性能，并且在从7B到70B不同规模的模型上都显示出良好的扩展性。我们的研究展示了通过模型外推来挖掘LLM潜力的有效性，为未来的探索指明了一个充满希望的方向。",
    "title_cn": "从弱到强的外推过程，有效地促进了数据的对齐工作。",
    "tags": [
      "LLM理论",
      "人工智能",
      ""
    ]
  },
  {
    "title": "SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension",
    "submit_datetime": "2024年04月25日",
    "abstract": "Comprehending text-rich visual content is paramount for the practical application of Multimodal Large Language Models (MLLMs), since text-rich scenarios are ubiquitous in the real world, which are characterized by the presence of extensive texts embedded within images. Recently, the advent of MLLMs with impressive versatility has raised the bar for what we can expect from MLLMs. However, their proficiency in text-rich scenarios has yet to be comprehensively and objectively assessed, since current MLLM benchmarks primarily focus on evaluating general visual comprehension. In this work, we introduce SEED-Bench-2-Plus, a benchmark specifically designed for evaluating \\textbf{text-rich visual comprehension} of MLLMs. Our benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs, each of which covers a wide spectrum of text-rich scenarios in the real world. These categories, due to their inherent complexity and diversity, effectively simulate real-world text-rich environments. We further conduct a thorough evaluation involving 34 prominent MLLMs (including GPT-4V, Gemini-Pro-Vision and Claude-3-Opus) and emphasize the current limitations of MLLMs in text-rich visual comprehension. We hope that our work can serve as a valuable addition to existing MLLM benchmarks, providing insightful observations and inspiring further research in the area of text-rich visual comprehension with MLLMs. The dataset and evaluation code can be accessed at https://github.com/AILab-CVC/SEED-Bench.",
    "pdf_link": "https://arxiv.org/abs/2404.16790",
    "graphs": [],
    "abstract_cn": "掌握充满文本的视觉内容对于实际部署多模态大型语言模型（MLLMs）至关重要，因为现实世界中充斥着嵌入大量文本的图像场景。最近，功能强大的MLLMs的问世提升了我们对这些模型的期待。但是，MLLMs在处理文本密集场景时的能力尚未得到全面客观的评估，目前的主要基准测试更多地集中在一般视觉理解上。本研究提出了SEED-Bench-2-Plus，这是一个专为评估MLLMs的文本密集视觉理解能力而设计的基准测试。该基准测试包含2300道多项选择题，覆盖图表、地图和网页三大类别，每类均覆盖现实世界中多样化的文本密集场景，并配有精确的人工标注。通过对34个知名MLLMs（包括GPT-4V、Gemini-Pro-Vision和Claude-3-Opus）的深入评估，我们揭示了MLLMs在文本密集视觉理解方面的当前局限。我们期望本研究能为现有的MLLM基准测试增添价值，提供深刻的见解，并激励该领域进一步的研究。相关数据集和评估代码可在 https://github.com/AILab-CVC/SEED-Bench 获取。",
    "title_cn": "SEED-Bench-2-Plus：以文本为中心的视觉理解，为多模态大型语言模型设立新标杆",
    "tags": [
      "分类：LLM应用",
      "视觉理解",
      "多模态学习"
    ]
  },
  {
    "title": "Continual Learning of Large Language Models: A Comprehensive Survey",
    "submit_datetime": "2024年04月25日",
    "abstract": "The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as \"catastrophic forgetting\". While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.",
    "pdf_link": "https://arxiv.org/abs/2404.16789",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）在静态、预先收集的通用数据集上的训练成就，开启了众多研究新方向和应用场景。其中一个关键研究方向是如何将这些预训练的LLMs有效地融入到不断变化的数据分布、任务结构和用户偏好中。当这些模型为特定需求定制时，常常会在原有知识领域出现性能大幅下降，这一现象被称为“灾难性遗忘”。尽管在持续学习（CL）领域已有深入研究，但在LLMs的应用中仍展现出新的挑战。本文综述了LLMs在CL领域的研究进展，分为四个部分：首先介绍持续学习的LLMs概览，涉及垂直连续性和水平连续性两个维度（第3节）；接着总结LLMs在现代CL中的三个学习阶段：持续预训练（CPT）、领域自适应预训练（DAP）和持续微调（CFT）（第4节）；然后概述了LLMs持续学习的评估协议和可用数据资源（第5节）；最后探讨了LLMs持续学习中的一些引人入胜的问题（第6节）。相关论文的完整列表可访问 https://github.com/Wang-ML-Lab/llm-continual-learning-survey。",
    "title_cn": "深入探究大型语言模型的持续学习：全面综述",
    "tags": [
      "分类：LLM应用\n\n这篇论文摘要主要讨论了大型语言模型（LLMs）在持续学习（Continual Learning, CL）领域的应用和挑战。它概述了LLMs在CL中的研究进展，包括持续预训练、领域自适应预训练和持续微调等学习阶段，以及评估协议和数据资源。由于论文聚焦于LLMs在特定应用场景中的性能和挑战，因此将其归类为LLM应用。",
      "机器学习",
      "持续学习"
    ]
  },
  {
    "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
    "submit_datetime": "2024年04月25日",
    "abstract": "While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely \"superficial\". We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.16766",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16766v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16766/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16766v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16766/x2.png"
      }
    ],
    "abstract_cn": "监督式微调（SFT）虽是定制大型语言模型（LLM）输出以适应特定需求的直接手段，但其对齐深度却受到质疑，批评者认为这种对齐仅停留在表面。本文针对跨语言生成任务，对这一假设进行了深入探讨，并指出SFT的效能可能因其依赖于先前标记来指导跨语言生成而受限。为解决SFT所需非英语数据成本高昂且资源有限的问题，我们提出了一种创新的无训练对齐方法——PreTTY。该方法仅利用少量与任务相关的先前标记，便能将基础LLM与SFT LLM相连，达到无需训练即可比拟的性能。在八种语言的机器翻译和词性标注任务上的实验验证了PreTTY在跨语言应用中的高效性。尤为突出的是，基础LLM仅需一两个先前标记启动解码，就能与经过SFT的模型相媲美。这一方法不仅为SFT提供了经济高效的替代方案，也为多语言LLM的普及化进程迈出了重要一步。",
    "title_cn": "将前缀文本视作线索，引导基础语言模型实现非英语语种的对齐。",
    "tags": [
      "LLM应用",
      "机器翻译",
      ""
    ]
  },
  {
    "title": "RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT Analysis",
    "submit_datetime": "2024年04月25日",
    "abstract": "Developing generalist foundation model has recently attracted tremendous attention among researchers in the field of AI for Medicine (AI4Medicine). A pivotal insight in developing these models is their reliance on dataset scaling, which emphasizes the requirements on developing open-source medical image datasets that incorporate diverse supervision signals across various imaging modalities. In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE. Specifically, we leverage the latest powerful universal segmentation and large language models, to extend the original datasets (over 25,692 non-contrast 3D chest CT volume and reports from 20,000 patients) from the following aspects: (i) organ-level segmentation masks covering 197 categories, which provide intermediate reasoning visual clues for interpretation; (ii) 665 K multi-granularity grounded reports, where each sentence of the report is linked to the corresponding anatomical region of CT volume in the form of a segmentation mask; (iii) 1.3 M grounded VQA pairs, where questions and answers are all linked with reference segmentation masks, enabling models to associate visual evidence with textual explanations. All grounded reports and VQA pairs in the validation set have gone through manual verification to ensure dataset quality. We believe that RadGenome-Chest CT can significantly advance the development of multimodal medical foundation models, by training to generate texts based on given segmentation regions, which is unattainable with previous relevant datasets. We will release all segmentation masks, grounded reports, and VQA pairs to facilitate further research and development in this field.",
    "pdf_link": "https://arxiv.org/abs/2404.16754",
    "graphs": [],
    "abstract_cn": "医学人工智能领域对开发通用基础模型的兴趣日益浓厚。这些模型的开发关键在于依赖数据集的扩展，特别是需要创建包含不同成像方式下多样化监督信号的开源医学图像数据集。本文介绍了RadGenome-Chest CT，这是一个基于CT-RATE的全面、大规模、区域引导的3D胸部CT解释数据集。我们采用了先进的通用分割技术和大型语言模型，对原始数据集（超过25,692个非对比3D胸部CT图像和20,000名患者的报告）进行了以下扩展：（i）197个器官类别的分割掩模，为解读提供推理的视觉线索；（ii）665,000份多粒度的地面报告，报告中每句话都与CT图像中的相应解剖区域通过分割掩模关联；（iii）130万个地面VQA对，问题和答案均与参考分割掩模相连，使模型能够将视觉证据与文本解释相联系。验证集中的所有地面报告和VQA对都经过了人工验证，确保了数据集的质量。我们认为，RadGenome-Chest CT能够显著推动多模态医学基础模型的发展，通过训练模型基于特定分割区域生成文本，这在以往的相关数据集中是无法实现的。我们将公开所有分割掩模、地面报告和VQA对，以促进该领域的进一步研究和开发。",
    "title_cn": "RadGenome-Chest CT：一个为胸部 CT 影像分析量身定制的视觉与语言结合的数据集",
    "tags": [
      "分类：LLM应用",
      "医学图像处理",
      "人工智能"
    ]
  },
  {
    "title": "Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class",
    "submit_datetime": "2024年04月25日",
    "abstract": "Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today's best models exhibit skewed performance when objects are dissimilar from their typical depiction. Real world objects such as pears appear in a variety of forms -- from diced to whole, on a table or in a bowl -- yet standard VLM classifiers map all instances of a class to a \\it{single vector based on the class label}. We argue that to represent this rich diversity within a class, zero-shot classification should move beyond a single vector. We propose a method to encode and account for diversity within a class using inferred attributes, still in the zero-shot setting without retraining. We find our method consistently outperforms standard zero-shot classification over a large suite of datasets encompassing hierarchies, diverse object states, and real-world geographic diversity, as well finer-grained datasets where intra-class diversity may be less prevalent. Importantly, our method is inherently interpretable, offering faithful explanations for each inference to facilitate model debugging and enhance transparency. We also find our method scales efficiently to a large number of attributes to account for diversity -- leading to more accurate predictions for atypical instances. Finally, we characterize a principled trade-off between overall and worst class accuracy, which can be tuned via a hyperparameter of our method. We hope this work spurs further research into the promise of zero-shot classification beyond a single class vector for capturing diversity in the world, and building transparent AI systems without compromising performance.",
    "pdf_link": "https://arxiv.org/abs/2404.16717",
    "graphs": [],
    "abstract_cn": "视觉-语言模型实现了无需重新训练即可对物体进行开放性分类，这标志着一个重大的技术进步。然而，即便是顶尖模型，一旦物体形态偏离常规，也会出现性能偏差。以梨为例，它们可能被切块或完整呈现，放置在桌面或碗中，而传统的视觉-语言模型（VLM）分类器却将同一类别的所有实例简化为基于类别标签的\\it{单一向量}。我们提出，为了捕捉类别内部的丰富多样性，零样本分类不应局限于单一向量。本研究提出了一种新方法，利用推断出的属性在零样本学习框架内编码和考虑类别内部的多样性，无需重新训练。我们的方法在多个大型数据集上的表现均优于传统零样本分类，这些数据集覆盖了层级结构、多样化的物体状态、现实世界的地理多样性，以及类别内部多样性不那么显著的更细致的数据集。值得注意的是，我们的方法天生具有可解释性，为每次推断提供准确的解释，这有助于模型调试并提高透明度。此外，我们的方法能够高效扩展至大量属性，以更准确地预测非典型实例。最后，我们确定了整体准确性与最差类别准确性之间的权衡原则，并通过我们方法的超参数进行调整。我们期望本研究能推动零样本分类技术的发展，超越单一类别向量，以更好地捕捉世界的多样性，并构建性能不打折的透明AI系统。",
    "title_cn": "拥抱多样性：探索超越单一类别向量的零样本分类的可解释性",
    "tags": [
      "LLM应用",
      "计算机视觉",
      ""
    ]
  },
  {
    "title": "Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding",
    "submit_datetime": "2024年04月25日",
    "abstract": "We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.",
    "pdf_link": "https://arxiv.org/abs/2404.16710",
    "graphs": [],
    "abstract_cn": "本文介绍了 LayerSkip，一种旨在提升大型语言模型（LLMs）推理效率的端到端方法。在训练阶段，我们实施了层间丢弃策略，对前置层采用低丢弃率，对后置层采用高丢弃率，并引入了所有变换层共享的早期退出损失机制。在推理阶段，我们发现这种训练策略显著提升了早期层的退出准确性，且无需额外增加辅助层或模块。此外，我们还提出了一种创新的自我推测解码技术，该技术允许在早期层进行退出，并利用模型剩余层进行校验和修正。与传统推测解码方法相比，我们的自我推测解码技术占用更少的内存，并能从草稿和验证阶段共享的计算资源与激活中获益。我们在不同规模的 Llama 模型上进行了广泛实验，包括从零开始的预训练、持续预训练、特定数据域的微调以及特定任务的微调。我们实现了这一推理方案，并在 CNN/DM 文档摘要任务上实现了最高 2.16 倍的速度提升，在编码任务上达到 1.82 倍，在 TOPv2 语义解析任务上达到 2.0 倍的加速。",
    "title_cn": "层级跳转：实现快速退出推理与自主推测性解码",
    "tags": [
      "LLM应用",
      "",
      "计算效率"
    ]
  },
  {
    "title": "Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents",
    "submit_datetime": "2024年04月25日",
    "abstract": "In the rapidly evolving field of artificial intelligence, ensuring safe decision-making of Large Language Models (LLMs) is a significant challenge. This paper introduces Governance of the Commons Simulation (GovSim), a simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. Through this simulation environment, we explore the dynamics of resource sharing among AI agents, highlighting the importance of ethical considerations, strategic planning, and negotiation skills. GovSim is versatile and supports any text-based agent, including LLMs agents. Using the Generative Agent framework, we create a standard agent that facilitates the integration of different LLMs. Our findings reveal that within GovSim, only two out of 15 tested LLMs managed to achieve a sustainable outcome, indicating a significant gap in the ability of models to manage shared resources. Furthermore, we find that by removing the ability of agents to communicate, they overuse the shared resource, highlighting the importance of communication for cooperation. Interestingly, most LLMs lack the ability to make universalized hypotheses, which highlights a significant weakness in their reasoning skills. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.",
    "pdf_link": "https://arxiv.org/abs/2404.16698",
    "graphs": [],
    "abstract_cn": "在日新月异的人工智能世界中，保障大型语言模型（LLMs）的决策安全无疑是一项艰巨任务。本文提出了“公共资源治理模拟”（GovSim），一个专为研究LLMs中战略互动与合作决策而设计的模拟平台。该平台让我们得以深入探讨AI代理间资源共享的机制，同时凸显了道德考量、战略规划和谈判技巧的关键作用。GovSim的通用性使其能够支持包括LLMs在内的所有文本型代理。利用生成代理框架，我们开发了一个标准化代理，以促进不同LLMs的融合。研究发现，在GovSim平台上，15个受测的LLMs中仅有两个能够达成可持续的成果，这暴露了模型在共享资源管理上的能力不足。更进一步，当代理失去沟通能力时，会出现共享资源的过度使用，这强调了沟通在促进合作中的核心作用。值得注意的是，大多数LLMs在进行普遍化假设方面存在缺陷，这反映了它们在推理能力上的一个明显弱点。我们已经将研究成果的全套内容开源，包括模拟环境、代理提示和全面的网页界面。",
    "title_cn": "合作还是崩溃：探究大型语言模型（LLM）代理社会中可持续性行为的兴起",
    "tags": [
      "Agent",
      "人工智能",
      "资源管理"
    ]
  },
  {
    "title": "Influence of Solution Efficiency and Valence of Instruction on Additive and Subtractive Solution Strategies in Humans and GPT-4",
    "submit_datetime": "2024年04月25日",
    "abstract": "We explored the addition bias, a cognitive tendency to prefer adding elements over removing them to alter an initial state or structure, by conducting four preregistered experiments examining the problem-solving behavior of both humans and OpenAl's GPT-4 large language model. The experiments involved 588 participants from the U.S. and 680 iterations of the GPT-4 model. The problem-solving task was either to create symmetry within a grid (Experiments 1 and 3) or to edit a summary (Experiments 2 and 4). As hypothesized, we found that overall, the addition bias was present. Solution efficiency (Experiments 1 and 2) and valence of the instruction (Experiments 3 and 4) played important roles. Human participants were less likely to use additive strategies when subtraction was relatively more efficient than when addition and subtraction were equally efficient. GPT-4 exhibited the opposite behavior, with a strong addition bias when subtraction was more efficient. In terms of instruction valence, GPT-4 was more likely to add words when asked to \"improve\" compared to \"edit\", whereas humans did not show this effect. When we looked at the addition bias under different conditions, we found more biased responses for GPT-4 compared to humans. Our findings highlight the importance of considering comparable and sometimes superior subtractive alternatives, as well as reevaluating one's own and particularly the language models' problem-solving behavior.",
    "pdf_link": "https://arxiv.org/abs/2404.16692",
    "graphs": [],
    "abstract_cn": "通过四项预注册实验，我们深入探究了“添加偏好”这一认知倾向，即人们更倾向于通过增加而非减少元素来改变初始状态或结构。这些实验比较了人类与 OpenAI 的 GPT-4 大型语言模型在解决问题时的行为，共有 588 名美国参与者和 GPT-4 模型进行了 680 轮迭代。实验任务包括在网格中创造对称性（实验 1 和 3）和编辑摘要（实验 2 和 4）。正如预期，我们普遍观察到了添加偏好的存在。解决效率和指令的情感色彩对实验结果有显著影响。在减法更为高效的情况下，人类参与者较少采用累加策略。而 GPT-4 则表现出相反的倾向，即使在减法更有效时也展现出强烈的添加偏好。对于指令的情感色彩，GPT-4 在被要求“改进”时更倾向于增加词汇，而“编辑”时则不然，人类则未显示这种差异。在不同条件下观察添加偏好时，GPT-4 相比人类展现出更明显的偏差。这些发现提醒我们，在解决问题时，应考虑等效甚至更优的减法方法，并重新审视自己尤其是语言模型的解题策略。",
    "title_cn": "本文探讨了解决方案的效率性以及指令的正负价态如何影响人类和 GPT-4 在执行加法和减法策略时的行为模式。",
    "tags": [
      "LLM应用\n\n这篇论文主要研究了人类和大型语言模型（LLM）在解决问题时的行为差异，特别是关于“添加偏好”的认知倾向。通过比较人类和GPT-4模型在不同实验任务中的表现，论文探讨了解决效率、指令情感色彩等因素对实验结果的影响。由于论文主要关注LLM在实际应用中的表现和行为特点，因此可以归类为LLM应用。",
      "心理学",
      "人工智能"
    ]
  },
  {
    "title": "EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning",
    "submit_datetime": "2024年04月25日",
    "abstract": "Visual Instruction Tuning represents a novel learning paradigm involving the fine-tuning of pre-trained language models using task-specific instructions. This paradigm shows promising zero-shot results in various natural language processing tasks but is still unexplored in vision emotion understanding. In this work, we focus on enhancing the model's proficiency in understanding and adhering to instructions related to emotional contexts. Initially, we identify key visual clues critical to visual emotion recognition. Subsequently, we introduce a novel GPT-assisted pipeline for generating emotion visual instruction data, effectively addressing the scarcity of annotated instruction data in this domain. Expanding on the groundwork established by InstructBLIP, our proposed EmoVIT architecture incorporates emotion-specific instruction data, leveraging the powerful capabilities of Large Language Models to enhance performance. Through extensive experiments, our model showcases its proficiency in emotion classification, adeptness in affective reasoning, and competence in comprehending humor. The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs, providing valuable insights and opening avenues for future exploration in this domain. Our code is available at \\url{https://github.com/aimmemotion/EmoVIT}.",
    "pdf_link": "https://arxiv.org/abs/2404.16670",
    "graphs": [],
    "abstract_cn": "视觉指令调优是一种新颖的学习范式，它通过特定任务指令对预训练的语言模型进行精准调整。这一范式在多种自然语言处理任务中展现了出色的零样本性能，但在视觉情感理解领域尚待深入研究。本研究致力于提升模型对情感语境相关指令的理解和执行能力。我们首先识别了对视觉情感识别至关重要的关键视觉线索。接着，我们提出了一个创新的GPT辅助流程，用于生成情感视觉指令数据，有效解决了该领域标注指令数据不足的问题。在InstructBLIP的基础上，我们设计的EmoVIT架构整合了针对情感的指令数据，借助大型语言模型的强大功能，显著提升了性能。经过广泛的实验验证，我们的模型在情感分类、情感推理以及幽默理解方面展现了卓越的能力。这项比较分析为LLM时代的Emotion Visual Instruction Tuning提供了坚实的基准，并为未来在该领域的研究提供了宝贵的洞见和探索途径。相关代码已在 \\url{https://github.com/aimmemotion/EmoVIT} 上公开。",
    "title_cn": "EmoVIT：以视觉指令调校，引领情感洞察力的革新。",
    "tags": [
      "LLM应用",
      "",
      "情感分析"
    ]
  },
  {
    "title": "Benchmarking Mobile Device Control Agents across Diverse Configurations",
    "submit_datetime": "2024年04月25日",
    "abstract": "Developing autonomous agents for mobile devices can significantly enhance user interactions by offering increased efficiency and accessibility. However, despite the growing interest in mobile device control agents, the absence of a commonly adopted benchmark makes it challenging to quantify scientific progress in this area. In this work, we introduce B-MoCA: a novel benchmark designed specifically for evaluating mobile device control agents. To create a realistic benchmark, we develop B-MoCA based on the Android operating system and define 60 common daily tasks. Importantly, we incorporate a randomization feature that changes various aspects of mobile devices, including user interface layouts and language settings, to assess generalization performance. We benchmark diverse agents, including agents employing large language models (LLMs) or multi-modal LLMs as well as agents trained from scratch using human expert demonstrations. While these agents demonstrate proficiency in executing straightforward tasks, their poor performance on complex tasks highlights significant opportunities for future research to enhance their effectiveness. Our source code is publicly available at https://b-moca.github.io.",
    "pdf_link": "https://arxiv.org/abs/2404.16660",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16660v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16660/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16660v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16660/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16660v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16660/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16660v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16660/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16660v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16660/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16660v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16660/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16660v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16660/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16660v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16660/x9.png"
      }
    ],
    "abstract_cn": "开发适用于移动设备的自主智能体，能够通过提升效率和便捷性显著改善用户体验。尽管对移动设备控制智能体的兴趣不断上升，但缺少统一的评估标准使得衡量该领域的科学进展变得困难。本研究提出了 B-MoCA，这是一个专为评估移动设备控制智能体而设计的全新基准测试。我们基于 Android 系统构建了 B-MoCA，设定了 60 项日常任务，并引入了随机化特性，以模拟用户界面布局和语言设置的变化，从而测试智能体的泛化能力。我们对多种智能体进行了评估，包括利用大型语言模型（LLMs）或多模态 LLMs 的智能体，以及完全基于人类专家示范从头训练的智能体。尽管这些智能体在处理简单任务上表现出色，但在复杂任务上的表现不佳，这为未来研究提供了提升智能体效能的重要方向。我们的源代码已在 https://b-moca.github.io 公开发布。",
    "title_cn": "跨多样配置对移动设备控制代理进行性能评估。",
    "tags": [
      "Agent",
      "移动设备",
      "人工智能"
    ]
  },
  {
    "title": "Evolutionary Large Language Models for Hardware Security: A Comparative Survey",
    "submit_datetime": "2024年04月25日",
    "abstract": "Automating hardware (HW) security vulnerability detection and mitigation during the design phase is imperative for two reasons: (i) It must be before chip fabrication, as post-fabrication fixes can be costly or even impractical; (ii) The size and complexity of modern HW raise concerns about unknown vulnerabilities compromising CIA triad. While Large Language Models (LLMs) can revolutionize both HW design and testing processes, within the semiconductor context, LLMs can be harnessed to automatically rectify security-relevant vulnerabilities inherent in HW designs. This study explores the seeds of LLM integration in register transfer level (RTL) designs, focusing on their capacity for autonomously resolving security-related vulnerabilities. The analysis involves comparing methodologies, assessing scalability, interpretability, and identifying future research directions. Potential areas for exploration include developing specialized LLM architectures for HW security tasks and enhancing model performance with domain-specific knowledge, leading to reliable automated security measurement and risk mitigation associated with HW vulnerabilities.",
    "pdf_link": "https://arxiv.org/abs/2404.16651",
    "graphs": [],
    "abstract_cn": "在芯片制造前自动化检测和缓解硬件安全漏洞至关重要，这不仅因为后期修复成本高昂或不切实际，也因为现代硬件的庞大与复杂性可能隐藏着威胁信息安全的秘密漏洞。大型语言模型（LLMs）有潜力革新硬件设计与测试流程，尤其在半导体领域，它们能够被用来自动识别并修复硬件设计中的安全漏洞。本研究深入探讨了将LLMs融入寄存器传输级（RTL）设计的可能性，特别是它们在独立解决安全漏洞方面的能力。研究内容包括对比不同方法、评估模型的可扩展性和可解释性，并展望未来的研究方向。探索的潜在领域可能包括为硬件安全任务设计专门的LLM架构，以及通过特定领域的知识提升模型性能，以实现对硬件漏洞的可靠自动化安全评估和风险缓解。",
    "title_cn": "进化型大型语言模型在硬件安全领域的应用：一项对比研究",
    "tags": [
      "分类：LLM应用",
      "半导体制造",
      "硬件安全"
    ]
  },
  {
    "title": "Tele-FLM Technical Report",
    "submit_datetime": "2024年04月25日",
    "abstract": "Large language models (LLMs) have showcased profound capabilities in language understanding and generation, facilitating a wide array of applications. However, there is a notable paucity of detailed, open-sourced methodologies on efficiently scaling LLMs beyond 50 billion parameters with minimum trial-and-error cost and computational resources. In this report, we introduce Tele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that features a stable, efficient pre-training paradigm and enhanced factual judgment capabilities. Tele-FLM demonstrates superior multilingual language modeling abilities, measured by BPB on textual corpus. Besides, in both English and Chinese foundation model evaluation, it is comparable to strong open-sourced models that involve larger pre-training FLOPs, such as Llama2-70B and DeepSeek-67B. In addition to the model weights, we share the core designs, engineering practices, and training details, which we expect to benefit both the academic and industrial communities.",
    "pdf_link": "https://arxiv.org/abs/2404.16645",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）以其卓越的语言理解和生成能力，推动了众多应用的发展。尽管如此，目前对于如何高效扩展这些模型至500亿参数以上，同时减少试错成本和计算资源消耗的详尽且开源的方法论却相对匮乏。本报告中，我们推出了Tele-FLM（亦称FLM-2），一个52亿参数的开源多语言大型语言模型，它采用了稳定而高效的预训练方法，并具备了更强的事实判断力。在文本语料库的BPB测试中，Tele-FLM展现了其卓越的多语言建模能力。在英文和中文的基础模型评估中，它与那些拥有更大预训练运算次数（FLOPs）的开源模型，如Llama2-70B和DeepSeek-67B，表现相当。我们不仅提供了模型权重，还包括了核心设计理念、工程实践和训练细节，期待这些信息能够为学术界和工业界带来裨益。",
    "title_cn": "远程-FLM 技术报告",
    "tags": [
      "LLM应用",
      "",
      "机器学习"
    ]
  },
  {
    "title": "TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning",
    "submit_datetime": "2024年04月25日",
    "abstract": "Charts are important for presenting and explaining complex data relationships. Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in various chart understanding tasks. However, the sheer size of these models in terms of parameters and computational requirements limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through a Program-of-Thoughts (PoT) learning strategy, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences produced by the vision transformer for high-resolution images through a Vision Token Merging module, which gradually merges most similar vision tokens. Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on a variety of chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart understanding MLLM with up to 13B parameters such as ChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V on ChartQA. It also demonstrates its superior efficiency with higher throughput during inference due to a smaller model scale and more efficient vision encoding. Our code and model are available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart.",
    "pdf_link": "https://arxiv.org/abs/2404.16635",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16635v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16635/x11.png"
      }
    ],
    "abstract_cn": "图表在阐释复杂数据关系方面扮演着关键角色。近期，多模态大型语言模型（MLLMs）在图表理解领域展现出了卓越的能力。但是，这些模型因参数众多和计算需求巨大，难以在资源有限的环境中部署。本文介绍了TinyChart，一款仅含3B参数的高效MLLM，专为图表理解设计。TinyChart解决了两个主要问题：首先，采用程序化思维（PoT）学习策略，通过训练模型生成Python程序来执行数值计算，减轻了学习数值计算的负担；其次，通过视觉令牌合并模块，优化了视觉变换器处理高分辨率图像时产生的长序列视觉特征。大量实验证明，3B参数的TinyChart在ChartQA、Chart-to-Text、Chart-to-Table、OpenCQA和ChartX等多个图表理解基准测试中达到了最先进的性能。它不仅超越了参数量高达13B的ChartLlama和ChartAst等MLLM，还在ChartQA上超越了封闭源的通用MLLM GPT-4V。此外，由于模型规模较小和视觉编码更高效，TinyChart在推理过程中展现了更高的吞吐量和优越的效率。我们的代码和模型已在 https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart 上发布。",
    "title_cn": "TinyChart：采用视觉标记融合与思维过程学习，实现图表理解的高效性",
    "tags": [
      "分类：LLM应用",
      "数据分析",
      "人工智能"
    ]
  },
  {
    "title": "Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare",
    "submit_datetime": "2024年04月25日",
    "abstract": "The integration of Large Language Models (LLMs) into healthcare promises to transform medical diagnostics, research, and patient care. Yet, the progression of medical LLMs faces obstacles such as complex training requirements, rigorous evaluation demands, and the dominance of proprietary models that restrict academic exploration. Transparent, comprehensive access to LLM resources is essential for advancing the field, fostering reproducibility, and encouraging innovation in healthcare AI. We present Hippocrates, an open-source LLM framework specifically developed for the medical domain. In stark contrast to previous efforts, it offers unrestricted access to its training datasets, codebase, checkpoints, and evaluation protocols. This open approach is designed to stimulate collaborative research, allowing the community to build upon, refine, and rigorously evaluate medical LLMs within a transparent ecosystem. Also, we introduce Hippo, a family of 7B models tailored for the medical domain, fine-tuned from Mistral and LLaMA2 through continual pre-training, instruction tuning, and reinforcement learning from human and AI feedback. Our models outperform existing open medical LLMs models by a large-margin, even surpassing models with 70B parameters. Through Hippocrates, we aspire to unlock the full potential of LLMs not just to advance medical knowledge and patient care but also to democratize the benefits of AI research in healthcare, making them available across the globe.",
    "pdf_link": "https://arxiv.org/abs/2404.16621",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）在医疗领域的融合预示着医学诊断、研究和患者护理的革新。但这一进程遭遇了诸如培训复杂性、评估严格性以及专有模型的壁垒等挑战，这些壁垒限制了学术界的深入研究。为了推动医疗AI的发展、提高研究的可复制性并激发创新，对LLM资源的透明和全面访问显得尤为重要。我们推出了希波克拉底（Hippocrates），一个专为医疗领域设计的开源LLM框架，它与以往的尝试截然不同，提供了对其训练数据集、代码库、检查点和评估协议的完全开放访问。这种开放的姿态旨在促进合作研究，使社区能够在一个透明的生态系统中构建、改进和严格评估医学LLMs。同时，我们还推出了Hippo系列，这是一系列为医疗领域量身定制的7B模型，它们基于Mistral和LLaMA2，通过持续的预训练、指令调整和人机反馈的强化学习进行微调。我们的模型在性能上显著超越了现有的开放医学LLMs，甚至超过了参数量达70B的模型。通过希波克拉底，我们期望充分挖掘LLMs的潜力，不仅为了增进医学知识和提升患者护理水平，也为了在全球范围内普及AI研究在医疗保健中的益处。",
    "title_cn": "希波克拉底：一个开源框架，致力于在医疗保健领域推动大型语言模型的发展。",
    "tags": [
      "LLM应用",
      "",
      "人工智能"
    ]
  },
  {
    "title": "Understanding Privacy Risks of Embeddings Induced by Large Language Models",
    "submit_datetime": "2024年04月25日",
    "abstract": "Large language models (LLMs) show early signs of artificial general intelligence but struggle with hallucinations. One promising solution to mitigate these hallucinations is to store external knowledge as embeddings, aiding LLMs in retrieval-augmented generation. However, such a solution risks compromising privacy, as recent studies experimentally showed that the original text can be partially reconstructed from text embeddings by pre-trained language models. The significant advantage of LLMs over traditional pre-trained models may exacerbate these concerns. To this end, we investigate the effectiveness of reconstructing original knowledge and predicting entity attributes from these embeddings when LLMs are employed. Empirical findings indicate that LLMs significantly improve the accuracy of two evaluated tasks over those from pre-trained models, regardless of whether the texts are in-distribution or out-of-distribution. This underscores a heightened potential for LLMs to jeopardize user privacy, highlighting the negative consequences of their widespread use. We further discuss preliminary strategies to mitigate this risk.",
    "pdf_link": "https://arxiv.org/abs/2404.16587",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16587v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16587/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16587v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16587/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16587v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16587/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16587v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16587/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16587v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16587/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16587v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16587/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16587v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16587/x7.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）初显人工通用智能的端倪，却也面临着幻觉的挑战。一种缓解幻觉的潜在方法是将外部知识嵌入化，以增强LLMs的检索增强生成能力。但这种做法可能会侵犯隐私，因为研究表明，预训练的语言模型能够从文本嵌入中部分还原原始文本。LLMs相较于传统预训练模型的优势可能会加剧对隐私的担忧。本研究探讨了使用LLMs时，从嵌入中重建原始知识和预测实体属性的有效性。实验结果显示，LLMs在两项评估任务的准确性上显著超越了预训练模型，无论文本是否符合分布。这突显了LLMs可能对用户隐私构成更大威胁，也反映了它们普及使用的潜在负面影响。文章还进一步讨论了减轻这一风险的初步策略。",
    "title_cn": "深入探究大型语言模型所诱发的嵌入技术背后的隐私隐患",
    "tags": [
      "分类：LLM应用",
      "人工智能",
      "隐私保护"
    ]
  },
  {
    "title": "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark",
    "submit_datetime": "2024年04月25日",
    "abstract": "Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more. In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms. We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data. Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features. This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series. Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter. In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length.",
    "pdf_link": "https://arxiv.org/abs/2404.16563",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）展现了在自动进行时间序列分析与报告方面的潜力，这对于医疗、金融、气候、能源等多个领域都至关重要。本文提出了一套严格评估LLMs时间序列理解能力的方法框架，覆盖了单变量和多变量的时间序列。我们引入了一个全面的时间序列特征分类体系，这一体系详细划分了时间序列数据的多种内在特性。基于此分类体系，我们设计并构建了一个包含多样特征的时间序列数据集，为测试LLMs的时间序列理解能力提供了坚实的基准。实验结果显示了当前最先进LLMs在时间序列理解方面的优势与局限，指出了这些模型能够轻松理解的特征以及它们的不足之处。同时，我们还发现了LLMs对于数据格式、查询点位置以及时间序列长度等因素的敏感度。",
    "title_cn": "全面分类与基准测试：探究大型语言模型对时间序列特征的理解能力。",
    "tags": [
      "LLM应用",
      "时间序列分析",
      "多领域应用"
    ]
  },
  {
    "title": "Energy-Latency Manipulation of Multi-modal Large Language Models via Verbose Samples",
    "submit_datetime": "2024年04月25日",
    "abstract": "Despite the exceptional performance of multi-modal large language models (MLLMs), their deployment requires substantial computational resources. Once malicious users induce high energy consumption and latency time (energy-latency cost), it will exhaust computational resources and harm availability of service. In this paper, we investigate this vulnerability for MLLMs, particularly image-based and video-based ones, and aim to induce high energy-latency cost during inference by crafting an imperceptible perturbation. We find that high energy-latency cost can be manipulated by maximizing the length of generated sequences, which motivates us to propose verbose samples, including verbose images and videos. Concretely, two modality non-specific losses are proposed, including a loss to delay end-of-sequence (EOS) token and an uncertainty loss to increase the uncertainty over each generated token. In addition, improving diversity is important to encourage longer responses by increasing the complexity, which inspires the following modality specific loss. For verbose images, a token diversity loss is proposed to promote diverse hidden states. For verbose videos, a frame feature diversity loss is proposed to increase the feature diversity among frames. To balance these losses, we propose a temporal weight adjustment algorithm. Experiments demonstrate that our verbose samples can largely extend the length of generated sequences.",
    "pdf_link": "https://arxiv.org/abs/2404.16557",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16557v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16557/x15.png"
      }
    ],
    "abstract_cn": "多模态大型语言模型（MLLMs）虽性能卓越，但其运行却需巨额计算资源支撑。恶意用户一旦触发高能耗与延迟（能耗-延迟成本），便可能耗尽资源，影响服务的稳定性。本文聚焦MLLMs，尤其是基于图像和视频的模型，探讨了如何通过精心设计的难以察觉的扰动，在推理过程中制造高昂的能耗-延迟成本。研究发现，通过延长生成序列的长度，可以操控这一成本，这促使我们引入了冗长样本的概念，涵盖冗长的图像与视频。文章具体提出了两种非模态特定的损失函数：一是推迟序列结束（EOS）标记的损失，二是增加每个生成标记不确定性的损失。此外，提升多样性对于激励更长响应、增加复杂性同样关键，这启发了我们设计了以下模态特定的损失函数。对于冗长图像，提出了一种标记多样性损失，以增强隐藏状态的多样性；对于冗长视频，则提出了一种帧特征多样性损失，以提升帧间特征的多样性。为平衡这些损失，我们设计了一种时间权重调整算法。实验结果证明，我们的冗长样本能显著延长生成序列的长度。",
    "title_cn": "本文探讨了如何利用冗长样本对多模态大型语言模型的能耗与延迟进行精细调控。",
    "tags": [
      "分类：LLM应用",
      "计算机视觉",
      ""
    ]
  },
  {
    "title": "Evaluating Consistency and Reasoning Capabilities of Large Language Models",
    "submit_datetime": "2024年04月25日",
    "abstract": "Large Language Models (LLMs) are extensively used today across various sectors, including academia, research, business, and finance, for tasks such as text generation, summarization, and translation. Despite their widespread adoption, these models often produce incorrect and misleading information, exhibiting a tendency to hallucinate. This behavior can be attributed to several factors, with consistency and reasoning capabilities being significant contributors. LLMs frequently lack the ability to generate explanations and engage in coherent reasoning, leading to inaccurate responses. Moreover, they exhibit inconsistencies in their outputs. This paper aims to evaluate and compare the consistency and reasoning capabilities of both public and proprietary LLMs. The experiments utilize the Boolq dataset as the ground truth, comprising questions, answers, and corresponding explanations. Queries from the dataset are presented as prompts to the LLMs, and the generated responses are evaluated against the ground truth answers. Additionally, explanations are generated to assess the models' reasoning abilities. Consistency is evaluated by repeatedly presenting the same query to the models and observing for variations in their responses. For measuring reasoning capabilities, the generated explanations are compared to the ground truth explanations using metrics such as BERT, BLEU, and F-1 scores. The findings reveal that proprietary models generally outperform public models in terms of both consistency and reasoning capabilities. However, even when presented with basic general knowledge questions, none of the models achieved a score of 90\\% in both consistency and reasoning. This study underscores the direct correlation between consistency and reasoning abilities in LLMs and highlights the inherent reasoning challenges present in current language models.",
    "pdf_link": "https://arxiv.org/abs/2404.16478",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）广泛应用于学术、研究、商业和金融等多个领域，承担着文本生成、摘要和翻译等任务。然而，尽管这些模型广受欢迎，它们却常常制造出错误和误导性的信息，似乎有一种“幻想”倾向。这种倾向的形成，一致性和推理能力是主要的影响因素。大型语言模型往往在生成解释和进行逻辑推理方面力不从心，从而导致了应答的不准确。此外，它们的输出结果也常常自相矛盾。本文的目标是对比评估公共与私有的大型语言模型在一致性和推理能力上的表现。实验采用了Boolq数据集作为基准，该数据集包含了问题、答案及相应的解释。将数据集中的查询作为提示输入给大型语言模型，然后将其生成的回答与基准答案进行比较。同时，为了评估模型的推理能力，还生成了解释。通过重复向模型提出相同的问题，观察其回答的变化，以此来衡量一致性。而推理能力的测量，则是通过将生成的解释与基准解释使用BERT、BLEU和F-1等指标进行对比。研究发现，私有模型在一致性和推理能力上普遍优于公共模型。但是，即便面对基础的常识性问题，也没有一个模型能够在一致性和推理上都达到90%的准确率。本研究揭示了大型语言模型中一致性与推理能力之间的密切联系，并指出了当前语言模型所面临的推理挑战。",
    "title_cn": "探究大型语言模型的内在一致性与逻辑推理功能",
    "tags": [
      "分类：LLM理论",
      "学术研究",
      ""
    ]
  },
  {
    "title": "Large Language Models Perform on Par with Experts Identifying Mental Health Factors in Adolescent Online Forums",
    "submit_datetime": "2024年04月25日",
    "abstract": "Mental health in children and adolescents has been steadily deteriorating over the past few years [ 1 ]. The recent advent of Large Language Models (LLMs) offers much hope for cost and time efficient scaling of monitoring and intervention, yet despite specifically prevalent issues such as school bullying and eating disorders, previous studies on have not investigated performance in this domain or for open information extraction where the set of answers is not predetermined. We create a new dataset of Reddit posts from adolescents aged 12-19 annotated by expert psychiatrists for the following categories: TRAUMA, PRECARITY, CONDITION, SYMPTOMS, SUICIDALITY and TREATMENT and compare expert labels to annotations from two top performing LLMs (GPT3.5 and GPT4). In addition, we create two synthetic datasets to assess whether LLMs perform better when annotating data as they generate it. We find GPT4 to be on par with human inter-annotator agreement and performance on synthetic data to be substantially higher, however we find the model still occasionally errs on issues of negation and factuality and higher performance on synthetic data is driven by greater complexity of real data rather than inherent advantage.",
    "pdf_link": "https://arxiv.org/abs/2404.16461",
    "graphs": [],
    "abstract_cn": "近年来，儿童和青少年的心理健康问题日益严重。大型语言模型（LLMs）的兴起为心理健康监测和干预提供了成本效益和时间效率的新途径。尽管如此，对于学校欺凌、饮食失调等普遍问题，先前研究并未深入探讨LLMs在这些领域的应用，尤其是在开放信息提取方面，即答案集未预先设定的情况下。本研究构建了一个新的数据集，包含12至19岁青少年在Reddit上的帖子，由专业精神病学家标注为创伤、不稳定、状况、症状、自杀倾向、治疗等类别。研究比较了专家的标注与两款顶尖LLMs（GPT3.5和GPT4）的标注结果。此外，研究还创建了两个合成数据集，用以评估LLMs在生成数据时进行标注的效果。研究发现，GPT4在一致性和性能上与人类专家相当，且在合成数据上的表现显著优于真实数据。然而，模型在处理否定和事实性问题时仍存在偏差，而合成数据上的性能提升更多是由于真实数据的复杂性，而非模型的内在优势。",
    "title_cn": "在分析青少年在线论坛中的心理健康状况时，大型语言模型展现出与专家相媲美的表现。",
    "tags": [
      "LLM应用",
      "心理健康",
      ""
    ]
  },
  {
    "title": "Asking and Answering Questions to Extract Event-Argument Structures",
    "submit_datetime": "2024年04月25日",
    "abstract": "This paper presents a question-answering approach to extract document-level event-argument structures. We automatically ask and answer questions for each argument type an event may have. Questions are generated using manually defined templates and generative transformers. Template-based questions are generated using predefined role-specific wh-words and event triggers from the context document. Transformer-based questions are generated using large language models trained to formulate questions based on a passage and the expected answer. Additionally, we develop novel data augmentation strategies specialized in inter-sentential event-argument relations. We use a simple span-swapping technique, coreference resolution, and large language models to augment the training instances. Our approach enables transfer learning without any corpora-specific modifications and yields competitive results with the RAMS dataset. It outperforms previous work, and it is especially beneficial to extract arguments that appear in different sentences than the event trigger. We also present detailed quantitative and qualitative analyses shedding light on the most common errors made by our best model.",
    "pdf_link": "https://arxiv.org/abs/2404.16413",
    "graphs": [],
    "abstract_cn": "本研究介绍了一种基于问答的文档级事件-论元结构提取方法。我们针对每个事件可能涉及的论元类型，自动生成并回答相关问题。这些问题利用预先定义的模板和生成式变换器来构建。模板问题通过上下文文档中的角色特定 wh-words 和事件触发器来生成，而变换器问题则依赖于经过训练的大型语言模型，这些模型能够根据给定段落和预期答案来构建问题。此外，我们还开发了创新的数据增强策略，专注于加强句子间的事件-论元关系。通过简单的跨度交换技术、共指消解以及大型语言模型，我们扩充了训练样本。这种方法支持无需针对特定语料库进行修改的迁移学习，并在 RAMS 数据集上取得了优异的成绩，超越了先前的研究。它特别擅长提取那些与事件触发器位于不同句子中的论元。我们还提供了详尽的定量和定性分析，深入探讨了我们最先进模型所犯的常见错误。",
    "title_cn": "通过提问和回答，我们能够抽取事件及其论元的结构。",
    "tags": [
      "LLM应用",
      "",
      "事件抽取"
    ]
  },
  {
    "title": "U2++ MoE: Scaling 4.7x parameters with minimal impact on RTF",
    "submit_datetime": "2024年04月25日",
    "abstract": "Scale has opened new frontiers in natural language processing, but at a high cost. In response, by learning to only activate a subset of parameters in training and inference, Mixture-of-Experts (MoE) have been proposed as an energy efficient path to even larger and more capable language models and this shift towards a new generation of foundation models is gaining momentum, particularly within the field of Automatic Speech Recognition (ASR). Recent works that incorporating MoE into ASR models have complex designs such as routing frames via supplementary embedding network, improving multilingual ability for the experts, and utilizing dedicated auxiliary losses for either expert load balancing or specific language handling. We found that delicate designs are not necessary, while an embarrassingly simple substitution of MoE layers for all Feed-Forward Network (FFN) layers is competent for the ASR task. To be more specific, we benchmark our proposed model on a large scale inner-source dataset (160k hours), the results show that we can scale our baseline Conformer (Dense-225M) to its MoE counterparts (MoE-1B) and achieve Dense-1B level Word Error Rate (WER) while maintaining a Dense-225M level Real Time Factor (RTF). Furthermore, by applying Unified 2-pass framework with bidirectional attention decoders (U2++), we achieve the streaming and non-streaming decoding modes in a single MoE based model, which we call U2++ MoE. We hope that our study can facilitate the research on scaling speech foundation models without sacrificing deployment efficiency.",
    "pdf_link": "https://arxiv.org/abs/2404.16407",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16407v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16407/u2pp-moe.png"
      }
    ],
    "abstract_cn": "规模的扩大为自然语言处理领域带来了新的突破，但这也伴随着高昂的成本。为了应对这一挑战，专家混合（MoE）模型通过在训练和推理过程中仅激活部分参数，提出了一种更为节能的解决方案，以实现语言模型的进一步扩大和能力提升。这种向新一代基础模型的演进，特别是在自动语音识别（ASR）领域，正逐渐成为主流。近期的研究工作将MoE集成到ASR模型中，采用了如通过辅助嵌入网络路由帧、增强专家的多语言处理能力、以及使用专门的辅助损失进行专家负载均衡或特定语言处理等复杂设计。然而，我们的研究发现，这些复杂的设计并非必需，简单地将所有前馈网络（FFN）层替换为MoE层，就足以胜任ASR任务。具体而言，我们在大规模内部数据集（160,000小时）上对模型进行了基准测试，结果表明，我们可以将基线Conformer模型（Dense-225M）扩展到MoE版本（MoE-1B），在保持Dense-225M级别的实时因子（RTF）的同时，达到Dense-1B级别的词错误率（WER）。此外，通过采用带有双向注意力解码器的统一两遍框架（U2++），我们在基于MoE的单一模型中实现了流式和非流式解码模式，即U2++ MoE模型。我们期望本研究能够推动在不牺牲部署效率的前提下，对扩展语音基础模型的研究工作。",
    "title_cn": "U2++ MoE：在几乎不影响实时性能的情况下，实现了参数量的4.7倍扩展。",
    "tags": [
      "LLM理论",
      "",
      "自动语音识别"
    ]
  },
  {
    "title": "Efficiency in Focus: LayerNorm as a Catalyst for Fine-tuning Medical Visual Language Pre-trained Models",
    "submit_datetime": "2024年04月25日",
    "abstract": "In the realm of Medical Visual Language Models (Med-VLMs), the quest for universal efficient fine-tuning mechanisms remains paramount, especially given researchers in interdisciplinary fields are often extremely short of training resources, yet largely unexplored. Given the unique challenges in the medical domain, such as limited data scope and significant domain-specific requirements, evaluating and adapting Parameter-Efficient Fine-Tuning (PEFT) methods specifically for Med-VLMs is essential. Most of the current PEFT methods on Med-VLMs have yet to be comprehensively investigated but mainly focus on adding some components to the model's structure or input. However, fine-tuning intrinsic model components often yields better generality and consistency, and its impact on the ultimate performance of Med-VLMs has been widely overlooked and remains understudied. In this paper, we endeavour to explore an alternative to traditional PEFT methods, especially the impact of fine-tuning LayerNorm layers, FFNs and Attention layers on the Med-VLMs. Our comprehensive studies span both small-scale and large-scale Med-VLMs, evaluating their performance under various fine-tuning paradigms across tasks such as Medical Visual Question Answering and Medical Imaging Report Generation. The findings reveal unique insights into the effects of intrinsic parameter fine-tuning methods on fine-tuning Med-VLMs to downstream tasks and expose fine-tuning solely the LayerNorm layers not only surpasses the efficiency of traditional PEFT methods but also retains the model's accuracy and generalization capabilities across a spectrum of medical downstream tasks. The experiments show LayerNorm fine-tuning's superior adaptability and scalability, particularly in the context of large-scale Med-VLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.16385",
    "graphs": [],
    "abstract_cn": "在医学视觉语言模型（Med-VLMs）的研究领域，探索一种通用且高效的微调机制显得尤为关键，特别是对于资源匮乏的跨学科研究者来说，这一需求尚未得到充分满足。面对医学领域的特殊挑战，如数据的有限性和领域特定的高标准要求，对Med-VLMs进行参数高效微调（PEFT）的评估与适配显得尤为重要。目前，大多数PEFT方法尚未全面探究，而是更多关注于模型结构或输入的局部改进。然而，对模型内部组件进行微调往往能够带来更佳的泛化和一致性，这一点在Med-VLMs的最终性能影响上却鲜有关注。本文旨在探索一种替代传统PEFT方法的新途径，特别是研究微调LayerNorm层、FFN和注意力层对Med-VLMs的影响。我们对不同规模的Med-VLMs进行了深入研究，包括小规模和大规模模型，并在多种微调框架下，评估了它们在医学视觉问答和医学影像报告生成等任务上的表现。研究发现，内部参数微调的方法在微调Med-VLMs以适应下游任务时具有独特的效果，尤其是仅微调LayerNorm层的方法，不仅在效率上超越了传统PEFT方法，而且在一系列医学相关下游任务中保持了模型的准确性和泛化能力。实验结果进一步证明，LayerNorm微调在大型Med-VLMs中的应用展现出了卓越的适应性和可扩展性。",
    "title_cn": "聚焦效率：LayerNorm 助力医学视觉语言预训练模型的精准调优。",
    "tags": [
      "LLM应用",
      "",
      "人工智能"
    ]
  },
  {
    "title": "List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs",
    "submit_datetime": "2024年04月25日",
    "abstract": "Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image. These tags, marked with alphanumerics, can be indexed via text tokens for easy reference. Despite the extraordinary performance from GPT-4V, we observe that other Multimodal Large Language Models (MLLMs) struggle to understand these visual tags. To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: \"list items one by one,\" which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags. By integrating our curated dataset with other visual instruction tuning datasets, we are able to equip existing MLLMs with the SoM prompting ability. Furthermore, we evaluate our finetuned SoM models on five MLLM benchmarks. We find that this new dataset, even in a relatively small size (10k-30k images with tags), significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs. Perhaps surprisingly, these improvements persist even when the visual tags are omitted from input images during inference. This suggests the potential of \"list items one by one\" as a new paradigm for training MLLMs, which strengthens the object-text alignment through the use of visual tags in the training stage. Finally, we conduct analyses by probing trained models to understand the working mechanism of SoM. Our code and data are available at \\url{https://github.com/zzxslp/SoM-LLaVA}.",
    "pdf_link": "https://arxiv.org/abs/2404.16375",
    "graphs": [],
    "abstract_cn": "Set-of-Mark（SoM）提示技术通过在图像上插入带有字母数字标签的视觉对象，显著提升了 GPT-4V 的视觉定位功能，使得模型能够轻松地通过文本标记引用这些标签。尽管 GPT-4V 表现出色，但其他多模态大型语言模型（MLLMs）在解析视觉标签上仍面临挑战。为此，我们提出了一种创新的学习模式——“逐一列举”，要求模型按照标签的字母数字顺序，对图像上的所有视觉标签进行枚举和描述。通过结合我们精选的数据集和现有的视觉指令调整数据集，成功地为现有 MLLMs 赋予了 SoM 提示的能力。我们在五个 MLLM 基准测试中对微调后的 SoM 模型进行了评估，发现即便是在较小的数据集规模（10k-30k 张带有标签的图像）下，也能显著提升 MLLMs 的视觉推理能力，并减少幻觉现象。令人意外的是，即便在推理时省略了输入图像中的视觉标签，这些提升依然能够保持。这一发现表明“逐一列举”有潜力成为训练 MLLMs 的新范式，它通过训练阶段的视觉标签使用，加强了对象与文本之间的对应关系。最后，我们通过深入分析训练模型，揭示了 SoM 的工作原理。相关代码和数据已在 \\url{https://github.com/zzxslp/SoM-LLaVA} 上公开。",
    "title_cn": "一一列举项目：开辟多模态大型语言模型（LLMs）的新数据来源与学习模式。",
    "tags": [
      "LLM应用",
      "计算机视觉",
      "人工智能"
    ]
  },
  {
    "title": "Don't Say No: Jailbreaking LLM by Suppressing Refusal",
    "submit_datetime": "2024年04月25日",
    "abstract": "Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values. Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to \"jailbreaking\" attacks, where carefully crafted prompts elicit them to produce toxic content. One category of jailbreak attacks is reformulating the task as adversarial attacks by eliciting the LLM to generate an affirmative response. However, the typical attack in this category GCG has very limited attack success rate. In this study, to better study the jailbreak attack, we introduce the DSN (Don't Say No) attack, which prompts LLMs to not only generate affirmative responses but also novelly enhance the objective to suppress refusals. In addition, another challenge lies in jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the attack. The existing evaluation such as refusal keyword matching has its own limitation as it reveals numerous false positive and false negative instances. To overcome this challenge, we propose an ensemble evaluation pipeline incorporating Natural Language Inference (NLI) contradiction assessment and two external LLM evaluators. Extensive experiments demonstrate the potency of the DSN and the effectiveness of ensemble evaluation compared to baseline methods.",
    "pdf_link": "https://arxiv.org/abs/2404.16369",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/examples.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/DSN_mainfig.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/sliding.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/ASR_step_Llama_only_searching.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/ASR_step_Llama_both.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/ASR_step_Vicuna_only_searching.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/ASR_step_Vicuna_both.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/max_ASR_vs_alpha_Llama.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/max_ASR_vs_alpha_Vicuna.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/AUROC.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/max_ASR_vs_alpha_Llama_eval_ensemble.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16369v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16369/max_ASR_vs_alpha_Vicuna_eval_ensemble.png"
      }
    ],
    "abstract_cn": "保障大型语言模型（LLMs）的安全对齐是生成符合人类价值观响应的关键。尽管LLMs能够识别并规避有害查询，但它们仍易受到“越狱”攻击，这类攻击通过精心构造的提示诱使模型生成有害内容。其中一种越狱攻击手法是将任务转化为对抗性攻击，诱使LLMs给出肯定回答。然而，这类攻击中的典型手段——GCG攻击——成功率并不高。本研究为了深入探究越狱攻击，提出了DSN（不要说不）攻击，它不仅诱导LLMs生成肯定回答，还创新性地增加了抑制拒绝的目标。此外，越狱攻击的评估工作同样充满挑战，因为直接且精确地评估攻击的危害性极为困难。现有的评估方法，例如拒绝关键词匹配，存在局限性，因为它会产生大量误报和漏报。为了应对这一挑战，我们设计了一个集成评估流程，包括自然语言推理（NLI）矛盾评估和两个外部LLM评估器。广泛的实验证明了DSN攻击的威力以及集成评估方法相比传统基线方法的有效性。",
    "title_cn": "别拒绝：抑制拒绝行为以解锁大型语言模型的潜力。",
    "tags": [
      "LLM应用",
      "人工智能安全",
      ""
    ]
  },
  {
    "title": "Training-Free Unsupervised Prompt for Vision-Language Models",
    "submit_datetime": "2024年04月25日",
    "abstract": "Prompt learning has become the most effective paradigm for adapting large pre-trained vision-language models (VLMs) to downstream tasks. Recently, unsupervised prompt tuning methods, such as UPL and POUF, directly leverage pseudo-labels as supervisory information to fine-tune additional adaptation modules on unlabeled data. However, inaccurate pseudo labels easily misguide the tuning process and result in poor representation capabilities. In light of this, we propose Training-Free Unsupervised Prompts (TFUP), which maximally preserves the inherent representation capabilities and enhances them with a residual connection to similarity-based prediction probabilities in a training-free and labeling-free manner. Specifically, we integrate both instance confidence and prototype scores to select representative samples, which are used to customize a reliable Feature Cache Model (FCM) for training-free inference. Then, we design a Multi-level Similarity Measure (MSM) that considers both feature-level and semantic-level similarities to calculate the distance between each test image and the cached sample as the weight of the corresponding cached label to generate similarity-based prediction probabilities. In this way, TFUP achieves surprising performance, even surpassing the training-base method on multiple classification datasets. Based on our TFUP, we propose a training-based approach (TFUP-T) to further boost the adaptation performance. In addition to the standard cross-entropy loss, TFUP-T adopts an additional marginal distribution entropy loss to constrain the model from a global perspective. Our TFUP-T achieves new state-of-the-art classification performance compared to unsupervised and few-shot adaptation approaches on multiple benchmarks. In particular, TFUP-T improves the classification accuracy of POUF by 3.3% on the most challenging Domain-Net dataset.",
    "pdf_link": "https://arxiv.org/abs/2404.16339",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16339v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16339/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16339v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16339/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16339v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16339/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16339v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16339/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16339v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16339/x5.png"
      }
    ],
    "abstract_cn": "提示学习已经发展成为调整大型预训练视觉-语言模型以适应下游任务的最有效方法。最近，像UPL和POUF这样的无监督提示调整技术，通过伪标签提供监督信息，直接对未标记数据进行微调。但是，伪标签的不准确性往往会误导微调过程，影响模型的表示能力。为此，我们提出了一种无需训练的无监督提示方法（TFUP），它在无需训练和标注的情况下，最大限度地保持并增强了模型的固有表示能力，通过残差连接与基于相似性的预测概率相结合。具体而言，我们融合了实例置信度和原型分数来挑选具有代表性的样本，并利用这些样本定制了一个可靠的特征缓存模型（FCM），以便进行无需训练的推理。此外，我们构建了一个多级相似性度量（MSM），它同时考虑了特征层面和语义层面的相似性，以计算测试图像与缓存样本之间的距离，并将此距离作为相应缓存标签权重，用以生成基于相似性的预测概率。这种方法使得TFUP在多个分类数据集上取得了惊人的成绩，甚至超过了基于训练的方法。基于TFUP，我们还提出了一种基于训练的方法（TFUP-T），以进一步提升模型的适应性能。TFUP-T不仅采用了标准的交叉熵损失，还引入了边际分布熵损失，从宏观角度对模型进行约束。在多个基准测试中，TFUP-T与无监督和少样本适应方法相比，展现出了新的最高水平的分类性能。特别是在最具挑战性的Domain-Net数据集上，TFUP-T将POUF的分类准确率提升了3.3%。",
    "title_cn": "免训练的无监督提示技术，为视觉-语言模型注入新动力。",
    "tags": [
      "LLM应用",
      "计算机视觉",
      "机器学习"
    ]
  },
  {
    "title": "AI Coders Are Among Us: Rethinking Programming Language Grammar Towards Efficient Code Generation",
    "submit_datetime": "2024年04月25日",
    "abstract": "Besides humans and machines, Artificial Intelligence (AI) models have emerged to be another important audience of programming languages, as we come to the era of large language models (LLMs). LLMs can now excel at coding competitions and even program like developers to address various tasks, such as math calculation. Yet, the grammar and layout of existing programs are designed for humans. Particularly, abundant grammar tokens and formatting tokens are included to make the code more readable to humans. While beneficial, such a human-centric design imposes an unnecessary computational burden on LLMs where each token, either consumed or generated, consumes computational resources. To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar, which aims to represent the code in a way that better suits the working mechanism of AI models. Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively. To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named Simple Python (SimPy). SimPy is crafted by revising the original Python grammar through a series of heuristic rules. Programs written in SimPy maintain identical Abstract Syntax Tree (AST) structures to those in standard Python, allowing execution via a modified AST parser. In addition, we explore methods to enable existing LLMs to proficiently understand and use SimPy, and ensure the changes remain imperceptible for human developers. Compared with the original Python, SimPy not only reduces token usage by 13.5% and 10.4% for CodeLlama and GPT-4, but can also achieve equivalent, even improved, performance over the models trained on Python code.",
    "pdf_link": "https://arxiv.org/abs/2404.16333",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16333v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16333/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16333v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16333/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16333v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16333/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16333v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16333/x4.png"
      }
    ],
    "abstract_cn": "随着大型语言模型（LLM）时代的到来，人工智能（AI）模型已成为编程语言的新兴重要受众。如今，LLM不仅在编程竞赛中大放异彩，还能像专业开发者一样编写代码，处理包括数学计算在内的多种任务。但现有的程序设计以人为中心，包含了大量提高代码可读性的语法和格式标记，这给LLM带来了额外的计算负担。为了提升推理效率和降低成本，我们提出了一种面向AI的语法概念，旨在以更适合AI模型工作方式的形式来表示代码。采用这种语法的代码去除了不必要的格式，以最精简的标记数量有效传递代码含义。为了验证这一概念，我们探索并实现了首个面向AI的Python语法——Simple Python（SimPy）。SimPy通过一系列启发式规则对传统Python语法进行了优化，保持了与标准Python相同的抽象语法树（AST）结构，并通过改进的AST解析器执行。我们还研究了如何使现有的LLM熟练掌握并使用SimPy，同时确保这些改变对人类开发者无影响。相较于传统Python，SimPy在CodeLlama和GPT-4上的标记使用量分别减少了13.5%和10.4%，并且能够在训练有素的Python代码模型上达到甚至超越原有性能。",
    "title_cn": "AI 程序员悄然崛起：探索编程语言的语法革新，以提升代码生成的效率。",
    "tags": [
      "LLM应用",
      "",
      "人工智能"
    ]
  },
  {
    "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
    "submit_datetime": "2024年04月25日",
    "abstract": "Retrieval-Augmented Generation (RAG) has shown significant improvements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, RAG introduces long sequence generation and leads to high computation and memory costs. We propose RAGCache, a novel multilevel dynamic caching system tailored for RAG. Our analysis benchmarks current RAG systems, pinpointing the performance bottleneck (i.e., long sequence due to knowledge injection) and optimization opportunities (i.e., caching knowledge's intermediate states). Based on these insights, we design RAGCache, which organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. RAGCache proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns. It also dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. We implement RAGCache and evaluate it on vLLM, a state-of-the-art LLM inference system and Faiss, a state-of-the-art vector database. The experimental results show that RAGCache reduces the time to first token (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to vLLM integrated with Faiss.",
    "pdf_link": "https://arxiv.org/abs/2404.12457",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x19.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12457v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12457/x20.png"
      }
    ],
    "abstract_cn": "检索增强生成（RAG）融合了大型语言模型（LLM）与外部知识库的优势，在自然语言处理任务上取得了显著进步。但这种方法也带来了长序列生成的问题，增加了计算和内存的开销。为此，我们设计了RAGCache，这是一个创新的多层次动态缓存系统，专为优化RAG性能而开发。通过对现有RAG系统的深入分析，我们识别出了影响性能的关键瓶颈——知识注入导致的序列过长，以及潜在的优化点——缓存知识的中间状态。RAGCache通过知识树结构来组织和管理检索到的知识的中间状态，并在GPU和主机内存中进行缓存。它还引入了一种智能替换策略，能够根据LLM的推理特性和RAG的检索模式进行调整。此外，RAGCache能够动态地并行处理检索和推理过程，有效减少了整体的延迟。我们在顶尖的LLM推理系统vLLM和向量数据库Faiss上部署并测试了RAGCache，实验数据显示，其将首次令牌生成时间缩短了至多4倍，吞吐量提升了高达2.1倍。",
    "title_cn": "RAGCache：一种高效的知识缓存机制，专为检索增强生成而设计。",
    "tags": [
      "分类：RAG",
      "",
      "缓存系统"
    ]
  },
  {
    "title": "OneChart: Purify the Chart Structural Extraction via One Auxiliary Token",
    "submit_datetime": "2024年04月25日",
    "abstract": "Chart parsing poses a significant challenge due to the diversity of styles, values, texts, and so forth. Even advanced large vision-language models (LVLMs) with billions of parameters struggle to handle such tasks satisfactorily. To address this, we propose OneChart: a reliable agent specifically devised for the structural extraction of chart information. Similar to popular LVLMs, OneChart incorporates an autoregressive main body. Uniquely, to enhance the reliability of the numerical parts of the output, we introduce an auxiliary token placed at the beginning of the total tokens along with an additional decoder. The numerically optimized (auxiliary) token allows subsequent tokens for chart parsing to capture enhanced numerical features through causal attention. Furthermore, with the aid of the auxiliary token, we have devised a self-evaluation mechanism that enables the model to gauge the reliability of its chart parsing results by providing confidence scores for the generated content. Compared to current state-of-the-art (SOTA) chart parsing models, e.g., DePlot, ChartVLM, ChartAst, OneChart significantly outperforms in Average Precision (AP) for chart structural extraction across multiple public benchmarks, despite enjoying only 0.2 billion parameters. Moreover, as a chart parsing agent, it also brings 10%+ accuracy gains for the popular LVLM (LLaVA-1.6) in the downstream ChartQA benchmark.",
    "pdf_link": "https://arxiv.org/abs/2404.09987",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.09987v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09987/chart_6_v2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09987v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09987/chart_3_v3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09987v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09987/chart_2_v5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09987v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09987/chart-infer2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09987v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09987/append1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09987v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09987/append2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09987v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09987/append4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09987v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09987/append3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09987v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09987/chart_qa_all.png"
      }
    ],
    "abstract_cn": "图表解析因风格迥异、数值多样、文本繁杂等因素而充满挑战，即便是参数众多的先进视觉-语言大型模型（LVLMs）也常感力不从心。为此，我们设计了OneChart：一款专为图表信息的结构化抽取而打造的可信代理。OneChart沿用了主流LVLMs的自回归主体架构，并创新性地在总令牌序列的起始位置引入辅助令牌及附加解码器，以提升输出中数值部分的准确性。这一数值优化的辅助令牌，使得后续令牌在解析图表时能够通过因果注意力机制捕捉更为精细的数值特征。我们还引入了一种自我评估机制，使模型能够通过生成内容的置信度评分来评估其图表解析的可靠性。在与当前顶尖的图表解析模型如DePlot、ChartVLM、ChartAst等的比较中，OneChart在多个公共基准测试中的平均精度（AP）上取得了显著的领先优势，尽管其参数量仅20亿。此外，作为图表解析代理，OneChart还显著提升了流行LVLM（如LLaVA-1.6）在ChartQA基准测试中的准确率，增幅超过10%。",
    "title_cn": "OneChart：引入单一辅助标记，精炼图表结构的提取过程",
    "tags": [
      "Agent",
      "图表解析",
      ""
    ]
  },
  {
    "title": "What Makes Multimodal In-Context Learning Work?",
    "submit_datetime": "2024年04月25日",
    "abstract": "Large Language Models have demonstrated remarkable performance across various tasks, exhibiting the capacity to swiftly acquire new skills, such as through In-Context Learning (ICL) with minimal demonstration examples. In this work, we present a comprehensive framework for investigating Multimodal ICL (M-ICL) in the context of Large Multimodal Models. We consider the best open-source multimodal models (e.g., IDEFICS, OpenFlamingo) and a wide range of multimodal tasks. Our study unveils several noteworthy findings: (1) M-ICL primarily relies on text-driven mechanisms, showing little to no influence from the image modality. (2) When used with advanced-ICL strategy (like RICES), M-ICL is not better than a simple strategy based on majority voting over context examples. Moreover, we identify several biases and limitations of M-ICL that warrant consideration prior to deployment. Code available at https://gitlab.com/folbaeni/multimodal-icl",
    "pdf_link": "https://arxiv.org/abs/2404.15736",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/image.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/remove_image_radar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/remove_modality_normalized_full.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/remove_question_radar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/ngrams.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/rices_diff_bar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/rices_no_modality_radar_image.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/rices_vqa_bar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/rices_oracle_radar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/coco_similarity_rouge.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/similarity_vqa_final.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/coco_vqa_repetition_avanced.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/remove_modality_full.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/rices_key_full.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/rices_reverse_full.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15736/rices_no_image_full.png"
      }
    ],
    "abstract_cn": "大型语言模型在多样化任务上展现了非凡的能力，能够通过极少量的示例迅速掌握新技能，这主要得益于上下文学习（ICL）。本研究构建了一个深入的框架，旨在探讨大型多模态模型中的多模态ICL（M-ICL）。我们评估了顶尖的开源多模态模型（如IDEFICS、OpenFlamingo）在一系列多模态任务上的表现。研究发现，M-ICL主要依赖文本驱动机制，图像模态的影响微乎其微。此外，采用高级ICL策略（例如RICES）时，M-ICL的性能并不超越简单的多数投票策略。我们还指出了M-ICL存在的一些偏见和限制，这些在实际应用前需要被充分考虑。相关代码已在 https://gitlab.com/folbaeni/multimodal-icl 上公开。",
    "title_cn": "多模态上下文内学习为何有效？",
    "tags": [
      "LLM应用",
      "多模态学习",
      "人工智能"
    ]
  },
  {
    "title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
    "submit_datetime": "2024年04月25日",
    "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether smaller-size (<= 13B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs. We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities. First, we leverage correct solutions to guide the model in critiquing their incorrect responses. Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement. Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.",
    "pdf_link": "https://arxiv.org/abs/2404.17140",
    "graphs": [],
    "abstract_cn": "自我纠错技术正成为提升大型语言模型推理能力的有效途径，它通过自我批评来精确识别并修正错误。本研究旨在探究小型语言模型（规模不超过13B）在几乎不依赖更大型语言模型输入的情况下，是否具备在推理任务上的自我纠错功能。我们设计了一种创新的流程，激励这些小型模型收集自我纠错数据，以培养其自我完善的能力。首先，我们使用正确答案引导模型自我批评错误回答；其次，经过筛选的批评意见被用于对自我纠错推理器进行有监督的微调，以细化解决方案。实验结果表明，在涵盖数学和常识推理的五个数据集中，两种模型的自我纠错能力均有所增强，尤其是在与基于GPT-4的强大验证器结合使用时，性能提升尤为明显。然而，当使用较弱的自我验证器来决定何时进行纠错时，也暴露出了一些限制。",
    "title_cn": "为了自我修正推理过程，小型语言模型亟需配备强有力的验证机制。",
    "tags": [
      "分类：LLM应用\n\n这篇论文主要探讨了如何提升小型语言模型在推理任务上的自我纠错功能，通过设计一种创新的流程来培养模型的自我完善能力。这属于LLM应用的范畴，因为它涉及到如何更有效地利用现有的语言模型来提高其性能。",
      "人工智能",
      ""
    ]
  },
  {
    "title": "Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study",
    "submit_datetime": "2024年04月25日",
    "abstract": "The Natural Language to Visualization (NL2Vis) task aims to transform natural-language descriptions into visual representations for a grounded table, enabling users to gain insights from vast amounts of data. Recently, many deep learning-based approaches have been developed for NL2Vis. Despite the considerable efforts made by these approaches, challenges persist in visualizing data sourced from unseen databases or spanning multiple tables. Taking inspiration from the remarkable generation capabilities of Large Language Models (LLMs), this paper conducts an empirical study to evaluate their potential in generating visualizations, and explore the effectiveness of in-context learning prompts for enhancing this task. In particular, we first explore the ways of transforming structured tabular data into sequential text prompts, as to feed them into LLMs and analyze which table content contributes most to the NL2Vis. Our findings suggest that transforming structured tabular data into programs is effective, and it is essential to consider the table schema when formulating prompts. Furthermore, we evaluate two types of LLMs: finetuned models (e.g., T5-Small) and inference-only models (e.g., GPT-3.5), against state-of-the-art methods, using the NL2Vis benchmarks (i.e., nvBench). The experimental results reveal that LLMs outperform baselines, with inference-only models consistently exhibiting performance improvements, at times even surpassing fine-tuned models when provided with certain few-shot demonstrations through in-context learning. Finally, we analyze when the LLMs fail in NL2Vis, and propose to iteratively update the results using strategies such as chain-of-thought, role-playing, and code-interpreter. The experimental results confirm the efficacy of iterative updates and hold great potential for future study.",
    "pdf_link": "https://arxiv.org/abs/2404.17136",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x19.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x20.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17136v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17136/x21.png"
      }
    ],
    "abstract_cn": "自然语言到可视化（NL2Vis）任务致力于将自然语言描述转换成可视化的表格数据，以便用户能从海量数据中洞察信息。尽管基于深度学习的多种方法已应运而生，但在将未见数据库或跨多表的数据可视化时，仍面临挑战。本文借鉴大型语言模型（LLMs）的卓越生成能力，通过实证研究来评估其在创建可视化方面的能力，并探讨使用上下文学习提示来提升任务效果的可能性。我们首先研究了如何将结构化表格数据转化为连续文本提示，以便输入LLMs，并分析了哪些表格内容对NL2Vis最为关键。研究结果显示，将表格数据转化为程序文本是有效的，同时在构建提示时考虑表格结构是必要的。我们还对比了两种LLMs：经过微调的模型（如T5-Small）和仅限推理的模型（如GPT-3.5），并与现有最先进方法进行了比较。实验结果显示，LLMs在性能上超越了基线，尤其是仅限推理模型在上下文学习中通过少量示例的引导，有时甚至能超越微调模型。最后，我们深入分析了LLMs在NL2Vis中的不足，并提出了通过链式思考、角色扮演和代码解释等策略来迭代优化结果。实验结果证明了迭代更新策略的有效性，并为未来的研究开辟了新的可能性。",
    "title_cn": "本研究探索了利用大型语言模型将自然语言自动转换为数据可视化的过程。",
    "tags": [
      "LLM应用",
      "数据可视化",
      ""
    ]
  },
  {
    "title": "2M-NER: Contrastive Learning for Multilingual and Multimodal NER with Language and Modal Fusion",
    "submit_datetime": "2024年04月25日",
    "abstract": "Named entity recognition (NER) is a fundamental task in natural language processing that involves identifying and classifying entities in sentences into pre-defined types. It plays a crucial role in various research fields, including entity linking, question answering, and online product recommendation. Recent studies have shown that incorporating multilingual and multimodal datasets can enhance the effectiveness of NER. This is due to language transfer learning and the presence of shared implicit features across different modalities. However, the lack of a dataset that combines multilingualism and multimodality has hindered research exploring the combination of these two aspects, as multimodality can help NER in multiple languages simultaneously. In this paper, we aim to address a more challenging task: multilingual and multimodal named entity recognition (MMNER), considering its potential value and influence. Specifically, we construct a large-scale MMNER dataset with four languages (English, French, German and Spanish) and two modalities (text and image). To tackle this challenging MMNER task on the dataset, we introduce a new model called 2M-NER, which aligns the text and image representations using contrastive learning and integrates a multimodal collaboration module to effectively depict the interactions between the two modalities. Extensive experimental results demonstrate that our model achieves the highest F1 score in multilingual and multimodal NER tasks compared to some comparative and representative baselines. Additionally, in a challenging analysis, we discovered that sentence-level alignment interferes a lot with NER models, indicating the higher level of difficulty in our dataset.",
    "pdf_link": "https://arxiv.org/abs/2404.17122",
    "graphs": [],
    "abstract_cn": "命名实体识别（NER）是自然语言处理领域的基石，其核心在于识别并归类句子中的实体。它对于实体链接、问答系统和在线商品推荐等多个研究领域至关重要。最新研究揭示，融合多语言和多模态数据集能够显著提升NER的性能，这归功于语言迁移学习以及不同模态间共有的隐含特征。然而，由于缺少一个同时包含多语言和多模态特性的数据集，限制了对这两个要素结合的研究。本文旨在攻克更具挑战性的多语言多模态命名实体识别（MMNER）任务，探讨其潜在的研究价值和影响力。我们构建了一个包含四种语言（英、法、德、西）和两种模态（文本和图像）的大规模MMNER数据集。为解决这一挑战，我们提出了一个创新模型——2M-NER，该模型采用对比学习方法对齐文本与图像表示，并集成了一个多模态协作模块，以有效捕捉两种模态间的交互作用。广泛的实验结果显示，2M-NER在多语言多模态NER任务中取得了最高的F1分数，超越了其他基准模型。此外，我们通过深入分析发现，句子级别的对齐对NER模型构成了较大干扰，反映出我们数据集的高难度特性。",
    "title_cn": "2M-NER：一种采用对比学习方法，针对多语言和多模态命名实体识别任务，实现语言与模态融合的技术。",
    "tags": [
      "Agent",
      "",
      "多模态学习"
    ]
  },
  {
    "title": "Talking Nonsense: Probing Large Language Models' Understanding of Adversarial Gibberish Inputs",
    "submit_datetime": "2024年04月25日",
    "abstract": "Large language models (LLMs) exhibit excellent ability to understand human languages, but do they also understand their own language that appears gibberish to us? In this work we delve into this question, aiming to uncover the mechanisms underlying such behavior in LLMs. We employ the Greedy Coordinate Gradient optimizer to craft prompts that compel LLMs to generate coherent responses from seemingly nonsensical inputs. We call these inputs LM Babel and this work systematically studies the behavior of LLMs manipulated by these prompts. We find that the manipulation efficiency depends on the target text's length and perplexity, with the Babel prompts often located in lower loss minima compared to natural prompts. We further examine the structure of the Babel prompts and evaluate their robustness. Notably, we find that guiding the model to generate harmful texts is not more difficult than into generating benign texts, suggesting lack of alignment for out-of-distribution prompts.",
    "pdf_link": "https://arxiv.org/abs/2404.17120",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/diagram.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/length.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/perplexity.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/umap.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/intersecting_tokens.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/vicuna_word_frequency.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/robustness.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/umap_llama13b.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/umap_vicuna.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/umap_vicuna13b.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/robustness13b.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/llama_word_frequency.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/llama13b_word_frequency.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/vicuna13_word_frequency.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/umap_autoprompt.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/similarity_hist_llama.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17120v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17120/similarity_hist_vicuna.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）在理解人类语言方面表现出色，但它们是否能理解那些对我们而言毫无意义的自身语言呢？本研究旨在探究这一谜题，揭示LLMs这种行为背后的原理。我们利用贪心坐标梯度优化器精心构造提示，激发LLMs从看似荒谬的输入中产出有意义的回答。我们将这类输入命名为“语言模型巴别塔”（LM Babel），并系统性地分析了LLMs在这些提示操控下的行为模式。研究发现，操控的效率与目标文本的长度和困惑度密切相关，巴别塔提示往往处于比自然提示更低的损失最小值区域。我们还深入分析了巴别塔提示的结构，并对其鲁棒性进行了评估。尤为引人注意的是，我们发现引导模型产出有害文本的难度并不大于产出良性文本，这暗示了对于非典型提示，模型的对齐度存在缺失。",
    "title_cn": "探究谬误：深入理解大型语言模型如何处理对抗性的无意义输入",
    "tags": [
      "分类：LLM理论",
      "",
      "人工智能"
    ]
  },
  {
    "title": "Software Vulnerability Prediction in Low-Resource Languages: An Empirical Study of CodeBERT and ChatGPT",
    "submit_datetime": "2024年04月25日",
    "abstract": "Background: Software Vulnerability (SV) prediction in emerging languages is increasingly important to ensure software security in modern systems. However, these languages usually have limited SV data for developing high-performing prediction models. Aims: We conduct an empirical study to evaluate the impact of SV data scarcity in emerging languages on the state-of-the-art SV prediction model and investigate potential solutions to enhance the performance. Method: We train and test the state-of-the-art model based on CodeBERT with and without data sampling techniques for function-level and line-level SV prediction in three low-resource languages - Kotlin, Swift, and Rust. We also assess the effectiveness of ChatGPT for low-resource SV prediction given its recent success in other domains. Results: Compared to the original work in C/C++ with large data, CodeBERT's performance of function-level and line-level SV prediction significantly declines in low-resource languages, signifying the negative impact of data scarcity. Regarding remediation, data sampling techniques fail to improve CodeBERT; whereas, ChatGPT showcases promising results, substantially enhancing predictive performance by up to 34.4% for the function level and up to 53.5% for the line level. Conclusion: We have highlighted the challenge and made the first promising step for low-resource SV prediction, paving the way for future research in this direction.",
    "pdf_link": "https://arxiv.org/abs/2404.17110",
    "graphs": [],
    "abstract_cn": "研究背景：随着软件安全的重要性日益凸显，对新兴编程语言中的软件漏洞预测变得尤为关键。但这些新语言往往缺乏足够的漏洞数据，制约了高效预测模型的开发。研究目的：本研究旨在实证评估新兴语言中漏洞数据不足对顶尖漏洞预测模型的影响，并探索提升模型性能的可能方法。研究方法：我们选取了Kotlin、Swift和Rust三种资源稀缺语言，采用CodeBERT模型进行了函数级和代码行级的漏洞预测训练与测试，同时对比了有无数据采样技术的应用效果。此外，我们还检验了ChatGPT在低资源漏洞预测中的潜力，考虑到其在其他领域的显著成效。研究结果：与C/C++等资源丰富语言相比，CodeBERT在资源稀缺语言中的预测性能大幅下降，凸显了数据不足的挑战。尽管数据采样技术未能有效提升CodeBERT的性能，但ChatGPT却展现出显著的潜力，将预测准确率提升了34.4%（函数级）和53.5%（代码行级）。研究结论：本研究不仅揭示了低资源语言漏洞预测的难题，也为未来的研究方向指明了有希望的第一步。",
    "title_cn": "面向低资源语言的软件漏洞预测：CodeBERT 与 ChatGPT 的实证比较研究",
    "tags": [
      "分类：LLM应用\n\n这篇论文主要探讨了在新兴编程语言中，由于漏洞数据不足，如何利用大型语言模型（如CodeBERT和ChatGPT）来提高漏洞预测的性能。这属于LLM应用的范畴，因为它直接将大型语言模型应用于解决实际问题，即软件安全中的漏洞预测。",
      "软件安全",
      "编程语言"
    ]
  },
  {
    "title": "Open-Set Video-based Facial Expression Recognition with Human Expression-sensitive Prompting",
    "submit_datetime": "2024年04月25日",
    "abstract": "In Video-based Facial Expression Recognition (V-FER), models are typically trained on closed-set datasets with a fixed number of known classes. However, these V-FER models cannot deal with unknown classes that are prevalent in real-world scenarios. In this paper, we introduce a challenging Open-set Video-based Facial Expression Recognition (OV-FER) task, aiming at identifying not only known classes but also new, unknown human facial expressions not encountered during training. While existing approaches address open-set recognition by leveraging large-scale vision-language models like CLIP to identify unseen classes, we argue that these methods may not adequately capture the nuanced and subtle human expression patterns required by the OV-FER task. To address this limitation, we propose a novel Human Expression-Sensitive Prompting (HESP) mechanism to significantly enhance CLIP's ability to model video-based facial expression details effectively, thereby presenting a new CLIP-based OV-FER approach. Our proposed HESP comprises three components: 1) a textual prompting module with learnable prompt representations to complement the original CLIP textual prompts and enhance the textual representations of both known and unknown emotions, 2) a visual prompting module that encodes temporal emotional information from video frames using expression-sensitive attention, equipping CLIP with a new visual modeling ability to extract emotion-rich information, 3) a delicately designed open-set multi-task learning scheme that facilitates prompt learning and encourages interactions between the textual and visual prompting modules. Extensive experiments conducted on four OV-FER task settings demonstrate that HESP can significantly boost CLIP's performance (a relative improvement of 17.93% on AUROC and 106.18% on OSCR) and outperform other state-of-the-art open-set video understanding methods by a large margin.",
    "pdf_link": "https://arxiv.org/abs/2404.17100",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17100v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17100/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17100v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17100/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17100v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17100/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17100v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17100/predict_result.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17100v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17100/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17100v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17100/x5.png"
      }
    ],
    "abstract_cn": "在视频驱动的面部表情识别（V-FER）领域，模型多在有限类别的封闭数据集上训练，却难以应对现实世界中常见的未知类别。本文提出了一项前沿的开放集视频面部表情识别（OV-FER）任务，目标是识别出训练时未见过的新兴及未知的人脸表情。尽管现有技术借助如CLIP这样的大规模视觉-语言模型来识别新类别，但这些方法在捕捉OV-FER任务所需的精细人类表情模式方面仍有不足。为克服此局限，我们设计了创新的人类表情敏感提示（HESP）机制，显著提升了CLIP对视频面部表情细节的建模能力，进而推出了一种创新的基于CLIP的OV-FER解决方案。HESP机制包含三个关键部分：首先，文本提示模块通过可学习的提示表示来强化原有CLIP的文本提示，提升已知与未知情感的文本表达；其次，视觉提示模块利用表情敏感的注意力机制从视频帧中编码时间序列情感信息，赋予CLIP更强大的视觉建模能力；最后，一个精巧设计的开放集多任务学习框架，促进了提示学习并加强了文本与视觉模块间的互动。在四项OV-FER任务设置上的广泛实验证明，HESP显著提升了CLIP的性能（AUROC提升17.93%，OSCR提升106.18%），并在其他尖端开放集视频理解方法中脱颖而出。",
    "title_cn": "视频驱动的开放集人脸识别技术，引入了对人类表情变化敏感的提示机制。",
    "tags": [
      "分类：Agent\n\n这篇论文提出了一个创新的解决方案，通过设计人类表情敏感提示（HESP）机制来提高大规模视觉-语言模型CLIP在视频驱动的面部表情识别（V-FER）任务中的性能。这个解决方案包括文本提示模块、视觉提示模块和一个开放集多任务学习框架，这些组件共同工作以提高对视频面部表情细节的建模能力。由于这个研究涉及到设计和实现一个智能代理（即HESP机制），以提高模型在特定任务上的性能，因此这篇论文可以归类为Agent。",
      "面部识别",
      "情感识别"
    ]
  },
  {
    "title": "CyNetDiff -- A Python Library for Accelerated Implementation of Network Diffusion Models",
    "submit_datetime": "2024年04月25日",
    "abstract": "In recent years, there has been increasing interest in network diffusion models and related problems. The most popular of these are the independent cascade and linear threshold models. Much of the recent experimental work done on these models requires a large number of simulations conducted on large graphs, a computationally expensive task suited for low-level languages. However, many researchers prefer the use of higher-level languages (such as Python) for their flexibility and shorter development times. Moreover, in many research tasks, these simulations are the most computationally intensive task, so it would be desirable to have a library for these with an interface to a high-level language with the performance of a low-level language. To fill this niche, we introduce CyNetDiff, a Python library with components written in Cython to provide improved performance for these computationally intensive diffusion tasks.",
    "pdf_link": "https://arxiv.org/abs/2404.17059",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17059v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17059/simple_benchmark_screenshot.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.17059v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17059/simple_benchmark_screenshot_real.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.17059v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17059/x1.png"
      }
    ],
    "abstract_cn": "近年来，网络扩散模型及其相关议题日益受到广泛关注，尤其是独立级联和线性阈值模型。当前的实验研究往往需要在大型图集上进行大量的模拟，这一过程计算成本高昂，更适合使用低级语言。尽管如此，研究者们更倾向于使用高级语言如 Python，以享受其带来的灵活性和快速开发的优势。在众多研究任务中，模拟过程往往是最为消耗计算资源的环节，因此，能够提供一个既兼容高级语言接口又具备低级语言性能的库将大有裨益。正是为了满足这一需求，我们推出了 CyNetDiff——一个用 Cython 编写关键组件的 Python 库，旨在为这些计算密集型的扩散任务提供性能上的显著提升。",
    "title_cn": "CyNetDiff —— 一款旨在提高网络扩散模型实现效率的 Python 编程库。",
    "tags": [
      "分类：RAG",
      "网络科学",
      ""
    ]
  },
  {
    "title": "Near to Mid-term Risks and Opportunities of Open Source Generative AI",
    "submit_datetime": "2024年04月25日",
    "abstract": "In the next few years, applications of Generative AI are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about potential risks and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. This regulation is likely to put at risk the budding field of open source Generative AI. We argue for the responsible open sourcing of generative AI models in the near and medium term. To set the stage, we first introduce an AI openness taxonomy system and apply it to 40 current large language models. We then outline differential benefits and risks of open versus closed source AI and present potential risk mitigation, ranging from best practices to calls for technical and scientific contributions. We hope that this report will add a much needed missing voice to the current public discourse on near to mid-term AI safety and other societal impact.",
    "pdf_link": "https://arxiv.org/abs/2404.17047",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17047v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17047/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17047v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17047/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17047v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17047/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17047v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17047/x4.png"
      }
    ],
    "abstract_cn": "未来数年，生成性AI的应用有望在科学、医学、教育等多个领域引发变革。这些可能的变革激起了对潜在风险的广泛讨论，并促使一些在AI领域领先的大型科技公司呼吁加强监管。这种监管可能会对开源生成性AI的萌芽领域构成威胁。我们提倡在短期内以负责任的态度开放生成性AI模型的源代码。为此，我们首先提出了一个AI开放性分类体系，并将其应用于40种现行的大型语言模型。接着，我们分析了开源与闭源AI的利弊，并提出了风险缓解策略，从最佳实践到技术与科学贡献的呼吁。我们期望这份报告能为当前关于AI安全与社会影响的公共讨论贡献一个迫切需要的声音。",
    "title_cn": "开源生成式AI的近期至中期风险与机遇探讨",
    "tags": [
      "LLM应用",
      "AI安全",
      "开源软件"
    ]
  },
  {
    "title": "Player-Driven Emergence in LLM-Driven Game Narrative",
    "submit_datetime": "2024年04月25日",
    "abstract": "We explore how interaction with large language models (LLMs) can give rise to emergent behaviors, empowering players to participate in the evolution of game narratives. Our testbed is a text-adventure game in which players attempt to solve a mystery under a fixed narrative premise, but can freely interact with non-player characters generated by GPT-4, a large language model. We recruit 28 gamers to play the game and use GPT-4 to automatically convert the game logs into a node-graph representing the narrative in the player's gameplay. We find that through their interactions with the non-deterministic behavior of the LLM, players are able to discover interesting new emergent nodes that were not a part of the original narrative but have potential for being fun and engaging. Players that created the most emergent nodes tended to be those that often enjoy games that facilitate discovery, exploration and experimentation.",
    "pdf_link": "https://arxiv.org/abs/2404.17027",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17027v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17027/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17027v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17027/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17027v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17027/narrative-emergence.png"
      }
    ],
    "abstract_cn": "本研究深入探讨了与大型语言模型（LLM）互动如何催生新行为，让玩家在游戏叙事的演进中扮演角色。实验平台是一个文本冒险游戏，玩家需在既定叙事框架下解开谜题，同时能与GPT-4生成的非玩家角色自由互动。我们邀请了28位玩家参与游戏，利用GPT-4将游戏记录自动转化为节点图，以此映射玩家游戏过程中的叙事结构。研究发现，玩家与LLM的非确定性互动促使他们发掘出原本叙事之外的新颖节点，这些节点虽非初衷，却同样引人入胜。那些创造出最多新颖节点的玩家，往往是热衷于探索、发现和实验性游戏的爱好者。",
    "title_cn": "在大型语言模型（LLM）驱动的游戏叙事中，玩家的主动参与催生了新的发展。",
    "tags": [
      "分类：LLM应用",
      "游戏开发",
      "人工智能"
    ]
  },
  {
    "title": "How Does Conversation Length Impact User's Satisfaction? A Case Study of Length-Controlled Conversations with LLM-Powered Chatbots",
    "submit_datetime": "2024年04月25日",
    "abstract": "Users can discuss a wide range of topics with large language models (LLMs), but they do not always prefer solving problems or getting information through lengthy conversations. This raises an intriguing HCI question: How does instructing LLMs to engage in longer or shorter conversations affect conversation quality? In this paper, we developed two Slack chatbots using GPT-4 with the ability to vary conversation lengths and conducted a user study. Participants asked the chatbots both highly and less conversable questions, engaging in dialogues with 0, 3, 5, and 7 conversational turns. We found that the conversation quality does not differ drastically across different conditions, while participants had mixed reactions. Our study demonstrates LLMs' ability to change conversation length and the potential benefits for users resulting from such changes, but we caution that changes in text form may not necessarily imply changes in quality or content.",
    "pdf_link": "https://arxiv.org/abs/2404.17025",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/Should-Ask-More.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/Enough-Count.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/Helpfulness.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/Satisfaction.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/mturk_helpfulness.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/mturk_quantity.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/mturk_relevance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/mturk_repetitiveness.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/mturk_clarity.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/mturk_ambiguity.png"
      },
      {
        "url": "https://arxiv.org/html/2404.17025v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17025/slackbot_mturk_interface_0.png"
      }
    ],
    "abstract_cn": "用户与大型语言模型（LLMs）的交流话题广泛，但他们有时更倾向于简洁的对话而非冗长的问答。这引出了一个引人入胜的人机交互（HCI）问题：我们如何通过调整LLMs的对话长度来优化对话质量？本研究中，我们利用GPT-4构建了两个Slack聊天机器人，它们能够灵活调整对话的长短，并进行了一项用户调研。参与者向机器人提出了从高度互动到不太适合深入讨论的问题，并与机器人进行了0至7轮的对话。研究发现，不同对话长度对质量的影响并不显著，而用户对此有着不同的反馈。这项研究揭示了LLMs在调整对话长度方面的潜力，以及这种调整可能给用户带来的益处，但也指出了文本形式的变动并不总等同于质量或内容的提升。",
    "title_cn": "对话的长短如何影响用户的满意度？本研究通过案例分析，探讨了由大型语言模型（LLM）支持的聊天机器人在进行长度受控对话时的用户满意度。",
    "tags": [
      "分类：LLM应用",
      "人机交互",
      "聊天机器人"
    ]
  },
  {
    "title": "AutoGenesisAgent: Self-Generating Multi-Agent Systems for Complex Tasks",
    "submit_datetime": "2024年04月25日",
    "abstract": "The proliferation of large language models (LLMs) and their integration into multi-agent systems has paved the way for sophisticated automation in various domains. This paper introduces AutoGenesisAgent, a multi-agent system that autonomously designs and deploys other multi-agent systems tailored for specific tasks. AutoGenesisAgent comprises several specialized agents including System Understanding, System Design, Agent Generator, and several others that collectively manage the lifecycle of creating functional multi-agent systems from initial concept to deployment. Each agent in AutoGenesisAgent has distinct responsibilities ranging from interpreting input prompts to optimizing system performance, culminating, in the deployment of a ready-to-use system. This proof-of-concept study discusses the design, implementation, and lessons learned from developing AutoGenesisAgent, highlighting its capability to generate and refine multi-agent systems autonomously, thereby reducing the need for extensive human oversight in the initial stages of system design. Keywords: multi-agent systems, large language models, system design automation, agent architecture, autonomous systems, software deployment",
    "pdf_link": "https://arxiv.org/abs/2404.17017",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）的普及及其融入多智能体系统，为各行业的高级自动化开辟了新径。本文提出了AutoGenesisAgent，一个能够独立设计并定制部署特定任务多智能体系统的系统。它由多个专业智能体组成，包括系统理解、系统设计、智能体生成等，协同管理从概念构想到系统部署的全过程。每个智能体承担着从解析输入到优化性能的不同职责，最终实现系统的即时使用。本概念验证研究探讨了AutoGenesisAgent的设计和实现，以及开发过程中的心得体会，强调了其在自主生成和优化多智能体系统方面的能力，减少了系统设计初期对人工监督的依赖。关键词包括：多智能体系统、大型语言模型、系统设计自动化、智能体架构、自主系统、软件部署。",
    "title_cn": "AutoGenesisAgent：一种自生成多智能体系统，专为解决复杂任务而设计。",
    "tags": [
      "Agent",
      "自动化",
      "人工智能"
    ]
  },
  {
    "title": "Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models",
    "submit_datetime": "2024年04月25日",
    "abstract": "A backbone of knowledge graphs are their class membership relations, which assign entities to a given class. As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class. We evaluate the method using two publicly available knowledge graphs, Wikidata and CaLiGraph, and 7 large language models. Using the gpt-4-0125-preview large language model, the method's classification performance achieves a macro-averaged F1-score of 0.830 on data from Wikidata and 0.893 on data from CaLiGraph. Moreover, a manual analysis of the classification errors shows that 40.9% of errors were due to the knowledge graphs, with 16.0% due to missing relations and 24.9% due to incorrectly asserted relations. These results show how large language models can assist knowledge engineers in the process of knowledge graph refinement. The code and data are available on Github.",
    "pdf_link": "https://arxiv.org/abs/2404.17000",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.17000v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.17000/zero_shot_cot_classifier.png"
      }
    ],
    "abstract_cn": "知识图谱的核心在于其类别成员关系，这些关系负责将实体归类到特定类别。在知识工程的框架内，我们提出了一种创新的评估方法，通过零样本思维链分类器分析特定实体和类别的描述，并利用类别的自然语言定义来衡量这些关系的优劣。该方法在 Wikidata 和 CaLiGraph 两个知名知识图谱上进行了测试，并借助 7 种大型语言模型进行了验证。特别是，使用 gpt-4-0125-preview 模型，我们在 Wikidata 上达到了 0.830 的宏观平均 F1 分数，在 CaLiGraph 上达到了 0.893。手动分析分类误差后发现，40.9% 的错误与知识图谱本身有关，其中 16.0% 是因为缺少关联，24.9% 是因为关联错误。这一发现揭示了大型语言模型在知识图谱优化过程中的辅助潜力。相关代码和数据已在 Github 上公开。",
    "title_cn": "本文探讨了如何利用大型语言模型来评估知识图谱内类别成员之间的关系。",
    "tags": [
      "LLM应用",
      "知识图谱",
      "知识工程"
    ]
  },
  {
    "title": "PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning",
    "submit_datetime": "2024年04月25日",
    "abstract": "Vision-language pre-training has significantly elevated performance across a wide range of image-language applications. Yet, the pre-training process for video-related tasks demands exceptionally large computational and data resources, which hinders the progress of video-language models. This paper investigates a straightforward, highly efficient, and resource-light approach to adapting an existing image-language pre-trained model for dense video understanding. Our preliminary experiments reveal that directly fine-tuning pre-trained image-language models with multiple frames as inputs on video datasets leads to performance saturation or even a drop. Our further investigation reveals that it is largely attributed to the bias of learned high-norm visual features. Motivated by this finding, we propose a simple but effective pooling strategy to smooth the feature distribution along the temporal dimension and thus reduce the dominant impacts from the extreme features. The new model is termed Pooling LLaVA, or \\nameofmethod{} in short. \\nameofmethod{} achieves new state-of-the-art performance on modern benchmark datasets for both video question-answer and captioning tasks. Notably, on the recent popular Video ChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average of five evaluated dimensions, exceeding the previous SOTA results from GPT4V (IG-VLM) by 9\\%. On the latest multi-choice benchmark MVBench, PLLaVA achieves 58.1\\% accuracy on average across 20 sub-tasks, 14.5\\% higher than GPT4V (IG-VLM). Code is available at \\url{https://github.com/magic-research/PLLaVA}.",
    "pdf_link": "https://arxiv.org/abs/2404.16994",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16994v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16994/x19.png"
      }
    ],
    "abstract_cn": "视觉-语言预训练技术在图像-语言应用领域取得了显著成效。但视频任务的预训练却因对计算和数据资源的巨大需求而进展缓慢。本研究提出了一种简洁高效且资源消耗低的方法，用于优化现有的图像-语言预训练模型，以实现深度视频理解。我们的初步测试发现，直接对视频数据集进行多帧输入的预训练模型微调，可能会导致性能停滞甚至下降，这主要归咎于学习到的高范数视觉特征的偏颇。基于这一发现，我们设计了一种简单而有效的池化策略，用以沿时间维度平滑特征分布，减少极端特征的影响。新模型命名为 Pooling LLaVA（简称 PLLaVA）。PLLaVA 在视频问答和字幕制作等现代基准测试中刷新了最佳性能记录。特别值得一提的是，在最新的 Video ChatGPT 基准测试中，PLLaVA 的平均得分为 3.48 分（满分 5 分），较之前的最佳成绩提升了 9%。在多选基准测试 MVBench 中，PLLaVA 的平均准确度达到 58.1%，在 20 个子任务中比 IG-VLM 高出 14.5%。相关代码已在 \\url{https://github.com/magic-research/PLLaVA} 上发布。",
    "title_cn": "PLLaVA：一种无需额外参数即可将 LLaVA 从图像扩展到视频的技术，专为视频密集字幕生成设计。",
    "tags": [
      "分类：LLM应用\n\n这篇论文的摘要描述了一种针对视频任务的预训练模型优化方法，旨在实现深度视频理解。它提出了一种新的模型 PLLaVA，该模型在视频问答和字幕制作等任务中取得了优异的性能。这篇论文主要关注于如何改进现有的图像-语言预训练模型以适应视频任务，因此它属于LLM应用类别。",
      "视频理解",
      "人工智能"
    ]
  },
  {
    "title": "Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks",
    "submit_datetime": "2024年04月25日",
    "abstract": "Benchmarks have emerged as the central approach for evaluating Large Language Models (LLMs). The research community often relies on a model's average performance across the test prompts of a benchmark to evaluate the model's performance. This is consistent with the assumption that the test prompts within a benchmark represent a random sample from a real-world distribution of interest. We note that this is generally not the case; instead, we hold that the distribution of interest varies according to the specific use case. We find that (1) the correlation in model performance across test prompts is non-random, (2) accounting for correlations across test prompts can change model rankings on major benchmarks, (3) explanatory factors for these correlations include semantic similarity and common LLM failure points.",
    "pdf_link": "https://arxiv.org/abs/2404.16966",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x19.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x20.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x21.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x22.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x23.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x24.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x25.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x26.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x27.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16966/x28.png"
      }
    ],
    "abstract_cn": "基准测试已成为衡量大型语言模型（LLM）性能的核心手段。研究界通常根据模型在基准测试中各测试点的平均表现来进行评估。这种做法基于一个假设，即认为测试点是来自现实世界中我们感兴趣的某种分布的随机抽样。然而，实际情况往往并非如此，我们认为感兴趣的分布应当根据具体应用场景而有所不同。我们的研究发现：（1）模型在不同测试点的表现之间存在非随机的相关性；（2）考虑这些相关性可能会在主要基准测试中改变模型的排名；（3）这些相关性的解释因素包括语义相似性以及LLM常见的失败点。",
    "title_cn": "探究大型语言模型评估在面对基准测试分布假设时的稳健性。",
    "tags": [
      "LLM理论",
      "人工智能",
      ""
    ]
  },
  {
    "title": "A Survey of Generative Search and Recommendation in the Era of Large Language Models",
    "submit_datetime": "2024年04月25日",
    "abstract": "With the information explosion on the Web, search and recommendation are foundational infrastructures to satisfying users' information needs. As the two sides of the same coin, both revolve around the same core research problem, matching queries with documents or users with items. In the recent few decades, search and recommendation have experienced synchronous technological paradigm shifts, including machine learning-based and deep learning-based paradigms. Recently, the superintelligent generative large language models have sparked a new paradigm in search and recommendation, i.e., generative search (retrieval) and recommendation, which aims to address the matching problem in a generative manner. In this paper, we provide a comprehensive survey of the emerging paradigm in information systems and summarize the developments in generative search and recommendation from a unified perspective. Rather than simply categorizing existing works, we abstract a unified framework for the generative paradigm and break down the existing works into different stages within this framework to highlight the strengths and weaknesses. And then, we distinguish generative search and recommendation with their unique challenges, identify open problems and future directions, and envision the next information-seeking paradigm.",
    "pdf_link": "https://arxiv.org/abs/2404.16924",
    "graphs": [],
    "abstract_cn": "网络信息的海量增长使得搜索与推荐系统成为满足用户信息需求的基石。这两者虽各有侧重，却共同面对着将查询与文档、用户与商品精准匹配的核心课题。数十年来，搜索与推荐领域经历了技术革新，包括基于机器学习及深度学习的转型。近期，超智能的生成型大型语言模型为搜索和推荐带来了新的变革，即通过创造性的方法来解决匹配问题。本文全面梳理了这一新兴范式，并从统一视角对生成型搜索和推荐的最新进展进行了总结。我们不仅对现有研究进行了分类，还提炼出一个统一框架，将现有工作分解为框架内的不同阶段，以展现各自的优势与不足。此外，我们明确了生成型搜索和推荐面临的特殊挑战，指出了存在的问题和未来的研究方向，并对未来的信息检索模式进行了展望。",
    "title_cn": "本文综述了在大型语言模型盛行的当下，生成式搜索和推荐领域的研究进展。",
    "tags": [
      "LLM应用",
      "搜索与推荐系统",
      "信息检索"
    ]
  },
  {
    "title": "A Short Survey of Human Mobility Prediction in Epidemic Modeling from Transformers to LLMs",
    "submit_datetime": "2024年04月25日",
    "abstract": "This paper provides a comprehensive survey of recent advancements in leveraging machine learning techniques, particularly Transformer models, for predicting human mobility patterns during epidemics. Understanding how people move during epidemics is essential for modeling the spread of diseases and devising effective response strategies. Forecasting population movement is crucial for informing epidemiological models and facilitating effective response planning in public health emergencies. Predicting mobility patterns can enable authorities to better anticipate the geographical and temporal spread of diseases, allocate resources more efficiently, and implement targeted interventions. We review a range of approaches utilizing both pretrained language models like BERT and Large Language Models (LLMs) tailored specifically for mobility prediction tasks. These models have demonstrated significant potential in capturing complex spatio-temporal dependencies and contextual patterns in textual data.",
    "pdf_link": "https://arxiv.org/abs/2404.16921",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16921v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16921/humanmob.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16921v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16921/x1.png"
      }
    ],
    "abstract_cn": "本文全面梳理了运用机器学习技术，尤其是 Transformer 模型，预测疫情中人类活动模式的最新进展。深入理解疫情下的人群流动对于疾病传播模型的构建和有效应对策略的制定极为关键。准确预测人群移动对于公共卫生紧急事件中的流行病学模型构建和响应计划的制定具有重要意义。通过预测活动模式，有关部门可以更准确地预测疾病的地理和时间分布，更高效地配置资源，并实施精准干预。本研究回顾了多种方法，包括使用预训练的语言模型如 BERT，以及专为预测移动性而设计的定制大型语言模型（LLMs）。这些模型在捕捉文本数据中的复杂时空关系和上下文模式方面显示出巨大潜力。",
    "title_cn": "一篇关于从变换器到大型语言模型在流行病建模中预测人类移动性的简明调查。",
    "tags": [
      "LLM应用",
      "公共卫生",
      "机器学习"
    ]
  },
  {
    "title": "Prediction Is All MoE Needs: Expert Load Distribution Goes from Fluctuating to Stabilizing",
    "submit_datetime": "2024年04月25日",
    "abstract": "MoE facilitates the development of large models by making the computational complexity of the model no longer scale linearly with increasing parameters. The learning sparse gating network selects a set of experts for each token to be processed; however, this may lead to differences in the number of tokens processed by each expert over several successive iterations, i.e., the expert load fluctuations, which reduces computational parallelization and resource utilization. To this end, we traced and analyzed loads of each expert in the training iterations for several large language models in this work, and defined the transient state with \"obvious load fluctuation\" and the stable state with \"temporal locality\". Moreover, given the characteristics of these two states and the computational overhead, we deployed three classical prediction algorithms that achieve accurate expert load prediction results. For the GPT3 350M model, the average error rates for predicting the expert load proportion over the next 1,000 and 2,000 steps are approximately 1.3% and 1.8%, respectively. This work can provide valuable guidance for expert placement or resource allocation for MoE model training. Based on this work, we will propose an expert placement scheme for transient and stable states in our coming work.",
    "pdf_link": "https://arxiv.org/abs/2404.16914",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer2-load.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer8-load.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer6-w10-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer8-w10-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer12-w10-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer6-w100-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer8-w100-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer12-w100-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer2-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer4-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer6-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer8-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer10-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer12-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer2-range.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer4-range.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer6-range.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer8-range.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer10-range.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-layer12-range.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-LSTM-1k.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-ARIMA-1k.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-125m-AVG-1k.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-LSTM-1k.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-LSTM-1k-bar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-ARIMA-1k.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-ARIMA-1k-bar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-AVG-1k.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-AVG-1k-bar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-AVG-2k.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-AVG-2k-bar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer2-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer4-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer6-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer8-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer12-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer16-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer20-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer24-variance.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer2-range.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer4-range.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer6-range.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16914v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16914/gpt3-350m-layer8-range.png"
      }
    ],
    "abstract_cn": "MoE 技术通过改变计算复杂度与参数数量的线性关系，推动了大型模型的发展。在此过程中，稀疏门控网络负责为每个待处理的标记分配专家组，但这可能导致不同专家在连续迭代中的工作量出现波动，影响计算的并行性和资源的有效利用。在本研究中，我们对多个大型语言模型的专家负载进行了跟踪分析，并区分了“显著负载波动”的瞬态状态与“时间局部性”的稳定状态。针对这两种状态及计算开销，我们采用了三种经典预测算法，对专家负载进行了准确预测。以 GPT3 350M 模型为例，预测未来 1,000 步和 2,000 步的专家负载比例，平均误差率分别约为 1.3% 和 1.8%。本研究为 MoE 模型训练中的专家布局和资源配置提供了重要参考。未来，我们将进一步提出适用于瞬态和稳定状态的专家布局方案。",
    "title_cn": "预测即足矣：MoE 中的专家负载分配趋向稳定。",
    "tags": [
      "LLM理论",
      "人工智能",
      "计算优化"
    ]
  },
  {
    "title": "Evolve Cost-aware Acquisition Functions Using Large Language Models",
    "submit_datetime": "2024年04月25日",
    "abstract": "Many real-world optimization scenarios involve expensive evaluation with unknown and heterogeneous costs. Cost-aware Bayesian optimization stands out as a prominent solution in addressing these challenges. To approach the global optimum within a limited budget in a cost-efficient manner, the design of cost-aware acquisition functions (AFs) becomes a crucial step. However, traditional manual design paradigm typically requires extensive domain knowledge and involves a labor-intensive trial-and-error process. This paper introduces EvolCAF, a novel framework that integrates large language models (LLMs) with evolutionary computation (EC) to automatically design cost-aware AFs. Leveraging the crossover and mutation in the algorithm space, EvolCAF offers a novel design paradigm, significantly reduces the reliance on domain expertise and model training. The designed cost-aware AF maximizes the utilization of available information from historical data, surrogate models and budget details. It introduces novel ideas not previously explored in the existing literature on acquisition function design, allowing for clear interpretations to provide insights into its behavior and decision-making process. In comparison to the well-known EIpu and EI-cool methods designed by human experts, our approach showcases remarkable efficiency and generalization across various tasks, including 12 synthetic problems and 3 real-world hyperparameter tuning test sets.",
    "pdf_link": "https://arxiv.org/abs/2404.16906",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16906v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16906/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16906v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16906/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16906v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16906/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16906v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16906/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16906v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16906/x5.png"
      }
    ],
    "abstract_cn": "在众多现实世界的优化情境中，评估成本高昂且成本结构复杂多变。面对这一难题，成本感知贝叶斯优化策略以其显著效果脱颖而出。在预算有限的情况下，如何设计出成本感知的获取函数（AFs），以经济高效的方式逼近全局最优解，成为了一个关键问题。传统上，这一设计过程依赖于深入的领域知识和繁琐的试错环节。本论文提出了EvolCAF，这是一个创新的框架，它融合了大型语言模型（LLMs）和进化计算（EC），自动化地设计出成本感知的AFs。EvolCAF通过算法空间中的交叉和变异操作，开创了一种新的设计模式，大幅降低了对专业领域知识和模型训练的依赖。所设计的AF能够充分利用历史数据、替代模型和预算细节中的信息，其新颖的设计思路为获取函数的设计领域带来了新的视角，提供了对其行为和决策过程的清晰解释。相较于人类专家设计的EIpu和EI-cool方法，EvolCAF在多个任务上展现出了卓越的效率和泛化能力，这包括12个合成问题和3个真实世界的超参数调整测试集。",
    "title_cn": "利用大型语言模型，我们推进了成本感知采购函数的演化。",
    "tags": [
      "LLM应用",
      "优化算法",
      "自动化设计"
    ]
  },
  {
    "title": "How to Parameterize Asymmetric Quantization Ranges for Quantization-Aware Training",
    "submit_datetime": "2024年04月25日",
    "abstract": "This paper investigates three different parameterizations of asymmetric uniform quantization for quantization-aware training: (1) scale and offset, (2) minimum and maximum, and (3) beta and gamma. We perform a comprehensive comparative analysis of these parameterizations' influence on quantization-aware training, using both controlled experiments and real-world large language models. Our particular focus is on their changing behavior in response to critical training hyperparameters, bit width and learning rate. Based on our investigation, we propose best practices to stabilize and accelerate quantization-aware training with learnable asymmetric quantization ranges.",
    "pdf_link": "https://arxiv.org/abs/2404.16898",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/fig1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/fig2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/sz_mm_adam_3_norm.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/sz_mm_adam_10_norm.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/fig4_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/fig4_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/ext_mm_vs_bg_b3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/ext_mm_vs_bg_b10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/sz_asz1_asz2_ksz_b3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/sz_asz1_asz2_ksz_b10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/sym_all.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/sym_scale.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/relu_sz_vs_mm_3bit_1e2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/relu_sz_vs_mm_8bit_1e2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/normal_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16898v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16898/normal_8.png"
      }
    ],
    "abstract_cn": "本研究深入探讨了量化感知训练中三种非对称均匀量化参数化方案：（1）缩放和平移，（2）最小与最大值，（3）β与γ。通过精心设计的实验和在真实世界的大型语言模型上的应用，我们全面比较了这些参数化对训练过程的影响。研究重点在于它们如何随关键训练超参数——位宽和学习率——的变化而变化。根据我们的发现，我们提出了一系列最佳实践，旨在优化具有可学习非对称量化区间的量化感知训练的稳定性与效率。",
    "title_cn": "量化感知训练中非对称量化区间的参数化方法",
    "tags": [
      "分类：LLM理论",
      "机器学习",
      ""
    ]
  },
  {
    "title": "WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural Value Awareness of Language Models",
    "submit_datetime": "2024年04月24日",
    "abstract": "The awareness of multi-cultural human values is critical to the ability of language models (LMs) to generate safe and personalized responses. However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values. In this paper, we present WorldValuesBench, a globally diverse, large-scale benchmark dataset for the multi-cultural value prediction task, which requires a model to generate a rating response to a value question based on demographic contexts. Our dataset is derived from an influential social science project, World Values Survey (WVS), that has collected answers to hundreds of value questions (e.g., social, economic, ethical) from 94,728 participants worldwide. We have constructed more than 20 million examples of the type \"(demographic attributes, value question) $\\rightarrow$ answer\" from the WVS responses. We perform a case study using our dataset and show that the task is challenging for strong open and closed-source models. On merely $11.1\\%$, $25.0\\%$, $72.2\\%$, and $75.0\\%$ of the questions, Alpaca-7B, Vicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5 Turbo can respectively achieve $<0.2$ Wasserstein 1-distance from the human normalized answer distributions. WorldValuesBench opens up new research avenues in studying limitations and opportunities in multi-cultural value awareness of LMs.",
    "pdf_link": "https://arxiv.org/abs/2404.16308",
    "graphs": [],
    "abstract_cn": "语言模型在生成安全且个性化的回应时，对多元文化价值观的认知极为关键。然而，对于语言模型在这方面的认知，研究尚显不足，主要原因在于计算机科学界难以获取大规模的多元文化价值现实世界数据。本文介绍了WorldValuesBench，这是一个涵盖全球多样性、大规模的基准数据集，旨在预测多文化价值观，要求模型基于人口统计背景对价值问题给出评分反馈。该数据集基于著名的社会科学项目——世界价值观调查（WVS），该项目汇集了全球94,728名受访者对数百个价值问题的回答。我们从WVS的反馈中构建了超过2000万个“（人口统计特征，价值问题）→答案”的样本。通过案例研究，我们发现即使是先进的开源和闭源模型，面对这一任务也显得力不从心。Alpaca-7B、Vicuna-7B-v1.5、Mixtral-8x7B-Instruct-v0.1和GPT-3.5 Turbo在分别只有11.1%、25.0%、72.2%和75.0%的问题上，才能达到与人类答案分布的Wasserstein 1-距离小于0.2。WorldValuesBench的推出，为探索语言模型在多元文化价值认知上的局限与潜力提供了新的研究路径。",
    "title_cn": "WorldValuesBench：为语言模型打造的大型多文化价值意识基准数据集",
    "tags": [
      "分类：LLM应用\n\n这篇论文主要关注语言模型在多元文化价值观认知方面的应用，通过构建大规模基准数据集WorldValuesBench来评估模型性能。这属于LLM应用的范畴，因为它探讨了如何利用现有的大型语言模型来解决特定的问题（即预测多文化价值观），并提出了一个新的数据集来促进相关研究。",
      "计算机科学",
      "社会心理学"
    ]
  },
  {
    "title": "Semantically consistent Video-to-Audio Generation using Multimodal Language Large Model",
    "submit_datetime": "2024年04月24日",
    "abstract": "Existing works have made strides in video generation, but the lack of sound effects (SFX) and background music (BGM) hinders a complete and immersive viewer experience. We introduce a novel semantically consistent v ideo-to-audio generation framework, namely SVA, which automatically generates audio semantically consistent with the given video content. The framework harnesses the power of multimodal large language model (MLLM) to understand video semantics from a key frame and generate creative audio schemes, which are then utilized as prompts for text-to-audio models, resulting in video-to-audio generation with natural language as an interface. We show the satisfactory performance of SVA through case study and discuss the limitations along with the future research direction. The project page is available at https://huiz-a.github.io/audio4video.github.io/.",
    "pdf_link": "https://arxiv.org/abs/2404.16305",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16305v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16305/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16305v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16305/x2.png"
      }
    ],
    "abstract_cn": "在视频生成领域，尽管已有显著进展，但缺少音效和背景音乐影响了观众的沉浸体验。为此，我们提出了一个创新的语义一致性视频转音频生成框架——SVA，它能够自动生成与视频内容语义相匹配的音频。SVA框架借助多模态大型语言模型深入理解关键帧的语义，并创造性地设计音频方案，这些方案随后作为文本转音频模型的输入提示，实现了以自然语言为桥梁的视频到音频的生成。通过案例分析，我们证实了SVA的卓越性能，并对其局限性及未来研究的可能方向进行了探讨。项目详情可访问 https://huiz-a.github.io/audio4video.github.io/。",
    "title_cn": "借助多模态大型语言模型，实现视频到音频的语义一致性生成。",
    "tags": [
      "LLM应用",
      "视频生成",
      "音频生成"
    ]
  },
  {
    "title": "When Fuzzing Meets LLMs: Challenges and Opportunities",
    "submit_datetime": "2024年04月24日",
    "abstract": "Fuzzing, a widely-used technique for bug detection, has seen advancements through Large Language Models (LLMs). Despite their potential, LLMs face specific challenges in fuzzing. In this paper, we identified five major challenges of LLM-assisted fuzzing. To support our findings, we revisited the most recent papers from top-tier conferences, confirming that these challenges are widespread. As a remedy, we propose some actionable recommendations to help improve applying LLM in Fuzzing and conduct preliminary evaluations on DBMS fuzzing. The results demonstrate that our recommendations effectively address the identified challenges.",
    "pdf_link": "https://arxiv.org/abs/2404.16297",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16297v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16297/x1.png"
      }
    ],
    "abstract_cn": "模糊测试，这一广泛应用于漏洞探测的技术，借助大型语言模型（LLMs）取得了新的突破。然而，LLMs在模糊测试的应用上遭遇了特别的难题。本文中，我们归纳了LLM辅助模糊测试面临的五大挑战。为了验证这些发现，我们回顾了顶级会议上的最新论文，确认了这些挑战的普遍性。为此，我们提出了一系列切实可行的建议，旨在优化LLM在模糊测试中的应用，并在数据库管理系统（DBMS）模糊测试领域进行了初步的评估。评估结果证明，我们的建议对于应对这些挑战具有显著效果。",
    "title_cn": "模糊测试与大型语言模型（LLMs）的邂逅：面临的挑战与潜在的机遇。",
    "tags": [
      "LLM应用",
      "软件测试",
      "数据库管理"
    ]
  },
  {
    "title": "LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications",
    "submit_datetime": "2024年04月24日",
    "abstract": "Electronic health records (EHR) even though a boon for healthcare practitioners, are growing convoluted and longer every day. Sifting around these lengthy EHRs is taxing and becomes a cumbersome part of physician-patient interaction. Several approaches have been proposed to help alleviate this prevalent issue either via summarization or sectioning, however, only a few approaches have truly been helpful in the past. With the rise of automated methods, machine learning (ML) has shown promise in solving the task of identifying relevant sections in EHR. However, most ML methods rely on labeled data which is difficult to get in healthcare. Large language models (LLMs) on the other hand, have performed impressive feats in natural language processing (NLP), that too in a zero-shot manner, i.e. without any labeled data. To that end, we propose using LLMs to identify relevant section headers. We find that GPT-4 can effectively solve the task on both zero and few-shot settings as well as segment dramatically better than state-of-the-art methods. Additionally, we also annotate a much harder real world dataset and find that GPT-4 struggles to perform well, alluding to further research and harder benchmarks.",
    "pdf_link": "https://arxiv.org/abs/2404.16294",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16294v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16294/Handwritten_clinical_note.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16294v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16294/Section_Category.png"
      }
    ],
    "abstract_cn": "电子健康记录（EHR）为医疗专业人员带来了便利，但其内容日益庞杂和冗长，使得在这些繁复的记录中寻找信息变得既费力又繁琐。尽管提出了多种解决方案，如摘要化或分节，但真正有效的办法并不多。随着自动化技术的兴起，机器学习（ML）在识别 EHR 中的关键部分方面展现出潜力。不过，大多数 ML 方法需要依赖难以获得的标记数据。与此同时，大型语言模型（LLMs）在无需任何标记数据的情况下，在自然语言处理（NLP）领域取得了显著成就。基于此，我们提出利用 LLMs 来识别 EHR 中的相关章节标题。实验结果表明，GPT-4 不仅在零样本和少样本情境下能够有效完成任务，而且在数据分段上也显著优于现有技术。此外，我们还对一个更具挑战性的现实世界数据集进行了标注，发现 GPT-4 在此任务上的表现仍有提升空间，这为未来的研究和更严格的评估标准提供了方向。",
    "title_cn": "基于大型语言模型的章节识别器在开源项目中表现优异，然而在现实世界的应用场景中却步履蹒跚。",
    "tags": [
      "LLM应用",
      "",
      "机器学习"
    ]
  },
  {
    "title": "Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services",
    "submit_datetime": "2024年04月24日",
    "abstract": "The advent of large language models (LLMs) has transformed text-based services, enabling capabilities ranging from real-time translation to AI-driven chatbots. However, existing serving systems primarily focus on optimizing server-side aggregate metrics like token generation throughput, ignoring individual user experience with streamed text. As a result, under high and/or bursty load, a significant number of users can receive unfavorable service quality or poor Quality-of-Experience (QoE). In this paper, we first formally define QoE of text streaming services, where text is delivered incrementally and interactively to users, by considering the end-to-end token delivery process throughout the entire interaction with the user. Thereafter, we propose Andes, a QoE-aware serving system that enhances user experience for LLM-enabled text streaming services. At its core, Andes strategically allocates contended GPU resources among multiple requests over time to optimize their QoE. Our evaluations demonstrate that, compared to the state-of-the-art LLM serving systems like vLLM, Andes improves the average QoE by up to 3.2$\\times$ under high request rate, or alternatively, it attains up to 1.6$\\times$ higher request rate while preserving high QoE.",
    "pdf_link": "https://arxiv.org/abs/2404.16283",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x19.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x20.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x21.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x22.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x23.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x24.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x25.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x26.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x27.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x28.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x29.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x30.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x31.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x32.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x33.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x34.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x35.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x36.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x37.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x38.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x39.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x40.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x41.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x42.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x43.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x44.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x45.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x46.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x47.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x48.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x49.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x50.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x51.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16283v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16283/x52.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLM）的问世，彻底革新了文本服务领域，从即时翻译到AI聊天机器人，开启了多种可能性。但现有服务系统多集中于提升服务器端的总体性能指标，如令牌生成速率，而忽视了用户在接收流式文本时的个性化体验。这导致在高流量或流量高峰时段，许多用户可能会遭遇服务质量不佳或体验质量（QoE）下降的问题。本文首先明确了文本流式服务的QoE定义，该服务以递增和互动的方式向用户传递文本，全面考虑了与用户互动的整个过程中端到端的令牌传递。接着，我们提出了Andes，这是一个注重QoE的服务系统，旨在提升LLM支持的文本流式服务的用户体验。Andes的核心在于，它巧妙地在多个请求之间分配有限的GPU资源，以时间为基础优化用户体验。我们的评估结果证明，与现有的顶尖LLM服务系统相比，Andes在高请求率下平均QoE提升了最多3.2倍，或者在保持高QoE的前提下，实现了最多1.6倍的请求处理能力提升。",
    "title_cn": "Andes：为基于大型语言模型的文本流服务定义并提升用户体验质量",
    "tags": [
      "LLM应用",
      "文本服务",
      "用户体验"
    ]
  },
  {
    "title": "Interpreting Answers to Yes-No Questions in Dialogues from Multiple Domains",
    "submit_datetime": "2024年04月24日",
    "abstract": "People often answer yes-no questions without explicitly saying yes, no, or similar polar keywords. Figuring out the meaning of indirect answers is challenging, even for large language models. In this paper, we investigate this problem working with dialogues from multiple domains. We present new benchmarks in three diverse domains: movie scripts, tennis interviews, and airline customer service. We present an approach grounded on distant supervision and blended training to quickly adapt to a new dialogue domain. Experimental results show that our approach is never detrimental and yields F1 improvements as high as 11-34%.",
    "pdf_link": "https://arxiv.org/abs/2404.16262",
    "graphs": [],
    "abstract_cn": "人们在回答是非题时，往往不会直接使用“是”、“否”等极端词汇。对于大型语言模型而言，解读这些含蓄的回答颇为棘手。本文针对这一难题，通过分析多个领域的对话数据进行了探讨。我们为三个迥异的领域——电影剧本、网球访谈和航空公司客服——设立了新的评估标准。我们介绍了一种基于远程监督与混合训练的方法，旨在迅速适应新的对话领域。实验结果显示，这种方法不仅无害，还能显著提升 F1 分数，提升幅度在 11% 至 34% 之间。",
    "title_cn": "在跨领域的对话中，解读针对是非问题的答复",
    "tags": [
      "LLM应用",
      "对话系统",
      ""
    ]
  },
  {
    "title": "Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions",
    "submit_datetime": "2024年04月24日",
    "abstract": "Prompt leakage in large language models (LLMs) poses a significant security and privacy threat, particularly in retrieval-augmented generation (RAG) systems. However, leakage in multi-turn LLM interactions along with mitigation strategies has not been studied in a standardized manner. This paper investigates LLM vulnerabilities against prompt leakage across 4 diverse domains and 10 closed- and open-source LLMs. Our unique multi-turn threat model leverages the LLM's sycophancy effect and our analysis dissects task instruction and knowledge leakage in the LLM response. In a multi-turn setting, our threat model elevates the average attack success rate (ASR) to 86.2%, including a 99% leakage with GPT-4 and claude-1.3. We find that some black-box LLMs like Gemini show variable susceptibility to leakage across domains - they are more likely to leak contextual knowledge in the news domain compared to the medical domain. Our experiments measure specific effects of 6 black-box defense strategies, including a query-rewriter in the RAG scenario. Our proposed multi-tier combination of defenses still has an ASR of 5.3% for black-box LLMs, indicating room for enhancement and future direction for LLM security research.",
    "pdf_link": "https://arxiv.org/abs/2404.16251",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）中的提示泄露问题，对检索增强生成（RAG）系统构成了严重的安全和隐私风险。尽管如此，对于多轮LLM交互中的泄露现象及其缓解措施，目前还缺乏系统化的研究。本研究深入探讨了不同领域内，以及10种闭源和开源LLMs在面对提示泄露时的脆弱性。我们构建的多轮威胁模型巧妙利用了LLM的“拍马屁”效应，并通过深入分析，揭示了LLM回应中任务指令与知识泄露的关系。在模拟的多轮对话场景中，我们的模型将攻击者的平均成功率提升至86.2%，在某些情况下，如GPT-4和claude-1.3，泄露率甚至高达99%。我们还发现，部分黑盒LLMs，例如Gemini，在不同领域的泄露敏感性上表现出差异，它们在新闻领域比在医疗领域更容易泄露上下文信息。通过一系列实验，我们评估了六种黑盒防御策略的具体效果，包括RAG场景下的查询重写技术。我们提出的多层次防御策略组合，尽管将黑盒LLMs的平均攻击成功率降低到了5.3%，但仍显示出提升空间，为LLM安全研究的未来指明了方向。",
    "title_cn": "本研究旨在探究在多轮大型语言模型互动中出现的提示泄露现象，以及为这种交互模式设计的黑箱防御机制。",
    "tags": [
      "LLM应用",
      "安全与隐私",
      "人工智能"
    ]
  },
  {
    "title": "URL: Universal Referential Knowledge Linking via Task-instructed Representation Compression",
    "submit_datetime": "2024年04月24日",
    "abstract": "Linking a claim to grounded references is a critical ability to fulfill human demands for authentic and reliable information. Current studies are limited to specific tasks like information retrieval or semantic matching, where the claim-reference relationships are unique and fixed, while the referential knowledge linking (RKL) in real-world can be much more diverse and complex. In this paper, we propose universal referential knowledge linking (URL), which aims to resolve diversified referential knowledge linking tasks by one unified model. To this end, we propose a LLM-driven task-instructed representation compression, as well as a multi-view learning approach, in order to effectively adapt the instruction following and semantic understanding abilities of LLMs to referential knowledge linking. Furthermore, we also construct a new benchmark to evaluate ability of models on referential knowledge linking tasks across different scenarios. Experiments demonstrate that universal RKL is challenging for existing approaches, while the proposed framework can effectively resolve the task across various scenarios, and therefore outperforms previous approaches by a large margin.",
    "pdf_link": "https://arxiv.org/abs/2404.16248",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16248/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16248/x2.png"
      }
    ],
    "abstract_cn": "将信息主张与确凿的参考资源相连接是人类追求真实和可信信息的基本需求。目前的研究多聚焦于特定领域，如信息检索或语义匹配，这些领域的主张与参考关系明确且单一。相较之下，现实世界中的参照知识链接（RKL）要复杂和多变得多。本文提出了一种全新的通用参照知识链接（URL）概念，其目的是通过统一的模型框架，解决多样化的参照知识链接问题。我们引入了一种由大型语言模型（LLM）支持的任务驱动表示压缩技术，结合多视角学习方法，以提升模型在参照知识链接任务中的指令执行和语义理解能力。此外，我们还开发了一个新的评估基准，用以测试模型在不同情境下进行参照知识链接的表现。实验结果证明，现有的方法在处理通用RKL时面临挑战，而我们提出的框架能够在多种不同情境下有效解决问题，显著优于以往的解决方案。",
    "title_cn": "URL：通过任务驱动的表示压缩技术，实现知识的普遍引用链接。",
    "tags": [
      "LLM应用",
      "信息检索",
      "知识链接"
    ]
  },
  {
    "title": "Step Differences in Instructional Video",
    "submit_datetime": "2024年04月24日",
    "abstract": "Comparing a user video to a reference how-to video is a key requirement for AR/VR technology delivering personalized assistance tailored to the user's progress. However, current approaches for language-based assistance can only answer questions about a single video. We propose an approach that first automatically generates large amounts of visual instruction tuning data involving pairs of videos from HowTo100M by leveraging existing step annotations and accompanying narrations, and then trains a video-conditioned language model to jointly reason across multiple raw videos. Our model achieves state-of-the-art performance at identifying differences between video pairs and ranking videos based on the severity of these differences, and shows promising ability to perform general reasoning over multiple videos.",
    "pdf_link": "https://arxiv.org/abs/2404.16222",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16222v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16222/x14.png"
      }
    ],
    "abstract_cn": "在 AR/VR 技术中，将用户视频与教学视频进行比较是实现个性化辅助的关键。但现有基于语言的辅助方法仅能针对单个视频答疑。我们提出了一种新方法：首先，通过现有步骤注释和旁白，自动生成大量 HowTo100M 视频对的视觉指令调整数据；其次，训练一个视频驱动的语言模型，使其能够跨多个原始视频进行联合推理。该模型在识别视频对差异和基于差异严重性对视频排序方面达到了行业领先水平，并展现出处理多视频通用推理的潜力。",
    "title_cn": "教学视频的分步差异",
    "tags": [
      "分类：RAG",
      "增强现实/虚拟现实",
      ""
    ]
  },
  {
    "title": "Towards Efficient Patient Recruitment for Clinical Trials: Application of a Prompt-Based Learning Model",
    "submit_datetime": "2024年04月24日",
    "abstract": "Objective: Clinical trials are essential for advancing pharmaceutical interventions, but they face a bottleneck in selecting eligible participants. Although leveraging electronic health records (EHR) for recruitment has gained popularity, the complex nature of unstructured medical texts presents challenges in efficiently identifying participants. Natural Language Processing (NLP) techniques have emerged as a solution with a recent focus on transformer models. In this study, we aimed to evaluate the performance of a prompt-based large language model for the cohort selection task from unstructured medical notes collected in the EHR. Methods: To process the medical records, we selected the most related sentences of the records to the eligibility criteria needed for the trial. The SNOMED CT concepts related to each eligibility criterion were collected. Medical records were also annotated with MedCAT based on the SNOMED CT ontology. Annotated sentences including concepts matched with the criteria-relevant terms were extracted. A prompt-based large language model (Generative Pre-trained Transformer (GPT) in this study) was then used with the extracted sentences as the training set. To assess its effectiveness, we evaluated the model's performance using the dataset from the 2018 n2c2 challenge, which aimed to classify medical records of 311 patients based on 13 eligibility criteria through NLP techniques. Results: Our proposed model showed the overall micro and macro F measures of 0.9061 and 0.8060 which were among the highest scores achieved by the experiments performed with this dataset. Conclusion: The application of a prompt-based large language model in this study to classify patients based on eligibility criteria received promising scores. Besides, we proposed a method of extractive summarization with the aid of SNOMED CT ontology that can be also applied to other medical texts.",
    "pdf_link": "https://arxiv.org/abs/2404.16198",
    "graphs": [],
    "abstract_cn": "目标：临床试验对于推动药物干预至关重要，但筛选合适的参与者却是一个难题。尽管利用电子健康记录（EHR）招募参与者的方法日益普及，但非结构化医疗文本的复杂性使得高效识别参与者变得困难。自然语言处理（NLP）技术，特别是变换器模型，已成为解决这一问题的有效手段。本研究旨在评估一种基于提示的大型语言模型在从EHR中收集的非结构化医疗笔记中进行队列筛选任务的性能。方法：我们筛选出与临床试验资格标准最相关的句子来处理医疗记录。收集了与各资格标准相关的SNOMED CT概念，并基于SNOMED CT本体论对医疗记录进行了MedCAT注释。随后，提取了包含与标准相关术语相匹配概念的注释句子。我们采用了基于提示的大型语言模型（本研究中为生成预训练变换器GPT）和这些句子作为训练集。为了测试其效果，我们使用了2018年n2c2挑战的数据集来评估模型性能，该挑战旨在通过NLP技术对311名患者的医疗记录根据13个资格标准进行分类。结果：我们所提出的模型在整体微观和宏观F测量值上分别达到了0.9061和0.8060，这在该数据集上的所有实验中得分名列前茅。结论：本研究中应用的基于提示的大型语言模型在根据资格标准对患者进行分类的任务上取得了令人鼓舞的成绩。此外，我们还提出了一种利用SNOMED CT本体论进行提取式摘要的方法，该方法同样适用于其他医疗文本的处理。",
    "title_cn": "高效招募临床试验患者：应用一种基于提示的学习模型",
    "tags": [
      "LLM应用",
      "",
      ""
    ]
  },
  {
    "title": "Improving Multi-label Recognition using Class Co-Occurrence Probabilities",
    "submit_datetime": "2024年04月24日",
    "abstract": "Multi-label Recognition (MLR) involves the identification of multiple objects within an image. To address the additional complexity of this problem, recent works have leveraged information from vision-language models (VLMs) trained on large text-images datasets for the task. These methods learn an independent classifier for each object (class), overlooking correlations in their occurrences. Such co-occurrences can be captured from the training data as conditional probabilities between a pair of classes. We propose a framework to extend the independent classifiers by incorporating the co-occurrence information for object pairs to improve the performance of independent classifiers. We use a Graph Convolutional Network (GCN) to enforce the conditional probabilities between classes, by refining the initial estimates derived from image and text sources obtained using VLMs. We validate our method on four MLR datasets, where our approach outperforms all state-of-the-art methods.",
    "pdf_link": "https://arxiv.org/abs/2404.16193",
    "graphs": [],
    "abstract_cn": "多标签识别（MLR）技术致力于在单张图像中识别多个对象。面对这一问题的复杂性，最新研究开始借助视觉-语言模型（VLMs）——它们在海量图文数据集上进行训练——来提升任务效率。传统方法通常为每个类别独立训练一个分类器，却忽视了类别间存在的共现关系。我们提出一种新框架，通过整合类别对的共现信息，优化独立分类器的性能。该框架利用图卷积网络（GCN）来调整类别间条件概率，从而精细化VLMs从图像和文本源得到的初步预测。经过在四个MLR数据集上的测试，我们的方法在性能上超越了所有现有的顶尖技术。",
    "title_cn": "通过类别共现概率提升多标签识别的性能",
    "tags": [
      "分类：Agent\n\n这篇论文提出了一种新的框架，通过整合类别对的共现信息，优化独立分类器的性能。该框架利用图卷积网络（GCN）来调整类别间条件概率，从而精细化视觉-语言模型（VLMs）从图像和文本源得到的初步预测。这表明该研究涉及到智能代理（Agent）在多标签识别（MLR）任务中的应用，因此将其归类为Agent。",
      "计算机视觉",
      "机器学习"
    ]
  },
  {
    "title": "Fusion of Domain-Adapted Vision and Language Models for Medical Visual Question Answering",
    "submit_datetime": "2024年04月24日",
    "abstract": "Vision-language models, while effective in general domains and showing strong performance in diverse multi-modal applications like visual question-answering (VQA), struggle to maintain the same level of effectiveness in more specialized domains, e.g., medical. We propose a medical vision-language model that integrates large vision and language models adapted for the medical domain. This model goes through three stages of parameter-efficient training using three separate biomedical and radiology multi-modal visual and text datasets. The proposed model achieves state-of-the-art performance on the SLAKE 1.0 medical VQA (MedVQA) dataset with an overall accuracy of 87.5% and demonstrates strong performance on another MedVQA dataset, VQA-RAD, achieving an overall accuracy of 73.2%.",
    "pdf_link": "https://arxiv.org/abs/2404.16192",
    "graphs": [],
    "abstract_cn": "视觉-语言模型在通用领域表现出色，并在多模态应用如视觉问答（VQA）中表现强劲。但在专业领域如医学中，其效能有所下降。为此，我们设计了一款专门针对医学领域的视觉-语言模型，该模型融合了专为医学定制的大型视觉和语言模型。通过三个阶段的参数高效训练，分别使用了三个不同的生物医学及放射学多模态视觉与文本数据集。在SLAKE 1.0医学VQA数据集上，该模型以87.5%的准确率刷新了最佳成绩，并在VQA-RAD数据集上也以73.2%的准确率展现了优异的性能。",
    "title_cn": "将领域适应的视觉与语言模型相结合，以提升医学视觉问答的性能。",
    "tags": [
      "分类：RAG\n\n这篇论文摘要描述了一种针对医学领域的视觉-语言模型，它通过三个阶段的参数高效训练，使用了生物医学及放射学多模态视觉与文本数据集。这种模型的设计和训练方法属于RAG（Retrieval-Augmented Generation）的范畴，因为它结合了视觉和语言信息，并且针对特定领域进行了定制。",
      "",
      "视觉问答"
    ]
  },
  {
    "title": "Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall",
    "submit_datetime": "2024年04月24日",
    "abstract": "Large language models (LLMs) have shown remarkable performance on a variety of NLP tasks, and are being rapidly adopted in a wide range of use cases. It is therefore of vital importance to holistically evaluate the factuality of their generated outputs, as hallucinations remain a challenging issue.\n  In this work, we focus on assessing LLMs' ability to recall factual knowledge learned from pretraining, and the factors that affect this ability. To that end, we construct FACT-BENCH, a representative benchmark covering 20 domains, 134 property types, 3 answer types, and different knowledge popularity levels. We benchmark 31 models from 10 model families and provide a holistic assessment of their strengths and weaknesses. We observe that instruction-tuning hurts knowledge recall, as pretraining-only models consistently outperform their instruction-tuned counterparts, and positive effects of model scaling, as larger models outperform smaller ones for all model families. However, the best performance from GPT-4 still represents a large gap with the upper-bound. We additionally study the role of in-context exemplars using counterfactual demonstrations, which lead to significant degradation of factual knowledge recall for large models. By further decoupling model known and unknown knowledge, we find the degradation is attributed to exemplars that contradict a model's known knowledge, as well as the number of such exemplars. Lastly, we fine-tune LLaMA-7B in different settings of known and unknown knowledge. In particular, fine-tuning on a model's known knowledge is beneficial, and consistently outperforms fine-tuning on unknown and mixed knowledge. We will make our benchmark publicly available.",
    "pdf_link": "https://arxiv.org/abs/2404.16164",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）在众多自然语言处理（NLP）任务上展现了卓越性能，并迅速被应用于多种场景。全面评估其输出内容的真实性极为重要，以应对生成幻觉这一棘手问题。本研究着眼于分析LLMs回忆预训练阶段学习到的事实知识的能力及其影响因素。为此，我们开发了FACT-BENCH，一个涵盖20个领域、134种属性类型、3种答案类型以及不同知识知名度水平的基准测试。我们对10个模型家族的31个模型进行了全面评估，揭示了它们的优势与不足。研究发现，指令调整对知识回忆有负面影响，因为仅经过预训练的模型总是比经过指令调整的模型表现更好；同时，模型规模的扩大对性能有积极影响，大型模型在所有家族中均优于小型模型。但即便是GPT-4的最佳表现，与最佳性能仍有较大差距。我们还探讨了上下文示例在反事实演示中的作用，发现这会导致大型模型的事实知识回忆能力显著下降。进一步分析表明，这种下降是由于与模型已知知识相冲突的示例以及这类示例的数量所导致的。最后，我们在已知和未知知识的不同设置下对LLaMA-7B进行了微调，发现针对模型已知知识的微调尤其有益，其性能一致超越了针对未知和混合知识进行的微调。我们将向公众开放这一基准测试。",
    "title_cn": "迈向对大型语言模型在事实知识记忆能力上的全面评估",
    "tags": [
      "LLM应用",
      "",
      "基准测试"
    ]
  },
  {
    "title": "Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant",
    "submit_datetime": "2024年04月24日",
    "abstract": "Large language models (LLMs) have demonstrated impressive generalization capabilities on specific tasks with human-written instruction data. However, the limited quantity, diversity, and professional expertise of such instruction data raise concerns about the performance of LLMs in psychotherapy tasks when provided with domain-specific instructions. To address this, we firstly propose Domain-Specific Assistant Instructions based on AlexanderStreet therapy, and secondly, we use an adaption fine-tuning method and retrieval augmented generation method to improve pre-trained LLMs. Through quantitative evaluation of linguistic quality using automatic and human evaluation, we observe that pre-trained LLMs on Psychotherapy Assistant Instructions outperform state-of-the-art LLMs response baselines. Our Assistant-Instruction approach offers a half-annotation method to align pre-trained LLMs with instructions and provide pre-trained LLMs with more psychotherapy knowledge.",
    "pdf_link": "https://arxiv.org/abs/2404.16160",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）在特定任务上展现出卓越的泛化能力，尤其是在接收到人类编写的指令数据时。但现有指令数据的数量、多样性和专业深度的不足，让人对LLMs在心理治疗领域的应用效果存疑，尤其是在面对特定领域的指导时。为此，我们首先基于AlexanderStreet治疗资料，提出了一套针对性的助理指令；其次，通过采用适应性微调和检索增强生成技术，对预训练的LLMs进行了优化。通过自动和人工评估对语言质量的定量分析，我们发现，这些针对心理治疗助理指令预训练的LLMs在响应上超越了当前最顶尖的LLMs。我们的这种方法为预训练的LLMs提供了一种半注释手段，使其更好地与指令相匹配，并赋予了它们更丰富的心理治疗知识。",
    "title_cn": "借助助手，对心理治疗聊天机器人进行专业领域的优化提升。",
    "tags": [
      "LLM应用",
      "心理治疗",
      "人工智能"
    ]
  },
  {
    "title": "The Feasibility of Implementing Large-Scale Transformers on Multi-FPGA Platforms",
    "submit_datetime": "2024年04月24日",
    "abstract": "FPGAs are rarely mentioned when discussing the implementation of large machine learning applications, such as Large Language Models (LLMs), in the data center. There has been much evidence showing that single FPGAs can be competitive with GPUs in performance for some computations, especially for low latency, and often much more efficient when power is considered. This suggests that there is merit to exploring the use of multiple FPGAs for large machine learning applications. The challenge with using multiple FPGAs is that there is no commonly-accepted flow for developing and deploying multi-FPGA applications, i.e., there are no tools to describe a large application, map it to multiple FPGAs and then deploy the application on a multi-FPGA platform. In this paper, we explore the feasibility of implementing large transformers using multiple FPGAs by developing a scalable multi-FPGA platform and some tools to map large applications to the platform. We validate our approach by designing an efficient multi-FPGA version of the I-BERT transformer and implement one encoder using six FPGAs as a working proof-of-concept to show that our platform and tools work. Based on our proof-of-concept prototype and the estimations of performance using the latest FPGAs compared to GPUs, we conclude that there can be a place for FPGAs in the world of large machine learning applications. We demonstrate a promising first step that shows that with the right infrastructure and tools it is reasonable to continue to explore the possible benefits of using FPGAs for applications such as LLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.16158",
    "graphs": [],
    "abstract_cn": "在数据中心部署大型机器学习应用，如大型语言模型（LLMs），通常不会提及现场可编程门阵列（FPGA）。然而，有证据显示，单个FPGA在某些计算任务上的性能可与GPU媲美，尤其在低延迟计算上，且在能耗效率上往往更胜一筹。这一发现激励我们考虑利用多个FPGA来执行大型机器学习任务。面临的挑战在于，目前缺乏一套广泛接受的开发和部署多FPGA应用的流程，也就是说，缺少能够描述、映射并部署大型应用到多FPGA平台的工具。本文中，我们探讨了利用多个FPGA实现大型变换器的可能性，包括开发一个可扩展的多FPGA平台和相应的映射工具。我们通过设计高效的多FPGA版本的I-BERT变换器，并用六个FPGA实现一个编码器作为实际概念验证，来验证我们的方法。根据我们的概念验证原型以及与GPU相比的最新FPGA性能估算，我们得出结论，FPGA在大型机器学习应用领域确实有其一席之地。我们的初步成果表明，只要有合适的基础设施和工具，进一步探索FPGA在LLMs等应用中的潜在优势是值得的。",
    "title_cn": "探讨在多 FPGA 平台上部署大型变换器的可能性",
    "tags": [
      "LLM应用",
      "数据中心",
      "机器学习"
    ]
  },
  {
    "title": "Chat2Scenario: Scenario Extraction From Dataset Through Utilization of Large Language Model",
    "submit_datetime": "2024年04月24日",
    "abstract": "The advent of Large Language Models (LLM) provides new insights to validate Automated Driving Systems (ADS). In the herein-introduced work, a novel approach to extracting scenarios from naturalistic driving datasets is presented. A framework called Chat2Scenario is proposed leveraging the advanced Natural Language Processing (NLP) capabilities of LLM to understand and identify different driving scenarios. By inputting descriptive texts of driving conditions and specifying the criticality metric thresholds, the framework efficiently searches for desired scenarios and converts them into ASAM OpenSCENARIO and IPG CarMaker text files. This methodology streamlines the scenario extraction process and enhances efficiency. Simulations are executed to validate the efficiency of the approach. The framework is presented based on a user-friendly web app and is accessible via the following link: https://github.com/ftgTUGraz/Chat2Scenario.",
    "pdf_link": "https://arxiv.org/abs/2404.16147",
    "graphs": [],
    "abstract_cn": "随着大型语言模型（LLM）的兴起，我们获得了验证自动驾驶系统（ADS）的新视角。本研究提出了一种创新的方法，用于从自然驾驶数据集中提炼出各种驾驶场景。我们引入了一个名为Chat2Scenario的框架，该框架借助LLM的先进自然语言处理（NLP）技术，来理解并识别多样化的驾驶情境。用户只需输入关于驾驶环境的描述，并设定关键性指标的阈值，框架便能高效地筛选并锁定目标场景，进而将这些场景转换成ASAM OpenSCENARIO和IPG CarMaker格式的文本文件。这一流程不仅简化了场景提取工作，还显著提升了工作效率。为了验证此方法的有效性，我们进行了一系列的仿真测试。Chat2Scenario框架以一个易于操作的Web应用形式呈现，并通过以下链接向公众开放：https://github.com/ftgTUGraz/Chat2Scenario。",
    "title_cn": "Chat2Scenario：利用大型语言模型从数据集中提炼场景",
    "tags": [
      "分类：LLM应用\n\n这篇论文摘要描述了一种利用大型语言模型（LLM）的自然语言处理（NLP）技术，从自然驾驶数据集中提炼出各种驾驶场景的方法。这种方法被称为Chat2Scenario框架，它允许用户输入关于驾驶环境的描述，并根据关键性指标的阈值来筛选和锁定目标场景。然后将这些场景转换成特定格式的文本文件，以便于自动驾驶系统（ADS）的验证。这篇论文的焦点在于应用LLM技术来解决实际问题，因此它应该被归类为LLM应用。",
      "自动驾驶",
      ""
    ]
  },
  {
    "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
    "submit_datetime": "2024年04月24日",
    "abstract": "The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as \"What are the main themes in the dataset?\", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a naïve RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.",
    "pdf_link": "https://arxiv.org/abs/2404.16130",
    "graphs": [],
    "abstract_cn": "通过检索增强生成（RAG）技术，大型语言模型（LLMs）能够从外部知识库中检索信息，从而回答涉及私人或未见过的文档集合的问题。但面对针对整个文本集合的全局性问题，如“数据集中的主要主题有哪些？”，RAG 显得无能为力，因为这实际上是一个查询聚焦的摘要任务，而非简单的检索任务。传统的查询聚焦摘要方法也因无法扩展至 RAG 系统所索引的大量文本而受限。为了融合这些方法的优势，我们提出了一种基于图的 RAG 方法，它能够适应私人文本语料库的问答需求，同时满足用户问题的广泛性和源文本数量的扩展性。该方法利用 LLM 分两步构建基于图的文本索引：首先从源文档中提取实体知识图谱，然后为所有紧密相关的实体群体预生成社区摘要。面对问题时，每个社区摘要用于生成部分答案，所有部分答案再汇总成最终回答。在处理大约百万个令牌规模的数据集上的全局性理解问题时，我们证明了图 RAG 在答案的全面性和多样性上相较于传统 RAG 有显著提升。相关的开源 Python 实现，包括全局和本地图 RAG 方法，即将在 https://aka.ms/graphrag 发布。",
    "title_cn": "探索全局视角：图卷积RAG技术在查询导向摘要中的应用。",
    "tags": [
      "RAG",
      "信息检索",
      ""
    ]
  },
  {
    "title": "Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts",
    "submit_datetime": "2024年04月24日",
    "abstract": "Retrieval-augmented generation (RAG) mitigates many problems of fully parametric language models, such as temporal degradation, hallucinations, and lack of grounding. In RAG, the model's knowledge can be updated from documents provided in context. This leads to cases of conflict between the model's parametric knowledge and the contextual information, where the model may not always update its knowledge. Previous work studied knowledge conflicts by creating synthetic documents that contradict the model's correct parametric answers. We present a framework for studying knowledge conflicts in a realistic setup. We update incorrect parametric knowledge using real conflicting documents. This reflects how knowledge conflicts arise in practice. In this realistic scenario, we find that knowledge updates fail less often than previously reported. In cases where the models still fail to update their answers, we find a parametric bias: the incorrect parametric answer appearing in context makes the knowledge update likelier to fail. These results suggest that the factual parametric knowledge of LLMs can negatively influence their reading abilities and behaviors. Our code is available at https://github.com/kortukov/realistic_knowledge_conflicts/.",
    "pdf_link": "https://arxiv.org/abs/2404.16032",
    "graphs": [],
    "abstract_cn": "检索增强生成（RAG）有效解决了全参数化语言模型的诸多难题，如信息过时、产生幻觉和缺乏事实依据。在RAG框架下，模型能够根据提供的文档上下文更新其知识库。然而，这也可能引发模型内部参数知识与文档上下文信息的冲突，有时模型未必会选择更新知识。先前研究通过构建与模型正确参数答案相冲突的合成文档来探讨这类知识冲突。本研究提出了一个更贴近现实的框架，利用真实存在的冲突文档来修正模型中的错误参数知识，从而更真实地模拟知识冲突的产生过程。研究发现，在现实情境中，知识更新的失败率低于先前研究的报告。当模型未能更新答案时，存在一种参数偏好：如果错误的参数答案出现在上下文中，知识更新更可能失败。这些发现暗示大型语言模型（LLMs）的事实性参数知识可能会对其阅读理解和行为模式产生不利影响。相关代码已在 https://github.com/kortukov/realistic_knowledge_conflicts/ 上公开。",
    "title_cn": "探究现实知识冲突情境下大型语言模型的表现",
    "tags": [
      "分类：RAG",
      "",
      "机器学习"
    ]
  },
  {
    "title": "Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications",
    "submit_datetime": "2024年04月24日",
    "abstract": "The application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems in the telecommunication domain presents unique challenges, primarily due to the complex nature of telecom standard documents and the rapid evolution of the field. The paper introduces and open-sources Telco-RAG, a customized RAG framework designed to handle the specific needs of telecommunications standards, particularly 3rd Generation Partnership Project (3GPP) documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG implementation in other technical domains.",
    "pdf_link": "https://arxiv.org/abs/2404.15939",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15939v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15939/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15939v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15939/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15939v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15939/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15939v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15939/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15939v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15939/x5.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）与检索增强生成（RAG）系统在电信行业的应用遭遇了特殊难题，这主要是因为电信标准文档的复杂性以及该行业的迅猛发展。本文提出了 Telco-RAG，一个为电信标准量身定制的 RAG 框架，专门针对第三代合作伙伴计划（3GPP）等电信文档的需求。Telco-RAG 克服了在技术性极强的内容上部署 RAG 流水线的重大挑战，为 LLMs 在电信领域的应用开拓了新路，并为 RAG 在其他技术性领域的实施提供了宝贵参考。",
    "title_cn": "Telco-RAG：探索电信行业中检索增强语言模型的挑战与应对之道",
    "tags": [
      "分类：RAG",
      "",
      "人工智能"
    ]
  },
  {
    "title": "A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples",
    "submit_datetime": "2024年04月24日",
    "abstract": "The capabilities of a single large language model (LLM) agent for solving a complex task are limited. Connecting multiple LLM agents to a network can effectively improve overall performance. However, building an LLM agent network (LAN) requires a substantial amount of time and effort. In this paper, we introduce EasyLAN, a human-computer collaborative tool that helps developers construct LANs. EasyLAN initially generates a LAN containing only one agent based on the description of the desired task. Subsequently, EasyLAN leverages a few training examples to update the LAN. For each example, EasyLAN models the gap between the output and the ground truth and identifies the causes of the errors. These errors are addressed through carefully designed strategies. Users can intervene in EasyLAN's workflow or directly modify the LAN. Eventually, the LAN evolves from a single agent to a network of LLM agents. The experimental results indicate that developers can rapidly construct LANs with good performance.",
    "pdf_link": "https://arxiv.org/abs/2404.15974",
    "graphs": [],
    "abstract_cn": "单个大型语言模型（LLM）在处理复杂任务时力有未逮。通过构建大型语言模型代理网络（LAN），将多个LLM代理互联，可以有效提升整体效能。但搭建LAN是一项耗时耗力的工作。本文提出了EasyLAN，这是一个辅助开发者构建LAN的人机协作工具。EasyLAN根据用户对任务的描述，首先创建一个只包含单个代理的初始网络。然后，通过少量训练样本，EasyLAN对网络进行迭代优化，分析输出与标准答案之间的差异，找出并纠正错误。用户可以参与EasyLAN的工作流程，或直接对LAN进行编辑。通过这种方式，LAN逐步从一个单一代理发展成为一个由多个LLM代理组成的网络。实验数据显示，使用EasyLAN，开发者能够迅速搭建出性能优异的LAN。",
    "title_cn": "一种人机协作工具，通过少量示例将单一的大型语言模型训练成为网络的一部分。",
    "tags": [
      "Agent",
      "人工智能",
      ""
    ]
  },
  {
    "title": "KGValidator: A Framework for Automatic Validation of Knowledge Graph Construction",
    "submit_datetime": "2024年04月24日",
    "abstract": "This study explores the use of Large Language Models (LLMs) for automatic evaluation of knowledge graph (KG) completion models. Historically, validating information in KGs has been a challenging task, requiring large-scale human annotation at prohibitive cost. With the emergence of general-purpose generative AI and LLMs, it is now plausible that human-in-the-loop validation could be replaced by a generative agent. We introduce a framework for consistency and validation when using generative models to validate knowledge graphs. Our framework is based upon recent open-source developments for structural and semantic validation of LLM outputs, and upon flexible approaches to fact checking and verification, supported by the capacity to reference external knowledge sources of any kind. The design is easy to adapt and extend, and can be used to verify any kind of graph-structured data through a combination of model-intrinsic knowledge, user-supplied context, and agents capable of external knowledge retrieval.",
    "pdf_link": "https://arxiv.org/abs/2404.15923",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15923v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15923/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15923v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15923/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15923v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15923/x3.png"
      }
    ],
    "abstract_cn": "本研究着眼于利用大型语言模型（LLMs）自动评估知识图谱（KG）补全模型的应用。传统上，KG中的信息验证是一项成本高昂且充满挑战的任务，需要大量的人工注释。随着通用生成性AI和LLMs的兴起，现在有理由相信可以由生成性代理取代传统的人工验证环节。我们提出了一个框架，用于在采用生成模型进行KG验证时确保一致性和有效性。该框架依托于最新的开源进展，不仅涵盖了对LLMs输出的结构和语义验证，还包括了灵活的事实核查与验证方法，这些方法能够利用各种类型的外部知识源。设计上易于调整和扩展，能够通过结合模型内生知识、用户定义的上下文以及能够进行外部知识检索的代理，来验证各种图形化结构的数据。",
    "title_cn": "KGValidator：一套自动验证知识图谱构建的框架。",
    "tags": [
      "LLM应用",
      "知识图谱",
      "自动化验证"
    ]
  },
  {
    "title": "Automated Social Science: Language Models as Scientist and Subjects",
    "submit_datetime": "2024年04月24日",
    "abstract": "We present an approach for automatically generating and testing, in silico, social scientific hypotheses. This automation is made possible by recent advances in large language models (LLM), but the key feature of the approach is the use of structural causal models. Structural causal models provide a language to state hypotheses, a blueprint for constructing LLM-based agents, an experimental design, and a plan for data analysis. The fitted structural causal model becomes an object available for prediction or the planning of follow-on experiments. We demonstrate the approach with several scenarios: a negotiation, a bail hearing, a job interview, and an auction. In each case, causal relationships are both proposed and tested by the system, finding evidence for some and not others. We provide evidence that the insights from these simulations of social interactions are not available to the LLM purely through direct elicitation. When given its proposed structural causal model for each scenario, the LLM is good at predicting the signs of estimated effects, but it cannot reliably predict the magnitudes of those estimates. In the auction experiment, the in silico simulation results closely match the predictions of auction theory, but elicited predictions of the clearing prices from the LLM are inaccurate. However, the LLM's predictions are dramatically improved if the model can condition on the fitted structural causal model. In short, the LLM knows more than it can (immediately) tell.",
    "pdf_link": "https://arxiv.org/abs/2404.11794",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11794v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11794/system_overview.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.11794v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11794/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11794v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11794/example_agents.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11794v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11794/interaction_types.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.11794v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11794/x2.png"
      }
    ],
    "abstract_cn": "我们展示了一种自动生成并模拟测试社会科学假设的新方法。这一创新得益于大型语言模型（LLM）的突破性进展，但其核心在于结构因果模型的应用。这些模型不仅为我们提供了一种表述假设的方式，还为构建基于LLM的智能体、设计实验和规划数据分析提供了指导。经过拟合的模型进一步成为预测和后续实验规划的有力工具。我们通过多个场景——谈判、保释听证、求职面试和拍卖——来验证这一方法，系统在这些场景中提出并验证了因果关系，发现了一些关系的证据，而对其他关系则持保留态度。研究表明，通过这些模拟获得的对社会互动的洞察，是LLM无法仅通过直接询问得到的。尽管LLM能够准确预测估计效应的方向，但对于效应大小的预测却不够可靠。特别是在拍卖实验中，模拟结果与拍卖理论的预测高度吻合，而LLM直接引出的清算价格预测却存在偏差。但如果LLM能够依据拟合的因果模型进行条件预测，其预测准确度将大幅提升。总之，LLM的潜力远超其直接表达的能力。",
    "title_cn": "自动社会科学：语言模型的双重角色——既是研究者也是研究对象",
    "tags": [
      "LLM应用",
      "社会科学",
      "因果推理"
    ]
  },
  {
    "title": "Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models",
    "submit_datetime": "2024年04月24日",
    "abstract": "The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. The technologies of interacting with data particularly have an important entanglement with LLMs as efficient and intuitive data interactions are paramount. In this paper, we present DB-GPT, a revolutionary and product-ready Python library that integrates LLMs into traditional data interaction tasks to enhance user experience and accessibility. DB-GPT is designed to understand data interaction tasks described by natural language and provide context-aware responses powered by LLMs, making it an indispensable tool for users ranging from novice to expert. Its system design supports deployment across local, distributed, and cloud environments. Beyond handling basic data interaction tasks like Text-to-SQL with LLMs, it can handle complex tasks like generative data analysis through a Multi-Agents framework and the Agentic Workflow Expression Language (AWEL). The Service-oriented Multi-model Management Framework (SMMF) ensures data privacy and security, enabling users to employ DB-GPT with private LLMs. Additionally, DB-GPT offers a series of product-ready features designed to enable users to integrate DB-GPT within their product environments easily. The code of DB-GPT is available at Github(https://github.com/eosphoros-ai/DB-GPT) which already has over 10.7k stars. Please install DB-GPT for your own usage with the instructions(https://github.com/eosphoros-ai/DB-GPT#install) and watch a 5-minute introduction video on Youtube(https://youtu.be/n_8RI1ENyl4) to further investigate DB-GPT.",
    "pdf_link": "https://arxiv.org/abs/2404.10209",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.10209v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10209/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10209v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10209/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10209v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10209/x3.png"
      }
    ],
    "abstract_cn": "近期大型语言模型（LLMs）的创新进展，预示着软件行业的多领域将迎来变革。在这一变革中，与数据互动的技术尤为关键，它需要既高效又易于操作。本文介绍了DB-GPT，这是一款创新且即用型的Python库，它将LLMs融入到日常的数据交互流程中，旨在提升用户互动体验和操作便捷性。DB-GPT能够理解用自然语言描述的数据交互任务，并借助LLMs提供充满上下文感知的响应，无论是新手还是资深用户都能从中获益。该库的系统架构支持本地、分布式及云端环境的部署。DB-GPT不仅能够处理Text-to-SQL等基础数据交互任务，还能通过多代理框架和代理工作流表达语言（AWEL）来执行复杂的生成性数据分析任务。此外，面向服务的多模型管理框架（SMMF）确保了数据的隐私与安全，让用户能够安心使用私人LLMs。DB-GPT还提供了一系列即用型特性，方便用户轻松将其集成到产品生态中。感兴趣的用户可以在Github上找到DB-GPT的代码（https://github.com/eosphoros-ai/DB-GPT），项目已获得超过10.7k的星标认可。按照提供的指南（https://github.com/eosphoros-ai/DB-GPT#install）安装DB-GPT，并通过Youtube上的5分钟介绍视频（https://youtu.be/n_8RI1ENyl4）深入了解这款工具。",
    "title_cn": "展示 DB-GPT：新一代数据交互系统，由先进的大型语言模型提供支持。",
    "tags": [
      "LLM应用",
      "软件工程",
      "数据科学"
    ]
  },
  {
    "title": "Cantor: Inspiring Multimodal Chain-of-Thought of MLLM",
    "submit_datetime": "2024年04月24日",
    "abstract": "With the advent of large language models(LLMs) enhanced by the chain-of-thought(CoT) methodology, visual reasoning problem is usually decomposed into manageable sub-tasks and tackled sequentially with various external tools. However, such a paradigm faces the challenge of the potential \"determining hallucinations\" in decision-making due to insufficient visual information and the limitation of low-level perception tools that fail to provide abstract summaries necessary for comprehensive reasoning. We argue that converging visual context acquisition and logical reasoning is pivotal for tackling visual reasoning tasks. This paper delves into the realm of multimodal CoT to solve intricate visual reasoning tasks with multimodal large language models(MLLMs) and their cognitive capability. To this end, we propose an innovative multimodal CoT framework, termed Cantor, characterized by a perception-decision architecture. Cantor first acts as a decision generator and integrates visual inputs to analyze the image and problem, ensuring a closer alignment with the actual context. Furthermore, Cantor leverages the advanced cognitive functions of MLLMs to perform as multifaceted experts for deriving higher-level information, enhancing the CoT generation process. Our extensive experiments demonstrate the efficacy of the proposed framework, showing significant improvements in multimodal CoT performance across two complex visual reasoning datasets, without necessitating fine-tuning or ground-truth rationales. Project Page: https://ggg0919.github.io/cantor/ .",
    "pdf_link": "https://arxiv.org/abs/2404.16033",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.16033v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.16033/x18.png"
      }
    ],
    "abstract_cn": "随着思维链（CoT）方法加持的大型语言模型（LLMs）的出现，视觉推理问题被细化为易于管理的子任务，并借助多种外部工具逐步攻克。然而，这种模式在决策过程中可能会遇到“确定性幻觉”的问题，这源于视觉信息的不足以及低级感知工具无法提供进行综合推理所需的抽象总结。本文强调，将视觉上下文获取与逻辑推理相结合，对于解决视觉推理任务至关重要。本研究深入探讨了利用多模态大型语言模型（MLLMs）及其认知能力来解决复杂视觉推理任务的多模态CoT领域。我们提出了一个创新的多模态CoT框架——Cantor，它采用感知-决策架构。Cantor首先作为决策生成器，整合视觉输入以分析图像和问题，确保与实际情境的紧密结合。此外，Cantor还利用MLLMs的高级认知功能，充当多面手专家，以获取更高层次的信息，从而提升CoT生成过程。我们广泛的实验验证了所提框架的有效性，显示出在两个复杂的视觉推理数据集上，多模态CoT性能有了显著提升，且无需进行微调或依赖真实理由。项目页面：https://ggg0919.github.io/cantor/ 。",
    "title_cn": "Cantor：激发机器学习大型语言模型（MLLM）的多模态思维链",
    "tags": [
      "分类：LLM应用",
      "视觉推理",
      "人工智能"
    ]
  },
  {
    "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
    "submit_datetime": "2024年04月24日",
    "abstract": "Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.",
    "pdf_link": "https://arxiv.org/abs/2404.16006",
    "graphs": [],
    "abstract_cn": "大型视觉-语言模型（LVLMs）在多模态应用领域取得突破性进展，尤其在视觉对话和具身导航等通用任务中表现卓越。但现行的多模态评估标准仅覆盖了有限的多模态任务，主要测试基础能力，未能全面反映LVLMs的发展水平。本研究提出了MMT-Bench，这是一个全新的全面基准测试，用以评估LVLMs在一系列需要专业知识和深入视觉识别、定位、推理及规划能力的大规模多模态任务上的表现。MMT-Bench精心挑选了31,325道多项选择视觉问题，这些问题源自包括车辆驾驶和具身导航在内的多种多模态场景，覆盖了32个核心元任务和162个子任务。其广泛的任务覆盖范围使得MMT-Bench能够通过任务地图对LVLMs进行评估，帮助发现领域内和领域外的任务。对包括GPT-4V、GeminiProVision和InternVL-Chat在内的30个LVLMs的评估结果显示，MMT-Bench提出了重大挑战。我们期望MMT-Bench能够激发社区开发下一代多模态基础模型，以实现更广泛的多模态智能目标。",
    "title_cn": "MMT-Bench 是一个全面的多模态基准测试平台，旨在评估大型视觉-语言模型在实现多任务人工通用智能（AGI）方面的性能。",
    "tags": [
      "LLM应用",
      "多模态学习",
      "人工智能评估"
    ]
  },
  {
    "title": "Leveraging Large Language Models for Multimodal Search",
    "submit_datetime": "2024年04月24日",
    "abstract": "Multimodal search has become increasingly important in providing users with a natural and effective way to ex-press their search intentions. Images offer fine-grained details of the desired products, while text allows for easily incorporating search modifications. However, some existing multimodal search systems are unreliable and fail to address simple queries. The problem becomes harder with the large variability of natural language text queries, which may contain ambiguous, implicit, and irrelevant in-formation. Addressing these issues may require systems with enhanced matching capabilities, reasoning abilities, and context-aware query parsing and rewriting. This paper introduces a novel multimodal search model that achieves a new performance milestone on the Fashion200K dataset. Additionally, we propose a novel search interface integrating Large Language Models (LLMs) to facilitate natural language interaction. This interface routes queries to search systems while conversationally engaging with users and considering previous searches. When coupled with our multimodal search model, it heralds a new era of shopping assistants capable of offering human-like interaction and enhancing the overall search experience.",
    "pdf_link": "https://arxiv.org/abs/2404.15790",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15790v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15790/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15790v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15790/success.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15790v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15790/failures.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15790v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15790/demo_example.png"
      }
    ],
    "abstract_cn": "多模态搜索正日益成为用户表达搜索意图的自然而有效手段。图片能展示产品细节，文本便于搜索调整。但现有多模态搜索系统在处理简单查询时表现不稳定，且面对多样化的自然语言查询，这些查询可能含糊、隐晦或包含无关内容，系统面临更大挑战。要解决这些问题，系统需具备更强大的匹配、推理能力和能够感知上下文的查询解析及重写功能。本文提出的新型多模态搜索模型在Fashion200K数据集上创下了新的性能记录。我们还设计了一种新颖的搜索界面，整合了大型语言模型（LLMs），以便更自然地与用户互动。该界面在与用户对话的同时，会考虑用户之前的搜索记录，并将查询引导至搜索系统。结合我们的多模态搜索模型，它预示着购物助手将迈入一个新时代，能够提供更人性化的互动和更优的搜索体验。",
    "title_cn": "通过大型语言模型实现多模态搜索的潜力挖掘。",
    "tags": [
      "LLM应用",
      "电子商务",
      "搜索技术"
    ]
  },
  {
    "title": "Describe-then-Reason: Improving Multimodal Mathematical Reasoning through Visual Comprehension Training",
    "submit_datetime": "2024年04月24日",
    "abstract": "Open-source multimodal large language models (MLLMs) excel in various tasks involving textual and visual inputs but still struggle with complex multimodal mathematical reasoning, lagging behind proprietary models like GPT-4V(ision) and Gemini-Pro. Although fine-tuning with intermediate steps (i.e., rationales) elicits some mathematical reasoning skills, the resulting models still fall short in visual comprehension due to inadequate visual-centric supervision, which leads to inaccurate interpretation of math figures. To address this issue, we propose a two-step training pipeline VCAR, which emphasizes the Visual Comprehension training in Addition to mathematical Reasoning learning. It first improves the visual comprehension ability of MLLMs through the visual description generation task, followed by another training step on generating rationales with the assistance of descriptions. Experimental results on two popular benchmarks demonstrate that VCAR substantially outperforms baseline methods solely relying on rationale supervision, especially on problems with high visual demands.",
    "pdf_link": "https://arxiv.org/abs/2404.14604",
    "graphs": [],
    "abstract_cn": "开源的多模态大型语言模型（MLLMs）在处理文本和视觉输入的任务上表现优异，但面对复杂的多模态数学推理挑战时，相较于 GPT-4V(ision) 和 Gemini-Pro 等专有模型，仍显不足。尽管通过细化中间步骤（理由）的微调能够提升一定的数学推理能力，但现有模型在视觉理解上仍有欠缺，主要是由于缺乏针对视觉的充分指导，这导致了对数学图表的解读不够精准。为应对这一挑战，我们设计了 VCAR，一个两阶段训练流程，特别强化了数学推理之外的视觉理解能力。该流程首先通过视觉描述生成任务增强 MLLMs 的视觉理解力，随后进行第二阶段训练，即在已有描述的基础上生成理由。在两个广泛认可的基准测试中，VCAR 的表现显著超过了仅依赖理由指导的基线方法，特别是在视觉需求较高的问题上。",
    "title_cn": "通过视觉理解训练提升多模态数学推理“描述然后推理”：这一方法通过增强视觉理解训练，显著提升了处理多模态数学问题推理的能力。",
    "tags": [
      "分类：LLM应用",
      "教育技术",
      "人工智能"
    ]
  },
  {
    "title": "Zero-Shot Character Identification and Speaker Prediction in Comics via Iterative Multimodal Fusion",
    "submit_datetime": "2024年04月24日",
    "abstract": "Recognizing characters and predicting speakers of dialogue are critical for comic processing tasks, such as voice generation or translation. However, because characters vary by comic title, supervised learning approaches like training character classifiers which require specific annotations for each comic title are infeasible. This motivates us to propose a novel zero-shot approach, allowing machines to identify characters and predict speaker names based solely on unannotated comic images. In spite of their importance in real-world applications, these task have largely remained unexplored due to challenges in story comprehension and multimodal integration. Recent large language models (LLMs) have shown great capability for text understanding and reasoning, while their application to multimodal content analysis is still an open problem. To address this problem, we propose an iterative multimodal framework, the first to employ multimodal information for both character identification and speaker prediction tasks. Our experiments demonstrate the effectiveness of the proposed framework, establishing a robust baseline for these tasks. Furthermore, since our method requires no training data or annotations, it can be used as-is on any comic series.",
    "pdf_link": "https://arxiv.org/abs/2404.13993",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/zeroshot_1.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/zeroshot_2.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.13993v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13993/zeroshot_3.jpg"
      }
    ],
    "abstract_cn": "漫画处理任务，如声音生成或翻译，对角色识别和对话中说话者的预测极为关键。然而，由于角色随漫画标题的不同而变化，传统的监督学习方法，如训练需要特定标注的角色分类器，变得不切实际。为此，我们提出了一种创新的零样本方法，使机器能够仅通过未标注的漫画图像来识别角色和预测说话者的名字。尽管这些任务在现实世界的应用中极为重要，但由于故事理解的挑战和多模态整合的复杂性，它们至今未被充分研究。最新的大型语言模型（LLMs）在文本理解和推理方面展现了强大的能力，但其在多模态内容分析上的应用仍然是一个待解决的问题。为应对这一挑战，我们设计了一个迭代的多模态框架，首次将多模态信息应用于角色识别和说话者预测。我们的实验显示，该框架不仅有效，而且为这些任务设定了一个坚实的基准。更重要的是，由于该方法无需训练数据或标注，它可以无缝应用于任何漫画系列。",
    "title_cn": "本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。",
    "tags": [
      "分类：LLM应用",
      "漫画处理",
      ""
    ]
  },
  {
    "title": "Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models",
    "submit_datetime": "2024年04月24日",
    "abstract": "Counterfactual reasoning, as a crucial manifestation of human intelligence, refers to making presuppositions based on established facts and extrapolating potential outcomes. Existing multimodal large language models (MLLMs) have exhibited impressive cognitive and reasoning capabilities, which have been examined across a wide range of Visual Question Answering (VQA) benchmarks. Nevertheless, how will existing MLLMs perform when faced with counterfactual questions? To answer this question, we first curate a novel \\textbf{C}ounter\\textbf{F}actual \\textbf{M}ulti\\textbf{M}odal reasoning benchmark, abbreviated as \\textbf{CFMM}, to systematically assess the counterfactual reasoning capabilities of MLLMs. Our CFMM comprises six challenging tasks, each including hundreds of carefully human-labeled counterfactual questions, to evaluate MLLM's counterfactual reasoning capabilities across diverse aspects. Through experiments, interestingly, we find that existing MLLMs prefer to believe what they see, but ignore the counterfactual presuppositions presented in the question, thereby leading to inaccurate responses. Furthermore, we evaluate a wide range of prevalent MLLMs on our proposed CFMM. The significant gap between their performance on our CFMM and that on several VQA benchmarks indicates that there is still considerable room for improvement in existing MLLMs toward approaching human-level intelligence. On the other hand, through boosting MLLMs performances on our CFMM in the future, potential avenues toward developing MLLMs with advanced intelligence can be explored.",
    "pdf_link": "https://arxiv.org/abs/2404.12966",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12966/intro.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12966/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12966/exps.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12966v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12966/x2.png"
      }
    ],
    "abstract_cn": "反事实推理是人类智慧的关键体现，它涉及基于确凿事实提出假设并推演可能的结果。目前，多模态大型语言模型（MLLMs）在视觉问答（VQA）的多项基准测试中已展现出卓越的认知和推理技能。但当这些模型遭遇反事实问题时，它们的表现又将如何？为解答这一疑问，我们首次构建了一个名为CFMM的反事实多模态推理基准，旨在全面评估MLLMs的反事实推理能力。CFMM包含六项挑战性任务，每个任务都涵盖了数百个精心设计的反事实问题，用以多角度评价MLLMs的推理技能。实验结果显示，现有MLLMs倾向于信赖直观所见，却忽视了问题中提出的反事实前提，这导致了反应的不精确性。此外，我们在CFMM上对多种主流MLLMs进行了评估，发现它们在CFMM上的表现与VQA基准测试有显著差异，这表明MLLMs在达到人类智能水平方面还有很大的提升空间。展望未来，通过提升MLLMs在CFMM上的表现，我们有望探索出发展更高级智能MLLMs的潜在路径。",
    "title_cn": "眼睛或有误导：评估多模态大型语言模型的反事实推理能力。",
    "tags": [
      "分类：LLM应用\n\n这篇论文讨论了多模态大型语言模型（MLLMs）在处理反事实问题时的表现，并构建了一个名为CFMM的反事实多模态推理基准来评估MLLMs的反事实推理能力。这表明论文关注的是大型语言模型（LLM）在特定应用场景（即反事实推理）下的表现和潜在改进，因此将其归类为LLM应用。",
      "人工智能",
      "视觉问答"
    ]
  },
  {
    "title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
    "submit_datetime": "2024年04月24日",
    "abstract": "We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the Blink tasks can be solved by humans \"within a blink\" (e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70% accuracy on average, Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception.",
    "pdf_link": "https://arxiv.org/abs/2404.12390",
    "graphs": [],
    "abstract_cn": "我们推出了 Blink，这是一项针对多模态语言模型（LLMs）的新基准测试，旨在测试其他评测中未涉及的核心视觉感知技能。这些任务，如相对深度估计、视觉匹配、法医侦查和多视角推理，人类都能在瞬间解决。然而，我们发现这些对视觉感知要求极高的任务对当前的多模态 LLMs 构成了显著挑战，因为它们不容易通过自然语言来处理。Blink 将 14 个经典计算机视觉任务转化为 3,807 道多项选择题，每题都配有一张或多张图片和视觉提示。尽管人类平均正确率高达 95.70%，但 Blink 对现有多模态 LLMs 来说却颇具挑战性：即便是表现最佳的 GPT-4V 和 Gemini，准确率也仅为 51.26% 和 45.72%，仅比随机猜测高出 13.17% 和 7.63%，这表明这些感知技能在最新的多模态 LLMs 中尚未“显现”。我们的分析还指出，专业的计算机视觉模型能更好地解决这些问题，这为未来的提升指明了可能的方向。我们相信 Blink 将激励业界共同努力，推动多模态 LLMs 的视觉感知能力向人类水平看齐。",
    "title_cn": "BLINK：虽然多模态的大型语言模型具备视觉识别能力，但它们却无法实现深层次的感知理解。",
    "tags": [
      "LLM应用",
      "计算机视觉",
      "人工智能"
    ]
  },
  {
    "title": "Attacks on Third-Party APIs of Large Language Models",
    "submit_datetime": "2024年04月24日",
    "abstract": "Large language model (LLM) services have recently begun offering a plugin ecosystem to interact with third-party API services. This innovation enhances the capabilities of LLMs, but it also introduces risks, as these plugins developed by various third parties cannot be easily trusted. This paper proposes a new attacking framework to examine security and safety vulnerabilities within LLM platforms that incorporate third-party services. Applying our framework specifically to widely used LLMs, we identify real-world malicious attacks across various domains on third-party APIs that can imperceptibly modify LLM outputs. The paper discusses the unique challenges posed by third-party API integration and offers strategic possibilities to improve the security and safety of LLM ecosystems moving forward. Our code is released at https://github.com/vk0812/Third-Party-Attacks-on-LLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.16891",
    "graphs": [],
    "abstract_cn": "近期，大型语言模型（LLM）服务推出了插件生态系统，以便与第三方API服务进行互动，这不仅扩展了LLM的功能，也带来了安全隐患，因为这些第三方插件的可信度难以保证。本研究提出了一种新的攻击框架，旨在探究整合了第三方服务的LLM平台的安全与安全隐患。通过将该框架应用于多个广泛使用的LLM，我们发现了多个领域中的第三方API所遭受的真实世界恶意攻击，这些攻击能够悄无声息地改变LLM的输出结果。文章深入讨论了第三方API整合所带来的独特挑战，并提出了提升LLM生态系统未来安全性与安全性的战略性建议。相关代码已在 https://github.com/vk0812/Third-Party-Attacks-on-LLMs 上公开。",
    "title_cn": "针对大型语言模型第三方接口的攻击",
    "tags": [
      "LLM应用",
      "网络安全",
      "人工智能"
    ]
  },
  {
    "title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
    "submit_datetime": "2024年04月23日",
    "abstract": "Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach.",
    "pdf_link": "https://arxiv.org/abs/2404.15406",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/resized_encyclopedic_27c590f5dfe2d909.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/resized_encyclopedic_27e011f811e7b62e.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/resized_encyclopedic_41b81647e136ee90.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/resized_infoseek_val_00000016.jpeg"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/resized_infoseek_val_00000048.jpeg"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/resized_infoseek_val_00000131.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/resized_infoseek_val_00000210.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/resized_encyclopedic_fa07f04eea2fdcf0.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.15406v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15406/resized_infoseek_val_00000169.jpg"
      }
    ],
    "abstract_cn": "多模态大型语言模型（LLM）作为LLM的进阶形态，拓展了其功能，使其能够超越单一文本模态的限制。本研究致力于开发新型架构和视觉-语言适配器，专注于赋予模型以外部知识为基础回答问题的能力。我们提出的Wiki-LLaVA方法，旨在整合一个外部的多模态文档知识库，并通过分层检索流程进行访问。通过这种方法，能够从知识库中检索出相关文段，为LLM提供额外的上下文信息，从而提升对话生成的有效性和精确度。我们在视觉问答领域，针对包含外部数据的数据集进行了广泛的实验，验证了我们方法的有效性。",
    "title_cn": "Wiki-LLaVA：为多模态大型语言模型（LLM）引入分层检索增强生成技术",
    "tags": [
      "LLM应用",
      "视觉问答",
      "知识库"
    ]
  },
  {
    "title": "MedDr: Diagnosis-Guided Bootstrapping for Large-Scale Medical Vision-Language Learning",
    "submit_datetime": "2024年04月23日",
    "abstract": "The rapid advancement of large-scale vision-language models has showcased remarkable capabilities across various tasks. However, the lack of extensive and high-quality image-text data in medicine has greatly hindered the development of large-scale medical vision-language models. In this work, we present a diagnosis-guided bootstrapping strategy that exploits both image and label information to construct vision-language datasets. Based on the constructed dataset, we developed MedDr, a generalist foundation model for healthcare capable of handling diverse medical data modalities, including radiology, pathology, dermatology, retinography, and endoscopy. Moreover, during inference, we propose a simple but effective retrieval-augmented medical diagnosis strategy, which enhances the model's generalization ability. Extensive experiments on visual question answering, medical report generation, and medical image diagnosis demonstrate the superiority of our method.",
    "pdf_link": "https://arxiv.org/abs/2404.15127",
    "graphs": [],
    "abstract_cn": "大规模视觉-语言模型的迅猛进展在众多任务中展现了非凡的才能。但医学界高质量图文数据的匮乏，严重限制了这一模型在医疗领域的应用。本研究提出了一种新颖的诊断引导自举策略，充分利用图像与标签数据构建视觉-语言数据库。依托此数据库，我们研发了 MedDr，一个能够应对多种医疗数据形式的通用基础模型，涵盖了放射、病理、皮肤、视网膜和内窥镜等多个领域。在推理阶段，我们还引入了一种简洁高效的检索增强医疗诊断策略，进一步提升了模型的泛化性能。通过在视觉问答、医疗报告撰写和医疗图像诊断等任务上的广泛测试，证实了我们方法的优势。",
    "title_cn": "MedDr：一种为大规模医学视觉-语言学习设计的诊断引导自举策略",
    "tags": [
      "分类：LLM应用",
      "",
      "人工智能"
    ]
  },
  {
    "title": "From Matching to Generation: A Survey on Generative Information Retrieval",
    "submit_datetime": "2024年04月23日",
    "abstract": "Information Retrieval (IR) systems are crucial tools for users to access information, widely applied in scenarios like search engines, question answering, and recommendation systems. Traditional IR methods, based on similarity matching to return ranked lists of documents, have been reliable means of information acquisition, dominating the IR field for years. With the advancement of pre-trained language models, generative information retrieval (GenIR) has emerged as a novel paradigm, gaining increasing attention in recent years. Currently, research in GenIR can be categorized into two aspects: generative document retrieval (GR) and reliable response generation. GR leverages the generative model's parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. Reliable response generation, on the other hand, employs language models to directly generate the information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching, offering more flexibility, efficiency, and creativity, thus better meeting practical needs. This paper aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training, document identifier, incremental learning, downstream tasks adaptation, multi-modal GR and generative recommendation, as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, generating response with citations and personal information assistant. We also review the evaluation, challenges and future prospects in GenIR systems. This review aims to offer a comprehensive reference for researchers in the GenIR field, encouraging further development in this area.",
    "pdf_link": "https://arxiv.org/abs/2404.14851",
    "graphs": [],
    "abstract_cn": "信息检索系统对于用户获取信息至关重要，它们在搜索引擎、问答系统和推荐系统等场景中发挥着重要作用。传统上，这些系统依赖于相似性匹配技术，以提供有序的文档列表，这一方法多年来一直是信息检索的主流。然而，随着预训练语言模型的发展，生成式信息检索（GenIR）作为一种创新的检索范式，正逐渐受到业界的广泛关注。GenIR的研究主要分为两大方向：生成式文档检索（GR）和可靠的响应生成。GR通过利用生成模型的参数记忆文档内容，实现了无需显式索引即可直接生成相关文档标识符的检索方式。而可靠的响应生成则利用语言模型直接产出用户所需的信息，超越了传统IR在文档细节和相关性匹配上的局限，提供了更高的灵活性和效率。本文的目的是对GenIR领域的最新研究进展进行系统性的梳理。我们将概述GR在模型训练、文档标识、增量学习、任务适应、多模态检索和推荐系统等方面的进展，以及在知识记忆、知识增强、引用生成和个人助理等方面的可靠响应生成技术。此外，我们还将探讨GenIR系统的评估标准、面临的挑战及未来的发展方向。本篇综述的目标是为GenIR领域的研究者提供一个全面的参考框架，以促进该领域的持续进步。",
    "title_cn": "探索信息检索的演进：从匹配到生成的生成性信息检索综述",
    "tags": [
      "LLM应用",
      "信息检索",
      ""
    ]
  },
  {
    "title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
    "submit_datetime": "2024年04月23日",
    "abstract": "A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various NLP and multi-mode tasks to answer users' arbitrary questions and specific-domain content generation. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing remaining challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graph (KG) based augmented retrieval, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.",
    "pdf_link": "https://arxiv.org/abs/2404.14809",
    "graphs": [],
    "abstract_cn": "图作为基本的数据模型，广泛用于描绘社会和自然界中的实体及其错综复杂的联系，如社交网络、交通系统、金融体系和生物医学网络。近期，大型语言模型（LLMs）在处理多样化的自然语言处理（NLP）和多模态任务方面展现出了卓越的泛化能力，能够响应用户的随机查询和特定领域的内容创作。相较于传统的图学习模型，LLMs在处理图任务时更具优势，它们避免了图学习模型的训练需求，同时降低了手动标注的成本。本篇综述深入探讨了LLM在图数据分析领域的研究进展，归纳了高级LLM模型解决的图分析任务，并指出了当前面临的挑战与未来的研究趋势。具体而言，我们聚焦于基于LLM的生成图分析（LLM-GGA）的三大核心问题：基于LLM的图查询处理（LLM-GQP）、基于LLM的图推理与学习（LLM-GIL），以及图与LLM结合的应用。LLM-GQP着重于图分析技术与LLM提示的融合，涵盖图理解及基于知识图谱的增强检索；LLM-GIL则专注于图上的学习和推理，包括图学习、图形态推理和图表示。我们归纳了LLM在处理各类图任务时采用的有效提示。此外，我们还总结了LLM模型的评估方法、基准数据集/任务，并深入剖析了LLM模型的优势与局限。同时，我们也探讨了LLM与图分析这一激动人心的跨学科研究领域的未解之谜及未来可能的研究方向。",
    "title_cn": "大型语言模型在生成图分析领域的研究综述：探讨查询、学习及应用",
    "tags": [
      "LLM应用",
      "图数据分析",
      ""
    ]
  },
  {
    "title": "Simulating Task-Oriented Dialogues with State Transition Graphs and Large Language Models",
    "submit_datetime": "2024年04月23日",
    "abstract": "This paper explores SynTOD, a new synthetic data generation approach for developing end-to-end Task-Oriented Dialogue (TOD) Systems capable of handling complex tasks such as intent classification, slot filling, conversational question-answering, and retrieval-augmented response generation, without relying on crowdsourcing or real-world data. SynTOD utilizes a state transition graph to define the desired behavior of a TOD system and generates diverse, structured conversations through random walks and response simulation using large language models (LLMs). In our experiments, using graph-guided response simulations leads to significant improvements in intent classification, slot filling and response relevance compared to naive single-prompt simulated conversations. We also investigate the end-to-end TOD effectiveness of different base and instruction-tuned LLMs, with and without the constructed synthetic conversations. Finally, we explore how various LLMs can evaluate responses in a TOD system and how well they are correlated with human judgments. Our findings pave the path towards quick development and evaluation of domain-specific TOD systems. We release our datasets, models, and code for research purposes.",
    "pdf_link": "https://arxiv.org/abs/2404.14772",
    "graphs": [],
    "abstract_cn": "本研究介绍了 SynTOD，这是一种创新的合成数据生成技术，旨在培育能够执行复杂任务的全链条面向任务对话（TOD）系统。这些任务包括意图识别、槽位填充、对话式问答以及增强检索的响应生成，且整个过程无需依赖众包或现实世界数据。SynTOD 通过状态转移图明确 TOD 系统的目标行为，并通过随机游走结合大型语言模型（LLMs）的响应模拟，创造出多样化且结构化的对话。实验结果显示，采用图引导的响应模拟方法，在意图识别、槽位填充和响应相关性方面，相较于传统的单一提示模拟对话，有显著的性能提升。此外，我们还深入探讨了不同基础型和指令调优型 LLMs 在 TOD 系统中的应用效果，无论是否结合了合成对话数据。最终，我们评估了各种 LLMs 在 TOD 系统中的响应评估能力，并分析了它们与人类评判的一致性。这些发现为快速开发和评估特定领域的 TOD 系统提供了新思路。为了促进研究，我们公开了数据集、模型和代码。",
    "title_cn": "通过状态转移图和大型语言模型来模拟以任务为导向的对话流程。",
    "tags": [
      "分类：RAG\n\n这篇论文介绍了一种名为SynTOD的合成数据生成技术，用于培育能够执行复杂任务的全链条面向任务对话（TOD）系统。这项技术涉及到意图识别、槽位填充、对话式问答以及增强检索的响应生成。论文中提到了大型语言模型（LLMs）在生成多样化且结构化的对话中的作用，以及对不同基础型和指令调优型LLMs在TOD系统中的应用效果的探讨。这些内容表明，这篇论文主要关注的是利用合成数据和大型语言模型来生成和评估对话系统，因此它属于RAG（Retrieval-Augmented Generation）的范畴。",
      "对话系统",
      "数据生成"
    ]
  },
  {
    "title": "Retrieval Augmented Generation for Domain-specific Question Answering",
    "submit_datetime": "2024年04月23日",
    "abstract": "Question answering (QA) has become an important application in the advanced development of large language models. General pre-trained large language models for question-answering are not trained to properly understand the knowledge or terminology for a specific domain, such as finance, healthcare, education, and customer service for a product. To better cater to domain-specific understanding, we build an in-house question-answering system for Adobe products. We propose a novel framework to compile a large question-answer database and develop the approach for retrieval-aware finetuning of a Large Language model. We showcase that fine-tuning the retriever leads to major improvements in the final generation. Our overall approach reduces hallucinations during generation while keeping in context the latest retrieval information for contextual grounding.",
    "pdf_link": "https://arxiv.org/abs/2404.14760",
    "graphs": [],
    "abstract_cn": "问答系统在大型语言模型的高级应用中占据了重要地位。然而，通用的预训练模型往往缺乏对特定领域，如金融、医疗、教育或产品客户服务的专业理解。为了提升对特定领域的精准把握，我们为 Adobe 产品专门打造了一套内部问答系统。本系统采用了创新框架，旨在构建庞大的问答数据库，并针对大型语言模型进行了检索感知的微调训练。实践证明，对检索器进行微调能显著提升最终输出的质量。我们的策略在确保生成内容的准确性的同时，有效减少了生成过程中可能出现的误导性信息。",
    "title_cn": "为特定领域问答设计的检索增强生成技术",
    "tags": [
      "LLM应用",
      "客户服务",
      "问答系统"
    ]
  },
  {
    "title": "Retrieval-Augmented Audio Deepfake Detection",
    "submit_datetime": "2024年04月23日",
    "abstract": "With recent advances in speech synthesis including text-to-speech (TTS) and voice conversion (VC) systems enabling the generation of ultra-realistic audio deepfakes, there is growing concern about their potential misuse. However, most deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a single model, resulting in performance bottlenecks and transparency issues. Inspired by retrieval-augmented generation (RAG), we propose a retrieval-augmented detection (RAD) framework that augments test samples with similar retrieved samples for enhanced detection. We also extend the multi-fusion attentive classifier to integrate it with our proposed RAD framework. Extensive experiments show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets. Further sample analysis indicates that the retriever consistently retrieves samples mostly from the same speaker with acoustic characteristics highly consistent with the query audio, thereby improving detection performance.",
    "pdf_link": "https://arxiv.org/abs/2404.13892",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13892v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13892/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13892v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13892/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13892v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13892/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13892v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13892/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13892v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13892/x5.png"
      }
    ],
    "abstract_cn": "随着文本到语音和声音转换技术的进步，我们能够创造出极为逼真的音频深度伪造，这引发了对其潜在滥用的担忧。目前，大多数深度伪造检测方法主要依赖单一模型的模糊知识，这不仅限制了检测性能，也带来了透明度的挑战。借鉴检索增强生成的理念，我们提出了一种新颖的检索增强检测框架，通过引入相似的检索样本来提升检测效果。此外，我们还对多融合注意力分类器进行了扩展，以适配我们的检测框架。大量实验证明，我们的框架在多个数据集上均优于现有方法，尤其在ASVspoof 2021的深度伪造数据集上取得了突破性成果，并在2019年和2021年的LA数据集上也展现了强劲的竞争力。深入分析显示，检索器能够精准地找到与查询音频声学特征高度相似的同一说话人的样本，这显著提升了检测的准确性。",
    "title_cn": "增强检索音频深度伪造检测",
    "tags": [
      "分类：LLM应用\n\n这篇论文提出了一种新颖的检索增强检测框架，用于检测音频深度伪造。它通过引入相似的检索样本来提升检测效果，并扩展了多融合注意力分类器以适配检测框架。这项工作主要关注于应用大型语言模型（LLM）的技术来解决实际问题，即提高深度伪造检测的性能。因此，它应该被归类为LLM应用。",
      "音频处理",
      ""
    ]
  },
  {
    "title": "BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis",
    "submit_datetime": "2024年04月23日",
    "abstract": "This paper presents BattleAgent, an emulation system that combines the Large Vision-Language Model and Multi-agent System. This novel system aims to simulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time. It emulates both the decision-making processes of leaders and the viewpoints of ordinary participants, such as soldiers. The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes. It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging. These components collaborate to recreate historical events in a lively and comprehensive manner while offering insights into the thoughts and feelings of individuals from diverse viewpoints. The technological foundations of BattleAgent establish detailed and immersive settings for historical battles, enabling individual agents to partake in, observe, and dynamically respond to evolving battle scenarios. This methodology holds the potential to substantially deepen our understanding of historical events, particularly through individual accounts. Such initiatives can also aid historical research, as conventional historical narratives often lack documentation and prioritize the perspectives of decision-makers, thereby overlooking the experiences of ordinary individuals. BattelAgent illustrates AI's potential to revitalize the human aspect in crucial social events, thereby fostering a more nuanced collective understanding and driving the progressive development of human society.",
    "pdf_link": "https://arxiv.org/abs/2404.15532",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/battle_map.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/Battle_of_crecy_froissart.jpeg"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/general_process.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/observation.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/agent_structure.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/battlefield_interaction.jpeg"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/crecy.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/Agincourt.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/Poitiers.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/Falkirk.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/battle_field.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15532v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15532/x1.png"
      }
    ],
    "abstract_cn": "本文推出了一款名为 BattleAgent 的仿真系统，它融合了大型视觉-语言模型与多智能体系统。该系统致力于模拟多个智能体之间的复杂互动，以及智能体与其环境之间的长期交互。它不仅模拟了领导者的决策机制，也反映了普通参与者如士兵的看法。该仿真系统精细地展现了智能体的能力，实现了智能体与环境之间的多模态互动。系统还设计了可定制的智能体结构，以适应不同的战斗需求，如侦察和挖掘战壕等。这些元素共同作用，生动全面地再现了历史事件，同时深入探讨了不同角色的内心世界。BattleAgent 的技术基础为历史战役提供了一个详尽而沉浸式的背景，让每个智能体都能参与、观察并灵活应对战斗的演变。这种方法有望显著提升我们对历史事件的认识，尤其是通过个人经历的视角。此外，它还能辅助历史研究，因为传统叙事往往忽略了普通人的经历，而 BattleAgent 则利用人工智能的力量，恢复了社会重大事件中的人文关怀，促进了对人类社会更深层次的理解与发展。",
    "title_cn": "BattleAgent：通过在历史战场上进行多模态动态模拟，为历史研究提供补充分析。",
    "tags": [
      "Agent",
      "历史研究",
      "人工智能"
    ]
  },
  {
    "title": "IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection & Correction Task On the Shoulders of Medical Agents",
    "submit_datetime": "2024年04月23日",
    "abstract": "In natural language processing applied to the clinical domain, utilizing large language models has emerged as a promising avenue for error detection and correction on clinical notes, a knowledge-intensive task for which annotated data is scarce. This paper presents MedReAct'N'MedReFlex, which leverages a suite of four LLM-based medical agents. The MedReAct agent initiates the process by observing, analyzing, and taking action, generating trajectories to guide the search to target a potential error in the clinical notes. Subsequently, the MedEval agent employs five evaluators to assess the targeted error and the proposed correction. In cases where MedReAct's actions prove insufficient, the MedReFlex agent intervenes, engaging in reflective analysis and proposing alternative strategies. Finally, the MedFinalParser agent formats the final output, preserving the original style while ensuring the integrity of the error correction process. One core component of our method is our RAG pipeline based on our ClinicalCorp corpora. Among other well-known sources containing clinical guidelines and information, we preprocess and release the open-source MedWiki dataset for clinical RAG application. Our results demonstrate the central role of our RAG approach with ClinicalCorp leveraged through the MedReAct'N'MedReFlex framework. It achieved the ninth rank on the MEDIQA-CORR 2024 final leaderboard.",
    "pdf_link": "https://arxiv.org/abs/2404.15488",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15488v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15488/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15488v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15488/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15488v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15488/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15488v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15488/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15488v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15488/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15488v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15488/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15488v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15488/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15488v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15488/x8.png"
      }
    ],
    "abstract_cn": "在临床领域的自然语言处理应用中，借助大型语言模型来检测和修正临床记录中的错误，这一方法显示出巨大的潜力，尤其是在标注数据难以获得的复杂任务中。本研究提出了一个创新的系统——MedReAct'N'MedReFlex，它整合了四个基于大型语言模型的医疗智能体。MedReAct 智能体首先通过观察、分析和执行操作来启动错误检测流程，生成路径以定位临床记录中的潜在错误。接着，MedEval 智能体动用五个评估器对目标错误及其修正建议进行评估。当 MedReAct 的修正措施不足时，MedReFlex 智能体会介入，进行深入分析并提出新的策略。最终，MedFinalParser 智能体负责将修正后的内容格式化输出，既保持了原文的风格，又确保了错误修正过程的准确性。我们的方法还包括一个基于 ClinicalCorp 语料库的 RAG 流程，这是我们的核心组成部分。我们还预处理并开源了 MedWiki 数据集，以支持临床 RAG 应用。实验结果显示，我们的 RAG 方法在 MedReAct'N'MedReFlex 框架下，通过 ClinicalCorp 数据集的加持，发挥了关键作用，在 MEDIQA-CORR 2024 的最终排行榜上荣获第九名。",
    "title_cn": "IryoNLP 亮相 2024 年 MEDIQA-CORR，携手医疗代理共同攻克医疗错误检测与纠正的挑战。",
    "tags": [
      "Agent",
      "",
      ""
    ]
  },
  {
    "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
    "submit_datetime": "2024年04月23日",
    "abstract": "We study interactive learning of language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER achieves the lowest edit distance cost and learns preferences that show significant similarity to the ground truth preferences",
    "pdf_link": "https://arxiv.org/abs/2404.15269",
    "graphs": [],
    "abstract_cn": "本研究探讨了语言代理如何通过用户对其输出的编辑进行交互式学习。在写作助手等典型应用场景中，用户与语言代理互动，根据特定上下文生成回复，并可根据自己的内在偏好对代理的回复进行编辑，以实现个性化并提升准确性。这种编辑反馈自然产生，有助于提升代理与用户偏好的契合度，同时降低用户随时间增加的编辑成本。我们提出了一个名为Prelude的学习框架，它能够基于历史编辑数据推断用户的潜在偏好，并据此制定提示策略，引导未来的回复生成，避免了对代理进行成本高昂且难以扩展的微调，同时可能影响其在其他任务上的表现。此外，通过学习描述性偏好，增强了模型的可解释性，使用户能够查看并调整已学习到的偏好。然而，用户偏好复杂多变，受上下文影响，学习起来颇具挑战。为此，我们设计了一个简单高效的算法CIPHER，它利用大型语言模型（LLM）分析用户编辑，从而推断出特定上下文下的用户偏好。CIPHER在未来能够从历史中检索与当前上下文最相似的k个偏好，并综合这些偏好以生成回复。我们还构建了摘要和电子邮件写作两个交互式环境，通过GPT-4模拟用户进行评估。在比较中，CIPHER不仅在两个任务上都实现了最低的编辑成本，而且学习到的偏好与真实偏好有显著的一致性。",
    "title_cn": "通过用户编辑学习用户的潜在偏好，以此来校准大型语言模型（LLM）代理的行为。",
    "tags": [
      "Agent",
      "写作辅助",
      "用户偏好学习"
    ]
  },
  {
    "title": "CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based Reasoning",
    "submit_datetime": "2024年04月23日",
    "abstract": "Large Language Models (LLMs) and multi-agent systems have shown impressive capabilities in natural language tasks but face challenges in clinical trial applications, primarily due to limited access to external knowledge. Recognizing the potential of advanced clinical trial tools that aggregate and predict based on the latest medical data, we propose an integrated solution to enhance their accessibility and utility. We introduce Clinical Agent System (CT-Agent), a Clinical multi-agent system designed for clinical trial tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct reasoning technology. This integration not only boosts LLM performance in clinical contexts but also introduces novel functionalities. Our system autonomously manages the entire clinical trial process, demonstrating significant efficiency improvements in our evaluations, which include both computational benchmarks and expert feedback.",
    "pdf_link": "https://arxiv.org/abs/2404.14777",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）与多智能体系统在处理自然语言任务时表现出色，但在临床试验应用上却遭遇难题，这主要是因为它们难以获取外部知识。为了克服这一限制，我们提出了一种集成方案，旨在提升临床试验工具的可用性和效能。我们设计了临床代理系统（CT-Agent），这是一个专为临床试验任务打造的多智能体系统，它整合了GPT-4、多智能体架构、LEAST-TO-MOST策略以及ReAct推理技术。这一集成方案不仅极大提升了LLM在临床领域的性能，还带来了创新功能。我们的系统能够自动执行整个临床试验流程，在计算基准测试和专家评审中均展现出显著的效率提升。",
    "title_cn": "CT-Agent：融合大型语言模型推理能力的临床试验多智能体系统",
    "tags": [
      "Agent",
      "临床试验",
      "人工智能"
    ]
  },
  {
    "title": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering",
    "submit_datetime": "2024年04月23日",
    "abstract": "To address the issue of insufficient knowledge and the tendency to generate hallucination in Large Language Models (LLMs), numerous studies have endeavored to integrate LLMs with Knowledge Graphs (KGs). However, all these methods are evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where the factual triples involved in each question are entirely covered by the given KG. In this situation, LLM mainly acts as an agent to find answer entities by exploring the KG, rather than effectively integrating internal and external knowledge sources. However, in real-world scenarios, KGs are often incomplete to cover all the knowledge required to answer questions. To simulate real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, in this paper, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the given KG doesn't include all the factual triples involved in each question. To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG) that can generate new factual triples while exploring on KGs. Specifically, we propose a selecting-generating-answering framework, which not only treat the LLM as an agent to explore on KGs, but also treat it as a KG to generate new facts based on the explored subgraph and its inherent knowledge. Experimental results on two datasets demonstrate that our GoG can solve IKGQA to a certain extent, while almost all previous methods cannot perform well on IKGQA.",
    "pdf_link": "https://arxiv.org/abs/2404.14741",
    "graphs": [],
    "abstract_cn": "为应对大型语言模型（LLMs）知识匮乏和易于产生幻觉的问题，众多研究致力于将LLMs与知识图谱（KGs）相结合。但这些方法多在完备的知识图谱问答（KGQA）环境中进行评估，即每个问题所涉及的事实三元组均被知识图谱全面覆盖。在此环境下，LLMs主要扮演搜索知识图谱以发现答案实体的角色，而非真正融合内外部知识。然而现实情况中，知识图谱往往无法全面覆盖所有问答所需的知识。本文旨在模拟现实世界环境，评估LLMs整合内外部知识的能力，提出了在不完全知识图谱（IKGQA）下进行问答的LLMs应用。在IKGQA中，所给知识图谱并未包含每个问题涉及的所有事实三元组。为此，我们提出了一种无需训练的方法——图上生成（GoG），它能够在探索知识图谱的同时产生新的事实三元组。具体而言，我们构建了一个选择-生成-回答框架，将LLMs视作探索知识图谱的智能体，同时也将其作为能够基于探索到的子图及其内在知识生成新事实的知识图谱。实验结果显示，我们的GoG方法在IKGQA任务上取得了一定成效，而大多数现有方法在此类任务上表现不佳。",
    "title_cn": "在不完整的知识图谱问答任务中，我们将大型语言模型（LLM）既视为智能代理，也视为知识库，以生成答案。",
    "tags": [
      "LLM应用",
      "知识图谱",
      "问答系统"
    ]
  },
  {
    "title": "Large Language Models for Synthetic Participatory Planning of Synergistic Transportation Systems",
    "submit_datetime": "2024年04月23日",
    "abstract": "Unleashing the synergies of rapidly evolving mobility technologies in a multi-stakeholder landscape presents unique challenges and opportunities for addressing urban transportation problems. This paper introduces a novel synthetic participatory method, critically leveraging large language models (LLMs) to create digital avatars representing diverse stakeholders to plan shared automated electric mobility systems (SAEMS). These calibratable agents collaboratively identify objectives, envision and evaluate SAEMS alternatives, and strategize implementation under risks and constraints. The results of a Montreal case study indicate that a structured and parameterized workflow provides outputs with high controllability and comprehensiveness on an SAEMS plan than generated using a single LLM-enabled expert agent. Consequently, the approach provides a promising avenue for cost-efficiently improving the inclusivity and interpretability of multi-objective transportation planning, suggesting a paradigm shift in how we envision and strategize for sustainable and equitable transportation systems.",
    "pdf_link": "https://arxiv.org/abs/2404.12317",
    "graphs": [],
    "abstract_cn": "在多元利益主体的格局中，迅速发展的移动性技术为我们解决城市交通问题提供了特殊的挑战与机遇。本论文提出了一种创新的综合参与式方法，充分利用大型语言模型（LLMs）打造代表多样利益方的数字形象，共同规划共享的自动化电动移动系统（SAEMS）。这些可调节的智能体协同确定目标，构想和评估SAEMS的不同方案，并在风险与限制下制定实施策略。通过对蒙特利尔案例的研究，我们发现，与传统的单一LLM驱动的专家智能体相比，一个结构化且可参数化的流程能够更可控、更全面地输出SAEMS计划。这一发现预示着在提升多目标交通规划的包容性与可解释性方面，我们有了一条成本效益高的新途径，这也可能引领我们对可持续和公平交通系统规划的全新思考方式。",
    "title_cn": "大型语言模型在合成协同交通系统的参与式规划中的应用。",
    "tags": [
      "Agent",
      "城市交通规划",
      "自动化电动移动系统"
    ]
  },
  {
    "title": "ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction",
    "submit_datetime": "2024年04月23日",
    "abstract": "Existing datasets for attribute value extraction (AVE) predominantly focus on explicit attribute values while neglecting the implicit ones, lack product images, are often not publicly available, and lack an in-depth human inspection across diverse domains. To address these limitations, we present ImplicitAVE, the first, publicly available multimodal dataset for implicit attribute value extraction. ImplicitAVE, sourced from the MAVE dataset, is carefully curated and expanded to include implicit AVE and multimodality, resulting in a refined dataset of 68k training and 1.6k testing data across five domains. We also explore the application of multimodal large language models (MLLMs) to implicit AVE, establishing a comprehensive benchmark for MLLMs on the ImplicitAVE dataset. Six recent MLLMs with eleven variants are evaluated across diverse settings, revealing that implicit value extraction remains a challenging task for MLLMs. The contributions of this work include the development and release of ImplicitAVE, and the exploration and benchmarking of various MLLMs for implicit AVE, providing valuable insights and potential future research directions. Dataset and code are available at https://github.com/HenryPengZou/ImplicitAVE",
    "pdf_link": "https://arxiv.org/abs/2404.15592",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15592v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15592/x13.png"
      }
    ],
    "abstract_cn": "目前针对属性值提取（AVE）的数据集多集中于明显属性值，而对隐性属性值视而不见，往往缺乏产品图像，不对外开放，且在多个领域内缺少详尽的人工审查。为克服这些不足，我们推出了ImplicitAVE——首个公开的多模态隐式属性值提取数据集。ImplicitAVE基于MAVE数据集，经过精心筛选和扩充，纳入了隐性AVE和多模态特性，形成了一个优化后的数据集，包含68,000条训练样本和1,600条测试样本，覆盖五个不同的领域。我们还研究了多模态大型语言模型（MLLMs）在隐式AVE中的应用，并为MLLMs在ImplicitAVE数据集上设立了一个全面的评估标准。对六种最新的MLLMs及其十一种变体进行了广泛设置的评估，发现隐性值提取对于MLLMs而言仍是一大挑战。本研究的贡献包括开发并发布了ImplicitAVE数据集，以及对多种MLLMs在隐式AVE任务中的探索和性能评估，为未来研究提供了宝贵的洞见和可能的研究方向。相关数据集和代码已在 https://github.com/HenryPengZou/ImplicitAVE 上公开。",
    "title_cn": "ImplicitAVE 推出了一个开源数据集，并为隐式属性值提取设立了多模态大型语言模型（LLM）的基准测试。",
    "tags": [
      "LLM应用",
      "属性值提取",
      "多模态学习"
    ]
  },
  {
    "title": "Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation",
    "submit_datetime": "2024年04月23日",
    "abstract": "Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects. We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer. To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators. Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models.",
    "pdf_link": "https://arxiv.org/abs/2404.15100",
    "graphs": [],
    "abstract_cn": "最新研究表明，通过利用人类偏好数据集来优化文本到图像的生成模型，可以显著提升生成图像与文本描述的匹配度。然而，目前构建这样的数据集要么成本过高，要么偏好维度的多样性不足，这限制了其在开源文本到图像生成模型中的指令调优应用，并影响了进一步的研究。为了解决这些问题，我们运用多模态大型语言模型，开发了 VisionPrefer——一个捕捉多元偏好维度的高质量、细致入微的偏好数据集。我们汇总了 AI 注释者在四个方面——遵循提示、审美、忠实度和无害性——的反馈，构建了 VisionPrefer。为了证明其有效性，我们基于该数据集训练了一个奖励模型 VP-Score，用以指导文本到图像生成模型的训练，其偏好预测的准确度与人类注释者不相上下。此外，我们采用两种强化学习方法对生成模型进行监督微调，并使用 VisionPrefer 进行性能评估，广泛的实验结果显示，VisionPrefer 在多样化维度上显著提升了文本与图像的一致性，比如审美，并在不同图像分布上比以往的人类偏好指标更具泛化能力。更重要的是，VisionPrefer 的研究指出，将 AI 生成的合成数据作为监督信号，是提高视觉生成模型与人类偏好一致性的一个充满希望的方向。",
    "title_cn": "多模态大型语言模型，作为文本到图像生成任务中的人类对齐注释器，展现出卓越的性能。",
    "tags": [
      "LLM应用",
      "图像生成",
      "人工智能"
    ]
  },
  {
    "title": "DesignProbe: A Graphic Design Benchmark for Multimodal Large Language Models",
    "submit_datetime": "2024年04月23日",
    "abstract": "A well-executed graphic design typically achieves harmony in two levels, from the fine-grained design elements (color, font and layout) to the overall design. This complexity makes the comprehension of graphic design challenging, for it needs the capability to both recognize the design elements and understand the design. With the rapid development of Multimodal Large Language Models (MLLMs), we establish the DesignProbe, a benchmark to investigate the capability of MLLMs in design. Our benchmark includes eight tasks in total, across both the fine-grained element level and the overall design level. At design element level, we consider both the attribute recognition and semantic understanding tasks. At overall design level, we include style and metaphor. 9 MLLMs are tested and we apply GPT-4 as evaluator. Besides, further experiments indicates that refining prompts can enhance the performance of MLLMs. We first rewrite the prompts by different LLMs and found increased performances appear in those who self-refined by their own LLMs. We then add extra task knowledge in two different ways (text descriptions and image examples), finding that adding images boost much more performance over texts.",
    "pdf_link": "https://arxiv.org/abs/2404.14801",
    "graphs": [],
    "abstract_cn": "精心设计的图形设计在细节元素（如色彩、字体和版式）和整体布局上通常达到和谐统一。这种设计复杂性的理解颇为不易，它要求既要识别设计元素，又要理解设计的深层含义。随着多模态大型语言模型（MLLMs）的迅猛发展，我们创建了DesignProbe这一基准测试，用以探究MLLMs在设计领域的能力。该基准测试包含八个任务，覆盖了从设计元素的细节到整体设计的宏观层面。在设计元素层面，我们既关注属性识别也注重语义理解。在整体设计层面，我们考量了风格和隐喻的使用。我们对9种MLLMs进行了测试，并采用GPT-4作为评判标准。此外，进一步的实验发现，优化提示可以显著提升MLLMs的表现。我们尝试用不同的LLMs重构提示，发现那些能够自我优化提示的模型性能有所提升。我们还尝试了两种添加额外任务知识的方法——文本描述和图像示例，结果表明，图像示例对性能的提升远超过文本描述。",
    "title_cn": "DesignProbe：为多模态大型语言模型量身打造的图形设计性能评估工具。",
    "tags": [
      "LLM应用",
      "图形设计",
      "人工智能"
    ]
  },
  {
    "title": "Single-temporal Supervised Remote Change Detection for Domain Generalization",
    "submit_datetime": "2024年04月23日",
    "abstract": "Change detection is widely applied in remote sensing image analysis. Existing methods require training models separately for each dataset, which leads to poor domain generalization. Moreover, these methods rely heavily on large amounts of high-quality pair-labelled data for training, which is expensive and impractical. In this paper, we propose a multimodal contrastive learning (ChangeCLIP) based on visual-language pre-training for change detection domain generalization. Additionally, we propose a dynamic context optimization for prompt learning. Meanwhile, to address the data dependency issue of existing methods, we introduce a single-temporal and controllable AI-generated training strategy (SAIN). This allows us to train the model using a large number of single-temporal images without image pairs in the real world, achieving excellent generalization. Extensive experiments on series of real change detection datasets validate the superiority and strong generalization of ChangeCLIP, outperforming state-of-the-art change detection methods. Code will be available.",
    "pdf_link": "https://arxiv.org/abs/2404.11326",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11326v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11326/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11326v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11326/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11326v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11326/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11326v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11326/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11326v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11326/x5.png"
      }
    ],
    "abstract_cn": "变化检测技术在遥感图像分析领域扮演着重要角色。然而，传统方法需针对不同数据集分别训练模型，这限制了其泛化能力。它们还过分依赖大量高质量配对标记数据，这不仅成本高昂，也难以实现。本文提出了一种新颖的多模态对比学习方法ChangeCLIP，它基于视觉-语言预训练，旨在提升变化检测任务的领域泛化能力。我们还引入了动态上下文优化技术以增强提示学习的效果。为了克服现有方法对数据的依赖性，我们开发了一种单时相可控的人工智能生成训练策略SAIN，允许模型在无需成对图像的情况下，使用大量单时相图像进行训练，从而获得卓越的泛化性能。通过在多个真实变化检测数据集上的实验，我们证明了ChangeCLIP的优越性和强大的泛化能力，其性能超越了当前最先进方法。相关代码将公开提供。",
    "title_cn": "单时相的遥感变化检测技术，用于实现领域的泛化应用。",
    "tags": [
      "分类：Agent",
      "遥感图像分析",
      "人工智能"
    ]
  },
  {
    "title": "Review of Data-centric Time Series Analysis from Sample, Feature, and Period",
    "submit_datetime": "2024年04月23日",
    "abstract": "Data is essential to performing time series analysis utilizing machine learning approaches, whether for classic models or today's large language models. A good time-series dataset is advantageous for the model's accuracy, robustness, and convergence, as well as task outcomes and costs. The emergence of data-centric AI represents a shift in the landscape from model refinement to prioritizing data quality. Even though time-series data processing methods frequently come up in a wide range of research fields, it hasn't been well investigated as a specific topic. To fill the gap, in this paper, we systematically review different data-centric methods in time series analysis, covering a wide range of research topics. Based on the time-series data characteristics at sample, feature, and period, we propose a taxonomy for the reviewed data selection methods. In addition to discussing and summarizing their characteristics, benefits, and drawbacks targeting time-series data, we also introduce the challenges and opportunities by proposing recommendations, open problems, and possible research topics.",
    "pdf_link": "https://arxiv.org/abs/2404.16886",
    "graphs": [],
    "abstract_cn": "数据是运用机器学习进行时间序列分析的关键，无论是传统模型还是现代的大型语言模型。优质的时间序列数据集能够提升模型的精确度、稳定性和收敛速度，同时也能优化任务成果与成本效益。当前，以数据为核心的人工智能正引领着从模型优化到数据质量提升的转变。尽管时间序列数据处理在众多研究领域内频繁出现，但作为独立议题的研究尚显不足。本文旨在填补这一研究空白，系统性地审视时间序列分析中的多种数据驱动方法，并广泛覆盖了不同的研究议题。我们根据时间序列数据在样本、特征和周期上的特性，提出了一种数据选择方法的分类体系。文章不仅探讨并总结了这些方法的特点、优势和局限性，还针对时间序列数据的挑战和机遇提出了建议、开放性问题及潜在的研究课题。",
    "title_cn": "本文综述了以数据为核心的时间序列分析方法，涵盖了样本选择、特征提取和周期划分等关键方面。",
    "tags": [
      "分类：LLM应用",
      "时间序列分析",
      "数据科学"
    ]
  },
  {
    "title": "Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering",
    "submit_datetime": "2024年04月22日",
    "abstract": "Multi-hop question answering is a knowledge-intensive complex problem. Large Language Models (LLMs) use their Chain of Thoughts (CoT) capability to reason complex problems step by step, and retrieval-augmentation can effectively alleviate factual errors caused by outdated and unknown knowledge in LLMs. Recent works have introduced retrieval-augmentation in the CoT reasoning to solve multi-hop question answering. However, these chain methods have the following problems: 1) Retrieved irrelevant paragraphs may mislead the reasoning; 2) An error in the chain structure may lead to a cascade of errors.\n  In this paper, we propose a dynamic retrieval framework called Tree of Reviews (ToR), where the root node is the question, and the other nodes are paragraphs from retrieval, extending different reasoning paths from the root node to other nodes. Our framework dynamically decides to initiate a new search, reject, or accept based on the paragraphs on the reasoning paths. Compared to related work, we introduce a tree structure to handle each retrieved paragraph separately, alleviating the misleading effect of irrelevant paragraphs on the reasoning path; the diversity of reasoning path extension reduces the impact of a single reasoning error on the whole. We conducted experiments on three different multi-hop question answering datasets. The results show that compared to the baseline methods, ToR achieves state-of-the-art performance in both retrieval and response generation. In addition, we propose two tree-based search optimization strategies, pruning and effective expansion, to reduce time overhead and increase the diversity of path extension. We will release our code.",
    "pdf_link": "https://arxiv.org/abs/2404.14464",
    "graphs": [],
    "abstract_cn": "多跳问答是一项复杂且知识密集的任务。大型语言模型（LLMs）通过思维链（CoT）逐步分析并解决复杂问题，而检索增强技术有效减少了因知识陈旧或未知所导致的错误。近期研究将检索增强应用于CoT推理，以应对多跳问答的挑战。但现有链式方法存在两大问题：一是检索到的无关段落可能干扰推理过程；二是链结构中的任何错误都可能引发连锁反应。本文提出了一种新颖的动态检索框架——评审树（ToR），其以问题为根节点，其他节点为检索所得的段落，构建从根节点到其他节点的多样化推理路径。该框架能够根据推理路径上的段落动态决定是否发起新搜索、拒绝或接受。与现有方法相比，ToR通过树状结构独立处理每个检索段落，减少了无关信息对推理的干扰；同时，推理路径的多样性也降低了单一错误对整体推理的影响。我们在三个不同的多跳问答数据集上进行了实验，ToR在检索和回答生成方面均实现了业界领先的性能。此外，我们还提出了两种基于树的搜索优化策略——剪枝和有效扩展，旨在减少时间消耗并提升路径扩展的多样性。我们将公开我们的代码。",
    "title_cn": "《评论之树：面向多步问答的基于树状动态迭代检索框架》",
    "tags": [
      "LLM应用",
      "问答系统",
      "信息检索"
    ]
  },
  {
    "title": "LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation",
    "submit_datetime": "2024年04月22日",
    "abstract": "Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating outdated knowledge or hallucination by supplying LLMs with updated and relevant knowledge. However, there are still several difficulties for RAG in understanding complex multi-hop query and retrieving relevant documents, which require LLMs to perform reasoning and retrieve step by step. Inspired by human's reasoning process in which they gradually search for the required information, it is natural to ask whether the LLMs could notice the missing information in each reasoning step. In this work, we first experimentally verified the ability of LLMs to extract information as well as to know the missing. Based on the above discovery, we propose a Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the identification of missing information to generate a targeted query that steers the subsequent knowledge retrieval. Besides, we design a sentence-level re-ranking filtering approach to filter the irrelevant content out from document, along with the information extraction capability of LLMs to extract useful information from cleaned-up documents, which in turn to bolster the overall efficacy of RAG. Extensive experiments conducted on multiple public datasets reveal the superiority of the proposed MIGRES method, and analytical experiments demonstrate the effectiveness of our proposed modules.",
    "pdf_link": "https://arxiv.org/abs/2404.14043",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.14043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.14043/x1.png"
      }
    ],
    "abstract_cn": "检索增强生成（RAG）通过向大型语言模型（LLMs）注入最新和贴切的知识，有效缓解了知识过时或产生幻觉的问题。尽管如此，RAG 在解析复杂多跳查询和定位相关文档上仍面临挑战，这要求 LLMs 逐步进行逻辑推理和检索。借鉴人类逐步搜寻所需信息的推理方式，我们自然会产生疑问：LLMs 是否能够察觉到每个逻辑推理步骤中的信息空缺。在本研究中，我们首先通过实验确认了 LLMs 在提取信息和识别缺失信息方面的能力。基于这一发现，我们提出了一种新的范式——缺失信息引导的检索-提取-解决（MIGRES），利用识别出的信息缺失点来生成针对性的查询，从而引导接下来的知识检索过程。此外，我们还设计了一种基于句子级别的重新排序过滤方法，用以排除文档中的无关内容，并结合 LLMs 的信息提取功能，从净化后的文档中抽取有用信息，以此提升 RAG 的整体效能。我们在多个公共数据集上进行的广泛实验显示了 MIGRES 方法的优越性，而深入的分析实验也验证了我们所提模块的有效性。",
    "title_cn": "大型语言模型（LLMs）自有所需：借助缺失信息导向框架，提升检索增强型生成能力。",
    "tags": [
      "RAG",
      "信息检索",
      ""
    ]
  },
  {
    "title": "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations",
    "submit_datetime": "2024年04月22日",
    "abstract": "The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.",
    "pdf_link": "https://arxiv.org/abs/2404.13948",
    "graphs": [],
    "abstract_cn": "随着大型语言模型（LLMs）在多个领域和实际应用中的广泛应用，其鲁棒性变得日益关键。检索增强生成（RAG）作为一种应对LLMs局限性的解决方案，展现出巨大潜力。但是，目前对RAG鲁棒性的研究往往忽略了RAG组件间的内在联系以及现实世界数据库中常见的潜在威胁，如微小的文本错误。在本研究中，我们深入探讨了评估RAG鲁棒性的两个未被充分研究的维度：一是对含噪声文档的低层次扰动的敏感性；二是对RAG鲁棒性的全面评估。此外，我们提出了一种创新的攻击方法——遗传攻击RAG（GARAG），专门针对这些维度。GARAG设计用来暴露各个组件的弱点，并检验系统在面对噪声文档时的整体功能。我们通过将GARAG应用于标准问答（QA）数据集，并结合多种检索器和LLMs，来验证RAG的鲁棒性。实验结果显示，GARAG在攻击成功率上持续保持高水平，显著降低了各组件及其协同工作的性能，从而突显了现实世界中微小文本错误对RAG系统稳定性构成的重大威胁。",
    "title_cn": "错字引发的 RAG 崩溃：通过细微扰动在现实文档中模拟，对 RAG 流程发起基因级攻击",
    "tags": [
      "分类：RAG",
      "",
      "信息检索"
    ]
  },
  {
    "title": "\"Where am I?\" Scene Retrieval with Language",
    "submit_datetime": "2024年04月22日",
    "abstract": "Natural language interfaces to embodied AI are becoming more ubiquitous in our daily lives. This opens further opportunities for language-based interaction with embodied agents, such as a user instructing an agent to execute some task in a specific location. For example, \"put the bowls back in the cupboard next to the fridge\" or \"meet me at the intersection under the red sign.\" As such, we need methods that interface between natural language and map representations of the environment. To this end, we explore the question of whether we can use an open-set natural language query to identify a scene represented by a 3D scene graph. We define this task as \"language-based scene-retrieval\" and it is closely related to \"coarse-localization,\" but we are instead searching for a match from a collection of disjoint scenes and not necessarily a large-scale continuous map. Therefore, we present Text2SceneGraphMatcher, a \"scene-retrieval\" pipeline that learns joint embeddings between text descriptions and scene graphs to determine if they are matched. The code, trained models, and datasets will be made public.",
    "pdf_link": "https://arxiv.org/abs/2404.14565",
    "graphs": [],
    "abstract_cn": "自然语言界面在具身智能领域的应用日益广泛，为我们与智能代理进行基于语言的互动提供了更多可能，如指导代理在特定地点完成任务。举例来说，用户可能会说：“把碗放回冰箱旁的橱柜里”，或者“在那个有红色标志的十字路口见我”。为了实现这一功能，我们需要一种能够将自然语言与环境地图表示形式相连接的方法。基于此，我们研究了一个关键问题：我们能否通过开放式自然语言查询来识别由3D场景图表示的场景。我们将这一任务命名为“基于语言的场景检索”，它与“粗略定位”有关，但我们的目标是在一系列独立的3D场景集合中寻找匹配项，而非在大规模连续地图上。因此，我们开发了Text2SceneGraphMatcher，这是一个学习文本描述与场景图之间联合嵌入的“场景检索”流程，用以判断它们是否匹配。相关的代码、训练好的模型以及数据集将对公众开放。",
    "title_cn": "“我身在何方？”通过语言技术实现场景检索",
    "tags": [
      "Agent",
      "具身智能",
      ""
    ]
  },
  {
    "title": "A Survey on Self-Evolution of Large Language Models",
    "submit_datetime": "2024年04月22日",
    "abstract": "Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.14387",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.14387v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.14387/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.14387v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.14387/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.14387v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.14387/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.14387v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.14387/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.14387v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.14387/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.14387v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.14387/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.14387v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.14387/x7.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）在多个领域和智能代理应用上取得了突破性进展。尽管如此，依赖人类或外部模型监督的LLMs不仅成本高昂，而且随着任务的复杂和多变，它们的性能提升也遇到了瓶颈。为突破这一局限，一种新兴的自我进化方法应运而生，它允许LLMs自主地从自身生成的经验中学习、提炼和进化。这种训练新范式，模仿人类的体验学习过程，为LLMs向超智能的跨越式发展提供了可能。在本研究中，我们全面审视了LLMs中的自我进化策略。我们首先构建了一个自我进化的概念性框架，并将其演进过程划分为四个循环迭代的阶段：经验获取、经验优化、更新和评估。接着，我们对LLMs及其代理的进化目标进行了分类，并综述了相关文献，为每个模块提供了系统的分类和深入的洞见。最后，我们指出了当前面临的挑战，并对未来的研究方向提出了建议，旨在为研究人员提供宝贵的见解，以加速自我进化LLMs的研究进程。",
    "title_cn": "大型语言模型的自我进化研究综述",
    "tags": [
      "分类：LLM理论",
      "人工智能",
      ""
    ]
  },
  {
    "title": "FINEMATCH: Aspect-based Fine-grained Image and Text Mismatch Detection and Correction",
    "submit_datetime": "2024年04月22日",
    "abstract": "Recent progress in large-scale pre-training has led to the development of advanced vision-language models (VLMs) with remarkable proficiency in comprehending and generating multimodal content. Despite the impressive ability to perform complex reasoning for VLMs, current models often struggle to effectively and precisely capture the compositional information on both the image and text sides. To address this, we propose FineMatch, a new aspect-based fine-grained text and image matching benchmark, focusing on text and image mismatch detection and correction. This benchmark introduces a novel task for boosting and evaluating the VLMs' compositionality for aspect-based fine-grained text and image matching. In this task, models are required to identify mismatched aspect phrases within a caption, determine the aspect's class, and propose corrections for an image-text pair that may contain between 0 and 3 mismatches. To evaluate the models' performance on this new task, we propose a new evaluation metric named ITM-IoU for which our experiments show a high correlation to human evaluation. In addition, we also provide a comprehensive experimental analysis of existing mainstream VLMs, including fully supervised learning and in-context learning settings. We have found that models trained on FineMatch demonstrate enhanced proficiency in detecting fine-grained text and image mismatches. Moreover, models (e.g., GPT-4V, Gemini Pro Vision) with strong abilities to perform multimodal in-context learning are not as skilled at fine-grained compositional image and text matching analysis. With FineMatch, we are able to build a system for text-to-image generation hallucination detection and correction.",
    "pdf_link": "https://arxiv.org/abs/2404.14715",
    "graphs": [],
    "abstract_cn": "最新在大规模预训练领域的进展催生了视觉-语言模型（VLMs）的突破，这些模型在理解和创造多模态内容上展现出非凡的才能。尽管VLMs在复杂推理方面表现出色，但现有模型在准确捕捉图像和文本的组合信息上仍面临挑战。为此，我们引入了FineMatch，这是一个创新的基于方面的细粒度文本与图像匹配基准，专注于检测和纠正文本与图像的不匹配。该基准旨在推动和评估VLMs在细粒度文本与图像匹配方面的组合能力。在这项新任务中，模型需识别出标题中的不匹配方面短语，确定其类别，并为可能存在0至3处不匹配的图像-文本对提出修正建议。我们设计了一个新的评估指标ITM-IoU来衡量模型在这一新任务上的表现，实验结果显示其与人工评估高度相关。此外，我们还对现有的主流VLMs进行了全面的实验分析，包括全面监督学习和上下文学习环境。研究发现，通过FineMatch训练的模型在识别细粒度文本和图像不匹配方面更加精准。同时，我们也发现，虽然一些模型（如GPT-4V，Gemini Pro Vision）在多模态上下文学习方面表现出色，但在进行细粒度组合图像和文本匹配分析时却不够精细。FineMatch的推出，使我们得以构建一个系统，用于检测和纠正文本到图像生成中的幻觉问题。",
    "title_cn": "FINEMATCH：面向细粒度的图像与文本不匹配问题的基于方面的检测与校正技术",
    "tags": [
      "LLM应用",
      "计算机视觉",
      ""
    ]
  },
  {
    "title": "WangLab at MEDIQA-M3G 2024: Multimodal Medical Answer Generation using Large Language Models",
    "submit_datetime": "2024年04月22日",
    "abstract": "This paper outlines our submission to the MEDIQA2024 Multilingual and Multimodal Medical Answer Generation (M3G) shared task. We report results for two standalone solutions under the English category of the task, the first involving two consecutive API calls to the Claude 3 Opus API and the second involving training an image-disease label joint embedding in the style of CLIP for image classification. These two solutions scored 1st and 2nd place respectively on the competition leaderboard, substantially outperforming the next best solution. Additionally, we discuss insights gained from post-competition experiments. While the performance of these two solutions have significant room for improvement due to the difficulty of the shared task and the challenging nature of medical visual question answering in general, we identify the multi-stage LLM approach and the CLIP image classification approach as promising avenues for further investigation.",
    "pdf_link": "https://arxiv.org/abs/2404.14567",
    "graphs": [],
    "abstract_cn": "本文介绍了我们为 MEDIQA2024 多语言多模态医学答案生成（M3G）任务所提交的研究成果。在该任务的英语部分，我们提出了两种独立方法：第一种方法是连续两次调用 Claude 3 Opus API，第二种方法是采用 CLIP 技术训练图像与疾病标签的联合嵌入模型以进行图像分类。这两种方法在竞赛排行榜上分别荣获第一和第二名，大幅领先于其他解决方案。文章还分享了赛后实验的深刻见解。尽管面对任务的复杂性和医学视觉问答的普遍挑战，这两种解决方案的表现尚有提升空间，但我们看好多阶段大型语言模型（LLM）方法和 CLIP 图像分类方法，认为它们是未来研究的有前景方向。",
    "title_cn": "在 2024 年的 MEDIQA-M3G 大会上，WangLab 展示了一项创新成果：运用先进的大型语言模型，实现了多模态医学问题的智能答案生成。",
    "tags": [
      "分类：LLM应用\n\n这篇论文摘要介绍了两个独立的方法，它们在MEDIQA2024多语言多模态医学答案生成任务的英语部分中表现出色。第一个方法涉及连续两次调用Claude 3 Opus API，而第二个方法则采用CLIP技术来训练图像与疾病标签的联合嵌入模型。这两种方法在竞赛中获得了第一名和第二名，表明了大型语言模型（LLM）在实际应用中的潜力。此外，论文还提到了对赛后实验的深入见解，以及对未来研究方向的展望。因此，这篇论文可以归类为LLM应用。",
      "",
      "图像识别"
    ]
  },
  {
    "title": "Graphic Design with Large Multimodal Model",
    "submit_datetime": "2024年04月22日",
    "abstract": "In the field of graphic design, automating the integration of design elements into a cohesive multi-layered artwork not only boosts productivity but also paves the way for the democratization of graphic design. One existing practice is Graphic Layout Generation (GLG), which aims to layout sequential design elements. It has been constrained by the necessity for a predefined correct sequence of layers, thus limiting creative potential and increasing user workload. In this paper, we present Hierarchical Layout Generation (HLG) as a more flexible and pragmatic setup, which creates graphic composition from unordered sets of design elements. To tackle the HLG task, we introduce Graphist, the first layout generation model based on large multimodal models. Graphist efficiently reframes the HLG as a sequence generation problem, utilizing RGB-A images as input, outputs a JSON draft protocol, indicating the coordinates, size, and order of each element. We develop new evaluation metrics for HLG. Graphist outperforms prior arts and establishes a strong baseline for this field. Project homepage: https://github.com/graphic-design-ai/graphist",
    "pdf_link": "https://arxiv.org/abs/2404.14368",
    "graphs": [],
    "abstract_cn": "在平面设计界，自动化地将设计元素融合为一个和谐的多层艺术作品，既提升了效率，也为设计的普及化开辟了新径。目前，图形布局生成（GLG）技术正致力于有序地排列设计元素，但这种做法因要求预先设定正确的层级顺序而受限，这不仅束缚了创意的翅膀，也加重了用户的工作负担。本文提出了一种新颖的分层布局生成（HLG）方法，它以一种更灵活、更实用的框架，从无序的设计元素集合中创造出图形构图。为应对HLG挑战，我们推出了Graphist——首个基于大型多模态模型的布局生成模型。Graphist巧妙地将HLG问题转化为序列生成问题，输入RGB-A图像，输出包含坐标、尺寸和元素顺序信息的JSON草稿协议。我们还为HLG任务设计了全新的评估标准，Graphist在这些标准上超越了先前的技术，并为该领域设定了新的基准。项目网址：https://github.com/graphic-design-ai/graphist",
    "title_cn": "大型多模态模型在图形设计中的应用",
    "tags": [
      "LLM应用",
      "平面设计",
      "自动化设计"
    ]
  },
  {
    "title": "UrbanCross: Enhancing Satellite Image-Text Retrieval with Cross-Domain Adaptation",
    "submit_datetime": "2024年04月22日",
    "abstract": "Urbanization challenges underscore the necessity for effective satellite image-text retrieval methods to swiftly access specific information enriched with geographic semantics for urban applications. However, existing methods often overlook significant domain gaps across diverse urban landscapes, primarily focusing on enhancing retrieval performance within single domains. To tackle this issue, we present UrbanCross, a new framework for cross-domain satellite image-text retrieval. UrbanCross leverages a high-quality, cross-domain dataset enriched with extensive geo-tags from three countries to highlight domain diversity. It employs the Large Multimodal Model (LMM) for textual refinement and the Segment Anything Model (SAM) for visual augmentation, achieving a fine-grained alignment of images, segments and texts, yielding a 10% improvement in retrieval performance. Additionally, UrbanCross incorporates an adaptive curriculum-based source sampler and a weighted adversarial cross-domain fine-tuning module, progressively enhancing adaptability across various domains. Extensive experiments confirm UrbanCross's superior efficiency in retrieval and adaptation to new urban environments, demonstrating an average performance increase of 15% over its version without domain adaptation mechanisms, effectively bridging the domain gap.",
    "pdf_link": "https://arxiv.org/abs/2404.14241",
    "graphs": [],
    "abstract_cn": "随着城市化进程的加速，对能够迅速检索富含地理语义信息的卫星图像-文本方法的需求日益迫切。然而，现有技术往往忽略了城市景观多样性之间的领域差异，过于集中于单一领域的检索性能提升。为应对这一挑战，我们推出了 UrbanCross，这是一个创新的跨领域卫星图像-文本检索框架。该框架依托于一个涵盖三国广泛地理标签的高质量跨领域数据集，以展现不同领域的丰富性。它结合了大型多模态模型（LMM）对文本进行精细化处理，以及分段任何模型（SAM）对视觉内容进行增强，实现了图像、分段与文本之间的精准匹配，提升了10%的检索效率。此外，UrbanCross 还融入了自适应课程源采样器和加权对抗性跨领域微调模块，逐步增强了对不同领域环境的适应能力。广泛的实验结果表明，UrbanCross 在检索效率和新城市环境适应性方面具有显著优势，相较于未采用领域适应机制的版本，平均性能提升了15%，有效缩小了不同领域之间的差距。",
    "title_cn": "UrbanCross：借助跨域适应技术，提升卫星图像与文本的检索能力",
    "tags": [
      "分类：Agent",
      "",
      "城市化"
    ]
  },
  {
    "title": "Boter: Bootstrapping Knowledge Selection and Question Answering for Knowledge-based VQA",
    "submit_datetime": "2024年04月22日",
    "abstract": "Knowledge-based Visual Question Answering (VQA) requires models to incorporate external knowledge to respond to questions about visual content. Previous methods mostly follow the \"retrieve and generate\" paradigm. Initially, they utilize a pre-trained retriever to fetch relevant knowledge documents, subsequently employing them to generate answers. While these methods have demonstrated commendable performance in the task, they possess limitations: (1) they employ an independent retriever to acquire knowledge solely based on the similarity between the query and knowledge embeddings, without assessing whether the knowledge document is truly conducive to helping answer the question; (2) they convert the image into text and then conduct retrieval and answering in natural language space, which may not ensure comprehensive acquisition of all image information. To address these limitations, we propose Boter, a novel framework designed to bootstrap knowledge selection and question answering by leveraging the robust multimodal perception capabilities of the Multimodal Large Language Model (MLLM). The framework consists of two modules: Selector and Answerer, where both are initialized by the MLLM and parameter-efficiently finetuned in a simple cycle: find key knowledge in the retrieved knowledge documents using the Selector, and then use them to finetune the Answerer to predict answers; obtain the pseudo-labels of key knowledge documents based on the predictions of the Answerer and weak supervision labels, and then finetune the Selector to select key knowledge; repeat. Our framework significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83%.",
    "pdf_link": "https://arxiv.org/abs/2404.13947",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13947v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13947/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13947v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13947/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13947v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13947/x3.png"
      }
    ],
    "abstract_cn": "基于知识的可视化问答（VQA）任务要求模型整合外部信息以回答视觉内容的问题。传统“检索-生成”范式的方法虽表现不俗，但存在不足：它们依赖独立检索器，仅基于查询与知识嵌入的相似性获取信息，忽略了知识文档是否真正有助于问题解答；同时，将图像信息转换为文本后进行检索和回答，可能无法全面捕捉图像的全部信息。为克服这些限制，我们设计了Boter框架，它通过利用多模态大型语言模型（MLLM）的多模态感知能力，优化了知识选择和问答过程。该框架包含选择器和回答器两个模块，均由MLLM初始化，并通过简洁的循环高效微调：选择器筛选出检索到的知识文档中的关键信息，回答器据此预测答案；基于回答器的预测结果和弱监督标签，为关键知识文档生成伪标签，进一步微调选择器以精选关键知识；如此循环。此框架在开放领域知识型VQA基准测试OK-VQA上显著提升了基线模型的性能，达到了62.83%的先进准确率。",
    "title_cn": "Boter：一种引导式的知识选择与问答方法，专为基于知识的 VQA（视觉问答）而设计。",
    "tags": [
      "LLM应用",
      "可视化问答",
      "人工智能"
    ]
  },
  {
    "title": "Augmented Object Intelligence: Making the Analog World Interactable with XR-Objects",
    "submit_datetime": "2024年04月22日",
    "abstract": "Seamless integration of physical objects as interactive digital entities remains a challenge for spatial computing. This paper introduces Augmented Object Intelligence (AOI), a novel XR interaction paradigm designed to blur the lines between digital and physical by equipping real-world objects with the ability to interact as if they were digital, where every object has the potential to serve as a portal to vast digital functionalities. Our approach utilizes object segmentation and classification, combined with the power of Multimodal Large Language Models (MLLMs), to facilitate these interactions. We implement the AOI concept in the form of XR-Objects, an open-source prototype system that provides a platform for users to engage with their physical environment in rich and contextually relevant ways. This system enables analog objects to not only convey information but also to initiate digital actions, such as querying for details or executing tasks. Our contributions are threefold: (1) we define the AOI concept and detail its advantages over traditional AI assistants, (2) detail the XR-Objects system's open-source design and implementation, and (3) show its versatility through a variety of use cases and a user study.",
    "pdf_link": "https://arxiv.org/abs/2404.13274",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/F6study-setup.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/F11discover.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/F12productivity.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/F13learning.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13274v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13274/F14IOT.png"
      }
    ],
    "abstract_cn": "将实体对象无障碍地融入为互动的数字化实体，在空间计算领域仍是一大难题。本文提出了一种名为增强对象智能（AOI）的新型XR互动模式，旨在通过赋予现实世界物体以数字互动的能力，打破数字与物理的界限，让每个物体都可能成为通往丰富数字功能的入口。我们采用对象分割与分类技术，并借助多模态大型语言模型（MLLMs）的强大功能，以促进这些交互体验。AOI理念以XR-Objects的形式得以实现，这是一个开源的原型系统，为用户提供了一个与物理环境进行丰富且具有情境相关性的互动平台。该系统不仅让传统对象能够传递信息，还能触发数字化操作，如查询详细信息或执行任务。我们的研究成果包括三个方面：（1）明确了AOI的概念，并阐述了其相较于传统AI助手的优势；（2）详细介绍了XR-Objects系统的开源架构与实现；（3）通过多样化的应用案例和用户研究，展示了该系统的广泛适用性。",
    "title_cn": "增强对象智能：赋予实体世界以 XR-Objects，实现与扩展现实的互动。",
    "tags": [
      "Agent",
      "空间计算",
      "增强现实"
    ]
  },
  {
    "title": "AccidentBlip2: Accident Detection With Multi-View MotionBlip2",
    "submit_datetime": "2024年04月22日",
    "abstract": "Intelligent vehicles have demonstrated excellent capabilities in many transportation scenarios, but the complex on-board sensors and the inference capabilities of on-board neural networks limit the accuracy of intelligent vehicles for accident detection in complex transportation systems. In this paper, we present AccidentBlip2, a pure vision-based multimodal large model Blip2 accident detection method. Our method first processes the multi-view through ViT-14g and inputs the multi-view features into the cross attention layer of the Qformer, while our self-designed Motion Qformer replaces the self-attention layer in Blip2's Qformer with the Temporal Attention layer in the In the inference process, the query generated in the previous frame is input into the Temporal Attention layer to realize the inference for temporal information. Then we detect whether there is an accident in the surrounding environment by performing autoregressive inference on the query input to the MLP. We also extend our approach to a multi-vehicle cooperative system by deploying Motion Qformer on each vehicle and simultaneously inputting the inference-generated query into the MLP for autoregressive inference. Our approach detects the accuracy of existing video large language models and also adapts to multi-vehicle systems, making it more applicable to intelligent transportation scenarios.",
    "pdf_link": "https://arxiv.org/abs/2404.12149",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12149v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12149/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12149v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12149/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12149v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12149/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12149v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12149/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12149v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12149/x5.png"
      }
    ],
    "abstract_cn": "智能车辆在多样化的交通情境中表现出色，但其搭载的复杂传感器和神经网络的推理功能在复杂交通系统中对事故检测的精准度仍有局限。本文介绍了一种全新的基于纯视觉的多模态大型模型——AccidentBlip2，用于事故检测。该方法首先利用 ViT-14g 对多视角图像进行处理，并将处理后的特征送入 Qformer 的交叉注意力层。我们独创的 Motion Qformer 则用时间注意力层替代了 Blip2 中 Qformer 的自注意力层。在推理阶段，前一帧生成的查询会输入到时间注意力层，以实现对时间信息的推理。接着，通过自回归推理分析查询，以判断周围环境是否发生了事故。此外，我们将此方法扩展至多车协作系统，通过在每辆车上部署 Motion Qformer 并输入推理生成的查询至 MLP 进行自回归推理，进一步提升了检测的准确性。这种方法不仅提高了现有视频大型语言模型的检测精度，而且适应于多车系统，更符合智能交通场景的应用需求。",
    "title_cn": "AccidentBlip2：融合多视角 MotionBlip2 技术，精准捕捉事故瞬间",
    "tags": [
      "Agent",
      "智能交通",
      "事故检测"
    ]
  },
  {
    "title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation",
    "submit_datetime": "2024年04月21日",
    "abstract": "Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model's performance based on query-document relevance labels shows a small correlation with the RAG system's downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's $τ$ correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.",
    "pdf_link": "https://arxiv.org/abs/2404.13781",
    "graphs": [],
    "abstract_cn": "对检索增强生成（RAG）进行评估并非易事，尤其是在这些系统中的检索模型。传统的端到端评估方式不仅计算成本高，而且基于查询与文档相关性标签的检索模型性能评估与RAG系统的下游性能关联度并不高。为此，我们提出了一种创新的评估方案——eRAG。在eRAG中，RAG系统中的大型语言模型会单独处理检索列表中的每一个文档，并针对每个文档生成的输出进行下游任务真值标签的评估。这样，每个文档的下游表现就成为了其相关性标签。我们采用多种下游任务的度量标准，对文档进行逐级注解，并利用基于集合或排名的度量进行汇总。广泛的实验结果表明，eRAG与RAG系统的下游性能相关性更高，Kendall的$τ$相关性提升显著，从0.168增至0.494。同时，eRAG在计算上具有显著优势，不仅缩短了运行时间，而且相较于端到端评估，GPU内存消耗减少了高达50倍。",
    "title_cn": "在检索增强生成领域，对检索质量的评估至关重要。",
    "tags": [
      "分类：RAG",
      "信息检索",
      ""
    ]
  },
  {
    "title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
    "submit_datetime": "2024年04月21日",
    "abstract": "Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios. This poses a pressing demand to develop new real-world scenario benchmarks. We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions). Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method.",
    "pdf_link": "https://arxiv.org/abs/2404.13627",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/BackGround.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/Desire.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/Belief.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.13627v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13627/Intention.jpg"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）在心理理论（ToM）能力的潜在发展上引起了广泛的兴趣和讨论。当前的心理理论评估通常依赖于机器生成的数据或易导致捷径和偶然关联的游戏环境，这并未充分评估机器在现实人际互动中的ToM能力。因此，迫切需要创建新的现实世界情境基准。我们提出了NegotiationToM，这是一个新型基准测试，用以在真实世界的谈判中对机器的ToM进行严格测试，涉及多维度的心理状态，包括欲望、信念和意图。该基准测试基于信念-欲望-意图（BDI）代理模型理论，并进行了必要的实证实验以评估LLMs。我们的研究结果显示，即使是最先进的LLMs在NegotiationToM上也面临挑战，它们的表现一致地显著低于人类水平，即便采用了思维链（CoT）方法。",
    "title_cn": "NegotiationToM：一个专门设计来对机器理论思维进行压力测试的谈判基准",
    "tags": [
      "Agent",
      "心理学",
      "人工智能"
    ]
  },
  {
    "title": "EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking Enhances Visual Commonsense Reasoning",
    "submit_datetime": "2024年04月21日",
    "abstract": "Visual Commonsense Reasoning (VCR) is a cognitive task, challenging models to answer visual questions requiring human commonsense, and to provide rationales explaining why the answers are correct. With emergence of Large Language Models (LLMs), it is natural and imperative to explore their applicability to VCR. However, VCR task demands more external knowledge to tackle its challenging questions, necessitating special designs to activate LLMs' commonsense reasoning abilities. Also, most existing Multimodal LLMs adopted an abstraction of entire input image, which makes it difficult to comprehend VCR's unique co-reference tags between image regions and text, posing challenges for fine-grained alignment. To address these issues, we propose EventLens that leverages Event-Aware Pretraining and Cross-modal Linking and EnhanceS VCR. First, by emulating the cognitive process of human reasoning, an Event-Aware Pretraining auxiliary task is introduced to better activate LLM's global comprehension of intricate scenarios. Second, during fine-tuning, we further utilize reference tags to bridge RoI features with texts, while preserving both modality semantics. Finally, we use instruct-style prompts to narrow the gap between pretraining and fine-tuning, and task-specific adapters to better integrate LLM's inherent knowledge with new commonsense. Experimental results show the effectiveness of our proposed auxiliary task and fine-grained linking strategy.",
    "pdf_link": "https://arxiv.org/abs/2404.13847",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13847v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13847/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13847v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13847/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13847v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13847/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13847v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13847/x4.png"
      }
    ],
    "abstract_cn": "视觉常识推理（VCR）考验模型回答需依赖人类常识的视觉问题，并需阐明答案正确的理由。大型语言模型（LLMs）的兴起，使得探究其在VCR领域的应用变得既自然又迫切。但VCR的挑战性问题要求模型具备更丰富的外部知识，这就需要特别设计以激发LLMs的常识推理潜能。目前，多数多模态LLMs处理输入图像时采取整体抽象方法，难以捕捉VCR中图像区域与文本间的特定共指关系，这对精确对齐构成挑战。为应对这些挑战，我们设计了EventLens，它通过事件感知预训练和跨模态链接来增强VCR性能。首先，我们引入了一个模仿人类认知推理过程的事件感知预训练辅助任务，以提升LLM对复杂情境的全面理解。其次，在微调阶段，我们使用参考标签将感兴趣区域（RoI）的特征与文本相连，确保两种模态的语义信息得以保留。最后，我们采用指导性提示减少预训练与微调间的差异，并利用任务特定的适配器，使LLM的内在知识与新常识更好地融合。实验结果显示，我们提出的辅助任务和细粒度链接策略效果显著。",
    "title_cn": "EventLens：通过事件驱动的预训练和跨模态关联，提升视觉常识推理能力。",
    "tags": [
      "分类：LLM应用\n\n这篇论文讨论了如何利用大型语言模型（LLMs）来解决视觉常识推理（VCR）问题。它提出了一种名为EventLens的方法，通过事件感知预训练和跨模态链接来增强VCR性能。这篇论文的重点在于如何设计和应用LLMs来解决特定的AI问题，因此它属于LLM应用类别。",
      "计算机视觉",
      "人工智能"
    ]
  },
  {
    "title": "General Item Representation Learning for Cold-start Content Recommendations",
    "submit_datetime": "2024年04月21日",
    "abstract": "Cold-start item recommendation is a long-standing challenge in recommendation systems. A common remedy is to use a content-based approach, but rich information from raw contents in various forms has not been fully utilized. In this paper, we propose a domain/data-agnostic item representation learning framework for cold-start recommendations, naturally equipped with multimodal alignment among various features by adopting a Transformer-based architecture. Our proposed model is end-to-end trainable completely free from classification labels, not just costly to collect but suboptimal for recommendation-purpose representation learning. From extensive experiments on real-world movie and news recommendation benchmarks, we verify that our approach better preserves fine-grained user taste than state-of-the-art baselines, universally applicable to multiple domains at large scale.",
    "pdf_link": "https://arxiv.org/abs/2404.13808",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13808v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13808/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13808v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13808/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13808v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13808/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13808v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13808/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13808v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13808/x5.png"
      }
    ],
    "abstract_cn": "冷启动商品推荐一直是推荐领域的难题。传统解决方案倾向于采用基于内容的方法，但往往未能充分利用多样化原始内容中的丰富信息。本文提出了一种新颖的领域/数据无关的商品表示学习框架，旨在解决冷启动推荐问题。该框架采用基于 Transformer 的架构，实现了不同特征间的自然多模态对齐。我们的模型无需依赖分类标签即可进行端到端的训练，避免了收集标签的高昂成本，同时优化了推荐场景下的表示学习方法。通过在电影和新闻推荐的实际基准测试中的广泛实验，我们证实了该方法在捕捉用户细微偏好方面超越了当前最先进的方法，并且具有跨领域的广泛适用性。",
    "title_cn": "通用项目表示学习：破解冷启动内容推荐难题",
    "tags": [
      "分类：Agent",
      "推荐系统",
      "机器学习"
    ]
  },
  {
    "title": "SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model",
    "submit_datetime": "2024年04月21日",
    "abstract": "Extraction and synthesis of structured knowledge from extensive scientific literature are crucial for advancing and disseminating scientific progress. Although many existing systems facilitate literature review and digest, they struggle to process multimodal, varied, and inconsistent information within and across the literature into structured data. We introduce SciDaSynth, a novel interactive system powered by large language models (LLMs) that enables researchers to efficiently build structured knowledge bases from scientific literature at scale. The system automatically creates data tables to organize and summarize users' interested knowledge in literature via question-answering. Furthermore, it provides multi-level and multi-faceted exploration of the generated data tables, facilitating iterative validation, correction, and refinement. Our within-subjects study with researchers demonstrates the effectiveness and efficiency of SciDaSynth in constructing quality scientific knowledge bases. We further discuss the design implications for human-AI interaction tools for data extraction and structuring.",
    "pdf_link": "https://arxiv.org/abs/2404.13765",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13765v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13765/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13765v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13765/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13765v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13765/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13765v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13765/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13765v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13765/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13765v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13765/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13765v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13765/x7.png"
      }
    ],
    "abstract_cn": "深入挖掘和整合科学文献中的结构化知识对科学发展和传播极为关键。尽管众多现有系统辅助进行文献回顾和理解，但它们在将文献中的多模态、多变和不统一的信息转换为结构化数据方面仍面临挑战。我们推出了 SciDaSynth，一个由大型语言模型 (LLMs) 支持的创新交互式系统，旨在帮助研究人员高效地从科学文献中构建大规模的结构化知识库。该系统能够自动生成数据表格，通过问答形式整理和汇总用户感兴趣的文献知识。同时，它还提供多层次、多角度的数据表格探索功能，以便进行迭代式的验证、修正和优化。我们与研究人员进行的个体内研究显示，SciDaSynth 在构建高质量的科学知识库方面既高效又有效。此外，我们还探讨了设计人工智能交互工具以进行数据提取和结构化时的考量要点。",
    "title_cn": "SciDaSynth：借助大型语言模型，实现从科学文献中进行互动式的知识结构化提取与合成。",
    "tags": [
      "LLM应用",
      "科学文献",
      "知识管理"
    ]
  },
  {
    "title": "FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization",
    "submit_datetime": "2024年04月21日",
    "abstract": "Zero-shot anomaly detection (ZSAD) methods entail detecting anomalies directly without access to any known normal or abnormal samples within the target item categories. Existing approaches typically rely on the robust generalization capabilities of multimodal pretrained models, computing similarities between manually crafted textual features representing \"normal\" or \"abnormal\" semantics and image features to detect anomalies and localize anomalous patches. However, the generic descriptions of \"abnormal\" often fail to precisely match diverse types of anomalies across different object categories. Additionally, computing feature similarities for single patches struggles to pinpoint specific locations of anomalies with various sizes and scales. To address these issues, we propose a novel ZSAD method called FiLo, comprising two components: adaptively learned Fine-Grained Description (FG-Des) and position-enhanced High-Quality Localization (HQ-Loc). FG-Des introduces fine-grained anomaly descriptions for each category using Large Language Models (LLMs) and employs adaptively learned textual templates to enhance the accuracy and interpretability of anomaly detection. HQ-Loc, utilizing Grounding DINO for preliminary localization, position-enhanced text prompts, and Multi-scale Multi-shape Cross-modal Interaction (MMCI) module, facilitates more accurate localization of anomalies of different sizes and shapes. Experimental results on datasets like MVTec and VisA demonstrate that FiLo significantly improves the performance of ZSAD in both detection and localization, achieving state-of-the-art performance with an image-level AUC of 83.9% and a pixel-level AUC of 95.9% on the VisA dataset.",
    "pdf_link": "https://arxiv.org/abs/2404.13671",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13671v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13671/x12.png"
      }
    ],
    "abstract_cn": "零样本异常检测（ZSAD）技术能够在无需任何已知正常或异常样本的情况下直接识别异常。传统方法多依赖于多模态预训练模型的泛化能力，通过比较文本和图像特征的相似度来识别并定位异常。但这些方法在精确描述不同类别中的多样化异常时常常力不从心，且在确定异常具体位置时也存在挑战。为此，我们引入了一种创新的ZSAD方法——FiLo，它由两个核心组件构成：细粒度描述（FG-Des）和高质量定位（HQ-Loc）。FG-Des利用大型语言模型为不同类别提供细致的异常描述，并通过自适应学习的文本模板提升检测的精确度和可解释性。HQ-Loc则结合了初步定位技术Grounding DINO、位置增强文本提示和多尺度多形状跨模态交互模块（MMCI），以更准确地定位各种尺寸和形状的异常。在MVTec和VisA等数据集上的测试显示，FiLo在异常检测和定位方面均有显著提升，尤其在VisA数据集上，图像级AUC达到了83.9%，像素级AUC更是高达95.9%，刷新了行业标准。",
    "title_cn": "FiLo：一种零样本异常检测方法，它利用细致的描述和精准的定位技术。",
    "tags": [
      "分类：LLM应用\n\n这篇论文摘要介绍了一种零样本异常检测（ZSAD）方法——FiLo，它利用大型语言模型（LLM）来提供异常的细致描述，并通过自适应学习的文本模板提高检测的精确度和可解释性。此外，FiLo结合了多种技术来更准确地定位异常。由于该方法涉及到使用大型语言模型进行异常检测和定位，因此可以归类为LLM应用。",
      "异常检测",
      "跨模态学习"
    ]
  },
  {
    "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
    "submit_datetime": "2024年04月21日",
    "abstract": "While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, $\\sim800\\times$ faster than existing optimization-based approaches. We train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.",
    "pdf_link": "https://arxiv.org/abs/2404.16873",
    "graphs": [],
    "abstract_cn": "近期，大型语言模型（LLMs）虽然取得了显著成就，却也容易遭受特定越狱攻击，从而产生不当或有害的内容。传统的手动红队战术，通过添加后缀等方式寻找对抗性提示，不仅效率低下，还耗费大量时间。而自动化的对抗性提示生成方法，往往产生无意义的攻击，容易被基于困惑度的过滤器识破，且可能需要目标LLM的梯度信息，或因优化过程繁琐而难以扩展。本文提出了一种创新方法，利用另一个名为AdvPrompter的LLM快速生成易于人类理解的对抗性提示，速度是现有优化方法的800倍。我们采用了一种新颖的算法训练AdvPrompter，无需获取目标LLM的梯度信息。该过程包括两个交替步骤：首先，优化AdvPrompter的预测以产生高质量的目标对抗性后缀；其次，使用这些后缀对AdvPrompter进行低秩微调。经过训练的AdvPrompter能够生成不改变指令含义的隐蔽后缀，诱使目标LLM作出有害回应。在多个流行的开源目标LLM上的实验表明，我们的方法在AdvBench数据集上达到了最佳效果，并且这些成果同样适用于封闭源的黑盒LLM API。此外，我们还证明了通过在AdvPrompter生成的合成数据集上进行微调，可以在保持高性能的同时，增强LLM对越狱攻击的抵抗力，即保持高MMLU得分。",
    "title_cn": "AdvPrompter：一种为大型语言模型（LLMs）量身定制的快速自适应对抗性提示工具",
    "tags": [
      "LLM应用",
      "网络安全",
      "人工智能"
    ]
  },
  {
    "title": "Retrieval-Augmented Generation-based Relation Extraction",
    "submit_datetime": "2024年04月20日",
    "abstract": "Information Extraction (IE) is a transformative process that converts unstructured text data into a structured format by employing entity and relation extraction (RE) methodologies. The identification of the relation between a pair of entities plays a crucial role within this framework. Despite the existence of various techniques for relation extraction, their efficacy heavily relies on access to labeled data and substantial computational resources. In addressing these challenges, Large Language Models (LLMs) emerge as promising solutions; however, they might return hallucinating responses due to their own training data. To overcome these limitations, Retrieved-Augmented Generation-based Relation Extraction (RAG4RE) in this work is proposed, offering a pathway to enhance the performance of relation extraction tasks.\n  This work evaluated the effectiveness of our RAG4RE approach utilizing different LLMs. Through the utilization of established benchmarks, such as TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to comprehensively evaluate the efficacy of our RAG4RE approach. In particularly, we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our investigation. The results of our study demonstrate that our RAG4RE approach surpasses performance of traditional RE approaches based solely on LLMs, particularly evident in the TACRED dataset and its variations. Furthermore, our approach exhibits remarkable performance compared to previous RE methodologies across both TACRED and TACREV datasets, underscoring its efficacy and potential for advancing RE tasks in natural language processing.",
    "pdf_link": "https://arxiv.org/abs/2404.13397",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13397v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13397/x12.png"
      }
    ],
    "abstract_cn": "信息抽取（IE）通过实体和关系抽取（RE）技术，将杂乱无章的文本信息转化为井然有序的数据结构，其中实体间关系的辨识尤为关键。尽管众多关系抽取技术已现世，但其成效往往受限于标注数据的可用性和计算资源的丰富度。面对这些难题，大型语言模型（LLMs）崭露头角，但有时也会因训练数据的局限而产生误导性结果。为突破这些瓶颈，本文提出了一种新颖的基于检索增强生成的关系抽取方法（RAG4RE），旨在提升关系抽取的性能。我们通过一系列知名大型语言模型（LLMs），如Flan T5、Llama2和Mistral，对RAG4RE方法进行了效果评估，并选取了TACRED、TACREV、Re-TACRED和SemEval RE等权威数据集作为测试基准。研究结果显示，RAG4RE在TACRED数据集及其衍生变体中的表现尤为突出，超越了仅依赖LLMs的传统关系抽取方法。此外，RAG4RE在TACRED和TACREV数据集上的表现也显著优于以往的关系抽取技术，彰显了其在自然语言处理领域推动关系抽取任务的潜力和效果。",
    "title_cn": "检索增强的生成式关系抽取",
    "tags": [
      "分类：RAG",
      "",
      "信息抽取"
    ]
  },
  {
    "title": "Enhancing Question Answering for Enterprise Knowledge Bases using Large Language Models",
    "submit_datetime": "2024年04月20日",
    "abstract": "Efficient knowledge management plays a pivotal role in augmenting both the operational efficiency and the innovative capacity of businesses and organizations. By indexing knowledge through vectorization, a variety of knowledge retrieval methods have emerged, significantly enhancing the efficacy of knowledge management systems. Recently, the rapid advancements in generative natural language processing technologies paved the way for generating precise and coherent answers after retrieving relevant documents tailored to user queries. However, for enterprise knowledge bases, assembling extensive training data from scratch for knowledge retrieval and generation is a formidable challenge due to the privacy and security policies of private data, frequently entailing substantial costs. To address the challenge above, in this paper, we propose EKRG, a novel Retrieval-Generation framework based on large language models (LLMs), expertly designed to enable question-answering for Enterprise Knowledge bases with limited annotation costs. Specifically, for the retrieval process, we first introduce an instruction-tuning method using an LLM to generate sufficient document-question pairs for training a knowledge retriever. This method, through carefully designed instructions, efficiently generates diverse questions for enterprise knowledge bases, encompassing both fact-oriented and solution-oriented knowledge. Additionally, we develop a relevance-aware teacher-student learning strategy to further enhance the efficiency of the training process. For the generation process, we propose a novel chain of thought (CoT) based fine-tuning method to empower the LLM-based generator to adeptly respond to user questions using retrieved documents. Finally, extensive experiments on real-world datasets have demonstrated the effectiveness of our proposed framework.",
    "pdf_link": "https://arxiv.org/abs/2404.08695",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08695v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08695/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08695v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08695/ablation.png"
      }
    ],
    "abstract_cn": "高效管理知识对于增强企业和组织的操作效率与创新力至关重要。通过知识向量化索引，催生了众多提升知识管理系统效率的检索方法。随着生成性自然语言处理技术的突飞猛进，现在能够从相关文档中提取出精确且连贯的答案。但对于企业知识库而言，由于数据隐私和安全的限制，从头构建大量的训练数据集以支持知识检索和生成任务，既困难又昂贵。为应对这一挑战，本文提出了EKRG，一种创新的基于大型语言模型（LLMs）的检索-生成框架，旨在以较低的标注成本为企业知识库提供问答服务。具体来说，在检索阶段，我们首先利用LLM引入了一种指令调整方法，以生成足够的文档-问题对，用以训练知识检索器。这种方法通过精心设计的指令，能够高效生成涵盖事实和解决方案导向的企业知识库问题。同时，我们还开发了一种基于相关性感知的师生学习策略，进一步提升训练效率。在生成阶段，我们提出了一种新颖的基于思维链（CoT）的微调方法，使LLM能够灵活运用检索到的文档，精准回应用户询问。最终，我们在真实世界数据集上的广泛实验验证了我们框架的有效性。",
    "title_cn": "利用大型语言模型提升企业知识库的问答能力",
    "tags": [
      "LLM应用",
      "企业知识管理",
      ""
    ]
  },
  {
    "title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
    "submit_datetime": "2024年04月20日",
    "abstract": "Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.",
    "pdf_link": "https://arxiv.org/abs/2404.13501",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13501v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13501/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13501v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13501/cx1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13501v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13501/cx2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13501v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13501/cx3.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLM）驱动的智能代理近期备受研究和产业界的瞩目。相较于传统LLM，这些代理以其自我演化的特性脱颖而出，这为解决涉及长期复杂交互的现实问题提供了可能。在这些交互中，代理的“记忆”功能扮演着核心角色。尽管先前的研究提出了众多有潜力的内存机制，但这些成果分散在众多文献中，尚未有系统性综述来全面总结和比较这些研究，以提炼出对未来研究有启发性的通用且有效设计模式。本文旨在填补这一研究空白，全面审视基于LLM的代理的内存机制。我们首先探讨了LLM代理中“内存是什么”以及“为何我们需要它”。接着，我们系统性地回顾了关于内存模块设计和评估的先前研究。此外，我们也展示了许多内存模块发挥关键作用的代理应用案例。文末，我们分析了现有研究的局限，并指出了未来研究的重要方向。为紧跟该领域的最新进展，我们建立了一个在线资源库，地址为 \\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}。",
    "title_cn": "本文综述了基于大型语言模型的智能代理的记忆机制。",
    "tags": [
      "Agent",
      "人工智能",
      "智能代理"
    ]
  },
  {
    "title": "Large Language Models as Test Case Generators: Performance Evaluation and Enhancement",
    "submit_datetime": "2024年04月20日",
    "abstract": "Code generation with Large Language Models (LLMs) has been extensively studied and achieved remarkable progress. As a complementary aspect to code generation, test case generation is of crucial importance in ensuring the quality and reliability of code. However, using LLMs as test case generators has been much less explored. Current research along this line primarily focuses on enhancing code generation with assistance from test cases generated by LLMs, while the performance of LLMs in test case generation alone has not been comprehensively examined. To bridge this gap, we conduct extensive experiments to study how well LLMs can generate high-quality test cases. We find that as the problem difficulty increases, state-of-the-art LLMs struggle to generate correct test cases, largely due to their inherent limitations in computation and reasoning. To mitigate this issue, we further propose a multi-agent framework called \\emph{TestChain} that decouples the generation of test inputs and test outputs. Notably, TestChain uses a ReAct format conversation chain for LLMs to interact with a Python interpreter in order to provide more accurate test outputs. Our results indicate that TestChain outperforms the baseline by a large margin. Particularly, in terms of the accuracy of test cases, TestChain using GPT-4 as the backbone achieves a 13.84\\% improvement over the baseline on the LeetCode-hard dataset.",
    "pdf_link": "https://arxiv.org/abs/2404.13340",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x19.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x20.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x21.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x22.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x23.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x24.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x25.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x26.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x27.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x28.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x29.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x30.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x31.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x32.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x33.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x34.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x35.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13340v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13340/x36.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）在代码生成领域的应用已取得显著成果，而其在测试用例生成方面的潜力却鲜少被挖掘。尽管当前研究多聚焦于利用LLMs辅助生成的测试用例来提升代码生成质量，但LLMs独立生成测试用例的能力尚未得到充分评估。为了填补这一研究空白，我们开展了深入的实验，探究LLMs生成高品质测试用例的能力。实验结果显示，面对日益复杂的挑战，即便是最先进的LLMs也难以生成无误的测试用例，这主要归咎于它们在计算和逻辑推理上的局限性。针对这一问题，我们设计了一种创新的多代理框架——\\emph{TestChain}，它将测试输入与输出的生成过程分离，并通过ReAct格式的对话链让LLMs与Python解释器进行互动，以生成更精确的测试输出。实验结果证明，TestChain在性能上大幅超越了传统基线，尤其是在测试用例的准确性上，采用GPT-4作为核心的TestChain在LeetCode-hard数据集上实现了13.84%的显著提升。",
    "title_cn": "将大型语言模型用作测试用例生成器：评估性能并进行优化",
    "tags": [
      "Agent",
      "软件工程",
      "测试自动化"
    ]
  },
  {
    "title": "FakeBench: Uncover the Achilles' Heels of Fake Images with Large Multimodal Models",
    "submit_datetime": "2024年04月20日",
    "abstract": "Recently, fake images generated by artificial intelligence (AI) models have become indistinguishable from the real, exerting new challenges for fake image detection models. To this extent, simple binary judgments of real or fake seem less convincing and credible due to the absence of human-understandable explanations. Fortunately, Large Multimodal Models (LMMs) bring possibilities to materialize the judgment process while their performance remains undetermined. Therefore, we propose FakeBench, the first-of-a-kind benchmark towards transparent defake, consisting of fake images with human language descriptions on forgery signs. FakeBench gropes for two open questions of LMMs: (1) can LMMs distinguish fake images generated by AI, and (2) how do LMMs distinguish fake images? In specific, we construct the FakeClass dataset with 6k diverse-sourced fake and real images, each equipped with a Question&Answer pair concerning the authenticity of images, which are utilized to benchmark the detection ability. To examine the reasoning and interpretation abilities of LMMs, we present the FakeClue dataset, consisting of 15k pieces of descriptions on the telltale clues revealing the falsification of fake images. Besides, we construct the FakeQA to measure the LMMs' open-question answering ability on fine-grained authenticity-relevant aspects. Our experimental results discover that current LMMs possess moderate identification ability, preliminary interpretation and reasoning ability, and passable open-question answering ability for image defake. The FakeBench will be made publicly available soon.",
    "pdf_link": "https://arxiv.org/abs/2404.13306",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13306v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13306/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13306v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13306/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13306v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13306/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13306v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13306/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13306v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13306/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13306v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13306/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13306v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13306/x7.png"
      }
    ],
    "abstract_cn": "近期，人工智能（AI）生成的假图像日益逼真，对假图像检测技术构成了严峻挑战。在这一背景下，缺乏清晰解释的简单真假判断已不再那么令人信服。然而，大型多模态模型（LMMs）的出现为这一问题提供了新的解决途径，尽管其效果尚待验证。我们因此引入了FakeBench，这是一个首创的、旨在实现透明去伪的基准测试平台，它包含了带有伪造迹象描述的假图像。FakeBench旨在解答关于LMMs的两大疑问：其一，LMMs是否能够识别出AI生成的假图像；其二，LMMs是如何进行这一识别的。具体而言，我们创建了FakeClass数据集，包含6000张来源多样的真假图像，每张图像都附带了关于其真实性的问答对，用以评估检测能力。为了测试LMMs的推理和解释能力，我们推出了FakeClue数据集，包含15000条揭露假图像伪造迹象的描述。此外，我们还构建了FakeQA，用以衡量LMMs在细节真实性方面开放性问题的回答能力。实验结果显示，目前的LMMs在图像去伪方面具备一定的识别能力，初步的解释和推理能力，以及尚可的开放性问题回答能力。FakeBench将很快向公众开放。",
    "title_cn": "FakeBench：借助大型多模态模型，挖掘伪造图像的软肋。",
    "tags": [
      "LLM应用",
      "人工智能",
      "图像识别"
    ]
  },
  {
    "title": "PCQA: A Strong Baseline for AIGC Quality Assessment Based on Prompt Condition",
    "submit_datetime": "2024年04月20日",
    "abstract": "The development of Large Language Models (LLM) and Diffusion Models brings the boom of Artificial Intelligence Generated Content (AIGC). It is essential to build an effective quality assessment framework to provide a quantifiable evaluation of different images or videos based on the AIGC technologies. The content generated by AIGC methods is driven by the crafted prompts. Therefore, it is intuitive that the prompts can also serve as the foundation of the AIGC quality assessment. This study proposes an effective AIGC quality assessment (QA) framework. First, we propose a hybrid prompt encoding method based on a dual-source CLIP (Contrastive Language-Image Pre-Training) text encoder to understand and respond to the prompt conditions. Second, we propose an ensemble-based feature mixer module to effectively blend the adapted prompt and vision features. The empirical study practices in two datasets: AIGIQA-20K (AI-Generated Image Quality Assessment database) and T2VQA-DB (Text-to-Video Quality Assessment DataBase), which validates the effectiveness of our proposed method: Prompt Condition Quality Assessment (PCQA). Our proposed simple and feasible framework may promote research development in the multimodal generation field.",
    "pdf_link": "https://arxiv.org/abs/2404.13299",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13299v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13299/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13299v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13299/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13299v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13299/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13299v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13299/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13299v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13299/x5.png"
      }
    ],
    "abstract_cn": "随着大型语言模型（LLM）和扩散模型的兴起，人工智能生成内容（AIGC）迎来了蓬勃发展。为了对基于AIGC技术生成的多样化图像或视频进行量化评估，亟需建立一个高效的质量评估体系。AIGC生成的内容往往依赖于精心设计的提示，提示本身自然成为了质量评估的关键。本研究提出了一套创新的AIGC质量评估框架。首先，我们引入了一种混合提示编码技术，依托双源CLIP文本编码器，以深入理解和响应提示的要求。接着，我们设计了一个集成特征混合模块，用以高效融合调整后的提示与视觉特征。通过在AIGIQA-20K和T2VQA-DB两大数据库上的实证研究，我们验证了所提出的提示条件质量评估（PCQA）方法的有效性。这一简洁而实用的框架有望推动多模态生成研究领域的进一步发展。",
    "title_cn": "PCQA：一种基于提示条件的AI生成内容（AIGC）质量评估的强有力基准",
    "tags": [
      "LLM应用",
      "人工智能",
      "内容生成"
    ]
  },
  {
    "title": "Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented Generation",
    "submit_datetime": "2024年04月19日",
    "abstract": "While Retrieval-Augmented Generation (RAG) plays a crucial role in the application of Large Language Models (LLMs), existing retrieval methods in knowledge-dense domains like law and medicine still suffer from a lack of multi-perspective views, which are essential for improving interpretability and reliability. Previous research on multi-view retrieval often focused solely on different semantic forms of queries, neglecting the expression of specific domain knowledge perspectives. This paper introduces a novel multi-view RAG framework, MVRAG, tailored for knowledge-dense domains that utilizes intention-aware query rewriting from multiple domain viewpoints to enhance retrieval precision, thereby improving the effectiveness of the final inference. Experiments conducted on legal and medical case retrieval demonstrate significant improvements in recall and precision rates with our framework. Our multi-perspective retrieval approach unleashes the potential of multi-view information enhancing RAG tasks, accelerating the further application of LLMs in knowledge-intensive fields.",
    "pdf_link": "https://arxiv.org/abs/2404.12879",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12879v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12879/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12879v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12879/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12879v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12879/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12879v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12879/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12879v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12879/x5.png"
      }
    ],
    "abstract_cn": "检索增强生成（RAG）对于大型语言模型（LLM）的应用至关重要，但在法律和医学等知识密集型领域，现有检索方法因缺乏多视角视图而受限，这关键于提升模型的解释力和可靠性。过往研究多聚焦于查询的语义多样性，却忽略了领域知识视角的特定表达。本文提出了一个创新的多视图RAG框架——MVRAG，专为知识密集型领域设计，通过多领域视角的意图感知查询重写，增强检索的精准度，进而提升最终推理的效果。在法律和医学案例检索的实验中，我们的框架显著提升了召回率和精确率。这种多视角检索策略充分发挥了多视图信息的潜力，为RAG任务带来增益，推动了LLM在知识密集型行业的广泛应用。",
    "title_cn": "揭示知识密集型检索增强生成中的多维洞见。",
    "tags": [
      "分类：RAG",
      "",
      ""
    ]
  },
  {
    "title": "Generating Test Scenarios from NL Requirements using Retrieval-Augmented LLMs: An Industrial Study",
    "submit_datetime": "2024年04月19日",
    "abstract": "Test scenarios are specific instances of test cases that describe actions to validate a particular software functionality. By outlining the conditions under which the software operates and the expected outcomes, test scenarios ensure that the software functionality is tested in an integrated manner. Test scenarios are crucial for systematically testing an application under various conditions, including edge cases, to identify potential issues and guarantee overall performance and reliability. Specifying test scenarios is tedious and requires a deep understanding of software functionality and the underlying domain. It further demands substantial effort and investment from already time- and budget-constrained requirements engineers and testing teams. This paper presents an automated approach (RAGTAG) for test scenario generation using Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). RAG allows the integration of specific domain knowledge with LLMs' generation capabilities. We evaluate RAGTAG on two industrial projects from Austrian Post with bilingual requirements in German and English. Our results from an interview survey conducted with four experts on five dimensions -- relevance, coverage, correctness, coherence and feasibility, affirm the potential of RAGTAG in automating test scenario generation. Specifically, our results indicate that, despite the difficult task of analyzing bilingual requirements, RAGTAG is able to produce scenarios that are well-aligned with the underlying requirements and provide coverage of different aspects of the intended functionality. The generated scenarios are easily understandable to experts and feasible for testing in the project environment. The overall correctness is deemed satisfactory; however, gaps in capturing exact action sequences and domain nuances remain, underscoring the need for domain expertise when applying LLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.12772",
    "graphs": [],
    "abstract_cn": "测试场景是针对特定软件功能的测试用例的具体应用，它们通过定义软件运行条件和预期成果，确保软件功能的全面集成测试。这些场景对于在多变条件下系统性地测试应用程序、捕捉边缘情况、发现潜在问题以及确保软件的整体性能和可靠性至关重要。然而，制定测试场景既繁琐又复杂，它不仅要求深入掌握软件功能和相关领域知识，还对时间和预算本就紧张的需求工程师和测试团队提出了更高的要求。本论文提出了一种名为RAGTAG的自动化测试场景生成方法，该方法利用检索增强生成（RAG）技术和大型语言模型（LLM）。RAG技术使得特定领域的知识能够与LLM的生成能力相结合。我们在奥地利邮政的两个双语（德语和英语）工业项目中对RAGTAG进行了评估。通过与四位专家就相关性、覆盖度、准确性、连贯性和可行性五个方面进行的访谈调查，我们证实了RAGTAG在自动化生成测试场景方面的潜力。尽管面对分析双语需求的挑战，RAGTAG仍能生成与原始需求高度契合且全面覆盖预期功能各个方面的场景。这些场景不仅易于专家理解，也适合在项目环境中进行测试。尽管整体正确性得到了认可，但在捕捉精确动作序列和领域细节方面仍有改进空间，这表明在使用LLM时，领域专业知识的运用至关重要。",
    "title_cn": "本工业研究探讨了如何利用增强检索功能的大型语言模型，从自然语言需求中生成测试场景。",
    "tags": [
      "RAG",
      "软件测试",
      ""
    ]
  },
  {
    "title": "The Solution for the CVPR2024 NICE Image Captioning Challenge",
    "submit_datetime": "2024年04月19日",
    "abstract": "This report introduces a solution to the Topic 1 Zero-shot Image Captioning of 2024 NICE : New frontiers for zero-shot Image Captioning Evaluation. In contrast to NICE 2023 datasets, this challenge involves new annotations by humans with significant differences in caption style and content. Therefore, we enhance image captions effectively through retrieval augmentation and caption grading methods. At the data level, we utilize high-quality captions generated by image caption models as training data to address the gap in text styles. At the model level, we employ OFA (a large-scale visual-language pre-training model based on handcrafted templates) to perform the image captioning task. Subsequently, we propose caption-level strategy for the high-quality caption data generated by the image caption models and integrate them with retrieval augmentation strategy into the template to compel the model to generate higher quality, more matching, and semantically enriched captions based on the retrieval augmentation prompts. Our approach ranks first on the leaderboard, achieving a CIDEr score of 234.11 and 1st in all other metrics.",
    "pdf_link": "https://arxiv.org/abs/2404.12739",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12739v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12739/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12739v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12739/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12739v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12739/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12739v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12739/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12739v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12739/x5.png"
      }
    ],
    "abstract_cn": "本报告提出了一种针对2024年NICE挑战赛第1主题——零样本图像字幕生成的创新解决方案。与2023年的数据集相比，新挑战的人类注释在字幕的风格和内容上有了显著变化。我们通过改进的检索增强技术和字幕评分方法，有效提升了字幕的质量。在数据处理上，我们采用了由图像字幕模型生成的优质字幕作为训练素材，以弥补文本风格的差异。模型方面，我们部署了OFA——一种基于定制模板的大规模视觉-语言预训练模型，来执行字幕生成任务。此外，我们还提出了一种基于字幕级别的策略，将这些优质数据与检索增强技术相结合，推动模型产出更精准、语义更丰富的高质量字幕。这一方法在排行榜上勇夺第一，CIDEr得分高达234.11分，并在其他所有评价指标上均领跑。",
    "title_cn": "CVPR2024 NICE 图像标题挑战的应对之道",
    "tags": [
      "分类：LLM应用\n\n这篇论文的摘要描述了一种针对零样本图像字幕生成任务的解决方案，使用了大规模视觉-语言预训练模型（OFA）和字幕级别的策略来生成高质量的字幕。这表明了该研究在实际应用中使用了大型语言模型（LLM），因此可以归类为LLM应用。",
      "计算机视觉",
      ""
    ]
  },
  {
    "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
    "submit_datetime": "2024年04月19日",
    "abstract": "This paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models. Additionally, employing reasoning iterations on top of RAG delivers an even bigger jump in performance, enabling the Q&A systems to get closer to human-expert quality. We discuss the implications of such findings, propose a structured technical design space capturing major technical components of Q&A AI, and provide recommendations for making high-impact technical choices for such components. We plan to follow up on this work with actionable guides for AI teams and further investigations into the impact of domain-specific augmentation in RAG and into agentic AI capabilities such as advanced planning and reasoning.",
    "pdf_link": "https://arxiv.org/abs/2404.11792",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11792v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11792/ooda.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11792v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11792/ooda-rag.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11792v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11792/ooda-financebench.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11792v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11792/design-space.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11792v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11792/financebench-categories.png"
      }
    ],
    "abstract_cn": "本研究深入探讨了特定领域模型微调和推理机制在提升大型语言模型（LLMs）和增强检索生成（RAG）驱动的问答（Q&A）系统性能方面的作用。通过FinanceBench SEC财务文件数据集的分析，我们发现，将微调后的嵌入模型与LLM相结合，相较于通用模型，能够显著提升RAG的准确性，尤其是微调后的嵌入模型贡献更为显著。进一步地，叠加推理迭代能够显著提升性能，使得Q&A系统的表现更接近于人类专家水平。文章还讨论了这些发现的意义，提出了一个系统化的技术设计框架，涵盖了Q&A人工智能的关键技术要素，并为这些要素提供了技术选择的建议。未来，我们将继续开展工作，为AI团队提供实践指南，并深入研究RAG中特定领域增强的影响以及代理AI能力，如高级规划和推理等方面的影响。",
    "title_cn": "通过针对特定领域的精细调整和迭代推理过程，提升问答系统的性能：一项对比分析研究",
    "tags": [
      "分类：RAG",
      "",
      "问答系统"
    ]
  },
  {
    "title": "AutoCrawler: A Progressive Understanding Web Agent for Web Crawler Generation",
    "submit_datetime": "2024年04月19日",
    "abstract": "Web automation is a significant technique that accomplishes complicated web tasks by automating common web actions, enhancing operational efficiency, and reducing the need for manual intervention. Traditional methods, such as wrappers, suffer from limited adaptability and scalability when faced with a new website. On the other hand, generative agents empowered by large language models (LLMs) exhibit poor performance and reusability in open-world scenarios. In this work, we introduce a crawler generation task for vertical information web pages and the paradigm of combining LLMs with crawlers, which helps crawlers handle diverse and changing web environments more efficiently. We propose AutoCrawler, a two-stage framework that leverages the hierarchical structure of HTML for progressive understanding. Through top-down and step-back operations, AutoCrawler can learn from erroneous actions and continuously prune HTML for better action generation. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Resources of this paper can be found at \\url{https://github.com/EZ-hwh/AutoCrawler}",
    "pdf_link": "https://arxiv.org/abs/2404.12753",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12753v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12753/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12753v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12753/x2.png"
      }
    ],
    "abstract_cn": "网络自动化技术通过模拟常规网络行为，自动化执行复杂的网络任务，以此提升工作效率并降低手工操作的必要性。相较于传统方法如包装器，它们在适应新网站时存在适应性和扩展性的限制。与此同时，由大型语言模型（LLMs）驱动的生成性代理在开放环境下的性能和可复用性不尽人意。本研究提出了一种针对垂直信息网页的爬虫生成任务，以及一种融合LLMs与爬虫的新范式，以提高爬虫在多样化和动态变化的网络环境中的适应性和效率。我们设计了AutoCrawler，这是一个分两阶段的框架，它利用HTML的层级结构实现逐步理解。通过自上而下的操作和错误反馈学习，AutoCrawler能够不断优化HTML解析，以生成更精准的动作。我们对多个LLMs进行了广泛的测试，验证了我们框架的有效性。本文的相关资源可在 \\url{https://github.com/EZ-hwh/AutoCrawler} 获取。",
    "title_cn": "AutoCrawler：一种渐进式理解的网络代理，专为网络爬虫生成而设计。",
    "tags": [
      "Agent",
      "网络自动化",
      "网页爬虫"
    ]
  },
  {
    "title": "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works",
    "submit_datetime": "2024年04月19日",
    "abstract": "Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CroSS dataset from literature experts and assess the generated profiles by comparing ground truth references and their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. We believe our constructed resource will promote further research in this field. Resources are available at https://github.com/Joanna0123/character_profiling.",
    "pdf_link": "https://arxiv.org/abs/2404.12726",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）以其卓越的性能引领了众多AI应用的潮流，其中角色扮演代理（RPAs）因其独特的魅力而备受青睐，尤其是在虚构角色的塑造上。RPAs的核心在于LLMs对虚构作品中角色的深刻理解。尽管以往的研究尝试通过基础分类或特征模仿任务来评估LLMs的这一能力，但这些方法并未能充分捕捉到LLMs对角色理解的细腻之处。本文提出了一种新的评价方法——通过角色画像任务来衡量LLMs对角色的理解力，即从相关材料中提炼出角色画像，这一方法在RPAs的发展中虽被广泛应用，却鲜少受到学术界的关注。我们特别构建了CroSS数据集，并邀请文学专家参与，通过比对基准真实参考和角色画像在下游任务中的应用性来评估生成的画像。我们进行的一系列实验，包括多种总结方法和LLMs的测试，均取得了令人鼓舞的成果，这些成果充分证实了LLMs在角色理解方面的能力。我们期望本研究能够为该领域的深入探索提供动力。相关资源已在 https://github.com/Joanna0123/character_profiling 上发布。",
    "title_cn": "本研究通过分析虚构作品中的角色档案，探讨了大型语言模型对角色认知的深度。",
    "tags": [
      "分类：Agent\n\n这篇论文讨论了大型语言模型（LLMs）在角色扮演代理（RPAs）中的应用，特别是它们如何理解和塑造虚构角色。论文提出了一种新的评价方法，通过角色画像任务来衡量LLMs对角色的理解力。这表明论文的重点是LLMs在特定应用领域（即RPAs）的性能和应用，因此将其归类为Agent。",
      "人工智能",
      ""
    ]
  },
  {
    "title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context",
    "submit_datetime": "2024年04月19日",
    "abstract": "As the key component in multimodal large language models (MLLMs), the ability of the visual encoder greatly affects MLLM's understanding on diverse image content. Although some large-scale pretrained vision encoders such as vision encoders in CLIP and DINOv2 have brought promising performance, we found that there is still no single vision encoder that can dominate various image content understanding, e.g., the CLIP vision encoder leads to outstanding results on general image understanding but poor performance on document or chart content. To alleviate the bias of CLIP vision encoder, we first delve into the inherent behavior of different pre-trained vision encoders and then propose the MoVA, a powerful and novel MLLM, adaptively routing and fusing task-specific vision experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design a context-aware expert routing strategy to dynamically select the most suitable vision experts according to the user instruction, input image, and expertise of vision experts. This benefits from the powerful model function understanding ability of the large language model (LLM) equipped with expert-routing low-rank adaptation (LoRA). In the fine-grained stage, we elaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse task-specific knowledge from various experts. This coarse-to-fine paradigm effectively leverages representations from experts based on multimodal context and model expertise, further enhancing the generalization ability. We conduct extensive experiments to evaluate the effectiveness of the proposed approach. Without any bells and whistles, MoVA can achieve significant performance gains over current state-of-the-art methods in a wide range of challenging multimodal benchmarks. Codes and models will be available at https://github.com/TempleX98/MoVA.",
    "pdf_link": "https://arxiv.org/abs/2404.13046",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13046v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13046/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13046v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13046/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13046v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13046/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13046v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13046/x4.png"
      }
    ],
    "abstract_cn": "在多模态大型语言模型（MLLMs）中，视觉编码器扮演着核心角色，其性能直接影响模型对丰富图像内容的解析能力。尽管像CLIP和DINOv2这样的大规模预训练视觉编码器已经展现出不俗的表现，但目前尚未有单一编码器能在各类图像内容理解上都占据优势，例如CLIP在常规图像理解上表现出色，却在文档或图表类内容上不尽人意。为了克服CLIP视觉编码器的局限性，我们首先剖析了不同预训练视觉编码器的内在特性，并据此提出了MoVA——一种创新的MLLM，它能够通过由粗到细的机制，灵活地引导和整合特定任务的视觉专家。在粗粒度层面，我们开发了一种基于上下文的专家路由策略，能够根据用户的指令、输入图像以及视觉专家的专长，动态地选择最合适的视觉专家。这一策略得益于我们的大型语言模型（LLM）所具备的强大功能理解能力，以及专家路由低秩适应（LoRA）技术的加持。在细粒度层面，我们精心打造了混合视觉专家适配器（MoV-Adapter），用于从众多专家中提炼并融合任务相关的知识。这种由粗到细的处理模式，充分利用了多模态上下文和模型专长，有效提升了模型的泛化性能。我们通过大量实验验证了本方法的有效性，MoVA在无需额外修饰的情况下，便能在众多复杂的多模态基准测试中取得显著的性能提升。相关代码和模型将在 https://github.com/TempleX98/MoVA 提供。",
    "title_cn": "MoVA：将视觉专家的混合应用于多模态环境的适应性研究",
    "tags": [
      "LLM应用",
      "计算机视觉",
      ""
    ]
  },
  {
    "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
    "submit_datetime": "2024年04月19日",
    "abstract": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability. Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding. Such capabilities are built upon a localized visual tokenization mechanism, where an image input is decomposed into regions of interest and subsequently encoded into region tokens. By integrating region tokens into user instructions and model responses, we seamlessly enable Groma to understand user-specified region inputs and ground its textual output to images. Besides, to enhance the grounded chat ability of Groma, we curate a visually grounded instruction dataset by leveraging the powerful GPT-4V and visual prompting techniques. Compared with MLLMs that rely on the language model or external module for localization, Groma consistently demonstrates superior performances in standard referring and grounding benchmarks, highlighting the advantages of embedding localization into image tokenization. Project page: https://groma-mllm.github.io/.",
    "pdf_link": "https://arxiv.org/abs/2404.13013",
    "graphs": [],
    "abstract_cn": "我们推出了 Groma，这是一款具备精细视觉感知能力的多模态大型语言模型（MLLM）。Groma 精于区域级任务，例如区域字幕和视觉定位，超越了对图像的整体理解。这一能力基于一种将图像分解为关键区域并转化为区域标记的局部视觉标记机制。通过将这些区域标记融入用户指令和模型反馈，Groma 能够流畅地识别用户指定的区域并将其文本输出与图像紧密结合。此外，为了提升 Groma 的视觉基础对话能力，我们利用先进的 GPT-4V 和视觉提示技术，精心构建了一个视觉基础指令数据集。与那些依赖于语言模型或外部模块进行定位的 MLLM 相比，Groma 在标准的引用和定位基准测试中持续展现出更优的表现，彰显了将定位功能整合进图像标记化的优势。项目详情可见：https://groma-mllm.github.io/。",
    "title_cn": "Groma：为多模态大型语言模型提供精准视觉标记化技术",
    "tags": [
      "分类：LLM应用\n\n这篇论文介绍了一个名为 Groma 的多模态大型语言模型（MLLM），它在区域级任务（如区域字幕和视觉定位）中表现出色。Groma 的能力基于将图像分解为关键区域并转化为区域标记的局部视觉标记机制。此外，为了提升 Groma 的视觉基础对话能力，研究者利用了先进的 GPT-4V 和视觉提示技术。这篇论文主要关注 Groma 在实际应用中的表现和优势，因此将其归类为 LLM应用。",
      "视觉识别",
      "人工智能"
    ]
  },
  {
    "title": "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?",
    "submit_datetime": "2024年04月19日",
    "abstract": "The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly in-context learning, where MLLMs enhance task performance without updating pre-trained parameters. This effectiveness, however, hinges on the appropriate selection of in-context examples, a process that is currently biased towards visual data, overlooking textual information. Furthermore, the area of supervised retrievers for MLLMs, crucial for optimal in-context example selection, continues to be uninvestigated. Our study offers an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities. Responding to this, we introduce a novel supervised MLLM-retriever MSIER that employs a neural network to select examples that enhance multimodal in-context learning efficiency. This approach is validated through extensive testing across three distinct tasks, demonstrating the method's effectiveness. Additionally, we investigate the influence of modalities on our supervised retrieval method's training and pinpoint factors contributing to our model's success. This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data.",
    "pdf_link": "https://arxiv.org/abs/2404.12866",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12866v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12866/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12866v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12866/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12866v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12866/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12866v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12866/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12866v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12866/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12866v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12866/x6.png"
      }
    ],
    "abstract_cn": "随着多模态大型语言模型（MLLMs）参数量的扩大，它们在无需调整预训练参数的情况下，通过上下文学习显著提升了任务执行能力。然而，这种性能提升依赖于精心挑选的上下文示例，目前这一选择过程存在偏差，偏重视觉数据而忽略了文本信息。对于优化上下文示例选择至关重要的监督检索器领域，在MLLMs中的应用尚未得到充分探索。本研究深入分析了文本信息在多模态环境下对无监督选择上下文示例的影响，发现检索器的性能对所采用的信息模态极为敏感。为此，我们提出了一种创新的监督MLLM-检索器MSIER，利用神经网络精选示例以提高多模态上下文学习效率。该方法在三项不同任务上的广泛测试中证明了其有效性。我们还探讨了模态对我们监督检索方法训练的影响，并识别了推动模型成功的关键因素。这一研究为未来的技术进步奠定了基础，展示了通过策略性地运用多模态数据，对MLLMs中的上下文学习进行优化的巨大潜力。",
    "title_cn": "文本信息对多模态上下文内学习（MCL）检索过程的影响是怎样的？",
    "tags": [
      "LLM应用",
      "人工智能",
      "多模态学习"
    ]
  },
  {
    "title": "TextSquare: Scaling up Text-Centric Visual Instruction Tuning",
    "submit_datetime": "2024年04月19日",
    "abstract": "Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini, partly due to a lack of extensive, high-quality instruction tuning data. To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, which is generated using closed-source MLLMs. The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation. Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%). It even outperforms top-tier models like GPT4V and Gemini in 6 of 10 text-centric benchmarks. 2) Additionally, we demonstrate the critical role of VQA reasoning data in offering comprehensive contextual insights for specific questions. This not only improves accuracy but also significantly mitigates hallucinations. Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models. 3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M.",
    "pdf_link": "https://arxiv.org/abs/2404.12803",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12803v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12803/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12803v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12803/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12803v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12803/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12803v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12803/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12803v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12803/x5.png"
      }
    ],
    "abstract_cn": "随着多模态大型语言模型（MLLMs）的进展，文本中心视觉问答（VQA）领域取得了显著突破。尽管如此，开源模型相较于GPT4V和Gemini等顶尖模型仍有不足，这在一定程度上归咎于高质量指令调优数据的匮乏。为了解决这一问题，我们提出了一种创新方法，用于构建一个庞大且高质量的指令调优数据集——Square-10M，该数据集利用闭源MLLMs生成。整个数据构建流程，即Square流程，分为自我提问、解答、推理和评估四个阶段。通过Square-10M的实验，我们得出了三项重要发现：首先，我们的模型TextSquare在OCRBench上达到了62.2%的准确率，大幅超越了先前的开源顶尖模型，并为文本中心MLLMs设定了新的基准。其次，TextSquare在10个文本中心基准测试中的6个上超越了GPT4V和Gemini等顶级模型。此外，我们还证实了VQA推理数据在提供特定问题全面上下文洞察中的核心作用，这不仅提升了准确度，还显著减少了幻觉现象。具体来说，TextSquare在四个通用VQA和幻觉评估数据集上的平均准确率达到了75.1%，超越了之前的顶尖模型。最后，文本中心VQA数据集规模的扩展呈现出明显的规律：指令调优数据量的指数级增长与模型性能的提升成正比，这进一步证实了大规模和高质量数据集的必要性，以及Square-10M的卓越价值。",
    "title_cn": "TextSquare：扩展文本导向的视觉指令调优",
    "tags": [
      "LLM应用",
      "视觉问答",
      "数据集构建"
    ]
  },
  {
    "title": "Large Language Model Supply Chain: A Research Agenda",
    "submit_datetime": "2024年04月19日",
    "abstract": "The rapid advancements in pre-trained Large Language Models (LLMs) and Large Multimodal Models (LMMs) have ushered in a new era of intelligent applications, transforming fields ranging from natural language processing to content generation. The LLM supply chain represents a crucial aspect of the contemporary artificial intelligence landscape. It encompasses the entire lifecycle of pre-trained models, from its initial development and training to its final deployment and application in various domains. This paper presents a comprehensive overview of the LLM supply chain, highlighting its three core elements: 1) the model infrastructure, encompassing datasets and toolchain for training, optimization, and deployment; 2) the model lifecycle, covering training, testing, releasing, and ongoing maintenance; and 3) the downstream application ecosystem, enabling the integration of pre-trained models into a wide range of intelligent applications. However, this rapidly evolving field faces numerous challenges across these key components, including data privacy and security, model interpretability and fairness, infrastructure scalability, and regulatory compliance. Addressing these challenges is essential for harnessing the full potential of LLMs and ensuring their ethical and responsible use. This paper provides a future research agenda for the LLM supply chain, aiming at driving the continued advancement and responsible deployment of these transformative LLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.12736",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12736v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12736/x1.png"
      }
    ],
    "abstract_cn": "随着预训练的大型语言模型（LLMs）和大型多模态模型（LMMs）的突飞猛进，智能应用迎来了新纪元，从自然语言处理到内容创作等多个领域都经历了革新。LLM供应链作为现代人工智能领域的关键一环，涵盖了预训练模型从研发、训练到部署和跨领域应用的完整周期。本文深入探讨了LLM供应链的三大核心要素：模型架构、模型生命周期以及下游应用生态，强调了从数据集到工具链的全方位覆盖。尽管如此，这一迅猛发展的领域在数据隐私、模型透明度、基础设施扩展性以及法规遵循等方面仍面临诸多挑战。应对这些挑战对于释放LLMs的全部潜能并确保其合理、道德使用具有重要意义。文章还为LLM供应链的未来研究方向提出了议程，旨在推动这些变革性模型的持续发展和负责任的应用。",
    "title_cn": "探索大型语言模型的供应链：制定研究路线图",
    "tags": [
      "LLM理论",
      "人工智能",
      ""
    ]
  },
  {
    "title": "Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for Text-to-SQL",
    "submit_datetime": "2024年04月18日",
    "abstract": "The current state-of-the-art (SOTA) for automated text-to-SQL still falls well short of expert human performance as measured by execution accuracy (EX) on the BIRD-SQL benchmark. The most accurate methods are also slow and expensive. To advance the SOTA for text-to-SQL while reducing cost and improving speed, we explore the combination of low-cost fine tuning, novel methods for diverse retrieval-augmented generation (RAG) and new input and output formats that help large language models (LLMs) achieve higher EX. We introduce two new methods, Dubo-SQL v1 and v2. Dubo-SQL v1 sets a new record for EX on the holdout test set of BIRD-SQL. Dubo-SQL v2 achieves even higher performance on the BIRD-SQL dev set. Dubo-SQL v1 relies on LLMs from OpenAI, but uses the low-cost GPT-3.5 Turbo while exceeding the performance of the next-best model using OpenAI, which instead uses the more expensive GPT-4. Dubo-SQL v1 exceeds the performance of the next-best model using GPT-3.5 by over 20%. Dubo-SQL v2 uses GPT-4 Turbo and RAG in place of fine tuning to push EX higher.",
    "pdf_link": "https://arxiv.org/abs/2404.12560",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12560v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12560/dubov1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12560v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12560/dubov2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12560v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12560/examplesvaccuracy.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12560v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12560/tempvaccuracy.png"
      }
    ],
    "abstract_cn": "当前自动化文本到SQL技术的最前沿在BIRD-SQL基准的执行准确度（EX）上仍未达到专家水平，且顶尖方法不仅速度慢，成本也高。为了提升这一技术前沿并降低成本、加快速度，我们研究了低成本微调、创新的多样化检索增强生成（RAG）策略，以及新的输入输出格式，以助力大型语言模型（LLMs）提升EX。我们推出了两种新方法：Dubo-SQL v1和v2。Dubo-SQL v1在BIRD-SQL的独立测试集上刷新了EX的记录，而Dubo-SQL v2在开发集上展现了更出色的性能。Dubo-SQL v1借助OpenAI的LLMs，尤其是成本效益高的GPT-3.5 Turbo，超越了使用更昂贵GPT-4的竞争对手。与使用GPT-3.5的前最佳模型相比，Dubo-SQL v1的性能提升了20%以上。Dubo-SQL v2则采用了GPT-4 Turbo和RAG技术，而非微调，以进一步提升EX。",
    "title_cn": "Dubo-SQL：一种文本到SQL的生成技术，通过多样化检索增强和精准微调，以提升性能。",
    "tags": [
      "分类：RAG",
      "数据库",
      ""
    ]
  },
  {
    "title": "iRAG: An Incremental Retrieval Augmented Generation System for Videos",
    "submit_datetime": "2024年04月18日",
    "abstract": "Retrieval augmented generation (RAG) systems combine the strengths of language generation and information retrieval to power many real-world applications like chatbots. Use of RAG for combined understanding of multimodal data such as text, images and videos is appealing but two critical limitations exist: one-time, upfront capture of all content in large multimodal data as text descriptions entails high processing times, and not all information in the rich multimodal data is typically in the text descriptions. Since the user queries are not known apriori, developing a system for multimodal to text conversion and interactive querying of multimodal data is challenging.\n  To address these limitations, we propose iRAG, which augments RAG with a novel incremental workflow to enable interactive querying of large corpus of multimodal data. Unlike traditional RAG, iRAG quickly indexes large repositories of multimodal data, and in the incremental workflow, it uses the index to opportunistically extract more details from select portions of the multimodal data to retrieve context relevant to an interactive user query. Such an incremental workflow avoids long multimodal to text conversion times, overcomes information loss issues by doing on-demand query-specific extraction of details in multimodal data, and ensures high quality of responses to interactive user queries that are often not known apriori. To the best of our knowledge, iRAG is the first system to augment RAG with an incremental workflow to support efficient interactive querying of large, real-world multimodal data. Experimental results on real-world long videos demonstrate 23x to 25x faster video to text ingestion, while ensuring that quality of responses to interactive user queries is comparable to responses from a traditional RAG where all video data is converted to text upfront before any querying.",
    "pdf_link": "https://arxiv.org/abs/2404.12309",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12309v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12309/iRAG_demo_black.png"
      }
    ],
    "abstract_cn": "检索增强生成（RAG）系统融合了语言生成与信息检索的长处，广泛应用于聊天机器人等实际应用。然而，利用 RAG 来综合解析文本、图像和视频等多模态数据虽具吸引力，却面临两大挑战：一是将大量多模态数据一次性转换为文本描述需要较长的处理时间；二是文本描述往往无法涵盖所有多模态数据中的信息。鉴于用户查询无法预先得知，构建一个能够将多模态数据转换为文本并进行交互式查询的系统颇具难度。为应对这些挑战，我们提出了 iRAG，这是一种新型的增量式工作流程，它扩展了 RAG 系统，使其能够交互式地查询大型多模态数据集。与传统 RAG 相比，iRAG 能够迅速索引庞大的多模态数据库，并在增量式工作流程中，根据需求从多模态数据的选定部分提取更多细节，以获取与用户交互查询相关的上下文。这种方法不仅避免了长时间的多模态到文本的转换，还通过按需提取多模态数据中的细节来解决信息丢失问题，并确保了对用户交互查询的响应质量。据我们所知，iRAG 是首个采用增量式工作流程来增强 RAG 系统，以支持对大型、真实世界多模态数据进行高效交互式查询的系统。在真实长视频上的实验结果显示，视频到文本的转换速度提升了 23 至 25 倍，同时保证了对交互式用户查询的响应质量与传统 RAG 系统相当，后者在任何查询之前都将所有视频数据转换为文本。",
    "title_cn": "iRAG：一种用于视频的渐进式检索增强生成系统",
    "tags": [
      "RAG",
      "聊天机器人",
      "多模态数据处理"
    ]
  },
  {
    "title": "Aligning Actions and Walking to LLM-Generated Textual Descriptions",
    "submit_datetime": "2024年04月18日",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, including data augmentation and synthetic data generation. This work explores the use of LLMs to generate rich textual descriptions for motion sequences, encompassing both actions and walking patterns. We leverage the expressive power of LLMs to align motion representations with high-level linguistic cues, addressing two distinct tasks: action recognition and retrieval of walking sequences based on appearance attributes. For action recognition, we employ LLMs to generate textual descriptions of actions in the BABEL-60 dataset, facilitating the alignment of motion sequences with linguistic representations. In the domain of gait analysis, we investigate the impact of appearance attributes on walking patterns by generating textual descriptions of motion sequences from the DenseGait dataset using LLMs. These descriptions capture subtle variations in walking styles influenced by factors such as clothing choices and footwear. Our approach demonstrates the potential of LLMs in augmenting structured motion attributes and aligning multi-modal representations. The findings contribute to the advancement of comprehensive motion understanding and open up new avenues for leveraging LLMs in multi-modal alignment and data augmentation for motion analysis. We make the code publicly available at https://github.com/Radu1999/WalkAndText",
    "pdf_link": "https://arxiv.org/abs/2404.12192",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12192v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12192/gaitclip-diagram.drawio.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12192v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12192/gaitclip-diagram2.drawio.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12192v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12192/GaitCLIP-annotation.drawio.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12192v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12192/features.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12192v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12192/classif.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12192v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12192/ndgc.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12192v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12192/per_feature.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）在多个领域展现了非凡的才能，尤其是在数据扩充和合成数据创造方面。本研究利用LLMs为运动序列生成详尽的文本描述，涉及动作和行走模式。我们借助LLMs的表达力，将运动信息与高级语言信号相结合，以应对两项特定任务：动作识别和基于外观特征的行走序列检索。在动作识别任务中，我们利用LLMs为BABEL-60数据集中的动作生成文本描述，以实现运动序列与语言描述的匹配。在步态分析方面，我们通过LLMs对DenseGait数据集中的行走序列进行文本描述，探究外观特征如衣着和鞋履对行走模式的影响。这些描述捕捉了行走风格中的微妙差异。我们的方法证明了LLMs在增强结构化运动特征和整合多模态表示方面的潜力。这些研究成果不仅推动了对运动理解的深入，也为未来在多模态同步和运动分析中应用LLMs提供了新的思路。相关代码已在 https://github.com/Radu1999/WalkAndText 上公开。",
    "title_cn": "将行动和行走与大型语言模型生成的文本描述相匹配。",
    "tags": [
      "LLM应用",
      "运动分析",
      "多模态学习"
    ]
  },
  {
    "title": "RAGAR, Your Falsehood RADAR: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models",
    "submit_datetime": "2024年04月18日",
    "abstract": "The escalating challenge of misinformation, particularly in the context of political discourse, necessitates advanced solutions for fact-checking. We introduce innovative approaches to enhance the reliability and efficiency of multimodal fact-checking through the integration of Large Language Models (LLMs) with Retrieval-augmented Generation (RAG)- based advanced reasoning techniques. This work proposes two novel methodologies, Chain of RAG (CoRAG) and Tree of RAG (ToRAG). The approaches are designed to handle multimodal claims by reasoning the next questions that need to be answered based on previous evidence. Our approaches improve the accuracy of veracity predictions and the generation of explanations over the traditional fact-checking approach of sub-question generation with chain of thought veracity prediction. By employing multimodal LLMs adept at analyzing both text and images, this research advances the capability of automated systems in identifying and countering misinformation.",
    "pdf_link": "https://arxiv.org/abs/2404.12065",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/FrontPagee.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/pipelinefc_new.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/figCoragTorag-2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/ratings_graph.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/annot.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/Question_Generation.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/QA_Elimination.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/Veracity_Prediction.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/ZSCOTPROMPT.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/CoVeFigure.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/Verification_Prompt.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12065v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12065/Correction_Prompt.png"
      }
    ],
    "abstract_cn": "面对政治话语中日益严峻的虚假信息挑战，迫切需要更先进的事实核查方案。本文介绍了一种创新方法，通过融合大型语言模型（LLMs）和基于检索增强生成（RAG）的高级推理技术，以提升多模态事实核查的可信度和效率。我们提出了两种新颖的方法论：RAG 链（CoRAG）和 RAG 树（ToRAG），旨在通过分析先前证据来推理出需要回答的后续问题，以处理多模态的声明。这些方法不仅提高了真实性预测的精确度，还增强了解释生成的能力，相较于传统基于子问题生成和思维链真实性预测的事实核查方法，表现更为出色。本研究通过运用能够分析文本和图像的多模态 LLMs，进一步推动了自动化系统在识别和抵御虚假信息方面的能力。",
    "title_cn": "RAGAR——您的虚假信息侦测雷达：通过增强的 RAG（Retrieval-Augmented Generation）推理，利用多模态大型语言模型进行政治事实核查。",
    "tags": [
      "分类：RAG",
      "政治话语分析",
      "信息安全"
    ]
  },
  {
    "title": "Exploring the landscape of large language models: Foundations, techniques, and challenges",
    "submit_datetime": "2024年04月18日",
    "abstract": "In this review paper, we delve into the realm of Large Language Models (LLMs), covering their foundational principles, diverse applications, and nuanced training processes. The article sheds light on the mechanics of in-context learning and a spectrum of fine-tuning approaches, with a special focus on methods that optimize efficiency in parameter usage. Additionally, it explores how LLMs can be more closely aligned with human preferences through innovative reinforcement learning frameworks and other novel methods that incorporate human feedback. The article also examines the emerging technique of retrieval augmented generation, integrating external knowledge into LLMs. The ethical dimensions of LLM deployment are discussed, underscoring the need for mindful and responsible application. Concluding with a perspective on future research trajectories, this review offers a succinct yet comprehensive overview of the current state and emerging trends in the evolving landscape of LLMs, serving as an insightful guide for both researchers and practitioners in artificial intelligence.",
    "pdf_link": "https://arxiv.org/abs/2404.11973",
    "graphs": [],
    "abstract_cn": "本文深入剖析了大型语言模型（LLMs）的奥秘，从基本原理到应用场景，再到精细的训练策略，逐一展开。文章详细阐释了上下文学习的原理和多种微调技术，尤其着重于那些提高参数使用效率的策略。同时，探讨了通过前沿的强化学习框架和创新方法，将人类反馈融入其中，以期使LLMs更好地符合人类的偏好。此外，还审视了检索增强生成技术，这项技术通过整合外部知识来丰富LLMs的能力。文中还涉及了LLMs应用的伦理问题，强调了审慎和负责的重要性。文章以对未来发展路径的展望作为结尾，为当前LLMs的发展现状和趋势提供了精炼而全面的概览，对于人工智能领域的研究者和从业者而言，是一份颇具洞见的指南。",
    "title_cn": "深入剖析大型语言模型的疆域：从基础理论到技术应用，再到面临的挑战。",
    "tags": [
      "LLM理论",
      "人工智能",
      ""
    ]
  },
  {
    "title": "From Language Models to Practical Self-Improving Computer Agents",
    "submit_datetime": "2024年04月18日",
    "abstract": "We develop a simple and straightforward methodology to create AI computer agents that can carry out diverse computer tasks and self-improve by developing tools and augmentations to enable themselves to solve increasingly complex tasks. As large language models (LLMs) have been shown to benefit from non-parametric augmentations, a significant body of recent work has focused on developing software that augments LLMs with various capabilities. Rather than manually developing static software to augment LLMs through human engineering effort, we propose that an LLM agent can systematically generate software to augment itself. We show, through a few case studies, that a minimal querying loop with appropriate prompt engineering allows an LLM to generate and use various augmentations, freely extending its own capabilities to carry out real-world computer tasks. Starting with only terminal access, we prompt an LLM agent to augment itself with retrieval, internet search, web navigation, and text editor capabilities. The agent effectively uses these various tools to solve problems including automated software development and web-based tasks.",
    "pdf_link": "https://arxiv.org/abs/2404.11964",
    "graphs": [],
    "abstract_cn": "我们构建了一种简洁明了的方法论，用以培育能够执行多种计算机操作并自我提升的人工智能计算机代理。这些代理通过自我开发工具和增强功能，能够解决日益复杂的任务。鉴于大型语言模型（LLMs）能从非参数化增强中获益，当前研究热点在于开发能够赋予LLMs多元能力的软件。我们提出，与其人工编写静态软件来增强LLMs，不如让LLM代理自动生成软件以自我增强。通过几个案例研究，我们证明了仅需一个基础的查询循环和精心设计的提示，LLM便能生成并利用各种增强功能，从而扩展其执行现实世界计算机任务的能力。从一个简单的终端访问权限出发，我们引导LLM代理自我增强，包括信息检索、网络搜索、网页浏览和文本编辑等能力。该代理能够灵活运用这些工具，有效解决自动化软件开发和网络相关任务。",
    "title_cn": "探索语言模型向实用的自我完善计算机代理的转变之路。",
    "tags": [
      "Agent",
      "人工智能",
      "软件开发"
    ]
  },
  {
    "title": "Spiral of Silences: How is Large Language Model Killing Information Retrieval? -- A Case Study on Open Domain Question Answering",
    "submit_datetime": "2024年04月18日",
    "abstract": "The practice of Retrieval-Augmented Generation (RAG), which integrates Large Language Models (LLMs) with retrieval systems, has become increasingly prevalent. However, the repercussions of LLM-derived content infiltrating the web and influencing the retrieval-generation feedback loop are largely uncharted territories. In this study, we construct and iteratively run a simulation pipeline to deeply investigate the short-term and long-term effects of LLM text on RAG systems. Taking the trending Open Domain Question Answering (ODQA) task as a point of entry, our findings reveal a potential digital \"Spiral of Silence\" effect, with LLM-generated text consistently outperforming human-authored content in search rankings, thereby diminishing the presence and impact of human contributions online. This trend risks creating an imbalanced information ecosystem, where the unchecked proliferation of erroneous LLM-generated content may result in the marginalization of accurate information. We urge the academic community to take heed of this potential issue, ensuring a diverse and authentic digital information landscape.",
    "pdf_link": "https://arxiv.org/abs/2404.10496",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/pipeline.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/nq_sim.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/webq_sim.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/tqa_sim.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/pop_sim.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/nq_loop_retrieval.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/pop_loop_retrieval.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/nq_loop_qa.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/pop_loop_qa.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/percentage_nq.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/percentage_pop.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/nq_bleu.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/pop_bleu.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/context_nq.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/Overall_Average_Rank_Changes.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/Combined_Average_Ranks.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/webq_loop_retrieval.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/tqa_loop_retrieval.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/webq_loop_qa.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/tqa_loop_qa.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/percentage_webq.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/percentage_tqa.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/webq_bleu.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/tqa_bleu.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/mis_plot_nq_tsv.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/mis_plot_webq_tsv.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/mis_plot_tqa_tsv.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/mis_plot_pop_tsv.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/mis_context_nq.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/combined_qa_datasets.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/nq_filter_retrieval.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/webq_filter_retrieval.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/tqa_filter_retrieval.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/pop_filter_retrieval.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/nq_filter_qa.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/webq_filter_qa.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/tqa_filter_qa.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/pop_filter_qa.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/top5-plot_nq_tsv.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/top5-plot_nq-filter-bleu_tsv.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/top5-plot_nq-filter-source_tsv.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/filter_bleu_context_nq.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10496v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10496/filter_source_context_nq.png"
      }
    ],
    "abstract_cn": "检索增强生成（RAG）技术，通过将大型语言模型（LLMs）与检索系统相结合，正变得越来越流行。但LLM生成的内容如何影响网络生态和检索生成反馈回路，仍是一个较少被研究的新领域。本研究通过构建并迭代执行模拟流程，深入探讨了LLM文本对RAG系统影响的短期与长期效应。以当前热门的开放域问答（ODQA）任务为例，研究发现LLM生成的文本在搜索排名中屡屡超越人类创作，可能导致人类在线贡献的可见度和影响力降低，形成了一种潜在的数字“沉默螺旋”效应。这一趋势有可能导致信息生态失衡，错误信息的泛滥可能使准确信息被边缘化。我们呼吁学术界对此问题给予关注，以维护一个多元和真实的数字信息环境。",
    "title_cn": "沉默之螺旋：探究大型语言模型如何影响信息检索的效能——以开放领域问答为研究案例",
    "tags": [
      "分类：RAG",
      "网络生态",
      "信息检索"
    ]
  },
  {
    "title": "HalluciBot: Is There No Such Thing as a Bad Question?",
    "submit_datetime": "2024年04月18日",
    "abstract": "Hallucination continues to be one of the most critical challenges in the institutional adoption journey of Large Language Models (LLMs). In this context, an overwhelming number of studies have focused on analyzing the post-generation phase - refining outputs via feedback, analyzing logit output values, or deriving clues via the outputs' artifacts. We propose HalluciBot, a model that predicts the probability of hallucination $\\textbf{before generation}$, for any query imposed to an LLM. In essence, HalluciBot does not invoke any generation during inference. To derive empirical evidence for HalluciBot, we employ a Multi-Agent Monte Carlo Simulation using a Query Perturbator to craft $n$ variations per query at train time. The construction of our Query Perturbator is motivated by our introduction of a new definition of hallucination - $\\textit{truthful hallucination}$. Our training methodology generated 2,219,022 estimates for a training corpus of 369,837 queries, spanning 13 diverse datasets and 3 question-answering scenarios. HalluciBot predicts both binary and multi-class probabilities of hallucination, enabling a means to judge the query's quality with regards to its propensity to hallucinate. Therefore, HalluciBot paves the way to revise or cancel a query before generation and the ensuing computational waste. Moreover, it provides a lucid means to measure user accountability for hallucinatory queries.",
    "pdf_link": "https://arxiv.org/abs/2404.12535",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12535v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12535/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12535v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12535/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12535v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12535/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12535v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12535/x4.png"
      }
    ],
    "abstract_cn": "幻觉在大型语言模型（LLMs）的机构采纳之路上依旧是一个重大挑战。在此背景下，众多研究致力于分析生成后阶段——无论是通过反馈优化结果、解析对数输出值，还是从输出特征中寻找线索。我们提出了HalluciBot，这是一个在向LLM提出任何查询之前就能预测幻觉概率的模型。实质上，HalluciBot在推理时不进行任何生成操作。为了验证HalluciBot的有效性，我们采用了多智能体蒙特卡洛模拟，并通过查询扰动器在训练阶段为每个查询生成n个变体。这一查询扰动器的设计灵感来源于我们对幻觉的新定义——“真实幻觉”。我们的训练方法针对一个包含369,837个查询的训练语料库，生成了2,219,022个估计值，覆盖了13个多样化的数据集和3种问答情境。HalluciBot能够预测幻觉的二元和多类概率，从而评估查询可能导致幻觉的风险。这使得HalluciBot能够在生成之前对查询进行修订或取消，避免了不必要的计算资源浪费。同时，它还提供了一种明确的方法来评估用户对可能导致幻觉的查询的责任。",
    "title_cn": "HalluciBot：世上真有所谓的“坏问题”吗？",
    "tags": [
      "LLM应用",
      "人工智能",
      ""
    ]
  },
  {
    "title": "mABC: multi-Agent Blockchain-Inspired Collaboration for root cause analysis in micro-services architecture",
    "submit_datetime": "2024年04月18日",
    "abstract": "The escalating complexity of micro-services architecture in cloud-native technologies poses significant challenges for maintaining system stability and efficiency. To conduct root cause analysis (RCA) and resolution of alert events, we propose a pioneering framework, multi-Agent Blockchain-inspired Collaboration for root cause analysis in micro-services architecture (mABC), to revolutionize the AI for IT operations (AIOps) domain, where multiple agents based on the powerful large language models (LLMs) perform blockchain-inspired voting to reach a final agreement following a standardized process for processing tasks and queries provided by Agent Workflow. Specifically, seven specialized agents derived from Agent Workflow each provide valuable insights towards root cause analysis based on their expertise and the intrinsic software knowledge of LLMs collaborating within a decentralized chain. To avoid potential instability issues in LLMs and fully leverage the transparent and egalitarian advantages inherent in a decentralized structure, mABC adopts a decision-making process inspired by blockchain governance principles while considering the contribution index and expertise index of each agent. Experimental results on the public benchmark AIOps challenge dataset and our created train-ticket dataset demonstrate superior performance in accurately identifying root causes and formulating effective solutions, compared to previous strong baselines. The ablation study further highlights the significance of each component within mABC, with Agent Workflow, multi-agent, and blockchain-inspired voting being crucial for achieving optimal performance. mABC offers a comprehensive automated root cause analysis and resolution in micro-services architecture and achieves a significant improvement in the AIOps domain compared to existing baselines",
    "pdf_link": "https://arxiv.org/abs/2404.12135",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12135v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12135/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12135v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12135/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12135v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12135/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12135v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12135/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12135v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12135/x5.png"
      }
    ],
    "abstract_cn": "随着云原生技术中微服务架构的复杂性不断增加，维护系统的稳定性和效率变得越来越困难。为了深入分析问题根源并解决警报事件，我们提出了一个创新的框架——微服务架构根本原因分析的多代理区块链启发式协作（mABC），这一框架有望彻底改变AI运维（AIOps）领域。在这一框架中，多个基于先进的大型语言模型（LLMs）的代理通过区块链技术的投票机制，遵循标准化流程，共同处理由代理工作流提供的任务和查询，以达成共识。特别地，源自代理工作流的七位专业代理，依托其专业知识和LLMs的内在软件理解，为根本原因分析提供了独到的见解。mABC采用了区块链治理原则启发的决策流程，同时兼顾每位代理的贡献度和专业度，以避免LLMs的潜在不稳定因素，并充分利用去中心化结构的透明和公平优势。在公共AIOps挑战数据集和我们自行构建的火车票数据集上的实验结果显示，mABC在准确定位问题根源和制定有效解决方案方面，相较于传统方法有着显著的性能提升。进一步的消融研究强调了mABC中每个组成部分的重要性，其中代理工作流、多代理系统和区块链启发式投票对于实现最优性能至关重要。mABC为微服务架构提供了全面的自动化根本原因分析与解决方案，与现有技术相比，在AIOps领域实现了重大进步。",
    "title_cn": "mABC：一种受区块链启发的多代理协作机制，专为微服务架构中的根本原因分析而设计。",
    "tags": [
      "Agent",
      "云计算",
      "人工智能运维"
    ]
  },
  {
    "title": "Aligning Language Models to Explicitly Handle Ambiguity",
    "submit_datetime": "2024年04月18日",
    "abstract": "In spoken languages, utterances are often shaped to be incomplete or vague for efficiency. This can lead to varying interpretations of the same input, based on different assumptions about the context. To ensure reliable user-model interactions in such scenarios, it is crucial for models to adeptly handle the inherent ambiguity in user queries. However, conversational agents built upon even the most recent large language models (LLMs) face challenges in processing ambiguous inputs, primarily due to the following two hurdles: (1) LLMs are not directly trained to handle inputs that are too ambiguous to be properly managed; (2) the degree of ambiguity in an input can vary according to the intrinsic knowledge of the LLMs, which is difficult to investigate. To address these issues, this paper proposes a method to align LLMs to explicitly handle ambiguous inputs. Specifically, we introduce a proxy task that guides LLMs to utilize their intrinsic knowledge to self-disambiguate a given input. We quantify the information gain from the disambiguation procedure as a measure of the extent to which the models perceive their inputs as ambiguous. This measure serves as a cue for selecting samples deemed ambiguous from the models' perspectives, which are then utilized for alignment. Experimental results from several question-answering datasets demonstrate that the LLMs fine-tuned with our approach are capable of handling ambiguous inputs while still performing competitively on clear questions within the task.",
    "pdf_link": "https://arxiv.org/abs/2404.11972",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11972v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11972/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11972v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11972/x2.png"
      }
    ],
    "abstract_cn": "在日常对话中，为了效率，人们常使语句显得简略或含糊，这可能导致基于不同上下文假设的多样化解读。在这样的情境下，模型必须巧妙地应对用户提问中的歧义，以保证用户与模型之间的可靠互动。但是，即使是基于最新大型语言模型（LLMs）的对话系统，在解析含糊信息时也遇到了难题，这主要归咎于两点：首先，LLMs 并未被专门训练来处理那些过于含糊、难以有效管理的输入；其次，输入的含糊性会根据 LLMs 内在的知识水平而异，这使得问题的探究变得复杂。为了应对这些挑战，本文提出了一种方法，让 LLMs 能够明确地处理含糊的输入。具体而言，我们设计了一个代理任务，促使 LLMs 运用其内在的知识来自动消除输入信息的歧义。我们通过量化消歧过程中的信息增益，来衡量模型对输入歧义性的感知程度。这一衡量标准帮助我们从模型的视角筛选出那些被认为是含糊的样本，进而用于模型的优化。多个问答数据集的实验结果显示，采用我们方法进行微调的 LLMs 在处理含糊输入时表现出色，同时在处理明确问题时也保持了竞争力。",
    "title_cn": "调整语言模型以明确解决歧义问题",
    "tags": [
      "分类：LLM应用",
      "对话系统",
      ""
    ]
  },
  {
    "title": "AgentCoord: Visually Exploring Coordination Strategy for LLM-based Multi-Agent Collaboration",
    "submit_datetime": "2024年04月18日",
    "abstract": "The potential of automatic task-solving through Large Language Model (LLM)-based multi-agent collaboration has recently garnered widespread attention from both the research community and industry. While utilizing natural language to coordinate multiple agents presents a promising avenue for democratizing agent technology for general users, designing coordination strategies remains challenging with existing coordination frameworks. This difficulty stems from the inherent ambiguity of natural language for specifying the collaboration process and the significant cognitive effort required to extract crucial information (e.g. agent relationship, task dependency, result correspondence) from a vast amount of text-form content during exploration. In this work, we present a visual exploration framework to facilitate the design of coordination strategies in multi-agent collaboration. We first establish a structured representation for LLM-based multi-agent coordination strategy to regularize the ambiguity of natural language. Based on this structure, we devise a three-stage generation method that leverages LLMs to convert a user's general goal into an executable initial coordination strategy. Users can further intervene at any stage of the generation process, utilizing LLMs and a set of interactions to explore alternative strategies. Whenever a satisfactory strategy is identified, users can commence the collaboration and examine the visually enhanced execution result. We develop AgentCoord, a prototype interactive system, and conduct a formal user study to demonstrate the feasibility and effectiveness of our approach.",
    "pdf_link": "https://arxiv.org/abs/2404.11943",
    "graphs": [],
    "abstract_cn": "最近，利用大型语言模型（LLM）驱动的多代理协同来自动化解决任务引起了学术界和产业界的广泛关注。使用自然语言协调众多代理虽然为广大用户普及代理技术开辟了道路，但现有的协调框架在设计策略时仍面临挑战。这些挑战主要来自于自然语言在明确指定协作流程时的模糊性，以及在探索过程中从大量文本内容中提取关键信息（如代理间关系、任务依赖、结果对应）所需的巨大认知劳动。为此，我们提出了一个视觉探索框架，旨在简化多代理协同中的协调策略设计。我们首先为基于LLM的多代理协调策略创建了一个结构化表达，以减少自然语言的歧义。在此基础上，我们开发了一种三阶段的生成方法，该方法使用LLM将用户的总体目标转换为可执行的初始协调策略。用户可以在生成过程的任何阶段进行干预，通过LLM和一系列交互来探索不同的策略。一旦找到满意的策略，用户即可启动协作并查看视觉增强的执行结果。我们构建了一个名为AgentCoord的原型互动系统，并通过正式的用户研究展示了我们方法的可行性和有效性。",
    "title_cn": "AgentCoord：视觉探索基于大型语言模型的多智能体协作的协调策略",
    "tags": [
      "Agent",
      "人工智能",
      "自动化"
    ]
  },
  {
    "title": "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent",
    "submit_datetime": "2024年04月18日",
    "abstract": "A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions. Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging. In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications. To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters. Like GPT-4, our model can process both English and Chinese. We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi.",
    "pdf_link": "https://arxiv.org/abs/2404.11459",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11459v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11459/x1.png"
      }
    ],
    "abstract_cn": "多模态AI代理擅长处理和学习多种数据类型，如自然语言、视觉和音频输入，以指导其行为。尽管大型语言模型在整合视觉信息方面取得了进展，例如GPT-4V，但将图像数据转化为AI代理的可执行结果仍然充满挑战。本文提出了一个专为AI代理应用设计的多模态模型，引入了功能性标记的概念。为适应边缘设备，该模型被精简至不到10亿参数。与GPT-4相似，它支持英文和中文处理。我们展示了该模型在多种边缘设备上的高效运行能力，即便是在资源受限如树莓派这样的设备上。",
    "title_cn": "章鱼 v3：设备端亿级以下多模态人工智能代理技术报告",
    "tags": [
      "Agent",
      "AI代理",
      "边缘计算"
    ]
  },
  {
    "title": "MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale",
    "submit_datetime": "2024年04月18日",
    "abstract": "Medical Visual Question Answering (MedVQA), which offers language responses to image-based medical inquiries, represents a challenging task and significant advancement in healthcare. It assists medical experts to swiftly interpret medical images, thereby enabling faster and more accurate diagnoses. However, the model interpretability and transparency of existing MedVQA solutions are often limited, posing challenges in understanding their decision-making processes. To address this issue, we devise a semi-automated annotation process to streamlining data preparation and build new benchmark MedVQA datasets R-RAD and R-SLAKE. The R-RAD and R-SLAKE datasets provide intermediate medical decision-making rationales generated by multimodal large language models and human annotations for question-answering pairs in existing MedVQA datasets, i.e., VQA-RAD and SLAKE. Moreover, we design a novel framework which finetunes lightweight pretrained generative models by incorporating medical decision-making rationales into the training process. The framework includes three distinct strategies to generate decision outcomes and corresponding rationales, thereby clearly showcasing the medical decision-making process during reasoning. Extensive experiments demonstrate that our method can achieve an accuracy of 83.5% on R-RAD and 86.3% on R-SLAKE, significantly outperforming existing state-of-the-art baselines. Dataset and code will be released.",
    "pdf_link": "https://arxiv.org/abs/2404.12372",
    "graphs": [],
    "abstract_cn": "医学视觉问答（MedVQA）是一项前沿的医疗任务，它通过语言回答来解析医学图像，极大提升了医疗诊断的速度和准确性。尽管如此，现有 MedVQA 系统的模型透明度和可解释性不足，这限制了对其决策机制的理解。为克服这一难题，我们引入了一种半自动化的标注流程，以优化数据准备，并开发了两个新的基准数据集：R-RAD 和 R-SLAKE。这两个数据集不仅包含了由多模态大型语言模型和人工标注生成的医学决策理由，还涵盖了现有 MedVQA 数据集（VQA-RAD 和 SLAKE）中的问题-答案对。此外，我们构建了一个创新框架，它通过整合医学决策理由来微调轻量级预训练生成模型。该框架采用三种策略来生成决策结果及其理由，清晰地揭示了推理过程中的医学决策流程。实验结果表明，我们的模型在 R-RAD 数据集上达到了 83.5% 的准确率，在 R-SLAKE 数据集上达到了 86.3% 的准确率，均显著超过了当前的最佳水平。相关数据集和代码将向公众开放。",
    "title_cn": "MedThink：揭示医学视觉问答背后的多模决策机制",
    "tags": [
      "分类：LLM应用",
      "",
      "人工智能"
    ]
  },
  {
    "title": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning",
    "submit_datetime": "2024年04月18日",
    "abstract": "Video summarization aims to create short, accurate, and cohesive summaries of longer videos. Despite the existence of various video summarization datasets, a notable limitation is their limited amount of source videos, which hampers the effective fine-tuning of advanced large vision-language models (VLMs). Additionally, most existing datasets are created for video-to-video summarization, overlooking the contemporary need for multimodal video content summarization. Recent efforts have been made to expand from unimodal to multimodal video summarization, categorizing the task into three sub-tasks based on the summary's modality: video-to-video (V2V), video-to-text (V2T), and a combination of video and text summarization (V2VT). However, the textual summaries in previous multimodal datasets are inadequate. To address these issues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset featuring 30,000 diverse videos sourced from YouTube, with lengths ranging from 40 to 940 seconds and an average summarization ratio of 16.39\\%. Each video summary in Instruct-V2Xum is paired with a textual summary that references specific frame indexes, facilitating the generation of aligned video and textual summaries. In addition, we propose a new video summarization framework named V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the first framework that unifies different video summarization tasks into one large language model's (LLM) text decoder and achieves task-controllable video summarization with temporal prompts and task instructions. Experiments show that V2Xum-LLaMA outperforms strong baseline models on multiple video summarization tasks. Furthermore, we propose an enhanced evaluation metric for V2V and V2VT summarization tasks.",
    "pdf_link": "https://arxiv.org/abs/2404.12353",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12353v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12353/teaser.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12353v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12353/sankey.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12353v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12353/model.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12353v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12353/vera.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12353v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12353/grammar.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12353v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12353/sample0.png"
      }
    ],
    "abstract_cn": "视频摘要的目的是为长视频制作简短、精确且连贯的概述。尽管众多视频摘要数据集已经出现，但它们的一个明显短板是源视频数量不足，这限制了高级大型视觉-语言模型（VLMs）的有效微调。而且，大多数现有数据集都是为视频到视频摘要设计的，忽略了对多模态视频内容摘要的现代需求。近期，人们已经开始尝试从单模态向多模态视频摘要转变，将任务分为三个子任务：视频到视频（V2V）、视频到文本（V2T）以及视频和文本结合摘要（V2VT）。但是，以往的多模态数据集在文本摘要方面还不够完善。为了应对这些挑战，我们推出了Instruct-V2Xum，这是一个包含30,000个多样化视频的跨模态视频摘要数据集，这些视频源自YouTube，时长介于40至940秒之间，平均摘要率为16.39%。Instruct-V2Xum的每个视频摘要都与一个引用特定帧索引的文本摘要配对，这有助于生成同步的视频和文本摘要。此外，我们提出了一个名为V2Xum-LLM的新视频摘要框架。在本研究中，特别是V2Xum-LLaMA，是首个将不同视频摘要任务整合到一个大型语言模型（LLM）的文本解码器中，并利用时间提示和任务指令实现可控制的视频摘要任务的框架。实验结果表明，V2Xum-LLaMA在多个视频摘要任务上超越了强大的基线模型。而且，我们还为V2V和V2VT摘要任务提出了一个改进的评估指标。",
    "title_cn": "V2Xum-LLM：利用时间提示指令优化的跨模态视频摘要技术",
    "tags": [
      "LLM应用",
      "视频摘要",
      "多模态学习"
    ]
  },
  {
    "title": "Sequential Compositional Generalization in Multimodal Models",
    "submit_datetime": "2024年04月18日",
    "abstract": "The rise of large-scale multimodal models has paved the pathway for groundbreaking advances in generative modeling and reasoning, unlocking transformative applications in a variety of complex tasks. However, a pressing question that remains is their genuine capability for stronger forms of generalization, which has been largely underexplored in the multimodal setting. Our study aims to address this by examining sequential compositional generalization using \\textsc{CompAct} (\\underline{Comp}ositional \\underline{Act}ivities)\\footnote{Project Page: \\url{http://cyberiada.github.io/CompAct}}, a carefully constructed, perceptually grounded dataset set within a rich backdrop of egocentric kitchen activity videos. Each instance in our dataset is represented with a combination of raw video footage, naturally occurring sound, and crowd-sourced step-by-step descriptions. More importantly, our setup ensures that the individual concepts are consistently distributed across training and evaluation sets, while their compositions are novel in the evaluation set. We conduct a comprehensive assessment of several unimodal and multimodal models. Our findings reveal that bi-modal and tri-modal models exhibit a clear edge over their text-only counterparts. This highlights the importance of multimodality while charting a trajectory for future research in this domain.",
    "pdf_link": "https://arxiv.org/abs/2404.12013",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/audio-rinse.png"
      },
      {
        "url": "https://arxiv.org/html/2404.12013v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.12013/vision-fridge.png"
      }
    ],
    "abstract_cn": "大规模多模态模型的发展为生成建模和推理带来了革命性的进步，推动了复杂任务的创新应用。但一个关键问题尚未充分探讨：这些模型在更高级泛化能力上的真实表现如何。本研究通过分析 \\textsc{CompAct} 数据集——一个在自我中心厨房活动视频背景下精心构建、感知基础的数据集——来探讨顺序组合泛化问题。该数据集的每个案例都结合了原始视频、自然声音和众包的分步描述。我们的实验设计确保了训练集和测试集中概念的一致性，同时在测试集中引入了新颖的组合。我们对单模态和多模态模型进行了深入评估，发现双模态和三模态模型在性能上明显超越了纯文本模型。这一发现不仅凸显了多模态输入的重要性，也为未来研究方向提供了新的思路。",
    "title_cn": "多模态模型中的顺序组合性泛化",
    "tags": [
      "分类：LLM应用\n\n这篇论文探讨了大规模多模态模型在生成建模和推理方面的进步，并分析了这些模型在更高级泛化能力上的真实表现。通过使用一个精心构建的数据集，研究了顺序组合泛化问题，并对比了单模态和多模态模型的性能。这篇论文的重点是评估和改进多模态模型在特定任务上的应用，因此它属于LLM应用类别。",
      "计算机视觉",
      "人工智能"
    ]
  },
  {
    "title": "Deep and Dynamic Metabolic and Structural Imaging in Living Tissues",
    "submit_datetime": "2024年04月18日",
    "abstract": "Label-free imaging through two-photon autofluorescence (2PAF) of NAD(P)H allows for non-destructive and high-resolution visualization of cellular activities in living systems. However, its application to thick tissues and organoids has been restricted by its limited penetration depth within 300 $μ$m, largely due to tissue scattering at the typical excitation wavelength (~750 nm) required for NAD(P)H. Here, we demonstrate that the imaging depth for NAD(P)H can be extended to over 700 $μ$m in living engineered human multicellular microtissues by adopting multimode fiber (MMF)-based low-repetition-rate high-peak-power three-photon (3P) excitation of NAD(P)H at 1100 nm. This is achieved by having over 0.5 MW peak power at the band of 1100$\\pm$25 nm through adaptively modulating multimodal nonlinear pulse propagation with a compact fiber shaper. Moreover, the 8-fold increase in pulse energy at 1100 nm enables faster imaging of monocyte behaviors in the living multicellular models. These results represent a significant advance for deep and dynamic metabolic and structural imaging of intact living biosystems. The modular design (MMF with a slip-on fiber shaper) is anticipated to allow wide adoption of this methodology for demanding in vivo and in vitro imaging applications, including cancer research, autoimmune diseases, and tissue engineering.",
    "pdf_link": "https://arxiv.org/abs/2404.11901",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11901v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11901/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11901v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11901/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11901v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11901/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11901v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11901/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11901v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11901/x5.png"
      }
    ],
    "abstract_cn": "利用两光子自发荧光技术观察NAD(P)H，我们能够在活体系统中对细胞活动进行无损且高分辨率的成像。尽管如此，由于组织对约750纳米激发波长的散射，该技术在厚组织和类器官中的应用受到了穿透深度仅300微米的限制。本研究突破性地展示了，通过采用多模光纤（MMF）基低重复率、高峰值功率的三次光子（3P）技术，在1100纳米激发NAD(P)H，将成像深度提升至超过700微米。这一成就是通过对1100±25纳米波段的脉冲能量进行自适应调制，实现超过0.5兆瓦的峰值功率，并通过紧凑的光纤整形器精确控制。此外，1100纳米处脉冲能量的八倍提升，加快了活体多细胞模型中单个单核细胞行为的成像速度。这些成果为深入和动态地观察完整活体生物系统的代谢和结构提供了重大进步。预计这种模块化设计（结合滑动式光纤整形器的MMF）将促进这一方法在体内外成像应用中的广泛应用，包括癌症研究、自身免疫疾病和组织工程等领域。",
    "title_cn": "活体组织中的深度与动态代谢及结构成像技术。",
    "tags": [
      "分类：Agent\n\n这篇论文讨论了一种新的成像技术，通过使用多模光纤（MMF）基低重复率、高峰值功率的三次光子（3P）技术，在1100纳米激发NAD(P)H，将成像深度提升至超过700微米。这项技术可以用于观察活体生物系统的代谢和结构，具有广泛的应用前景，如癌症研究、自身免疫疾病和组织工程等领域。由于这项技术涉及到对活体系统的无损且高分辨率成像，它更像是一个Agent，用于观察和研究生物系统。",
      "生物医学成像",
      "细胞生物学"
    ]
  },
  {
    "title": "Retrieval-Augmented Embodied Agents",
    "submit_datetime": "2024年04月17日",
    "abstract": "Embodied agents operating in complex and uncertain environments face considerable challenges. While some advanced agents handle complex manipulation tasks with proficiency, their success often hinges on extensive training data to develop their capabilities. In contrast, humans typically rely on recalling past experiences and analogous situations to solve new problems. Aiming to emulate this human approach in robotics, we introduce the Retrieval-Augmented Embodied Agent (RAEA). This innovative system equips robots with a form of shared memory, significantly enhancing their performance. Our approach integrates a policy retriever, allowing robots to access relevant strategies from an external policy memory bank based on multi-modal inputs. Additionally, a policy generator is employed to assimilate these strategies into the learning process, enabling robots to formulate effective responses to tasks. Extensive testing of RAEA in both simulated and real-world scenarios demonstrates its superior performance over traditional methods, representing a major leap forward in robotic technology.",
    "pdf_link": "https://arxiv.org/abs/2404.11699",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11699v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11699/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11699v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11699/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11699v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11699/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11699v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11699/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11699v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11699/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11699v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11699/x6.png"
      }
    ],
    "abstract_cn": "具身代理在复杂且不确定的环境中运作，面临巨大挑战。尽管部分高级代理能够熟练地执行复杂操作，但其成功往往建立在大量训练数据之上。相较之下，人类通常依靠回忆过往经历和类似情境来应对新问题。为了在机器人技术中模拟人类的这种解决问题的方式，我们提出了检索增强型具身代理（RAEA）。这一创新系统赋予机器人共享记忆，显著提升了它们的性能。我们的方案结合了策略检索器，使机器人能够根据多模态输入从外部策略库中检索相关策略。同时，策略生成器的引入使得机器人能够将这些策略整合到学习过程中，从而有效应对各种任务。RAEA在模拟环境和现实世界中的广泛测试显示，其性能超越了传统方法，标志着机器人技术的一大飞跃。",
    "title_cn": "增强检索的具身智能体",
    "tags": [
      "Agent",
      "机器人技术",
      "人工智能"
    ]
  },
  {
    "title": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory",
    "submit_datetime": "2024年04月17日",
    "abstract": "While current large language models (LLMs) demonstrate some capabilities in knowledge-intensive tasks, they are limited by relying on their parameters as an implicit storage mechanism. As a result, they struggle with infrequent knowledge and temporal degradation. In addition, the uninterpretable nature of parametric memorization makes it challenging to understand and prevent hallucination. Parametric memory pools and model editing are only partial solutions. Retrieval Augmented Generation (RAG) $\\unicode{x2013}$ though non-parametric $\\unicode{x2013}$ has its own limitations: it lacks structure, complicates interpretability and makes it hard to effectively manage stored knowledge. In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLM's capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLM's performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation.",
    "pdf_link": "https://arxiv.org/abs/2404.11672",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11672v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11672/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11672v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11672/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11672v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11672/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11672v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11672/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11672v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11672/x5.png"
      }
    ],
    "abstract_cn": "尽管现有的大型语言模型在知识密集型任务上展现了一定的潜力，但它们依赖于参数作为隐式存储机制，这限制了它们应对罕见知识和时间衰减的能力。参数记忆的不透明性也使得防止幻觉的发生变得困难。虽然参数记忆库和模型编辑提供了部分解决方案，但检索增强生成（RAG）作为一种非参数方法，也存在结构缺失、解释性复杂和知识管理困难等问题。本文提出了MemLLM，这是一种创新的方法，通过整合一个结构化且明确的读写记忆模块来提升LLMs的性能。MemLLM通过动态与记忆模块交互，增强了LLMs利用存储知识的能力，有效应对了前述挑战。实验结果证明，MemLLM不仅提升了LLMs在语言建模方面的性能，也增强了其在知识密集型任务中的可解释性。我们视MemLLM为推动LLMs通过记忆增强变得更加贴近实际和事实基础的重要进展。",
    "title_cn": "MemLLM：为大型语言模型（LLM）定制优化，引入外显的读写记忆功能。",
    "tags": [
      "LLM理论",
      "人工智能",
      "知识管理"
    ]
  },
  {
    "title": "Position Engineering: Boosting Large Language Models through Positional Information Manipulation",
    "submit_datetime": "2024年04月17日",
    "abstract": "The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided. In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance. In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models. Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself. We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL). Our findings show that position engineering substantially improves upon the baseline in both cases. Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models.",
    "pdf_link": "https://arxiv.org/abs/2404.11216",
    "graphs": [],
    "abstract_cn": "大型语言模型的表现在很大程度上取决于所提供的提示质量。为此，研究者们设计了众多的提示工程策略，目的是通过调整提示文本来优化任务执行效果。本文提出了一种创新技术——位置工程，它以一种更为高效的方式引导大型语言模型。与需要大量修改LLM提示文本的提示工程相比，位置工程只需调整提示中的位置信息，而无需改动文本内容。我们在两个广泛应用的LLM场景——检索增强生成（RAG）和上下文学习（ICL）中，对位置工程进行了评估。评估结果显示，位置工程在这两种场景下均显著提升了性能基线。因此，位置工程成为了一种充满潜力的新策略，用于充分发挥大型语言模型的潜力。",
    "title_cn": "位置工程：通过巧妙操控位置信息，增强大型语言模型的性能",
    "tags": [
      "LLM应用",
      "人工智能",
      ""
    ]
  },
  {
    "title": "Open-Ended Wargames with Large Language Models",
    "submit_datetime": "2024年04月17日",
    "abstract": "Wargames are a powerful tool for understanding and rehearsing real-world decision making. Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes. There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses. Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames. We introduce \"Snow Globe,\" an LLM-powered multi-agent system for playing qualitative wargames. With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof. We describe its software architecture conceptually and release an open-source implementation alongside this publication. As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis. We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem.",
    "pdf_link": "https://arxiv.org/abs/2404.11446",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11446v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11446/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11446v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11446/x2.png"
      }
    ],
    "abstract_cn": "兵棋推演作为一种强有力的工具，能够帮助我们深入理解和模拟现实世界的决策过程。借助人工智能，兵棋推演能够实现超越人类对战的多种可能性，比如通过重复游戏来探索各种可能的结果。兵棋推演主要分为定量型和定性型两种，前者的走法类型是离散的，而后者则更侧重于开放式的回答。过去，自动化技术主要应用于定量型兵棋推演，但现在，大型语言模型（LLMs）的出现让定性型兵棋推演的自动化成为现实。我们推出了“Snow Globe”，这是一个基于LLM的多智能体系统，专门用于进行定性兵棋推演。使用Snow Globe，从情景设定到战后分析的每一步，都可以由人工智能、人类或两者协作完成。我们在概念上阐述了其软件架构，并随着本文的发布，提供了一个开源的实现版本。通过两个案例研究——一次关于AI应急响应的桌面演练和一次围绕地缘政治危机的政治兵棋推演——我们展示了这种方法的应用潜力，并探讨了它在整个兵棋推演生态系统中的定位。",
    "title_cn": "与大型语言模型进行的开放式战争模拟游戏",
    "tags": [
      "Agent",
      "兵棋推演",
      "人工智能"
    ]
  },
  {
    "title": "From Image to Video, what do we need in multimodal LLMs?",
    "submit_datetime": "2024年04月17日",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated profound capabilities in understanding multimodal information, covering from Image LLMs to the more complex Video LLMs. Numerous studies have illustrated their exceptional cross-modal comprehension. Recently, integrating video foundation models with large language models to build a comprehensive video understanding system has been proposed to overcome the limitations of specific pre-defined vision tasks. However, the current advancements in Video LLMs tend to overlook the foundational contributions of Image LLMs, often opting for more complicated structures and a wide variety of multimodal data for pre-training. This approach significantly increases the costs associated with these methods.In response to these challenges, this work introduces an efficient method that strategically leverages the priors of Image LLMs, facilitating a resource-efficient transition from Image to Video LLMs. We propose RED-VILLM, a Resource-Efficient Development pipeline for Video LLMs from Image LLMs, which utilizes a temporal adaptation plug-and-play structure within the image fusion module of Image LLMs. This adaptation extends their understanding capabilities to include temporal information, enabling the development of Video LLMs that not only surpass baseline performances but also do so with minimal instructional data and training resources. Our approach highlights the potential for a more cost-effective and scalable advancement in multimodal models, effectively building upon the foundational work of Image LLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.11865",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11865v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11865/pipeline_v4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11865v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11865/pooling_v2.png"
      }
    ],
    "abstract_cn": "多模态大型语言模型（MLLMs）在处理多模态信息方面展现出了非凡的洞察力，从图像语言模型（Image LLMs）到更为复杂的视频语言模型（Video LLMs）均有涉猎。大量研究已经证明了它们在跨模态理解上的卓越能力。最近，提出了结合视频基础模型与大型语言模型，以构建一个全面的视频理解系统，旨在突破特定预设视觉任务的限制。然而，当前视频LLMs的发展往往忽略了图像LLMs的基石作用，倾向于采用更复杂的结构和多样化的多模态数据进行预训练，这大幅提高了相关方法的成本。为应对这些挑战，本研究提出了一种高效的方法，它巧妙地利用了图像LLMs的已有优势，以资源节约的方式实现从图像LLMs向视频LLMs的转变。我们介绍了RED-VILLM，这是一个高效的视频LLMs开发流程，它基于图像LLMs，采用了时间适应性的即插即用结构，扩展了对时间信息的理解能力，从而开发出不仅性能超越基准，而且使用最少的指导数据和训练资源的视频LLMs。我们的方法展现了在多模态模型领域实现成本效益更高、更具扩展性进步的可能性，为图像LLMs的基石工作提供了有效的延伸。",
    "title_cn": "探索从静态图像到动态视频的转变，我们需明确多模态大型语言模型所需具备的关键要素。",
    "tags": [
      "LLM应用",
      "计算机视觉",
      "人工智能"
    ]
  },
  {
    "title": "3D object quality prediction for Metal Jet Printer with Multimodal thermal encoder",
    "submit_datetime": "2024年04月17日",
    "abstract": "With the advancements in 3D printing technologies, it is extremely important that the quality of 3D printed objects, and dimensional accuracies should meet the customer's specifications. Various factors during metal printing affect the printed parts' quality, including the power quality, the printing stage parameters, the print part's location inside the print bed, the curing stage parameters, and the metal sintering process. With the large data gathered from HP's MetJet printing process, AI techniques can be used to analyze, learn, and effectively infer the printed part quality metrics, as well as assist in improving the print yield. In-situ thermal sensing data captured by printer-installed thermal sensors contains the part thermal signature of fusing layers. Such part thermal signature contains a convoluted impact from various factors. In this paper, we use a multimodal thermal encoder network to fuse data of a different nature including the video data vectorized printer control data, and exact part thermal signatures with a trained encoder-decoder module. We explored the data fusing techniques and stages for data fusing, the optimized end-to-end model architecture indicates an improved part quality prediction accuracy.",
    "pdf_link": "https://arxiv.org/abs/2404.11776",
    "graphs": [],
    "abstract_cn": "3D打印技术的飞速发展要求打印出的物体不仅要质量上乘，尺寸精度也得精准符合客户需求。金属打印环节众多因素如功率稳定性、打印阶段参数、部件在打印台中的位置、固化阶段参数及金属烧结过程等，都会对成品质量产生影响。HP的MetJet打印技术所积累的海量数据，借助人工智能技术，不仅可以深入分析和学习，还能有效预测打印部件的质量指标，进而提升打印成功率。打印机内置的热传感器所捕获的实时热感测数据，记录了熔层的热特征，这些特征受到多种因素的综合影响。本文提出了一种多模态热编码器网络，用以整合视频数据、打印机控制数据以及精确的部件热签名等不同来源的信息，并通过训练有素的编码器-解码器模块进行处理。我们深入研究了数据融合的方法和阶段，最终优化出的端到端模型架构显著提高了部件质量预测的准确性。",
    "title_cn": "采用多模态热编码器对金属喷射打印技术的3D对象质量进行预测。",
    "tags": [
      "分类：Agent\n\n这篇论文主要研究了3D打印过程中的质量预测问题，提出了一种多模态热编码器网络来整合不同来源的信息，并通过训练有素的编码器-解码器模块进行处理。这个过程涉及到了智能代理（Agent）的概念，即利用人工智能技术来模拟人类专家的决策过程，以提高打印成功率。因此，这篇论文可以归类为Agent领域。",
      "3D打印",
      "人工智能"
    ]
  },
  {
    "title": "Pretraining Billion-scale Geospatial Foundational Models on Frontier",
    "submit_datetime": "2024年04月17日",
    "abstract": "As AI workloads increase in scope, generalization capability becomes challenging for small task-specific models and their demand for large amounts of labeled training samples increases. On the contrary, Foundation Models (FMs) are trained with internet-scale unlabeled data via self-supervised learning and have been shown to adapt to various tasks with minimal fine-tuning. Although large FMs have demonstrated significant impact in natural language processing and computer vision, efforts toward FMs for geospatial applications have been restricted to smaller size models, as pretraining larger models requires very large computing resources equipped with state-of-the-art hardware accelerators. Current satellite constellations collect 100+TBs of data a day, resulting in images that are billions of pixels and multimodal in nature. Such geospatial data poses unique challenges opening up new opportunities to develop FMs. We investigate billion scale FMs and HPC training profiles for geospatial applications by pretraining on publicly available data. We studied from end-to-end the performance and impact in the solution by scaling the model size. Our larger 3B parameter size model achieves up to 30% improvement in top1 scene classification accuracy when comparing a 100M parameter model. Moreover, we detail performance experiments on the Frontier supercomputer, America's first exascale system, where we study different model and data parallel approaches using PyTorch's Fully Sharded Data Parallel library. Specifically, we study variants of the Vision Transformer architecture (ViT), conducting performance analysis for ViT models with size up to 15B parameters. By discussing throughput and performance bottlenecks under different parallelism configurations, we offer insights on how to leverage such leadership-class HPC resources when developing large models for geospatial imagery applications.",
    "pdf_link": "https://arxiv.org/abs/2404.11706",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_io.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_configs.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_base.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_huge.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_giant.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_enormous.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/mem_full_1G.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_mem_act_1G.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_5B.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_15B.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/perf_mem_act_NG.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/power_vit_5b.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/mem_full_NG.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/train_loss.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/linprob_test_acc1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11706v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11706/linprob_test_acc5.png"
      }
    ],
    "abstract_cn": "随着AI工作负载日益复杂，泛化能力对于小型专用模型而言越发成为难题，这些模型对大量标记训练样本的需求也随之增长。与之相对，基础模型（FMs）通过自监督学习利用互联网规模的未标记数据进行训练，展现出对多种任务的适应能力，且仅需极少量微调。尽管大型FMs在自然语言处理和计算机视觉领域已产生显著影响，但针对地理空间应用的FMs开发因资源限制而多聚焦于较小规模模型。鉴于预训练更大规模模型需要巨大的计算资源和尖端硬件加速器，这一现象尤为明显。目前，卫星群每天产生超过100TB的数据，生成的图像不仅像素高达数十亿，还具有多模态特性，为地理空间数据的处理带来了新的挑战和机遇。本研究通过在公开数据集上预训练，探索了适用于地理空间应用的十亿规模FMs及高性能计算（HPC）训练策略。我们从整体上评估了模型规模扩展对解决方案性能和影响的提升，发现3B参数规模的模型在场景分类准确度上相较于100M参数模型提升了30%。此外，我们在Frontier超级计算机上进行了性能测试，这是美国首个E级计算系统，研究了使用PyTorch的Fully Sharded Data Parallel库实现的不同模型和数据并行方法。特别是，我们对视觉变换器架构（ViT）进行了性能分析，测试了高达15B参数规模的ViT模型。通过探讨不同并行配置下的吞吐量和性能瓶颈，我们为如何利用顶级高性能计算资源开发地理空间图像应用的大型模型提供了深刻见解。",
    "title_cn": "在前沿科技领域，我们正致力于预训练规模达十亿级的地理空间基础模型。",
    "tags": [
      "LLM应用",
      "地理空间数据处理",
      "高性能计算"
    ]
  },
  {
    "title": "Exploring the Transferability of Visual Prompting for Multimodal Large Language Models",
    "submit_datetime": "2024年04月17日",
    "abstract": "Although Multimodal Large Language Models (MLLMs) have demonstrated promising versatile capabilities, their performance is still inferior to specialized models on downstream tasks, which makes adaptation necessary to enhance their utility. However, fine-tuning methods require independent training for every model, leading to huge computation and memory overheads. In this paper, we propose a novel setting where we aim to improve the performance of diverse MLLMs with a group of shared parameters optimized for a downstream task. To achieve this, we propose Transferable Visual Prompting (TVP), a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model. We introduce two strategies to address the issue of cross-model feature corruption of existing visual prompting methods and enhance the transferability of the learned prompts, including 1) Feature Consistency Alignment: which imposes constraints to the prompted feature changes to maintain task-agnostic knowledge; 2) Task Semantics Enrichment: which encourages the prompted images to contain richer task-specific semantics with language guidance. We validate the effectiveness of TVP through extensive experiments with 6 modern MLLMs on a wide variety of tasks ranging from object recognition and counting to multimodal reasoning and hallucination correction.",
    "pdf_link": "https://arxiv.org/abs/2404.11207",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/seed_0_lr_200_add_prompt_False_model_name_instructblip_dataset_cifar10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/seed_0_lr_200_add_prompt_True_model_name_instructblip_dataset_cifar10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/seed_0_lr_200_add_prompt_False_model_name_bliva_dataset_cifar10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/seed_0_lr_200_add_prompt_True_model_name_bliva_dataset_cifar10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/small_CLEVR_True.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/small_Hatefulmemes_True.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11207v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11207/small_POPE_True.png"
      }
    ],
    "abstract_cn": "多模态大型语言模型（MLLMs）虽展现出潜力，但在特定任务上的表现仍不及专业模型，亟需优化以提升其应用价值。传统微调方法需对每个模型单独训练，造成了巨大的计算和存储资源消耗。本文提出了一种创新框架，通过共享参数集针对特定下游任务进行优化，以提升各类MLLMs的性能。我们引入了“可转移视觉提示”（TVP）技术，这是一种简洁高效的解决方案，能够生成可在不同模型间转移并提升任务表现的视觉提示，且仅需在一个模型上进行训练。为解决现有方法中跨模型特征干扰的问题，我们提出了两大策略：一是“特征一致性对齐”，确保提示特征变化不破坏任务通用知识；二是“任务语义丰富化”，通过语言引导使提示图像富含更多任务特定语义。我们通过在六种先进MLLMs上进行的广泛实验验证了TVP的有效性，实验任务包括物体识别、计数、多模态推理及幻觉校正等。",
    "title_cn": "本文旨在探究视觉提示在多模态大型语言模型中的应用迁移性。",
    "tags": [
      "LLM应用",
      "计算机视觉",
      ""
    ]
  },
  {
    "title": "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs",
    "submit_datetime": "2024年04月16日",
    "abstract": "Large language models (LLMs) have made significant advancements in various natural language processing tasks, including question answering (QA) tasks. While incorporating new information with the retrieval of relevant passages is a promising way to improve QA with LLMs, the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs. Augmenting retrieved passages via prompting has the potential to address this limitation, but this direction has been limitedly explored. To this end, we design a simple yet effective framework to enhance open-domain QA (ODQA) with LLMs, based on the summarized retrieval (SuRe). SuRe helps LLMs predict more accurate answers for a given question, which are well-supported by the summarized retrieval that could be viewed as an explicit rationale extracted from the retrieved passages. Specifically, SuRe first constructs summaries of the retrieved passages for each of the multiple answer candidates. Then, SuRe confirms the most plausible answer from the candidate set by evaluating the validity and ranking of the generated summaries. Experimental results on diverse ODQA benchmarks demonstrate the superiority of SuRe, with improvements of up to 4.6% in exact match (EM) and 4.0% in F1 score over standard prompting approaches. SuRe also can be integrated with a broad range of retrieval methods and LLMs. Finally, the generated summaries from SuRe show additional advantages to measure the importance of retrieved passages and serve as more preferred rationales by models and humans.",
    "pdf_link": "https://arxiv.org/abs/2404.13081",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.13081v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.13081/x18.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）在自然语言处理的多个领域取得了突破性进展，尤其是在问答（QA）任务上。尽管结合新信息和检索相关内容是提升LLMs QA性能的一条有前景的途径，但现有的技术往往需要额外的微调，这在最新的LLMs中变得不切实际。通过提示来增强检索段落，有望突破这一瓶颈，但这方面的研究还相对有限。为了解决这一问题，我们提出了一个基于总结检索（SuRe）的简洁而高效的框架，用于提升LLMs在开放域问答（ODQA）中的表现。SuRe通过为每个可能的答案构建检索段落的摘要，帮助LLMs更准确地预测问题的答案，这些摘要可视为从检索内容中提取的明确理由。SuRe首先为多个答案候选者生成摘要，然后通过评估这些摘要的准确性和排序来确定最可信的答案。在多个ODQA基准上的实验结果显示，SuRe的性能优于传统提示方法，精确匹配（EM）准确度提升了4.6%，F1分数提升了4.0%。此外，SuRe能够与多种检索方法和LLMs兼容。SuRe生成的摘要还具有额外的优势，它们不仅能够衡量检索内容的重要性，还能作为模型和人类更倾向的理由。",
    "title_cn": "SuRe：为大型语言模型（LLMs）的开放域问答任务，通过候选答案对检索结果进行精炼总结。",
    "tags": [
      "LLM应用",
      "",
      "问答系统"
    ]
  },
  {
    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
    "submit_datetime": "2024年04月16日",
    "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.10981",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.10981v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10981/RAG_example.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10981v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10981/RAG_framework.png"
      }
    ],
    "abstract_cn": "检索增强生成（RAG）技术通过结合检索技术和深度学习的最新进展，突破了大型语言模型（LLMs）的静态限制，实现了实时外部信息的动态融合。这一方法专注于文本领域，有效提升了LLMs生成结果的准确性与可靠性，避免了生成似是而非的错误响应。随着RAG技术不断演进，融合了多种可能影响其表现的概念，本文将RAG的框架划分为四个主要阶段：检索前处理、检索阶段、检索后处理和生成阶段，并从检索的角度提供了深入的分析。文章追溯了RAG的发展轨迹，并通过剖析关键研究，探讨了该领域的发展趋势。同时，本文还提出了RAG的评估方法，针对当前面临的挑战，并对未来的研究方向提出了建议。通过构建系统化的框架和清晰的分类，本文意在整合RAG的研究成果，阐释其技术原理，并强调其在扩展LLMs应用范围和适应性方面的潜力。",
    "title_cn": "一篇关于为大型语言模型增强检索文本生成的研究综述",
    "tags": [
      "分类：RAG",
      "文本生成",
      "信息检索"
    ]
  },
  {
    "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
    "submit_datetime": "2024年04月16日",
    "abstract": "Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.",
    "pdf_link": "https://arxiv.org/abs/2404.10774",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.10774v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10774/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10774v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10774/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10774v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10774/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10774v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10774/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10774v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10774/x5.png"
      }
    ],
    "abstract_cn": "在自然语言处理领域，判断大型语言模型（LLM）的输出是否能够依据证据是至关重要的，这涉及到检索增强生成、摘要、文档驱动对话等多个任务。现行的“事实核查”方法主要依赖于使用LLM对模型生成的内容逐项与潜在证据进行核对，但这一过程计算成本极高，需要多次调用LLM来核实单一回复。本研究展示了如何以400倍的成本降低，构建出性能媲美GPT-4的小型模型。我们通过利用GPT-4生成合成训练数据，创造性地构建出既真实又具有挑战性的事实错误案例，并通过结构化生成流程来实现。在这些数据上训练的模型学会了检查声明中的每个事实，并能够识别跨句的信息整合。为了评估，我们将现有的数据集整合成一个新的基准集LLM-AggreFact，它来源于近期关于LLM生成的事实核查和证据基础的研究。我们的最佳系统MiniCheck-FT5（参数量为7.7亿）在同等规模的系统中表现最优，并达到了GPT-4的准确度。我们还发布了LLM-AggreFact基准集、数据合成的代码以及模型。",
    "title_cn": "MiniCheck：一种高效的事实核查工具，专为在基础文档上验证大型语言模型（LLMs）的真实性而设计。",
    "tags": [
      "分类：LLM应用\n\n这篇论文的摘要主要讨论了如何提高大型语言模型（LLM）在事实核查任务中的效率和准确性。它提出了一种新的方法，通过生成合成训练数据和结构化生成流程来训练一个小型模型，该模型在性能上可以与大型模型相媲美，但成本降低了400倍。此外，论文还介绍了一个新的基准集LLM-AggreFact，用于评估模型在事实核查任务上的表现。这些内容都与LLM的应用相关，因此将这篇论文归类为LLM应用。",
      "",
      "事实核查"
    ]
  },
  {
    "title": "Generative Information Retrieval Evaluation",
    "submit_datetime": "2024年04月16日",
    "abstract": "This paper is a draft of a chapter intended to appear in a forthcoming book on generative information retrieval, co-edited by Chirag Shah and Ryen White. In this chapter, we consider generative information retrieval evaluation from two distinct but interrelated perspectives. First, large language models (LLMs) themselves are rapidly becoming tools for evaluation, with current research indicating that LLMs may be superior to crowdsource workers and other paid assessors on basic relevance judgement tasks. We review past and ongoing related research, including speculation on the future of shared task initiatives, such as TREC, and a discussion on the continuing need for human assessments. Second, we consider the evaluation of emerging LLM-based generative information retrieval (GenIR) systems, including retrieval augmented generation (RAG) systems. We consider approaches that focus both on the end-to-end evaluation of GenIR systems and on the evaluation of a retrieval component as an element in a RAG system. Going forward, we expect the evaluation of GenIR systems to be at least partially based on LLM-based assessment, creating an apparent circularity, with a system seemingly evaluating its own output. We resolve this apparent circularity in two ways: 1) by viewing LLM-based assessment as a form of \"slow search\", where a slower IR system is used for evaluation and training of a faster production IR system; and 2) by recognizing a continuing need to ground evaluation in human assessment, even if the characteristics of that human assessment must change.",
    "pdf_link": "https://arxiv.org/abs/2404.08137",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08137v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08137/GenIR-UI.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08137v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08137/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08137v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08137/GenIR-RAG.png"
      }
    ],
    "abstract_cn": "本章草稿将收录于即将出版的《生成式信息检索》一书，该书由 Chirag Shah 和 Ryen White 联合编辑。本章从两大视角探讨了生成式信息检索的评估问题：一是大型语言模型（LLMs）作为评估工具的兴起，研究显示它们在基础相关性判断任务上可能超越了众包工作者及其他付费评估者。我们梳理了相关研究的过去和现状，并对未来共享任务如TREC的发展趋势进行了展望，同时讨论了人类评估的必要性。二是对基于LLM的新兴生成式信息检索（GenIR）系统，包括检索增强生成（RAG）系统的评估方法进行了探讨。我们既关注了GenIR系统的整体评估，也关注了RAG系统中检索组件的评估。展望未来，GenIR系统的评估预计将部分依赖于LLM评估，这似乎形成了一种系统自我评估的循环性。我们通过两种方法来解决这一问题：首先，将LLM评估视作一种“慢速搜索”，利用较慢的IR系统来评估和训练更快的生产IR系统；其次，认识到即便人类评估的特性需要变化，基于人类评估的评估方法仍然是必要的。",
    "title_cn": "创造性信息检索评估",
    "tags": [
      "LLM应用",
      "信息检索",
      "人工智能"
    ]
  },
  {
    "title": "An Empirical Evaluation of Pre-trained Large Language Models for Repairing Declarative Formal Specifications",
    "submit_datetime": "2024年04月16日",
    "abstract": "Automatic Program Repair (APR) has garnered significant attention as a practical research domain focused on automatically fixing bugs in programs. While existing APR techniques primarily target imperative programming languages like C and Java, there is a growing need for effective solutions applicable to declarative software specification languages. This paper presents a systematic investigation into the capacity of Large Language Models (LLMs) for repairing declarative specifications in Alloy, a declarative formal language used for software specification. We propose a novel repair pipeline that integrates a dual-agent LLM framework, comprising a Repair Agent and a Prompt Agent. Through extensive empirical evaluation, we compare the effectiveness of LLM-based repair with state-of-the-art Alloy APR techniques on a comprehensive set of benchmarks. Our study reveals that LLMs, particularly GPT-4 variants, outperform existing techniques in terms of repair efficacy, albeit with a marginal increase in runtime and token usage. This research contributes to advancing the field of automatic repair for declarative specifications and highlights the promising potential of LLMs in this domain.",
    "pdf_link": "https://arxiv.org/abs/2404.11050",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.11050v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.11050/x16.png"
      }
    ],
    "abstract_cn": "自动程序修复（APR）因其致力于自动修正程序错误而成为备受关注的实用研究领域。目前，APR 技术多集中于命令式语言如 C 和 Java，但对声明式软件规范语言的解决方案需求正日益上升。本论文对大型语言模型（LLMs）在修复 Alloy 语言声明式规范方面的能力进行了深入探讨，Alloy 是一种广泛用于软件规范的声明式形式化语言。我们设计了一套创新的修复流程，融合了双代理 LLM 架构，涵盖修复代理与提示代理。经过广泛的实证评估，我们发现基于 LLM 的修复方法在修复效率上超越了现有的 Alloy APR 技术，尽管这以略微增加的运行时间和令牌使用为代价。本研究不仅推动了声明式规范自动修复技术的发展，也突显了 LLMs 在该领域的光明前景。",
    "title_cn": "本文通过实证研究，评估了预训练的大型语言模型在修复声明式形式规范方面的性能。",
    "tags": [
      "Agent",
      "软件工程",
      "自动化测试"
    ]
  },
  {
    "title": "Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans and Language Models",
    "submit_datetime": "2024年04月16日",
    "abstract": "As AI systems like language models are increasingly integrated into decision-making processes affecting people's lives, it's critical to ensure that these systems have sound moral reasoning. To test whether they do, we need to develop systematic evaluations. We provide a framework that uses a language model to translate causal graphs that capture key aspects of moral dilemmas into prompt templates. With this framework, we procedurally generated a large and diverse set of moral dilemmas -- the OffTheRails benchmark -- consisting of 50 scenarios and 400 unique test items. We collected moral permissibility and intention judgments from human participants for a subset of our items and compared these judgments to those from two language models (GPT-4 and Claude-2) across eight conditions. We find that moral dilemmas in which the harm is a necessary means (as compared to a side effect) resulted in lower permissibility and higher intention ratings for both participants and language models. The same pattern was observed for evitable versus inevitable harmful outcomes. However, there was no clear effect of whether the harm resulted from an agent's action versus from having omitted to act. We discuss limitations of our prompt generation pipeline and opportunities for improving scenarios to increase the strength of experimental effects.",
    "pdf_link": "https://arxiv.org/abs/2404.10975",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.10975v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10975/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10975v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10975/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10975v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10975/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10975v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10975/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10975v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10975/x5.png"
      }
    ],
    "abstract_cn": "随着语言模型等AI系统在影响人类生活的决策过程中扮演越来越重要的角色，确保它们具备坚实的道德推理能力变得尤为关键。为此，我们必须开展系统性的评估工作。本研究提出了一种框架，该框架利用语言模型将描述道德困境核心要素的因果图转换为提示模板。基于此框架，我们生成了一套内容丰富、多样化的道德困境数据集——OffTheRails基准测试，共包含50个情境和400个独特的测试案例。我们从人类参与者那里收集了对我们数据子集的道德许可性和意图性判断，并将其与两个语言模型（GPT-4和Claude-2）在八种不同条件下的判断进行了对比分析。研究发现，在伤害是必要手段而非副作用的道德困境中，无论是参与者还是语言模型，其许可性评价都较低，而意图性评价则较高。对于可避免与不可避免的伤害结果，也呈现出类似的趋势。然而，当伤害是由代理的行为引起与未采取行动引起时，并未发现明显的差异。我们讨论了当前提示生成流程的局限性，并提出了改进情景设计以增强实验效应强度的可能途径。",
    "title_cn": "生成程序性道德困境，以评估人类和语言模型的道德推理能力。",
    "tags": [
      "分类：LLM应用",
      "AI伦理",
      "道德评估"
    ]
  },
  {
    "title": "Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases",
    "submit_datetime": "2024年04月16日",
    "abstract": "Large Vision-Language Models (LVLMs), due to the remarkable visual reasoning ability to understand images and videos, have received widespread attention in the autonomous driving domain, which significantly advances the development of interpretable end-to-end autonomous driving. However, current evaluations of LVLMs primarily focus on the multi-faceted capabilities in common scenarios, lacking quantifiable and automated assessment in autonomous driving contexts, let alone severe road corner cases that even the state-of-the-art autonomous driving perception systems struggle to handle. In this paper, we propose CODA-LM, a novel vision-language benchmark for self-driving, which provides the first automatic and quantitative evaluation of LVLMs for interpretable autonomous driving including general perception, regional perception, and driving suggestions. CODA-LM utilizes the texts to describe the road images, exploiting powerful text-only large language models (LLMs) without image inputs to assess the capabilities of LVLMs in autonomous driving scenarios, which reveals stronger alignment with human preferences than LVLM judges. Experiments demonstrate that even the closed-sourced commercial LVLMs like GPT-4V cannot deal with road corner cases well, suggesting that we are still far from a strong LVLM-powered intelligent driving agent, and we hope our CODA-LM can become the catalyst to promote future development.",
    "pdf_link": "https://arxiv.org/abs/2404.10595",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x19.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10595v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10595/x20.png"
      }
    ],
    "abstract_cn": "大型视觉-语言模型（LVLMs）因其卓越的图像和视频理解能力，在自动驾驶领域引起了广泛关注，显著推动了可解释的端到端自动驾驶技术的进步。但目前对LVLMs的评估多聚焦于常规情境下的多维能力，缺少对自动驾驶场景的量化和自动化评估，尤其是对于那些即使是最尖端的自动驾驶感知系统也难以应对的极端路况。本文提出了CODA-LM，这是一个创新的自动驾驶视觉-语言基准测试，首次为LVLMs在可解释自动驾驶中的应用提供了自动化和量化的评估，涵盖常规感知、区域感知和驾驶建议等方面。CODA-LM通过文本描述道路图像，借助仅使用文本的大型语言模型（LLMs）来评估LVLMs在自动驾驶情境下的表现，这种方式更能反映人类的偏好。实验结果揭示，即便是市面上的闭源商业LVLMs，如GPT-4V，也难以妥善处理极端路况，这表明我们距离拥有一个强大的LVLM驱动的智能驾驶代理还有很长的路要走。我们期望CODA-LM能够成为推动未来技术发展的催化剂。",
    "title_cn": "本文探讨了如何对大型视觉-语言模型在自动驾驶领域的特殊情况下进行自动化评估。",
    "tags": [
      "Agent",
      "自动驾驶",
      "人工智能"
    ]
  },
  {
    "title": "White Men Lead, Black Women Help: Uncovering Gender, Racial, and Intersectional Bias in Language Agency",
    "submit_datetime": "2024年04月16日",
    "abstract": "Social biases can manifest in language agency. For instance, White individuals and men are often described as \"agentic\" and achievement-oriented, whereas Black individuals and women are frequently described as \"communal\" and as assisting roles. This study establishes agency as an important aspect of studying social biases in both human-written and Large Language Model (LLM)-generated texts. To accurately measure \"language agency\" at sentence level, we propose a Language Agency Classification dataset to train reliable agency classifiers. We then use an agency classifier to reveal notable language agency biases in 6 datasets of human- or LLM-written texts, including biographies, professor reviews, and reference letters. While most prior NLP research on agency biases focused on single dimensions, we comprehensively explore language agency biases in gender, race, and intersectional identities. We observe that (1) language agency biases in human-written texts align with real-world social observations; (2) LLM-generated texts demonstrate remarkably higher levels of language agency bias than human-written texts; and (3) critical biases in language agency target people of minority groups -- for instance, languages used to describe Black females exhibit the lowest level of agency across datasets. Our findings reveal intricate social biases in human- and LLM-written texts through the lens of language agency, warning against using LLM generations in social contexts without scrutiny.",
    "pdf_link": "https://arxiv.org/abs/2404.10508",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/bias_bios_gaps.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/ratemyprofessor_gaps.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/llm_race_bios_gender_bert_binary_average_overlay_histogram.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/llm_race_professor_gender_bert_binary_average_overlay_histogram.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/llm_race_rec_letter_gender_bert_binary_average_overlay_histogram.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/race_bios_bert_binary_average_overlay_histogram.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/race_professor_bert_binary_average_overlay_histogram.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/race_rec_letter_bert_binary_average_overlay_histogram.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/race_bios_gaps.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/race_professor_gaps.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10508v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10508/race_rec_letter_gaps.png"
      }
    ],
    "abstract_cn": "社会偏见在语言行为中表露无遗。通常，白人和男性被描绘为具有行动力和目标导向，而黑人和女性则多被形容为具有社群精神和辅助性。本研究将行为主体性视为审视人类撰写和大型语言模型（LLM）生成文本中社会偏见的关键要素。为精确评估句子级别的“语言行为主体性”，我们设计了一个语言行为分类数据集，用以培养高效的主体性分类器。随后，我们利用这一分类器揭露了6个由人类或LLM撰写的文本数据集中显著的语言行为偏见，涵盖了传记、教授评价和推荐信等。与以往专注于单一维度的NLP研究不同，我们全面探讨了性别、种族及交叉身份维度上的语言行为偏见。研究发现：（1）人类文本中的语言行为偏见与现实社会观察相吻合；（2）LLM生成的文本比人类文本展现出更高程度的语言行为偏见；（3）语言行为中的关键偏见集中于少数群体，如描述黑人女性的语言在所有数据集中代理性水平最低。这些发现通过语言行为主体性的视角，揭示了人类和LLM文本中的微妙社会偏见，提醒我们在社会情境中使用LLM生成的内容前需进行严格审查。",
    "title_cn": "白人男性担纲领导，黑人女性扮演辅助角色：本文揭露了语言机构中存在的性别、种族以及交叉性偏见。",
    "tags": [
      "LLM应用",
      "社会偏见分析",
      ""
    ]
  },
  {
    "title": "How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs' internal prior",
    "submit_datetime": "2024年04月15日",
    "abstract": "Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs). However, in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error? Conversely, in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error? To answer these questions, we systematically analyze the tug-of-war between a LLM's internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree. We test GPT-4 and other LLMs on question-answering abilities across datasets with and without reference documents. As expected, providing the correct retrieved information fixes most model mistakes (94% accuracy). However, when the reference document is perturbed with increasing levels of wrong values, the LLM is more likely to recite the incorrect, modified information when its internal prior is weaker but is more resistant when its prior is stronger. Similarly, we also find that the more the modified information deviates from the model's prior, the less likely the model is to prefer it. These results highlight an underlying tension between a model's prior knowledge and the information presented in reference documents.",
    "pdf_link": "https://arxiv.org/abs/2404.10198",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.10198v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10198/schematic4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10198v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10198/fig1-2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10198v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10198/examples4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10198v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10198/adherence-prompts6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10198v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10198/fig1combined2.png"
      }
    ],
    "abstract_cn": "检索增强生成（RAG）被广泛采用，旨在修正大型语言模型（LLMs）的误判并引入最新资讯。但当LLMs独自给出错误答案时，正确的检索内容是否总能纠正这一错误？反之，如果检索内容有误，LLMs能否识别并忽略这些错误信息，避免重复错误？为探究这一问题，我们对LLMs内部先验知识与检索信息之间的冲突进行了系统分析，特别是在它们相冲突的情况下。我们在有无参考文档的不同数据集上，对GPT-4及其他LLMs的问答能力进行了测试。测试结果显示，正确的检索信息能够修正大多数模型的误判，准确度高达94%。然而，当参考文档被逐渐增加的错误值干扰时，如果LLMs的内部先验较弱，它们更易接受错误的修改信息；而内部先验较强的模型则更具抵抗力。此外，我们同样发现，修改后的信息与模型先验的偏差越大，模型对其的偏好就越低。这些发现揭示了模型先验知识与参考文档所提供信息之间的内在张力。",
    "title_cn": "RAG模型的准确性究竟有多高？本文旨在量化分析RAG与大型语言模型内在先验之间的相互作用与影响。",
    "tags": [
      "分类：RAG",
      "信息检索",
      ""
    ]
  },
  {
    "title": "Memory Sharing for Large Language Model based Agents",
    "submit_datetime": "2024年04月15日",
    "abstract": "In the realm of artificial intelligence, the adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning for fixed-answer tasks such as common sense questions and yes/no queries. However, the application of In-context Learning to open-ended challenges, such as poetry creation, reveals substantial limitations due to the comprehensiveness of the provided examples and agent's ability to understand the content expressed in the problem, leading to outputs that often diverge significantly from expected results. Addressing this gap, our study introduces the Memory-Sharing (MS) framework for LLM multi-agents, which utilizes a real-time memory storage and retrieval system to enhance the In-context Learning process. Each \"memory\" within this system captures both the posed query and the corresponding real-time response from an LLM-based agent, aggregating these memories from a broad spectrum of similar agents to enrich the memory pool shared by all agents. This framework not only aids agents in identifying the most relevant examples for specific tasks but also evaluates the potential utility of their memories for future applications by other agents. Empirical validation across three distinct domains involving specialized functions of agents demonstrates that the MS framework significantly improve the agent's performance regrading the open-ended questions. Furthermore, we also discuss what type of memory pool and what retrieval strategy in MS can better help agents, offering a future develop direction of MS. The code and data are available at: https://github.com/GHupppp/MemorySharingLLM",
    "pdf_link": "https://arxiv.org/abs/2404.09982",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.09982v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09982/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09982v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09982/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09982v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09982/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09982v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09982/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09982v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09982/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09982v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09982/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09982v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09982/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09982v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09982/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09982v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09982/x9.png"
      }
    ],
    "abstract_cn": "在人工智能的世界中，大型语言模型（LLM）驱动的智能体通过自然语言指令执行任务，标志着一项重要进步，显著减少了对常识问答和是非题这类固定答案任务的显式再训练或微调需求。但是，当上下文学习应用于如诗歌创作这样的开放式任务时，由于示例的全面性和智能体对问题内容理解能力的局限，常常导致生成的结果与预期大相径庭。为填补这一空白，本研究提出了一种用于LLM多智能体的记忆共享（MS）框架，该框架通过实时记忆存储和检索系统来提升上下文学习过程。系统内的每一“记忆”单元都记录了提出的问题和LLM智能体的实时响应，通过收集广泛类似智能体的记忆，共同丰富了所有智能体共享的记忆库。这一框架不仅助力智能体为特定任务找到最贴切的示例，还评估了它们的记忆对未来其他智能体应用的潜在价值。在三个不同领域的实证验证中，MS框架显著提升了智能体处理开放式问题的表现。此外，我们还探讨了哪种记忆库类型和检索策略更能助力智能体，并为MS的未来发展方向提供了见解。相关代码和数据已在 https://github.com/GHupppp/MemorySharingLLM 上公开。",
    "title_cn": "大型语言模型驱动的代理之间的内存共享",
    "tags": [
      "Agent",
      "人工智能",
      ""
    ]
  },
  {
    "title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation",
    "submit_datetime": "2024年04月15日",
    "abstract": "Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the \"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.",
    "pdf_link": "https://arxiv.org/abs/2404.09127",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.09127v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09127/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09127v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09127/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09127v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09127/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09127v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09127/diagrams.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09127v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09127/ablation.png"
      }
    ],
    "abstract_cn": "当前的大型语言模型（LLMs）面临一个重大挑战：不确定性估计。这些模型往往校准不准确且过于自信，尤其是在采用人类反馈进行强化学习（RLHF）时。与人类可以通过日常观察调整决策和信心不同，现有的LLMs校准方法大多忽略了“集体智慧”——即多个LLMs之间的相互作用，这种互动能够共同提升模型的准确性和校准度。本研究提出了一种名为“协同校准”的无需后续训练的校准策略，该策略通过模拟小组讨论过程，充分发挥多个工具辅助的LLM代理的协作和表达能力。我们在不同领域的生成性问答任务中验证了协同校准的有效性，证明了其在提升模型预测可靠性方面的潜力，尤其是在集体校准信心评估的合理化方面。",
    "title_cn": "通过多智能体协商机制，实现大型语言模型的置信度校准与合理化解释",
    "tags": [
      "LLM应用",
      "人工智能",
      ""
    ]
  },
  {
    "title": "Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System: A~Case~Study~at~HCMUT",
    "submit_datetime": "2024年04月14日",
    "abstract": "In today's rapidly evolving landscape of Artificial Intelligence, large language models (LLMs) have emerged as a vibrant research topic. LLMs find applications in various fields and contribute significantly. Despite their powerful language capabilities, similar to pre-trained language models (PLMs), LLMs still face challenges in remembering events, incorporating new information, and addressing domain-specific issues or hallucinations. To overcome these limitations, researchers have proposed Retrieval-Augmented Generation (RAG) techniques, some others have proposed the integration of LLMs with Knowledge Graphs (KGs) to provide factual context, thereby improving performance and delivering more accurate feedback to user queries.\n  Education plays a crucial role in human development and progress. With the technology transformation, traditional education is being replaced by digital or blended education. Therefore, educational data in the digital environment is increasing day by day. Data in higher education institutions are diverse, comprising various sources such as unstructured/structured text, relational databases, web/app-based API access, etc. Constructing a Knowledge Graph from these cross-data sources is not a simple task. This article proposes a method for automatically constructing a Knowledge Graph from multiple data sources and discusses some initial applications (experimental trials) of KG in conjunction with LLMs for question-answering tasks.",
    "pdf_link": "https://arxiv.org/abs/2404.09296",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.09296v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09296/figure_05_course_withdraw.png"
      }
    ],
    "abstract_cn": "在人工智能迅猛发展的今天，大型语言模型（LLMs）已经成为研究的热点。这些模型在众多领域发挥着重要作用。然而，它们在记忆事件、整合新信息以及处理特定领域的挑战方面，仍面临着与预训练语言模型（PLMs）相似的难题。为解决这些问题，研究者们提出了检索增强生成（RAG）等技术，并探索将LLMs与知识图谱（KGs）结合，以提供准确的事实背景，优化性能并提升对用户查询的反馈精确度。教育对于人类的发展和进步至关重要。随着技术的革新，传统教育正逐渐向数字化或混合式教育转变。在此背景下，高等教育机构中的教育数据日益增长，涵盖了非结构化/结构化文本、关系数据库、网络/应用API接口等多种类型。从这些多样化的数据源中构建知识图谱是一项复杂工作。本文提出了一种自动化构建多源数据知识图谱的方法，并探讨了将知识图谱与LLMs结合在问答任务中的一些初步应用和实验。",
    "title_cn": "构建跨数据知识图谱以赋能大型语言模型（LLM）在教育问答系统中的应用：HCMUT案例研究。",
    "tags": [
      "分类：RAG",
      "",
      "知识图谱"
    ]
  },
  {
    "title": "Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts",
    "submit_datetime": "2024年04月14日",
    "abstract": "Reinforcement learning (RL) trains agents to accomplish complex tasks through environmental interaction data, but its capacity is also limited by the scope of the available data. To obtain a knowledgeable agent, a promising approach is to leverage the knowledge from large language models (LLMs). Despite previous studies combining LLMs with RL, seamless integration of the two components remains challenging due to their semantic gap. This paper introduces a novel method, Knowledgeable Agents from Language Model Rollouts (KALM), which extracts knowledge from LLMs in the form of imaginary rollouts that can be easily learned by the agent through offline reinforcement learning methods. The primary challenge of KALM lies in LLM grounding, as LLMs are inherently limited to textual data, whereas environmental data often comprise numerical vectors unseen to LLMs. To address this, KALM fine-tunes the LLM to perform various tasks based on environmental data, including bidirectional translation between natural language descriptions of skills and their corresponding rollout data. This grounding process enhances the LLM's comprehension of environmental dynamics, enabling it to generate diverse and meaningful imaginary rollouts that reflect novel skills. Initial empirical evaluations on the CLEVR-Robot environment demonstrate that KALM enables agents to complete complex rephrasings of task goals and extend their capabilities to novel tasks requiring unprecedented optimal behaviors. KALM achieves a success rate of 46% in executing tasks with unseen goals, substantially surpassing the 26% success rate achieved by baseline methods. Furthermore, KALM effectively enables the LLM to comprehend environmental dynamics, resulting in the generation of meaningful imaginary rollouts that reflect novel skills and demonstrate the seamless integration of large language models and reinforcement learning.",
    "pdf_link": "https://arxiv.org/abs/2404.09248",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/clevr_demo_begin.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/clevr_demo_end.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09248v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09248/x19.png"
      }
    ],
    "abstract_cn": "强化学习通过与环境的互动数据训练智能体以完成复杂任务，但其所处理的数据量也限制了其能力。要打造一个知识丰富的智能体，一个充满希望的途径是借助大型语言模型（LLMs）的洞见。尽管先前研究已尝试将LLMs与强化学习结合，两者之间的语义差异使得它们的完美融合颇具挑战。本文提出了一种创新方法——语言模型展开的知识智能体（KALM），它能够从LLMs中提取知识，形成易于智能体通过离线强化学习掌握的虚拟展开。KALM面临的主要难题是如何让LLMs与环境数据相匹配，因为LLMs通常只处理文本数据，而环境数据往往包含LLMs未曾接触过的数值向量。为此，KALM对LLMs进行微调，使其能够根据环境数据执行多样化任务，包括技能的自然语言描述与相应的展开数据之间的双向转换。这一匹配过程加深了LLMs对环境变化的理解，使其能够产生多样化且富有洞见的虚拟展开，反映出创新技能。在CLEVR-Robot环境上的初步实证评估显示，KALM能够使智能体完成复杂的任务目标重构，并将其能力拓展到需要全新最优行为的新任务上。KALM在执行未见目标任务时的成功率达到46%，显著超过了基线方法的26%。此外，KALM有效地促进了LLMs对环境动态的理解，生成了富有意义且反映新技能的虚拟展开，实现了大型语言模型与强化学习的和谐融合。",
    "title_cn": "通过利用大型语言模型的离线强化学习，我们培育出了见多识广的智能代理。",
    "tags": [
      "Agent",
      "人工智能",
      ""
    ]
  },
  {
    "title": "Interactive Generative AI Agents for Satellite Networks through a Mixture of Experts Transmission",
    "submit_datetime": "2024年04月13日",
    "abstract": "In response to the needs of 6G global communications, satellite communication networks have emerged as a key solution. However, the large-scale development of satellite communication networks is constrained by the complex system models, whose modeling is challenging for massive users. Moreover, transmission interference between satellites and users seriously affects communication performance. To solve these problems, this paper develops generative artificial intelligence (AI) agents for model formulation and then applies a mixture of experts (MoE) approach to design transmission strategies. Specifically, we leverage large language models (LLMs) to build an interactive modeling paradigm and utilize retrieval-augmented generation (RAG) to extract satellite expert knowledge that supports mathematical modeling. Afterward, by integrating the expertise of multiple specialized components, we propose an MoE-proximal policy optimization (PPO) approach to solve the formulated problem. Each expert can optimize the optimization variables at which it excels through specialized training through its own network and then aggregates them through the gating network to perform joint optimization. The simulation results validate the accuracy and effectiveness of employing a generative agent for problem formulation. Furthermore, the superiority of the proposed MoE-ppo approach over other benchmarks is confirmed in solving the formulated problem. The adaptability of MoE-PPO to various customized modeling problems has also been demonstrated.",
    "pdf_link": "https://arxiv.org/abs/2404.09134",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.09134v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09134/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09134v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09134/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09134v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09134/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09134v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09134/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09134v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09134/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09134v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09134/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09134v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09134/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09134v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09134/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09134v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09134/x9.png"
      }
    ],
    "abstract_cn": "为应对6G全球通信的挑战，卫星通信网络正成为核心解决方案。但这一进程因复杂的系统模型而受阻，大规模用户建模尤为困难。卫星与用户间的传输干扰也对通信质量构成了重大影响。为此，本研究提出了一种基于生成性AI代理的模型构建方法，并采用专家混合（MoE）策略来优化传输方案。我们借助大型语言模型（LLMs）打造交互式模型框架，并运用增强检索生成（RAG）技术提取卫星专家知识以支持数学建模。进一步地，通过融合多个专业模块的专长，我们提出了一种MoE-近端策略优化（PPO）算法来应对这一挑战。每位专家都能通过专门训练优化其擅长的变量，并通过门控网络实现协同优化。模拟实验证实了AI代理在问题构建上的准确性与效率，同时验证了MoE-PPO方法相较于其他基准的优越性。此外，MoE-PPO在多种定制化建模问题上的适应性也得到了充分展示。",
    "title_cn": "通过专家知识融合传输，打造卫星网络中的互动式生成AI代理。",
    "tags": [
      "Agent",
      "",
      "人工智能"
    ]
  },
  {
    "title": "Introducing Super RAGs in Mistral 8x7B-v1",
    "submit_datetime": "2024年04月13日",
    "abstract": "The relentless pursuit of enhancing Large Language Models (LLMs) has led to the advent of Super Retrieval-Augmented Generation (Super RAGs), a novel approach designed to elevate the performance of LLMs by integrating external knowledge sources with minimal structural modifications. This paper presents the integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM, and examines the resultant improvements in accuracy, speed, and user satisfaction. Our methodology uses a fine-tuned instruct model setup and a cache tuning fork system, ensuring efficient and relevant data retrieval. The evaluation, conducted over several epochs, demonstrates significant enhancements across all metrics. The findings suggest that Super RAGs can effectively augment LLMs, paving the way for more sophisticated and reliable AI systems. This research contributes to the field by providing empirical evidence of the benefits of Super RAGs and offering insights into their potential applications.",
    "pdf_link": "https://arxiv.org/abs/2404.08940",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08940v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08940/working.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08940v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08940/small_instruct.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08940v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08940/Instruct.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08940v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08940/cache.png"
      }
    ],
    "abstract_cn": "不断追求提升大型语言模型（LLMs）性能，催生了一种创新方法——超级检索增强生成（Super RAGs），它通过最小化结构改动，整合外部知识库以增强LLMs的能力。本文探讨了将Super RAGs集成至尖端的Mistral 8x7B v1 LLM，并评估了其在提升精确度、速度和用户满意度方面的成效。我们采用了微调指令模型和缓存优化系统，以确保数据检索的高效性和相关性。经过多轮测试，我们在所有评估指标上都观察到了显著的进步。研究结果揭示了Super RAGs在增强LLMs方面的潜力，为构建更高级、更可靠的人工智能系统提供了新途径。本研究通过实证数据证明了Super RAGs的优势，并对其可能的应用场景提供了深入见解，为该领域的发展做出了贡献。",
    "title_cn": "Mistral 8x7B-v1 版本中，我们迎来了超级 RAGs 的加入。",
    "tags": [
      "LLM应用",
      "人工智能",
      ""
    ]
  },
  {
    "title": "CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting",
    "submit_datetime": "2024年04月13日",
    "abstract": "In the field of Question Answering (QA), unifying large language models (LLMs) with external databases has shown great success. However, these methods often fall short in providing the advanced reasoning needed for complex QA tasks. To address these issues, we improve over a novel approach called Knowledge Graph Prompting (KGP), which combines knowledge graphs with a LLM-based agent to improve reasoning and search accuracy. Nevertheless, the original KGP framework necessitates costly fine-tuning with large datasets yet still suffers from LLM hallucination. Therefore, we propose a reasoning-infused LLM agent to enhance this framework. This agent mimics human curiosity to ask follow-up questions to more efficiently navigate the search. This simple modification significantly boosts the LLM performance in QA tasks without the high costs and latency associated with the initial KGP framework. Our ultimate goal is to further develop this approach, leading to more accurate, faster, and cost-effective solutions in the QA domain.",
    "pdf_link": "https://arxiv.org/abs/2404.09077",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.09077v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09077/questions.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09077v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09077/workflow.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09077v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09077/combined_performance_metrics.png"
      }
    ],
    "abstract_cn": "在问答技术的发展中，将大型语言模型（LLMs）与外部数据库融合的方法已经取得了显著成效。但这种方法在处理需要复杂推理能力的QA任务时，往往力有不逮。为克服这些挑战，我们对一种创新的方法——知识图谱提示（KGP）进行了优化。该方法通过将知识图谱与基于LLM的智能体相结合，有效提升了推理和搜索的精确度。尽管如此，传统的KGP框架仍需大量数据集进行昂贵的微调，并且难以摆脱LLM产生幻觉的问题。为此，我们设计了一个融入推理能力的LLM智能体，以强化这一框架。该智能体能够像人类一样好奇，通过追问来更高效地引导搜索过程。这一改进显著提升了LLM在QA任务中的表现，同时避免了KGP框架的高昂成本和延迟。我们的目标是继续完善这一方法，以期在QA领域提供更精准、迅速且经济的解决方案。",
    "title_cn": "CuriousLLM：融入推理知识图谱的提示，提升多文档问答的智能水平",
    "tags": [
      "Agent",
      "问答系统",
      "知识图谱"
    ]
  },
  {
    "title": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation",
    "submit_datetime": "2024年04月13日",
    "abstract": "With the rapid advancement of large language models (LLMs) and their remarkable capabilities in handling complex language tasks, an increasing number of studies are employing LLMs as agents to emulate the sequential decision-making processes of humans often represented as Markov decision-making processes (MDPs). The actions within this decision-making framework adhere to specific probability distributions and require iterative sampling. This arouses our curiosity regarding the capacity of LLM agents to comprehend probability distributions, thereby guiding the agent's behavioral decision-making through probabilistic sampling and generating behavioral sequences. To answer the above question, we divide the problem into two main aspects: simulation where the exact probability distribution is known, and generation of sequences where the probability distribution is ambiguous. In the first case, the agent is required to give the type and parameters of the probability distribution through the problem description, and then give the sampling sequence. However, our analysis shows that LLM agents perform poorly in this case, but the sampling success rate can be improved through programming tools. Real-world scenarios often entail unknown probability distributions. Thus, in the second case, we ask the agents to change the activity level in online social networks and analyze the frequency of actions. Ultimately, our analysis shows that LLM agents cannot sample probability distributions even using programming tools. Therefore, careful consideration is still required before directly applying LLM agents as agents to simulate human behavior.",
    "pdf_link": "https://arxiv.org/abs/2404.09043",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.09043v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.09043/x19.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）在处理复杂语言任务上的显著进步，促使众多研究将其作为模拟人类顺序决策过程（常以马尔可夫决策过程，MDPs表示）的代理。这些决策过程中的行动基于特定的概率分布，需通过迭代抽样来确定。这引发了我们对LLM代理能否理解概率分布，并以此指导其行为决策和生成行为序列的兴趣。为探究此问题，我们将研究分为两个方面：一是在已知确切概率分布的模拟中，代理需根据问题描述确定概率分布的类型和参数，然后生成抽样序列；二是在概率分布不明确的情况下生成序列。在第一种情况下，尽管LLM代理表现不佳，但通过编程工具可以提升其抽样成功率。然而，在第二种情况下，即使借助编程工具，LLM代理也无法准确抽样概率分布。因此，在将LLM代理直接应用于模拟人类行为之前，我们必须审慎考虑。",
    "title_cn": "大型语言模型（LLM）是否遵循随机性原则？本文深入探讨了在这些模型中运用概率分布抽样技术以进行行为模拟的可能性。",
    "tags": [
      "Agent",
      "人工智能",
      "决策过程"
    ]
  },
  {
    "title": "Generative AI Agent for Next-Generation MIMO Design: Fundamentals, Challenges, and Vision",
    "submit_datetime": "2024年04月12日",
    "abstract": "Next-generation multiple input multiple output (MIMO) is expected to be intelligent and scalable. In this paper, we study generative artificial intelligence (AI) agent-enabled next-generation MIMO design. Firstly, we provide an overview of the development, fundamentals, and challenges of the next-generation MIMO. Then, we propose the concept of the generative AI agent, which is capable of generating tailored and specialized contents with the aid of large language model (LLM) and retrieval augmented generation (RAG). Next, we comprehensively discuss the features and advantages of the generative AI agent framework. More importantly, to tackle existing challenges of next-generation MIMO, we discuss generative AI agent-enabled next-generation MIMO design, from the perspective of performance analysis, signal processing, and resource allocation. Furthermore, we present two compelling case studies that demonstrate the effectiveness of leveraging the generative AI agent for performance analysis in complex configuration scenarios. These examples highlight how the integration of generative AI agents can significantly enhance the analysis and design of next-generation MIMO systems. Finally, we discuss important potential research future directions.",
    "pdf_link": "https://arxiv.org/abs/2404.08878",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08878v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08878/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08878v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08878/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08878v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08878/x5.png"
      }
    ],
    "abstract_cn": "预计新一代多输入多输出（MIMO）技术将具备智能化和可扩展性。本文深入探讨了由生成性人工智能（AI）代理驱动的新一代MIMO设计。文章首先回顾了新一代MIMO技术的发展历程、基本原理及其面临的挑战。随后，提出了一种新型的生成性AI代理，该代理结合了大型语言模型（LLM）和检索增强生成（RAG）技术，能够生成定制化和专业化的内容。文章进一步详细阐述了生成性AI代理框架的特性和优势，并针对新一代MIMO技术面临的挑战，从性能分析、信号处理和资源分配等多个维度进行了深入讨论。此外，文中还展示了两个案例研究，证明了在复杂配置环境中，利用生成性AI代理进行性能分析的有效性。这些案例凸显了将生成性AI代理整合进系统，可以显著提升新一代MIMO系统的分析和设计水平。文末，作者提出了未来研究的重要潜在方向。",
    "title_cn": "为下一代多输入多输出（MIMO）设计而生的创造性人工智能代理：探索其基本原理、面临的挑战以及未来的发展方向。",
    "tags": [
      "Agent",
      "",
      "人工智能"
    ]
  },
  {
    "title": "LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Automation Task Evaluation",
    "submit_datetime": "2024年04月12日",
    "abstract": "The emergent large language/multimodal models facilitate the evolution of mobile agents, especially in the task of mobile UI automation. However, existing evaluation approaches, which rely on human validation or established datasets to compare agent-predicted actions with predefined ones, are unscalable and unfaithful. To overcome these limitations, this paper presents LlamaTouch, a testbed for on-device agent execution and faithful, scalable agent evaluation. By observing that the task execution process only transfers UI states, LlamaTouch employs a novel evaluation approach that only assesses whether an agent traverses all manually annotated, essential application/system states. LlamaTouch comprises three key techniques: (1) On-device task execution that enables mobile agents to interact with real mobile environments for task completion. (2) Fine-grained UI component annotation that merges pixel-level screenshots and textual screen hierarchies to explicitly identify and precisely annotate essential UI components with a rich set of designed annotation primitives. (3) A multi-level state matching algorithm that utilizes exact and fuzzy matching to accurately detect critical information in each screen with unpredictable UI layout/content dynamics. LlamaTouch currently incorporates four mobile agents and 495 UI automation tasks, encompassing both tasks in the widely-used datasets and our self-constructed ones for more diverse mobile applications. Evaluation results demonstrate the LlamaTouch's high faithfulness of evaluation in real environments and its better scalability than human validation. LlamaTouch also enables easy task annotation and integration of new mobile agents. Code and dataset are publicly available at https://github.com/LlamaTouch/LlamaTouch.",
    "pdf_link": "https://arxiv.org/abs/2404.16054",
    "graphs": [],
    "abstract_cn": "随着大型语言/多模态模型的出现，移动代理的进化，尤其是在移动用户界面(UI)自动化任务中，得到了显著推动。然而，目前依赖人工验证或现有数据集的评估方法存在扩展性差和准确性不足的问题。为了解决这一难题，本文介绍了 LlamaTouch，这是一个在设备上执行代理操作并进行准确、可扩展评估的测试平台。LlamaTouch 创新地仅关注任务执行过程中的 UI 状态转移，通过评估代理是否成功遍历了所有关键的、手动标注的应用/系统状态。该平台采用了三项核心技术：(1) 设备上的实时任务执行，让移动代理能够与真实移动环境互动完成任务；(2) 精细的 UI 组件标注，结合像素级截图和文本化屏幕层级，精确识别并标注关键 UI 组件；(3) 多级状态匹配算法，结合精确匹配和模糊匹配技术，准确捕捉每个屏幕中的关键信息，即使在 UI 布局和内容动态变化的情况下。LlamaTouch 已集成四种移动代理和 495 项 UI 自动化任务，不仅涵盖了广泛使用的公共数据集，还包括我们为多样化移动应用自建的任务。评估结果显示，LlamaTouch 在真实环境中的评估准确性高，且相较于人工验证，具有更好的可扩展性。此外，LlamaTouch 还简化了任务标注流程，并支持新移动代理的轻松集成。相关代码和数据集已在 https://github.com/LlamaTouch/LlamaTouch 上公开发布。",
    "title_cn": "LlamaTouch：一个忠实且可扩展的移动用户界面自动化任务评估测试平台。",
    "tags": [
      "Agent",
      "移动应用测试",
      "自动化测试"
    ]
  },
  {
    "title": "\"Don't forget to put the milk back!\" Dataset for Enabling Embodied Agents to Detect Anomalous Situations",
    "submit_datetime": "2024年04月12日",
    "abstract": "Home robots intend to make their users lives easier. Our work assists in this goal by enabling robots to inform their users of dangerous or unsanitary anomalies in their home. Some examples of these anomalies include the user leaving their milk out, forgetting to turn off the stove, or leaving poison accessible to children. To move towards enabling home robots with these abilities, we have created a new dataset, which we call SafetyDetect. The SafetyDetect dataset consists of 1000 anomalous home scenes, each of which contains unsafe or unsanitary situations for an agent to detect. Our approach utilizes large language models (LLMs) alongside both a graph representation of the scene and the relationships between the objects in the scene. Our key insight is that this connected scene graph and the object relationships it encodes enables the LLM to better reason about the scene -- especially as it relates to detecting dangerous or unsanitary situations. Our most promising approach utilizes GPT-4 and pursues a categorization technique where object relations from the scene graph are classified as normal, dangerous, unsanitary, or dangerous for children. This method is able to correctly identify over 90% of anomalous scenarios in the SafetyDetect Dataset. Additionally, we conduct real world experiments on a ClearPath TurtleBot where we generate a scene graph from visuals of the real world scene, and run our approach with no modification. This setup resulted in little performance loss. The SafetyDetect Dataset and code will be released to the public upon this papers publication.",
    "pdf_link": "https://arxiv.org/abs/2404.08827",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08827v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08827/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08827v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08827/Qualitative.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08827v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08827/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08827v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08827/x3.png"
      }
    ],
    "abstract_cn": "家用机器人的使命是简化我们的生活。本研究通过赋予机器人监测家中潜在危险或卫生问题的能力，助力这一使命。例如，提醒用户牛奶未冷藏、炉火未关或有毒物品被儿童触及等情况。为实现这一目标，我们开发了名为SafetyDetect的新型数据集，包含1000个包含安全隐患或卫生问题的异常家庭场景。我们的研究方法结合了大型语言模型（LLMs）以及场景的图形表示和物体间的关系。核心洞见在于，通过场景图中的物体关系，LLMs能更准确地理解和推理场景，尤其是在识别危险或不卫生情况方面。我们采用的最有前景的方法是利用GPT-4，通过将场景图中的物体关系分类为正常、危险、不卫生或儿童危险，以识别异常情况。这一方法在SafetyDetect数据集上的识别准确率超过90%。此外，我们在ClearPath TurtleBot上进行了实地测试，通过实景视觉生成场景图，并直接应用我们的算法，结果性能仅略有下降。随着本文的发表，SafetyDetect数据集和相关代码将向公众开放。",
    "title_cn": "\"别忘了把牛奶放回冰箱！\" 这个数据集旨在帮助具身智能体识别异常状况。",
    "tags": [
      "Agent",
      "家庭安全",
      "机器人技术"
    ]
  },
  {
    "title": "Inverse Kinematics for Neuro-Robotic Grasping with Humanoid Embodied Agents",
    "submit_datetime": "2024年04月12日",
    "abstract": "This paper introduces a novel zero-shot motion planning method that allows users to quickly design smooth robot motions in Cartesian space. A Bézier curve-based Cartesian plan is transformed into a joint space trajectory by our neuro-inspired inverse kinematics (IK) method CycleIK, for which we enable platform independence by scaling it to arbitrary robot designs. The motion planner is evaluated on the physical hardware of the two humanoid robots NICO and NICOL in a human-in-the-loop grasping scenario. Our method is deployed with an embodied agent that is a large language model (LLM) at its core. We generalize the embodied agent, that was introduced for NICOL, to also be embodied by NICO. The agent can execute a discrete set of physical actions and allows the user to verbally instruct various different robots. We contribute a grasping primitive to its action space that allows for precise manipulation of household objects. The new CycleIK method is compared to popular numerical IK solvers and state-of-the-art neural IK methods in simulation and is shown to be competitive with or outperform all evaluated methods when the algorithm runtime is very short. The grasping primitive is evaluated on both NICOL and NICO robots with a reported grasp success of 72% to 82% for each robot, respectively.",
    "pdf_link": "https://arxiv.org/abs/2404.08825",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08825v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08825/x1.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.08825v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08825/generator_alt_.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08825v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08825/cycle_ik_overview__.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08825v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08825/nico_trajectory_top_.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08825v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08825/nico_nicol_grasp_success_alt___.png"
      }
    ],
    "abstract_cn": "本研究提出了一种创新的零样本运动规划技术，它使得用户能够迅速在笛卡尔空间内规划出流畅的机器人动作。这种基于贝塞尔曲线的规划被我们的神经网络启发式逆运动学（IK）算法 CycleIK 转化为关节空间路径，并通过适配任意机器人设计实现了平台通用性。该运动规划系统在两个人形机器人 NICO 和 NICOL 上进行了实际测试，测试环境为包含人类操作的抓取任务。我们采用的具身代理核心是一个大型语言模型（LLM），并将其从 NICOL 扩展至 NICO。该代理能够执行一系列离散的物理动作，使用户能够通过语音指令控制多种不同的机器人。我们还为其动作库新增了一个抓取基本动作，以便对家用物品进行精细操控。在模拟环境中，CycleIK 与传统的数值 IK 解算器和先进的神经网络 IK 方法进行了比较，结果表明，在算法运行时间极短的情况下，CycleIK 能够与之竞争或取得更佳表现。此外，抓取基本动作在 NICOL 和 NICO 机器人上的测试结果显示，每台机器人的抓取成功率分别为 72% 到 82%。",
    "title_cn": "逆动力学在仿人机器人抓取任务中的应用，特别是与神经机器人结合时的实现。",
    "tags": [
      "Agent",
      "机器人技术",
      "运动规划"
    ]
  },
  {
    "title": "Training a Vision Language Model as Smartphone Assistant",
    "submit_datetime": "2024年04月12日",
    "abstract": "Addressing the challenge of a digital assistant capable of executing a wide array of user tasks, our research focuses on the realm of instruction-based mobile device control. We leverage recent advancements in large language models (LLMs) and present a visual language model (VLM) that can fulfill diverse tasks on mobile devices. Our model functions by interacting solely with the user interface (UI). It uses the visual input from the device screen and mimics human-like interactions, encompassing gestures such as tapping and swiping. This generality in the input and output space allows our agent to interact with any application on the device. Unlike previous methods, our model operates not only on a single screen image but on vision-language sentences created from sequences of past screenshots along with corresponding actions. Evaluating our method on the challenging Android in the Wild benchmark demonstrates its promising efficacy and potential.",
    "pdf_link": "https://arxiv.org/abs/2404.08755",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08755v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08755/x1.png"
      }
    ],
    "abstract_cn": "本研究旨在解决数字助手执行多种用户任务的挑战，特别是在基于指令的移动设备控制方面。借助大型语言模型（LLMs）的最新进展，我们推出了一款视觉语言模型（VLM），能够应对移动设备上的多样化任务。该模型通过与用户界面（UI）的互动来执行操作，利用设备屏幕的视觉输入，模拟人类的交互方式，如轻触和滑动等。这种设计使得我们的模型能够与设备上的所有应用程序进行互动。与传统方法相比，我们的模型不仅处理单个屏幕图像，还能处理由连续的屏幕截图序列及其对应动作生成的视觉-语言句子。在Android in the Wild这一具有挑战性的基准测试中，我们的方法展现了其显著的效率和潜力。",
    "title_cn": "打造一款视觉语言模型，助力智能手机成为你的得力助手。",
    "tags": [
      "Agent",
      "移动设备控制",
      "人工智能"
    ]
  },
  {
    "title": "Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation",
    "submit_datetime": "2024年04月12日",
    "abstract": "This paper introduces CRITICAL, a novel closed-loop framework for autonomous vehicle (AV) training and testing. CRITICAL stands out for its ability to generate diverse scenarios, focusing on critical driving situations that target specific learning and performance gaps identified in the Reinforcement Learning (RL) agent. The framework achieves this by integrating real-world traffic dynamics, driving behavior analysis, surrogate safety measures, and an optional Large Language Model (LLM) component. It is proven that the establishment of a closed feedback loop between the data generation pipeline and the training process can enhance the learning rate during training, elevate overall system performance, and augment safety resilience. Our evaluations, conducted using the Proximal Policy Optimization (PPO) and the HighwayEnv simulation environment, demonstrate noticeable performance improvements with the integration of critical case generation and LLM analysis, indicating CRITICAL's potential to improve the robustness of AV systems and streamline the generation of critical scenarios. This ultimately serves to hasten the development of AV agents, expand the general scope of RL training, and ameliorate validation efforts for AV safety.",
    "pdf_link": "https://arxiv.org/abs/2404.08570",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08570v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08570/Banner.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.08570v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08570/Scenario_Diagram.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.08570v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08570/Architecture.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.08570v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08570/loss_plot.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08570v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08570/risk_metrics.png"
      }
    ],
    "abstract_cn": "本文提出了 CRITICAL，一个创新的闭环系统，专为自动驾驶车辆（AV）的训练与测试设计。CRITICAL 的特色在于其能够创造出多样化的驾驶情境，特别关注那些针对强化学习（RL）代理中发现的学习与性能缺陷的关键驾驶时刻。该系统通过融合现实交通流动、驾驶行为解析、替代性安全指标，以及一个可选的大规模语言模型（LLM）模块来实现这一点。研究证实，数据生成流程与训练过程之间的闭环反馈机制能够提升训练效率，增强系统性能，并提高安全防护能力。通过近端策略优化（PPO）算法和 HighwayEnv 仿真环境的测试，我们发现结合关键案例生成和 LLM 分析可以显著提升性能，这表明 CRITICAL 在增强 AV 系统鲁棒性、优化关键情境生成以及加速 AV 代理开发和 RL 训练范围扩展方面具有巨大潜力，同时也有助于提高 AV 安全性的验证效率。",
    "title_cn": "融入语言模型并生成关键场景，提升自动驾驶车辆的训练效果。",
    "tags": [
      "Agent",
      "自动驾驶",
      "机器学习"
    ]
  },
  {
    "title": "Strategic Interactions between Large Language Models-based Agents in Beauty Contests",
    "submit_datetime": "2024年04月12日",
    "abstract": "The growing adoption of large language models (LLMs) presents substantial potential for deeper understanding of human behaviours within game theory frameworks through simulations. Leveraging on the diverse pool of LLM types and addressing the gap in research on competitive games, this paper examines the strategic interactions among multiple types of LLM-based agents in a classical game of beauty contest. Drawing parallels to experiments involving human subjects, LLM-based agents are assessed similarly in terms of strategic levels. They demonstrate varying depth of reasoning that falls within a range of level-0 and 1, and show convergence in actions in repeated settings. Furthermore, I also explore how variations in group composition of agent types influence strategic behaviours, where I found higher proportion of fixed-strategy opponents enhances convergence for LLM-based agents, and having a mixed environment with agents of differing relative strategic levels accelerates convergence for all agents. There could also be higher average payoffs for the more intelligent agents, albeit at the expense of the less intelligent agents. These results not only provide insights into outcomes for simulated agents under specified scenarios, it also offer valuable implications for understanding strategic interactions between algorithms.",
    "pdf_link": "https://arxiv.org/abs/2404.08492",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/histogram_of_choices_multillm.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/same_upperbound_c.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/mean_strategic_level_reference_average_c.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/fig2_mixed_one_shot_c.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/mean_payoffs_c.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/period1_average_n_c.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/average_normalized_chosen_number.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/freq_choices_6periods.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/freq_n_6periods.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/average_n_across_sessions_n.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/average_payoffs_across_periods.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/fixed_strategy_choices.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/fixed_strategy_choices_weak.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/fixed_strategy_convergence_rates.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/fixed_strategy_convergence_rates_weak.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/fixed_strategy_n_values.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/fixed_strategy_n_values_weak.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_set_up1_static.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_set_up2_static.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_set_up3_static.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_set_up1_static_weak.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_set_up2_static_weak.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_set_up3_static_weak.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/setup1_mixedllm_choices.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/setup2_mixedllm_choices.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/setup3_mixedllm_choices.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/setup4_mixedllm_choices.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/setup5_mixedllm_choices.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/convergence_mixedllm.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/n_values_mixedllm1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/n_values_mixedllm2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_mixedllm_setup1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_mixedllm_setup2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_mixedllm_setup3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_mixedllm_setup4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08492v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08492/payoffs_mixedllm_setup5.png"
      }
    ],
    "abstract_cn": "随着大型语言模型（LLMs）的广泛应用，我们有机会通过模拟更深入地洞察博弈论框架下的人类行为模式。本文聚焦于竞争性游戏研究的缺口，通过经典选美比赛这一经典游戏，探讨了基于LLM的多种智能体之间的战略互动。与人类参与者的实验相比较，这些基于LLM的智能体在战略层面上得到了类似的评估，展现出从0级到1级的不同程度的推理能力，并在多次重复的情境中趋于行动一致。研究还发现，智能体群体构成的变化对战略行为有显著影响：固定策略对手比例的提高有助于基于LLM的智能体更快趋同，而不同战略水平智能体混合的环境则能加速所有智能体的趋同过程。此外，智能体的智能程度越高，其平均收益也越高，尽管这可能会牺牲那些智能程度较低的智能体的利益。这些发现不仅为特定情境下模拟智能体的行为结果提供了深刻见解，同时也为理解算法间的战略互动提供了重要的启示。",
    "title_cn": "在选美比赛中，基于大型语言模型的智能体之间的策略互动。",
    "tags": [
      "Agent",
      "博弈论",
      "人工智能"
    ]
  },
  {
    "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
    "submit_datetime": "2024年04月11日",
    "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
    "pdf_link": "https://arxiv.org/abs/2404.08189",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08189v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08189/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08189v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08189/t2f_architecture.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.08189v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08189/t2f_input_output_example.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.08189v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08189/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08189v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08189/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08189v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08189/x4.png"
      }
    ],
    "abstract_cn": "生成性AI（GenAI）普遍存在的一大挑战是其易产生幻觉的特性。尽管大型语言模型（LLM）已经风靡全球，但如果不减少幻觉现象，GenAI在实际应用中可能会遭遇用户接受度的难题。在开发一款根据自然语言需求生成工作流程的企业级应用时，我们构建了一个系统，该系统采用检索增强生成（RAG）技术显著提升了所生成工作流程的结构化输出质量。得益于RAG的运用，我们的系统显著降低了输出中的幻觉现象，增强了LLM在非专业领域的泛化能力。此外，我们还发现，采用一个小型但训练精良的检索器编码器，可以有效减少所需LLM的规模，从而降低了部署基于LLM系统所需的资源。",
    "title_cn": "利用检索增强生成技术，有效降低结构化输出中的幻觉问题。",
    "tags": [
      "分类：RAG",
      "企业应用",
      "人工智能"
    ]
  },
  {
    "title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
    "submit_datetime": "2024年04月11日",
    "abstract": "Scientific Research, vital for improving human life, is hindered by its inherent complexity, slow pace, and the need for specialized experts. To enhance its productivity, we propose a ResearchAgent, a large language model-powered research idea writing agent, which automatically generates problems, methods, and experiment designs while iteratively refining them based on scientific literature. Specifically, starting with a core paper as the primary focus to generate ideas, our ResearchAgent is augmented not only with relevant publications through connecting information over an academic graph but also entities retrieved from an entity-centric knowledge store based on their underlying concepts, mined and shared across numerous papers. In addition, mirroring the human approach to iteratively improving ideas with peer discussions, we leverage multiple ReviewingAgents that provide reviews and feedback iteratively. Further, they are instantiated with human preference-aligned large language models whose criteria for evaluation are derived from actual human judgments. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showcasing its effectiveness in generating novel, clear, and valid research ideas based on human and model-based evaluation results.",
    "pdf_link": "https://arxiv.org/abs/2404.07738",
    "graphs": [],
    "abstract_cn": "科学研究对于提升人类生活质量至关重要，但其复杂性、缓慢的进展和对专家的依赖限制了其效率。为了提升科研生产力，我们设计了一个名为“研究代理”的智能写作助手，它由大型语言模型提供支持，能够自动生成问题、方法和实验设计，并通过科学文献不断迭代优化。该代理以一篇核心论文为起点，生成研究思路，并通过学术网络连接相关文献，以及从一个以实体为中心的知识库中检索与论文概念相关的实体，这些实体在多篇论文中被挖掘和共享。同时，借鉴人类通过同行评审不断改进研究思路的做法，我们引入了多个“审稿代理”，它们提供连续的评审和反馈。这些审稿代理是基于与人类偏好一致的大型语言模型构建的，其评审标准来源于实际的人类评价。我们在多个学科领域对“研究代理”进行了实验验证，结果表明，它在生成新颖、清晰且有效的研究构想方面具有显著效果，这一效果得到了人类评审和模型评估的双重认可。",
    "title_cn": "ResearchAgent：借助大型语言模型，对科学文献进行迭代式的研究创意生成",
    "tags": [
      "分类：Agent",
      "科学研究",
      "自动化写作"
    ]
  },
  {
    "title": "Rumour Evaluation with Very Large Language Models",
    "submit_datetime": "2024年04月11日",
    "abstract": "Conversational prompt-engineering-based large language models (LLMs) have enabled targeted control over the output creation, enhancing versatility, adaptability and adhoc retrieval. From another perspective, digital misinformation has reached alarming levels. The anonymity, availability and reach of social media offer fertile ground for rumours to propagate. This work proposes to leverage the advancement of prompting-dependent LLMs to combat misinformation by extending the research efforts of the RumourEval task on its Twitter dataset. To the end, we employ two prompting-based LLM variants (GPT-3.5-turbo and GPT-4) to extend the two RumourEval subtasks: (1) veracity prediction, and (2) stance classification. For veracity prediction, three classifications schemes are experimented per GPT variant. Each scheme is tested in zero-, one- and few-shot settings. Our best results outperform the precedent ones by a substantial margin. For stance classification, prompting-based-approaches show comparable performance to prior results, with no improvement over finetuning methods. Rumour stance subtask is also extended beyond the original setting to allow multiclass classification. All of the generated predictions for both subtasks are equipped with confidence scores determining their trustworthiness degree according to the LLM, and post-hoc justifications for explainability and interpretability purposes. Our primary aim is AI for social good.",
    "pdf_link": "https://arxiv.org/abs/2404.16859",
    "graphs": [],
    "abstract_cn": "对话式提示工程的LLMs为输出创造提供了精准控制，提升了模型的多功能性、适应性和即兴检索能力。然而，数字虚假信息的泛滥已引起社会广泛关注。社交媒体的匿名性、易得性和广泛传播为谣言的滋生提供了温床。本研究旨在借助先进的提示依赖型LLMs，通过扩展RumourEval任务在Twitter数据集上的研究，来对抗虚假信息。我们采用了两种基于提示的LLM变种（GPT-3.5-turbo和GPT-4），对RumourEval的两个子任务进行了扩展：（1）真实性预测；（2）立场分类。在真实性预测方面，我们对每个GPT变种进行了三种不同分类方案的实验，并在零次、一次和少次的设置中进行了测试。我们的最佳成绩显著超过了以往的成果。在立场分类方面，基于提示的方法与之前的结果相当，但并未超过微调方法的效果。此外，谣言立场子任务也被扩展，支持多类分类。所有生成的预测结果都附带了置信度评分，这些评分由LLM提供，用以评估其可信度，并附有事后解释，以增强模型的可解释性和可解释性。我们的核心宗旨是利用人工智能服务于社会公益。",
    "title_cn": "借助超大型语言模型，我们能够对谣言进行评估。",
    "tags": [
      "LLM应用",
      "社交媒体",
      "信息安全"
    ]
  },
  {
    "title": "Is Your LLM Outdated? Benchmarking LLMs & Alignment Algorithms for Time-Sensitive Knowledge",
    "submit_datetime": "2024年04月10日",
    "abstract": "We study the appropriateness of Large Language Models (LLMs) as knowledge repositories. We focus on the challenge of maintaining LLMs' factual knowledge up-to-date over time. Motivated by the lack of studies on identifying outdated knowledge within LLMs, we design and develop a dynamic benchmark with up-to-date ground truth answers for each target factual question. We evaluate eighteen open-source and closed-source state-of-the-art LLMs on time-sensitive knowledge retrieved in real-time from Wikidata. We select time-sensitive domain facts in politics, sports, and organizations, and estimate the recency of the information learned by the model during pre-training\\fine-tuning. In the second contribution, we evaluate the effectiveness of knowledge editing methods for aligning LLMs with up-to-date factual knowledge and compare their performance with Retrieval Augmented Generation. The dynamic benchmark is designed to be used as-is to assess LLMs's up-to-dateness, as well as to be extended to other domains by sharing the code, the dataset, as well as evaluation and visualization scripts.",
    "pdf_link": "https://arxiv.org/abs/2404.08700",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08700v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08700/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08700v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08700/observed_data.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08700v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08700/scala_number_and_percentage.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08700v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08700/ragdocs.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08700v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08700/observed_data_appendix1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08700v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08700/observed_data_appendix2.png"
      }
    ],
    "abstract_cn": "本研究探讨了大型语言模型（LLMs）作为知识库的适宜性，特别关注如何确保这些模型中的事实知识能够与时俱进。鉴于目前尚缺乏对LLMs中过时知识识别的研究，我们设计并构建了一个动态基准测试，它能够为每个目标事实问题提供最新的基准答案。我们对十八个开源和闭源的先进LLMs进行了评估，这些评估基于从Wikidata实时检索的时间敏感知识。我们挑选了政治、体育和组织等时间敏感领域的特定事实，并估算了模型在预训练和微调阶段所学习信息的时效性。在研究的第二部分，我们检验了知识编辑方法在更新LLMs事实知识方面的有效性，并将其与检索增强生成的性能进行了对比。这个动态基准测试旨在评估LLMs的时效性，并可通过共享代码、数据集以及评估和可视化工具，扩展应用于其他领域的评估。",
    "title_cn": "你的大型语言模型（LLM）是否已落伍？本文旨在对 LLM 及其对齐算法进行基准测试，以评估它们处理时间敏感知识的能力。",
    "tags": [
      "LLM应用",
      "知识库",
      "基准测试"
    ]
  },
  {
    "title": "LLMs in Biomedicine: A study on clinical Named Entity Recognition",
    "submit_datetime": "2024年04月10日",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedicine due to medical language complexities and data scarcity. This paper investigates the application of LLMs in the medical domain by exploring strategies to enhance their performance for the Named-Entity Recognition (NER) task. Specifically, our study reveals the importance of meticulously designed prompts in biomedicine. Strategic selection of in-context examples yields a notable improvement, showcasing ~15-20\\% increase in F1 score across all benchmark datasets for few-shot clinical NER. Additionally, our findings suggest that integrating external resources through prompting strategies can bridge the gap between general-purpose LLM proficiency and the specialized demands of medical NER. Leveraging a medical knowledge base, our proposed method inspired by Retrieval-Augmented Generation (RAG) can boost the F1 score of LLMs for zero-shot clinical NER. We will release the code upon publication.",
    "pdf_link": "https://arxiv.org/abs/2404.07376",
    "graphs": [],
    "abstract_cn": "大型语言模型（LLMs）在多样的自然语言处理（NLP）任务中展现了卓越能力，但在生物医学领域却因医学术语的复杂和数据的匮乏而面临挑战。本研究通过优化命名实体识别（NER）任务的策略，探讨了LLMs在医学领域的应用潜力。研究发现，精心设计的生物医学提示至关重要，策略性地挑选上下文实例能显著提升性能，使得临床NER的F1分数在所有基准数据集上平均提升了15-20%。此外，研究还指出，通过提示策略整合外部资源可以有效缩小通用型LLMs与专业医学NER需求之间的差异。我们提出的，受检索增强生成（RAG）启发的方法，能够显著提升LLMs在零次临床NER任务中的F1分数。相关代码将在论文发表后公开。",
    "title_cn": "生物医学领域的大型语言模型：专注于临床命名实体识别的深入研究",
    "tags": [
      "LLM应用",
      "生物医学",
      ""
    ]
  },
  {
    "title": "Apollonion: Profile-centric Dialog Agent",
    "submit_datetime": "2024年04月09日",
    "abstract": "The emergence of Large Language Models (LLMs) has innovated the development of dialog agents. Specially, a well-trained LLM, as a central process unit, is capable of providing fluent and reasonable response for user's request. Besides, auxiliary tools such as external knowledge retrieval, personalized character for vivid response, short/long-term memory for ultra long context management are developed, completing the usage experience for LLM-based dialog agents. However, the above-mentioned techniques does not solve the issue of \\textbf{personalization from user perspective}: agents response in a same fashion to different users, without consideration of their features, such as habits, interests and past experience. In another words, current implementation of dialog agents fail in ``knowing the user''. The capacity of well-description and representation of user is under development. In this work, we proposed a framework for dialog agent to incorporate user profiling (initialization, update): user's query and response is analyzed and organized into a structural user profile, which is latter served to provide personal and more precise response. Besides, we proposed a series of evaluation protocols for personalization: to what extend the response is personal to the different users.\n  The framework is named as \\method{}, inspired by inscription of ``Know Yourself'' in the temple of Apollo (also known as \\method{}) in Ancient Greek. Few works have been conducted on incorporating personalization into LLM, \\method{} is a pioneer work on guiding LLM's response to meet individuation via the application of dialog agents, with a set of evaluation methods for measurement in personalization.",
    "pdf_link": "https://arxiv.org/abs/2404.08692",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/logo2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x16.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x17.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x18.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x19.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08692v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08692/x20.png"
      }
    ],
    "abstract_cn": "大型语言模型（LLMs）的兴起为对话代理的发展带来了创新。特别是，一个经过良好训练的LLM，作为核心处理单元，能够为用户提供流畅且合理的回应。辅助工具如外部知识检索、个性化角色塑造、以及短/长期记忆管理等，进一步提升了基于LLM的对话代理的用户体验。尽管如此，现有技术仍未能解决用户个性化的问题：对话代理对不同用户的反应千篇一律，忽略了他们的习惯、兴趣和经验等个性化特征。换句话说，目前的对话代理在“了解用户”方面存在不足。用户特征的准确描述和表达仍在不断完善之中。在本研究中，我们提出了一个整合用户画像（包括初始化和更新）的对话代理框架：用户的查询和反馈被分析并整合成一个结构化的用户画像，以便提供更加个性化和精准的回应。此外，我们还设计了一系列评估个性化程度的协议。这个框架被命名为\\method{}，灵感来源于古希腊阿波罗神庙上的格言“认识你自己”。在将个性化融入LLM的领域中，\\method{}是一次开创性的尝试，旨在引导LLM通过对话代理的应答实现个性化，并建立了一套评估个性化效果的方法。",
    "title_cn": "Apollonion：专注于个人资料的对话代理",
    "tags": [
      "Agent",
      "对话系统",
      "个性化推荐"
    ]
  },
  {
    "title": "GUIDE: Graphical User Interface Data for Execution",
    "submit_datetime": "2024年04月09日",
    "abstract": "In this paper, we introduce GUIDE, a novel dataset tailored for the advancement of Multimodal Large Language Model (MLLM) applications, particularly focusing on Robotic Process Automation (RPA) use cases. Our dataset encompasses diverse data from various websites including Apollo(62.67\\%), Gmail(3.43\\%), Calendar(10.98\\%) and Canva(22.92\\%). Each data entry includes an image, a task description, the last action taken, CoT and the next action to be performed along with grounding information of where the action needs to be executed. The data is collected using our in-house advanced annotation tool NEXTAG (Next Action Grounding and Annotation Tool). The data is adapted for multiple OS, browsers and display types. It is collected by multiple annotators to capture the variation of design and the way person uses a website.\n  Through this dataset, we aim to facilitate research and development in the realm of LLMs for graphical user interfaces, particularly in tasks related to RPA. The dataset's multi-platform nature and coverage of diverse websites enable the exploration of cross-interface capabilities in automation tasks. We believe that our dataset will serve as a valuable resource for advancing the capabilities of multi-platform LLMs in practical applications, fostering innovation in the field of automation and natural language understanding. Using GUIDE, we build V-Zen, the first RPA model to automate multiple websites using our in-House Automation tool AUTONODE",
    "pdf_link": "https://arxiv.org/abs/2404.16048",
    "graphs": [],
    "abstract_cn": "本文提出了GUIDE，一个专为推动多模态大型语言模型（MLLM）应用而设计的新数据集，重点聚焦于机器人流程自动化（RPA）的实际应用。该数据集广泛采集了包括Apollo、Gmail、Calendar和Canva在内的多个网站数据，每个数据条目都包含了图像、任务说明、最近一次操作、上下文信息（CoT）、待执行的下一步操作及其执行位置的详细信息。我们利用自主研发的先进注释工具NEXTAG进行数据采集，确保数据能够适应不同的操作系统、浏览器和显示设备，并由多位注释者共同完成，以确保捕捉到网站设计多样性及用户使用习惯的差异。通过GUIDE数据集，我们期望推动LLM在图形用户界面领域的研究与开发，尤其是在RPA相关任务上。其跨平台的特性和对多样化网站的广泛覆盖，为探索自动化任务中的界面间协作能力提供了可能。我们相信GUIDE将成为提升多平台LLM实际应用能力、激发自动化和自然语言理解领域创新的重要资源。利用GUIDE，我们开发了V-Zen，这是首个利用我们自主研发的自动化工具AUTONODE，实现多网站自动化的RPA模型。",
    "title_cn": "GUIDE：图形用户界面数据执行指南",
    "tags": [
      "LLM应用",
      "机器人流程自动化",
      "自然语言理解"
    ]
  },
  {
    "title": "Information Retrieval with Entity Linking",
    "submit_datetime": "2024年04月07日",
    "abstract": "Despite the advantages of their low-resource settings, traditional sparse retrievers depend on exact matching approaches between high-dimensional bag-of-words (BoW) representations of both the queries and the collection. As a result, retrieval performance is restricted by semantic discrepancies and vocabulary gaps. On the other hand, transformer-based dense retrievers introduce significant improvements in information retrieval tasks by exploiting low-dimensional contextualized representations of the corpus. While dense retrievers are known for their relative effectiveness, they suffer from lower efficiency and lack of generalization issues, when compared to sparse retrievers. For a lightweight retrieval task, high computational resources and time consumption are major barriers encouraging the renunciation of dense models despite potential gains. In this work, I propose boosting the performance of sparse retrievers by expanding both the queries and the documents with linked entities in two formats for the entity names: 1) explicit and 2) hashed. A zero-shot end-to-end dense entity linking system is employed for entity recognition and disambiguation to augment the corpus. By leveraging the advanced entity linking methods, I believe that the effectiveness gap between sparse and dense retrievers can be narrowed. Experiments are conducted on the MS MARCO passage dataset using the original qrel set, the re-ranked qrels favoured by MonoT5 and the latter set further re-ranked by DuoT5. Since I am concerned with the early stage retrieval in cascaded ranking architectures of large information retrieval systems, the results are evaluated using recall@1000. The suggested approach is also capable of retrieving documents for query subsets judged to be particularly difficult in prior work.",
    "pdf_link": "https://arxiv.org/abs/2404.08678",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/query_expansion.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/star-adore.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/classifier.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_original_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_original_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_original_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_original_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_original_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_original_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_original_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_original_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_original_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_original_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_original_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_original_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_original_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_original_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_original_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_original_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_monoT5_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_monoT5_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_monoT5_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_monoT5_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_monoT5_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_monoT5_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_monoT5_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_monoT5_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_monoT5_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_monoT5_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_monoT5_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_monoT5_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_monoT5_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_monoT5_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_monoT5_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_monoT5_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_duoT5_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_duoT5_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_duoT5_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/all_duoT5_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_duoT5_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_duoT5_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_duoT5_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/veiled_duoT5_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_duoT5_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_duoT5_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_duoT5_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/pygmy_duoT5_1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_duoT5_2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_duoT5_3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_duoT5_4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.08678v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.08678/lesser_duoT5_1.png"
      }
    ],
    "abstract_cn": "尽管传统稀疏检索器在资源较少的环境中展现出优势，但它们仍需依赖于查询和文档集合间高维词袋（BoW）表示的精确匹配，这限制了检索性能，因为存在语义差异和词汇缺口。相比之下，基于变换器的密集检索器通过采用语料库的低维上下文化表示，在信息检索任务上取得了显著提升。尽管密集检索器因其高效性而受到推崇，但相较于稀疏检索器，它们在效率和泛化能力上仍有不足。在轻量级检索任务中，高昂的计算资源和时间成本成为了放弃潜在优势的密集模型的主要障碍。本文提出了一种提升稀疏检索器性能的方法，即通过两种格式（显式和哈希）将链接实体扩展到查询和文档中。引入了一个零样本端到端密集实体链接系统，用于实体识别和消歧，以增强语料库。借助先进的实体链接技术，有望缩小稀疏与密集检索器之间的性能差距。实验基于MS MARCO段落数据集，采用了原始qrel集、MonoT5优化后的重排qrels，以及DuoT5进一步优化的集合。鉴于研究重点在于大型信息检索系统中串联排名架构的初期检索阶段，因此采用recall@1000作为评估指标。本研究所提出的方法同样能够有效检索出以往研究中认为难度较大的查询子集的相关文档。",
    "title_cn": "通过实体链接进行信息检索",
    "tags": [
      "LLM应用",
      "信息检索",
      "实体链接"
    ]
  },
  {
    "title": "Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation",
    "submit_datetime": "2024年04月04日",
    "abstract": "Requirements elicitation, a critical, yet time-consuming and challenging step in product development, often fails to capture the full spectrum of user needs. This may lead to products that fall short of expectations. This paper introduces a novel framework that leverages Large Language Models (LLMs) to automate and enhance the requirements elicitation process. LLMs are used to generate a vast array of simulated users (LLM agents), enabling the exploration of a much broader range of user needs and unforeseen use cases. These agents engage in product experience scenarios, through explaining their actions, observations, and challenges. Subsequent agent interviews and analysis uncover valuable user needs, including latent ones. We validate our framework with three experiments. First, we explore different methodologies for diverse agent generation, discussing their advantages and shortcomings. We measure the diversity of identified user needs and demonstrate that context-aware agent generation leads to greater diversity. Second, we show how our framework effectively mimics empathic lead user interviews, identifying a greater number of latent needs than conventional human interviews. Third, we showcase that LLMs can be used to analyze interviews, capture needs, and classify them as latent or not. Our work highlights the potential of using LLM agents to accelerate early-stage product development, reduce costs, and increase innovation.",
    "pdf_link": "https://arxiv.org/abs/2404.16045",
    "graphs": [],
    "abstract_cn": "需求挖掘是产品开发中至关重要却费时费力的一环，往往难以全面捕捉用户的多元需求，从而造成产品不尽人意。本篇论文提出了一个创新框架，该框架借助大型语言模型（LLMs）自动化并优化需求挖掘流程。通过LLMs，我们能够创建出众多模拟用户（即LLM代理），这些代理能够探索更宽广的用户需求范围和预测之外的使用情境。在产品体验场景中，这些代理通过阐述自己的行为、观察和所遇挑战，深入参与互动。随后，通过代理访谈和深入分析，我们能够挖掘出包括潜在需求在内的宝贵用户需求。我们的框架通过三项实验得到验证：首先，我们探索了生成多样化代理的不同方法，并比较了它们的优劣；其次，我们展示了如何利用框架有效模拟富有同理心的领先用户访谈，从而识别出比传统人工访谈更多的潜在需求；最后，我们证明了LLMs不仅能够分析访谈内容、捕捉需求，还能将需求分类为显性或隐性。本研究凸显了运用LLM代理在产品开发的早期阶段加速进程、降低成本并提升创新的可能性。",
    "title_cn": "Elicitron：一款基于大型语言模型（LLM）的代理仿真框架，专为设计需求的挖掘而设计。",
    "tags": [
      "LLM应用",
      "产品开发",
      "人工智能"
    ]
  },
  {
    "title": "Concept-Guided LLM Agents for Human-AI Safety Codesign",
    "submit_datetime": "2024年04月03日",
    "abstract": "Generative AI is increasingly important in software engineering, including safety engineering, where its use ensures that software does not cause harm to people. This also leads to high quality requirements for generative AI. Therefore, the simplistic use of Large Language Models (LLMs) alone will not meet these quality demands. It is crucial to develop more advanced and sophisticated approaches that can effectively address the complexities and safety concerns of software systems. Ultimately, humans must understand and take responsibility for the suggestions provided by generative AI to ensure system safety. To this end, we present an efficient, hybrid strategy to leverage LLMs for safety analysis and Human-AI codesign. In particular, we develop a customized LLM agent that uses elements of prompt engineering, heuristic reasoning, and retrieval-augmented generation to solve tasks associated with predefined safety concepts, in interaction with a system model graph. The reasoning is guided by a cascade of micro-decisions that help preserve structured information. We further suggest a graph verbalization which acts as an intermediate representation of the system model to facilitate LLM-graph interactions. Selected pairs of prompts and responses relevant for safety analytics illustrate our method for the use case of a simplified automated driving system.",
    "pdf_link": "https://arxiv.org/abs/2404.15317",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15317v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15317/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15317v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15317/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15317v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15317/output.png"
      }
    ],
    "abstract_cn": "生成性AI在软件工程尤其是安全工程中的应用愈发关键，它确保软件不会对人造成伤害，这同时也对AI的质量提出了更高要求。仅依赖大型语言模型（LLM）的简单应用已不足以满足这些高标准。因此，迫切需要开发更先进、更精细的方法，以有效应对软件系统的复杂性和安全挑战。人类必须理解并负责监督生成性AI的建议，以保障系统的安全性。本研究提出了一种高效的混合策略，结合LLM进行安全分析与人机协同设计。我们设计了一款定制化的LLM代理，它结合了提示工程、启发式推理和增强检索生成技术，以解决与预定义安全概念相关的任务，并与系统模型图进行互动。这一过程通过一系列微决策引导，以保持信息的结构化。此外，我们还提出了一种图形化表述方法，作为系统模型的中间表示，以便更好地促进LLM与图形的交互。我们以简化的自动驾驶系统为例，展示了与安全分析相关的提示和响应对，以此来说明我们的方法。",
    "title_cn": "概念驱动的大型语言模型（LLM）代理助力人类与AI安全协同设计",
    "tags": [
      "Agent",
      "软件工程",
      "安全工程"
    ]
  },
  {
    "title": "Automated Assessment of Encouragement and Warmth in Classrooms Leveraging Multimodal Emotional Features and ChatGPT",
    "submit_datetime": "2024年04月01日",
    "abstract": "Classroom observation protocols standardize the assessment of teaching effectiveness and facilitate comprehension of classroom interactions. Whereas these protocols offer teachers specific feedback on their teaching practices, the manual coding by human raters is resource-intensive and often unreliable. This has sparked interest in developing AI-driven, cost-effective methods for automating such holistic coding. Our work explores a multimodal approach to automatically estimating encouragement and warmth in classrooms, a key component of the Global Teaching Insights (GTI) study's observation protocol. To this end, we employed facial and speech emotion recognition with sentiment analysis to extract interpretable features from video, audio, and transcript data. The prediction task involved both classification and regression methods. Additionally, in light of recent large language models' remarkable text annotation capabilities, we evaluated ChatGPT's zero-shot performance on this scoring task based on transcripts. We demonstrated our approach on the GTI dataset, comprising 367 16-minute video segments from 92 authentic lesson recordings. The inferences of GPT-4 and the best-trained model yielded correlations of r = .341 and r = .441 with human ratings, respectively. Combining estimates from both models through averaging, an ensemble approach achieved a correlation of r = .513, comparable to human inter-rater reliability. Our model explanation analysis indicated that text sentiment features were the primary contributors to the trained model's decisions. Moreover, GPT-4 could deliver logical and concrete reasoning as potential teacher guidelines. Our findings provide insights into using advanced, multimodal techniques for automated classroom observation, aiming to foster teacher training through frequent and valuable feedback.",
    "pdf_link": "https://arxiv.org/abs/2404.15310",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.15310v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15310/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15310v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15310/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15310v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15310/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15310v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15310/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.15310v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.15310/x5.png"
      }
    ],
    "abstract_cn": "课堂观察协议不仅统一了教学效果评价标准，还加深了对课堂互动的认识。这些协议虽然能为教师提供针对性的教学反馈，但人工编码过程既费时又可能存在误差。因此，研究者开始探索利用人工智能来自动化这一全面编码过程，以降低成本并提高可靠性。本研究采用了一种结合视觉和语音的多模态方法，旨在自动评估课堂中的关键元素——鼓励与温暖，这也是全球教学洞察研究中观察协议的一部分。我们利用面部和语音情感识别技术，结合情感分析，从视频、音频和文本资料中提取关键特征。研究中同时采用了分类和回归的预测方法。此外，考虑到当前大型语言模型在文本注解上的卓越表现，我们还测试了ChatGPT在没有先验训练的情况下，基于文本资料的评分能力。在GTI数据集上的实验显示，GPT-4和最佳训练模型与人工评分的相关性分别为0.341和0.441。通过整合两种模型的预测结果，我们得到了一个相关性为0.513的集成模型，这一结果与人工评分的一致性相当。模型解释分析揭示了文本情感特征在训练模型决策中起到了关键作用。GPT-4还能够提供逻辑性强、具体的推理，为教师提供潜在的指导。这些发现为利用先进的多模态技术自动化课堂观察提供了新思路，有助于通过持续而有价值的反馈促进教师的专业发展。",
    "title_cn": "通过融合多模态情感特征与ChatGPT技术，实现对课堂中鼓励与温暖的自动化评估。",
    "tags": [
      "LLM应用",
      "",
      "人工智能"
    ]
  },
  {
    "title": "Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations",
    "submit_datetime": "2024年03月23日",
    "abstract": "There is a compelling necessity from enterprises for fine tuning LLMs (Large Language Models) o get them trained on proprietary domain knowledge. The challenge is to imbibe the LLMs with domain specific knowledge using the most optimial resource and cost and in the best possible time. Many enterprises rely on RAG (Retrieval Augmented Generation) which does not need LLMs to be ine-tuned but they are limited by the quality of vector databases and their retrieval capabilities rather than the intrinsic capabilities of the LLMs themselves. In our current work we focus on fine tuning LLaMA, an open source LLM using proprietary documents and code from an enterprise repository and use the fine tuned models to evaluate the quality of responses. As part of this work, we aim to guide beginners on how to start with fine tuning an LLM for documentation and code by making educated guesses on size of GPU required and options that are available for formatting the data. We also propose pre processing recipes for both documentation and code to prepare dataset in different formats. The proposed methods of data preparation for document datasets are forming paragraph chunks, forming question and answer pairs and forming keyword and paragraph chunk pairs. For code dataset we propose forming summary and function pairs. Further, we qualitatively evaluate the results of the models for domain specific queries. Finally, we also propose practical guidelines and recommendations for fine tuning LLMs.",
    "pdf_link": "https://arxiv.org/abs/2404.10779",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.10779v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10779/text_finetune.jpg"
      },
      {
        "url": "https://arxiv.org/html/2404.10779v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10779/quantlatency.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10779v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10779/Llama2_7Bchat_modes.png"
      },
      {
        "url": "https://arxiv.org/html/2404.10779v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.10779/Llama2_13Bchat_modes.png"
      }
    ],
    "abstract_cn": "企业对大型语言模型（LLM）的精准调校有着迫切需求，目的是让这些模型能够掌握专有的领域知识。这一挑战的核心在于如何以最经济的资源、最低的成本以及最短的时间内，将特定领域的知识融入LLM。目前，众多企业采用RAG（检索增强生成）技术，该技术避免了对LLM的直接调校，但其性能受限于向量数据库的质量和检索系统的效能，而非LLM自身的潜力。在本项研究中，我们专注于对一个开源的LLM——LLaMA进行调校，利用企业存储库中的专有文档和代码，并通过调校后的模型来评估响应质量。此外，我们还旨在为初学者提供指导，帮助他们了解如何针对文档和代码对LLM进行调校，包括合理预估所需的GPU规模和数据格式化的可选方案。我们还为文档和代码数据集提出了数据预处理的方法，包括构建段落块、问答对、关键词与段落块对，以及为代码数据集构建摘要与功能对。我们进一步对模型在特定领域查询上的表现进行了定性评估，并最终提出了微调LLM的实用指南和建议。",
    "title_cn": "企业级大型语言模型微调：实用指南与建议。",
    "tags": [
      "分类：RAG",
      "企业知识管理",
      ""
    ]
  },
  {
    "title": "EduAgent: Generative Student Agents in Learning",
    "submit_datetime": "2024年03月23日",
    "abstract": "Student simulation in online education is important to address dynamic learning behaviors of students with diverse backgrounds. Existing simulation models based on deep learning usually need massive training data, lacking prior knowledge in educational contexts. Large language models (LLMs) may contain such prior knowledge since they are pre-trained from a large corpus. However, because student behaviors are dynamic and multifaceted with individual differences, directly prompting LLMs is not robust nor accurate enough to capture fine-grained interactions among diverse student personas, learning behaviors, and learning outcomes. This work tackles this problem by presenting a newly annotated fine-grained large-scale dataset and proposing EduAgent, a novel generative agent framework incorporating cognitive prior knowledge (i.e., theoretical findings revealed in cognitive science) to guide LLMs to first reason correlations among various behaviors and then make simulations. Our two experiments show that EduAgent could not only mimic and predict learning behaviors of real students but also generate realistic learning behaviors of virtual students without real data.",
    "pdf_link": "https://arxiv.org/abs/2404.07963",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/appendix_dataset2_distribution.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x5.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/appendix_gaze_simulation.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x6.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x7.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x8.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x9.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x10.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x11.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x12.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x13.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x14.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x15.png"
      },
      {
        "url": "https://arxiv.org/html/2404.07963v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2404.07963/x16.png"
      }
    ],
    "abstract_cn": "在网络教育领域，模拟学生行为对于理解和应对来自不同背景学生的多样化学习动态至关重要。尽管基于深度学习的模拟模型需依赖大量训练数据，它们在教育领域的先验知识方面往往不足。大型语言模型（LLMs）由于其庞大的预训练语料库，可能蕴含了丰富的教育相关先验知识。但是，学生行为的多样性和动态性要求我们不能简单地直接利用LLMs来进行模拟，因为这在捕捉学生个体差异、学习行为和成果之间的细微联系方面既不稳定也不够精确。本研究通过引入一个细粒度的大规模数据集，并设计了EduAgent——一个创新的生成代理框架，它融合了认知科学的理论发现作为认知先验知识，引导LLMs首先分析不同学习行为之间的关联，然后进行模拟。我们的两项实验证明，EduAgent不仅能够模拟和预测真实学生的行为模式，还能在没有实际数据的情况下生成虚拟学生的现实学习行为。",
    "title_cn": "EduAgent：在学习过程中应用的创造性学生代理",
    "tags": [
      "Agent",
      "网络教育",
      "人工智能"
    ]
  },
  {
    "title": "Content Knowledge Identification with Multi-Agent Large Language Models (LLMs)",
    "submit_datetime": "2024年03月21日",
    "abstract": "Teachers' mathematical content knowledge (CK) is of vital importance and need in teacher professional development (PD) programs. Computer-aided asynchronous PD systems are the most recent proposed PD techniques, which aim to help teachers improve their PD equally with fewer concerns about costs and limitations of time or location. However, current automatic CK identification methods, which serve as one of the core techniques of asynchronous PD systems, face challenges such as diversity of user responses, scarcity of high-quality annotated data, and low interpretability of the predictions. To tackle these challenges, we propose a Multi-Agent LLMs-based framework, LLMAgent-CK, to assess the user responses' coverage of identified CK learning goals without human annotations. By taking advantage of multi-agent LLMs in strong generalization ability and human-like discussions, our proposed LLMAgent-CK presents promising CK identifying performance on a real-world mathematical CK dataset MaCKT. Moreover, our case studies further demonstrate the working of the multi-agent framework.",
    "pdf_link": "https://arxiv.org/abs/2404.07960",
    "graphs": [],
    "abstract_cn": "数学内容知识（CK）对教师的专业成长至关重要，而计算机辅助的异步专业发展（PD）系统，作为最新的PD技术，旨在帮助教师在成本和时间、地点限制上减少顾虑，提升PD效果。尽管如此，现有的自动CK识别技术——异步PD系统的核心组成部分——仍面临着用户反馈多样性、高质量标注数据匮乏以及预测结果难以解释等问题。为解决这些问题，我们设计了一个基于多代理大型语言模型（LLM）的框架——LLMAgent-CK，它能够在无需人工注释的情况下，评估用户反馈对既定CK学习目标的覆盖度。利用多代理LLM的强大泛化能力和类人讨论特点，LLMAgent-CK在真实数学CK数据集MaCKT上显示出了出色的识别性能。此外，案例研究进一步证实了多代理框架的有效性。",
    "title_cn": "通过多代理大型语言模型（LLMs）实现内容知识的精准识别。",
    "tags": [
      "Agent",
      "",
      "人工智能"
    ]
  },
  {
    "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
    "submit_datetime": "2024年02月26日",
    "abstract": "To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.",
    "pdf_link": "https://arxiv.org/abs/2401.17464",
    "graphs": [
      {
        "url": "https://arxiv.org/html/2401.17464v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2401.17464/x1.png"
      },
      {
        "url": "https://arxiv.org/html/2401.17464v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2401.17464/x2.png"
      },
      {
        "url": "https://arxiv.org/html/2401.17464v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2401.17464/x3.png"
      },
      {
        "url": "https://arxiv.org/html/2401.17464v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2401.17464/x4.png"
      },
      {
        "url": "https://arxiv.org/html/2401.17464v1",
        "path": "/opt/data/Projects/HuggingArxiv/paper_images/2401.17464/x5.png"
      }
    ],
    "abstract_cn": "要实现符合人类预期的精准推理，大型语言模型（LLMs）必须将其推理植根于现实世界的知识体系中，如网络信息、数学和物理定律等。尽管现有工具能够帮助LLMs访问这些外部知识库，但在多步推理问题中，如何精准调整LLM代理（如Toolformer）以有效调用工具，仍是一个挑战，这需要对工具调用进行全局和高效的规划。在本研究中，我们提出了一种创新方法，旨在帮助LLMs在多步推理过程中更高效地利用工具。我们的方法称为抽象链（Chain-of-Abstraction，CoA），它训练LLMs首先识别出带有抽象占位符的推理链条，随后调用专业工具来填充具体知识，从而实现每个推理链条。这种基于抽象链条的规划方法不仅使LLMs能够掌握更为通用的推理策略，增强了对知识领域变化（如数学结果）的适应性，还能让LLMs同时进行解码和调用外部工具的操作，有效避免了因等待工具响应而产生的推理延迟。在数学推理和Wiki问答（Wiki QA）领域，我们的实验结果表明，相较于传统的链式思考和工具辅助方法，CoA方法在处理分布内和分布外测试集时，平均QA准确度提升了约6%，并且训练出的LLM代理在使用工具时更为高效，推理速度平均快了约1.4倍。",
    "title_cn": "通过抽象层次链推理实现高效工具运用",
    "tags": [
      "分类：LLM应用",
      "人工智能",
      ""
    ]
  },
  {
    "title": "HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring Unconstrained Photo Collections",
    "submit_datetime": "2024年02月14日",
    "abstract": "Internet image collections containing photos captured by crowds of photographers show promise for enabling digital exploration of large-scale tourist landmarks. However, prior works focus primarily on geometric reconstruction and visualization, neglecting the key role of language in providing a semantic interface for navigation and fine-grained understanding. In constrained 3D domains, recent methods have leveraged vision-and-language models as a strong prior of 2D visual semantics. While these models display an excellent understanding of broad visual semantics, they struggle with unconstrained photo collections depicting such tourist landmarks, as they lack expert knowledge of the architectural domain. In this work, we present a localization system that connects neural representations of scenes depicting large-scale landmarks with text describing a semantic region within the scene, by harnessing the power of SOTA vision-and-language models with adaptations for understanding landmark scene semantics. To bolster such models with fine-grained knowledge, we leverage large-scale Internet data containing images of similar landmarks along with weakly-related textual information. Our approach is built upon the premise that images physically grounded in space can provide a powerful supervision signal for localizing new concepts, whose semantics may be unlocked from Internet textual metadata with large language models. We use correspondences between views of scenes to bootstrap spatial understanding of these semantics, providing guidance for 3D-compatible segmentation that ultimately lifts to a volumetric scene representation. Our results show that HaLo-NeRF can accurately localize a variety of semantic concepts related to architectural landmarks, surpassing the results of other 3D models as well as strong 2D segmentation baselines. Our project page is at https://tau-vailab.github.io/HaLo-NeRF/.",
    "pdf_link": "https://arxiv.org/abs/2404.16845",
    "graphs": [],
    "abstract_cn": "互联网上的图片集，由众多摄影师捕捉的照片组成，为数字化探索大型旅游地标提供了可能性。但以往研究多集中于几何重建与可视化，忽略了语言在导航和精细理解中的语义界面作用。在3D领域的现有方法中，视觉与语言模型被用作2D视觉语义的强大预设，尽管这些模型对广泛的视觉语义有深刻理解，却难以应对无约束的照片集合，尤其是那些展示旅游地标的，因为它们缺少建筑学的专业知识。本研究提出了一种定位系统，该系统结合了最先进的视觉与语言模型，并对其进行了调整，以理解大型地标场景的语义，将场景的神经表示与描述场景内语义区域的文本相连接。我们利用大规模互联网数据，其中包含相似地标的图像和弱相关的文本信息，以此来丰富模型的精细知识。我们的方法基于一个前提：物理空间中的图像可以为新概念的定位提供强有力的监督信号，而这些概念的语义可能通过大型语言模型从互联网文本元数据中获得。我们利用场景视图之间的对应关系来引导空间语义的理解，并为3D兼容的分割提供指导，最终形成体积场景表示。我们的研究结果表明，HaLo-NeRF能够精确地定位与建筑地标相关的多种语义概念，超越了其他3D模型和2D分割基线的结果。项目详情可访问我们的网页 https://tau-vailab.github.io/HaLo-NeRF/。",
    "title_cn": "HaLo-NeRF：探索自由式照片集的几何引导语义学习。",
    "tags": [
      "分类：LLM应用\n\n这篇论文的摘要描述了一种结合了视觉与语言模型的定位系统，该系统利用互联网上的图像和文本信息来理解和导航大型旅游地标。虽然它涉及到了视觉和语言模型的应用，但主要关注的是如何利用大型语言模型（LLM）从互联网文本元数据中获取语义信息，以增强对场景的理解和导航。因此，这篇论文最符合\"LLM应用\"这一分类。",
      "",
      "3D建模"
    ]
  },
  {
    "title": "Machine Unlearning in Large Language Models",
    "submit_datetime": "2024年02月03日",
    "abstract": "Recently, large language models (LLMs) have emerged as a notable field, attracting significant attention for its ability to automatically generate intelligent contents for various application domains. However, LLMs still suffer from significant security and privacy issues. For example, LLMs might expose user privacy from hacking attacks or targeted prompts. To address this problem, this paper introduces a novel machine unlearning framework into LLMs. Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance. Experimental results show that our approach effectively meets unlearning objectives without substantially compromising model performance.",
    "pdf_link": "https://arxiv.org/abs/2404.16841",
    "graphs": [],
    "abstract_cn": "近期，大型语言模型（LLMs）作为新兴领域崭露头角，因其自动生成多样化应用领域智能内容的能力而备受瞩目。尽管如此，LLMs在安全与隐私方面仍存在不少挑战，如可能因黑客攻击或特定提示而泄露用户隐私。为应对这一问题，本研究提出了一种创新的机器退训练框架，旨在提升LLMs的安全性。我们的目标是确保LLMs不会生成有害、产生幻觉或侵犯隐私的响应，同时维持其标准的输出功能。通过采用评估模型精准识别需要退训练的对话，并引入距离损失作为负面损失，引导模型避免不良输出。同时，我们计算预期输出的簇均值，构建正面损失，以促进模型输出朝着更优结果发展，同时保证其推理和性能不受影响。实验结果显示，该方法在不牺牲模型性能的前提下，有效实现了退训练的目标。",
    "title_cn": "机器忘却在大型语言模型领域备受关注，这一话题在学术界和工业界都引起了广泛的讨论。",
    "tags": [
      "LLM应用",
      "人工智能安全",
      "隐私保护"
    ]
  },
  {
    "title": "Turning Machines: a simple algorithmic model for molecular robotics",
    "submit_datetime": "2022年01月24日",
    "abstract": "Molecular robotics is challenging, so it seems best to keep it simple. We consider an abstract molecular robotics model based on simple folding instructions that execute asynchronously. Turning Machines are a simple 1D to 2D folding model, also easily generalisable to 2D to 3D folding. A Turning Machine starts out as a line of connected monomers in the discrete plane, each with an associated turning number. A monomer turns relative to its neighbours, executing a unit-distance translation that drags other monomers along with it, and through collective motion the initial set of monomers eventually folds into a programmed shape. We provide a suite of tools for reasoning about Turning Machines by fully characterising their ability to execute line rotations: executing an almost-full line rotation of $5π/3$ radians is possible, yet a full $2π$ rotation is impossible. Furthermore, line rotations up to $5π/3$ are executed efficiently, in $O(\\log n)$ expected time in our continuous time Markov chain time model. We then show that such line-rotations represent a fundamental primitive in the model, by using them to efficiently and asynchronously fold shapes. In particular, arbitrarily large zig-zag-rastered squares and zig-zag paths are foldable, as are $y$-monotone shapes albeit with error (bounded by perimeter length). Finally, we give shapes that despite having paths that traverse all their points, are in fact impossible to fold, as well as techniques for folding certain classes of (scaled) shapes without error. Our approach relies on careful geometric-based analyses of the feats possible and impossible by a very simple robotic system, and pushes conceptional hardness towards mathematical analysis and away from molecular implementation.",
    "pdf_link": "https://arxiv.org/abs/2009.00755",
    "graphs": [],
    "abstract_cn": "分子机器人技术充满挑战，简化处理显得更为明智。我们提出了一个基于简易折叠指令的分子机器人抽象模型，这些指令能够异步运作。转向机模型以其简洁性著称，它从一维线性结构向二维形态转变，且易于扩展至三维。该模型以离散平面上的单体链起始，每个单体都配有特定的转向编号。单体相对于相邻单体进行旋转，带动其他单体同步移动，通过协同作用，原始的单体链最终折叠成预定的形态。我们开发了一系列工具，全面分析了转向机执行线旋转的能力：实现接近完整的 $5π/3$ 弧度旋转是可行的，但完整的 $2π$ 旋转则不可行。更重要的是，这些线旋转操作在连续时间马尔可夫链模型中，以 $O(\\log n)$ 的期望时间高效完成。进一步地，我们证明了这些线旋转是模型中的基础操作，利用它们可以高效、异步地折叠出各种形状。例如，任意大小的之字形栅格正方形和之字形路径均可实现折叠，而 $y$-单调形状虽有误差（受限于周长）也能完成折叠。最后，我们展示了一些尽管路径遍历所有点但实际无法折叠的形状，并提供了一些无误差折叠特定类别（缩放版）形状的技术。本研究的方法论基于对极简机器人系统可能与不可能完成的任务进行精细的几何分析，将概念上的复杂性转化为数学分析，而非分子层面的实现。",
    "title_cn": "转机：一种用于分子机器人领域的简洁算法模型",
    "tags": [
      "分类：Agent",
      "分子机器人技术",
      "计算机科学"
    ]
  }
]