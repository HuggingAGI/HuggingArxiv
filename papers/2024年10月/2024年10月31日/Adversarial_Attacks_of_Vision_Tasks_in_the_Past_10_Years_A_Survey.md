# 过去 10 年中视觉任务的对抗性攻击：一项调研

发布时间：2024年10月31日

`LLM应用` `机器学习`

> Adversarial Attacks of Vision Tasks in the Past 10 Years: A Survey

# 摘要

> 对抗性攻击，通过操纵输入数据来损害模型的可用性与完整性，在机器学习推理过程中带来了重大安全威胁。随着大型视觉语言模型（LVLMs）的问世，诸如认知偏差、提示注入和越狱技术等新的攻击手段也应运而生。理解这些攻击对于构建更强大的系统以及揭开神经网络的内在运作机制至关重要。然而，现有的综述通常聚焦于攻击分类，缺少全面深入的剖析。当前研究领域需要：1）对对抗性、可转移性和泛化性形成统一认识；2）对现有方法展开详尽评估；3）基于动机的攻击分类；4）对传统和 LVLMs 攻击持有综合视角。本文针对这些不足，对传统和 LVLMs 对抗性攻击予以全面总结，着重强调它们的关联与差异，并为未来研究提供了切实可行的见解。

> Adversarial attacks, which manipulate input data to undermine model availability and integrity, pose significant security threats during machine learning inference. With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such as cognitive bias, prompt injection, and jailbreak techniques, have emerged. Understanding these attacks is crucial for developing more robust systems and demystifying the inner workings of neural networks. However, existing reviews often focus on attack classifications and lack comprehensive, in-depth analysis. The research community currently needs: 1) unified insights into adversariality, transferability, and generalization; 2) detailed evaluations of existing methods; 3) motivation-driven attack categorizations; and 4) an integrated perspective on both traditional and LVLM attacks. This article addresses these gaps by offering a thorough summary of traditional and LVLM adversarial attacks, emphasizing their connections and distinctions, and providing actionable insights for future research.

[Arxiv](https://arxiv.org/abs/2410.23687)