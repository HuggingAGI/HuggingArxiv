# DetectRL：针对现实场景中大型语言模型生成的文本检测展开基准测试

发布时间：2024年10月31日

`LLM应用` `模型检测`

> DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios

# 摘要

> 检测大型语言模型（LLMs）生成的文本，近来备受关注。诸如DetectGPT之类的零样本方法，让检测能力达到了出色的水平。然而，现有检测器在实际应用中的可靠性尚未得到充分探究。在本研究中，我们推出了新基准DetectRL，指出即便是最先进（SOTA）的检测技术，在此任务中仍表现欠佳。我们从LLMs易被滥用的领域收集了人工编写的数据集。利用流行的LLMs，生成了更契合实际应用的数据。与以往研究不同，我们运用启发式规则创建对抗性的LLM生成文本，模拟高级提示用法、诸如单词替换的人工修订和写作错误。我们开发的DetectRL揭示了当前SOTA检测器的长处与局限。更为重要的是，我们剖析了写作风格、模型类型、攻击方法、文本长度以及现实中人工写作因素对不同类型检测器的潜在影响。我们认为DetectRL能够成为评估实际场景中检测器的有效基准，伴随先进攻击方法不断演进，从而提供更具挑战性的评估，推动更高效检测器的开发。数据和代码可在以下网址公开获取：https://github.com/NLP2CT/DetectRL。

> Detecting text generated by large language models (LLMs) is of great recent interest. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world applications remains underexplored. In this study, we present a new benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task. We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating advanced prompt usages, human revisions like word substitutions, and writing errors. Our development of DetectRL reveals the strengths and limitations of current SOTA detectors. More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe DetectRL could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors. Data and code are publicly available at: https://github.com/NLP2CT/DetectRL.

[Arxiv](https://arxiv.org/abs/2410.23746)