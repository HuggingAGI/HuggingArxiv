# 约束回译提升了大型语言模型遵循复杂指令的能力。

发布时间：2024年10月31日

`LLM应用` `人工智能`

> Constraint Back-translation Improves Complex Instruction Following of Large Language Models

# 摘要

> 大型语言模型（LLMs）在遵循格式、长度等复杂约束的指令时颇为吃力。遵循传统的指令调整做法，以往的研究将复杂指令输入先进的LLMs以生成复杂的指令-响应对，并对其开展后训练。然而，即便先进的LLMs也难以良好地遵循复杂指令，这就限制了生成数据的质量。在本研究中，我们发现现有数据集固有地包含隐性的复杂约束，并提出了一种新颖的数据生成技术——约束回译。具体而言，我们选取现有数据集中的高质量指令-响应对，仅让先进的LLMs把响应已满足的复杂约束添加到指令中，这自然降低了成本和数据噪声。在实验中，我们使用Llama3-70B-Instruct进行约束回译，创建了一个名为CRAB的高质量复杂指令-响应数据集。我们发现，在CRAB上进行后训练提升了多个骨干LLMs遵循复杂指令的能力，这在众多指令遵循基准测试中得到了评估。我们还进一步发现，约束回译在后训练中也能充当有用的辅助训练目标。我们的代码、数据和模型将会发布，以助力未来的研究。

> Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.

[Arxiv](https://arxiv.org/abs/2410.24175)