# 社会科学遇上大型语言模型：大型语言模型在社会模拟中究竟有多可靠？

发布时间：2024年10月30日

`LLM应用` `计算社会科学` `角色扮演代理`

> Social Science Meets LLMs: How Reliable Are Large Language Models in Social Simulations?

# 摘要

> 大型语言模型（LLMs）在模拟方面的应用日益广泛，于角色扮演代理和计算社会科学（CSS）领域均有落地。但这些模拟的可靠性尚未得到充分挖掘，致使人们对LLMs在这些应用中的可信度产生担忧。本文旨在解答“基于LLM的模拟究竟有多可靠？”为解决此问题，我们引入TrustSim这一涵盖10个CSS相关主题的评估数据集，对LLM模拟的可靠性展开系统研究。我们针对14个LLMs进行实验，发现基于LLM的模拟角色存在不一致的情况。而且，LLMs的一致性水平与其综合性能并无强关联。为增强LLMs在模拟中的可靠性，我们提出基于自适应学习率的ORPO（AdaORPO），这是一种基于强化学习的算法，能提升7个LLMs在模拟中的可靠性。我们的研究为未来探索更强大、更可信的基于LLM的模拟奠定了基础。

> Large Language Models (LLMs) are increasingly employed for simulations, enabling applications in role-playing agents and Computational Social Science (CSS). However, the reliability of these simulations is under-explored, which raises concerns about the trustworthiness of LLMs in these applications. In this paper, we aim to answer ``How reliable is LLM-based simulation?'' To address this, we introduce TrustSim, an evaluation dataset covering 10 CSS-related topics, to systematically investigate the reliability of the LLM simulation. We conducted experiments on 14 LLMs and found that inconsistencies persist in the LLM-based simulated roles. In addition, the consistency level of LLMs does not strongly correlate with their general performance. To enhance the reliability of LLMs in simulation, we proposed Adaptive Learning Rate Based ORPO (AdaORPO), a reinforcement learning-based algorithm to improve the reliability in simulation across 7 LLMs. Our research provides a foundation for future studies to explore more robust and trustworthy LLM-based simulations.

[Arxiv](https://arxiv.org/abs/2410.23426)