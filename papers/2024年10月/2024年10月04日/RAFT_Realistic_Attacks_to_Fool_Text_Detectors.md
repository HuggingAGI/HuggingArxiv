# RAFT：针对文本检测器的现实攻击策略

发布时间：2024年10月04日

`LLM应用` `网络安全` `人工智能`

> RAFT: Realistic Attacks to Fool Text Detectors

# 摘要

> 大型语言模型 (LLM) 在多任务中表现出色，但其不道德应用如传播虚假信息，引发了广泛担忧。尽管已有多种 LLM 检测方法，但其稳健性仍不明朗。本文介绍的 RAFT 是一种无语法错误、针对现有 LLM 检测器的黑盒攻击。与以往攻击不同，RAFT 利用 LLM 词嵌入的可转移性，同时保持文本质量。通过辅助嵌入，RAFT 贪婪选择词项干扰目标检测器。实验显示，RAFT 攻击在多领域内有效破坏检测器能力高达 99%，且可跨模型转移。人工评估表明，RAFT 生成的文本与人类书写无异。此外，RAFT 示例可用于训练更鲁棒的检测器。研究揭示，当前 LLM 检测器缺乏对抗鲁棒性，凸显了开发更强检测机制的紧迫性。

> Large language models (LLMs) have exhibited remarkable fluency across various tasks. However, their unethical applications, such as disseminating disinformation, have become a growing concern. Although recent works have proposed a number of LLM detection methods, their robustness and reliability remain unclear. In this paper, we present RAFT: a grammar error-free black-box attack against existing LLM detectors. In contrast to previous attacks for language models, our method exploits the transferability of LLM embeddings at the word-level while preserving the original text quality. We leverage an auxiliary embedding to greedily select candidate words to perturb against the target detector. Experiments reveal that our attack effectively compromises all detectors in the study across various domains by up to 99%, and are transferable across source models. Manual human evaluation studies show our attacks are realistic and indistinguishable from original human-written text. We also show that examples generated by RAFT can be used to train adversarially robust detectors. Our work shows that current LLM detectors are not adversarially robust, underscoring the urgent need for more resilient detection mechanisms.

[Arxiv](https://arxiv.org/abs/2410.03658)