# 探讨毒性分类器与大型语言模型在面对残疾歧视时的反应

发布时间：2024年10月04日

`LLM应用` `社交媒体` `残疾人`

> How Toxicity Classifiers and Large Language Models Respond to Ableism

# 摘要

> 残疾人常在网上遭遇仇视和微攻击。尽管在线平台使用机器学习模型来监管有害内容，但关于这些模型如何处理仇视的研究却寥寥无几。我们收集了 100 条针对残疾人的社交媒体评论，并邀请 160 名参与者评估其毒性和仇视程度。随后，我们让最先进的毒性分类器和大型语言模型进行同样的评估。结果显示，机器模型的毒性评分远低于残疾人，但在仇视评分上与残疾人相当。不过，机器模型在解释仇视时忽略了情感伤害，且缺乏对上下文的深入理解。未来，我们需克服设计残疾意识毒性分类器的难题，并推动从仇视检测向仇视解释的转变。

> People with disabilities (PwD) regularly encounter ableist hate and microaggressions online. While online platforms use machine learning models to moderate online harm, there is little research investigating how these models interact with ableism. In this paper, we curated a dataset of 100 social media comments targeted towards PwD, and recruited 160 participants to rate and explain how toxic and ableist these comments were. We then prompted state-of-the art toxicity classifiers (TCs) and large language models (LLMs) to rate and explain the harm. Our analysis revealed that TCs and LLMs rated toxicity significantly lower than PwD, but LLMs rated ableism generally on par with PwD. However, ableism explanations by LLMs overlooked emotional harm, and lacked specificity and acknowledgement of context, important facets of PwD explanations. Going forward, we discuss challenges in designing disability-aware toxicity classifiers, and advocate for the shift from ableism detection to ableism interpretation and explanation.

[Arxiv](https://arxiv.org/abs/2410.03448)