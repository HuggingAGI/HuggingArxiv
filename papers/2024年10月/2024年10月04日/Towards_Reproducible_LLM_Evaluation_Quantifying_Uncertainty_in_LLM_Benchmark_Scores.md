# 迈向可重复的 LLM 评估：量化基准分数中的不确定性

发布时间：2024年10月04日

`LLM理论` `人工智能`

> Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM Benchmark Scores

# 摘要

> LLM 具有随机性，即使将温度设为零并固定随机种子，也并非所有模型都能给出确定性答案。由于重复实验的时间和成本，很少有研究尝试量化这种不确定性。我们采用测试 LLM 对基本方向推理能力的基准，探讨了实验重复对平均分数和预测区间的影响。为此，我们提出了一种简单且经济的方法来量化基准分数的不确定性，并就如何进行可重复的 LLM 评估提出了建议。

> Large language models (LLMs) are stochastic, and not all models give deterministic answers, even when setting temperature to zero with a fixed random seed. However, few benchmark studies attempt to quantify uncertainty, partly due to the time and cost of repeated experiments. We use benchmarks designed for testing LLMs' capacity to reason about cardinal directions to explore the impact of experimental repeats on mean score and prediction interval. We suggest a simple method for cost-effectively quantifying the uncertainty of a benchmark score and make recommendations concerning reproducible LLM evaluation.

[Arxiv](https://arxiv.org/abs/2410.03492)