# Mentor-KD：让小型语言模型更擅长多步推理

发布时间：2024年10月11日

`LLM理论` `人工智能`

> Mentor-KD: Making Small Language Models Better Multi-step Reasoners

# 摘要

> 大型语言模型 (LLM) 通过 Chain-of-Thought (CoT) 提示在复杂任务中表现出色。最近的研究提出了推理蒸馏方法，通过微调多步推理语言模型来转移 LLM 的推理能力。然而，现有方法未充分考虑数据质量和软标签提供的挑战。为此，我们提出了 Mentor-KD，通过引入中间大小的任务特定微调模型作为导师，增强 CoT 注释并提供软标签，有效解决了这些问题。实验证明，Mentor-KD 在多种模型和复杂推理任务中表现优异。

> Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.

[Arxiv](https://arxiv.org/abs/2410.09037)