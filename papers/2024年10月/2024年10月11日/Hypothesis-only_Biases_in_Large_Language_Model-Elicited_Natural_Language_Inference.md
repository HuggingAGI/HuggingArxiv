# 大型语言模型在自然语言推理中仅依赖假设的偏差

发布时间：2024年10月11日

`LLM应用` `人工智能`

> Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference

# 摘要

> 我们探讨了用LLMs替代众包工人编写NLI假设是否会产生类似注释伪影。通过使用GPT-4、Llama-2和Mistral 7b重构斯坦福NLI语料库，并训练仅假设分类器，我们发现LLM生成的NLI数据集上，BERT分类器准确率高达86-96%，暗示存在仅假设伪影。此外，LLM生成的假设中常见“泄露”现象，如GPT-4生成的矛盾中频繁出现“在游泳池中游泳”。这表明，NLI中已知的偏见在LLM生成的数据中依然存在。

> We test whether replacing crowdsource workers with LLMs to write Natural Language Inference (NLI) hypotheses similarly results in annotation artifacts. We recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and Mistral 7b, and train hypothesis-only classifiers to determine whether LLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI datasets, BERT-based hypothesis-only classifiers achieve between 86-96% accuracy, indicating these datasets contain hypothesis-only artifacts. We also find frequent "give-aways" in LLM-generated hypotheses, e.g. the phrase "swimming in a pool" appears in more than 10,000 contradictions generated by GPT-4. Our analysis provides empirical evidence that well-attested biases in NLI can persist in LLM-generated data.

[Arxiv](https://arxiv.org/abs/2410.08996)