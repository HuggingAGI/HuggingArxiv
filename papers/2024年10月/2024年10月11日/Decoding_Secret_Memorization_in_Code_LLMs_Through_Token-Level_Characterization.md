# 揭秘代码 LLM 中的隐秘记忆：基于 Token 级别的特征分析

发布时间：2024年10月11日

`LLM应用` `软件开发` `网络安全`

> Decoding Secret Memorization in Code LLMs Through Token-Level Characterization

# 摘要

> 代码大型语言模型 (LLM) 在编程代码的生成、理解和操作方面表现出色，但其训练过程却无意中记忆了敏感信息，带来严重隐私风险。现有研究主要依赖提示工程技术，存在广泛幻觉和信息提取效率低下的问题。本文提出一种基于令牌概率的新方法，识别出四个关键特征，区分真实与虚假秘密。为克服现有局限，我们设计了 DESEC 两阶段方法，利用令牌级特征指导解码过程。实验证明，DESEC 在提高合理率和提取真实秘密方面表现优异。研究结果显示，我们的令牌级方法在评估代码 LLM 隐私泄露风险方面具有显著效果。

> Code Large Language Models (LLMs) have demonstrated remarkable capabilities in generating, understanding, and manipulating programming code. However, their training process inadvertently leads to the memorization of sensitive information, posing severe privacy risks. Existing studies on memorization in LLMs primarily rely on prompt engineering techniques, which suffer from limitations such as widespread hallucination and inefficient extraction of the target sensitive information. In this paper, we present a novel approach to characterize real and fake secrets generated by Code LLMs based on token probabilities. We identify four key characteristics that differentiate genuine secrets from hallucinated ones, providing insights into distinguishing real and fake secrets. To overcome the limitations of existing works, we propose DESEC, a two-stage method that leverages token-level features derived from the identified characteristics to guide the token decoding process. DESEC consists of constructing an offline token scoring model using a proxy Code LLM and employing the scoring model to guide the decoding process by reassigning token likelihoods. Through extensive experiments on four state-of-the-art Code LLMs using a diverse dataset, we demonstrate the superior performance of DESEC in achieving a higher plausible rate and extracting more real secrets compared to existing baselines. Our findings highlight the effectiveness of our token-level approach in enabling an extensive assessment of the privacy leakage risks associated with Code LLMs.

[Arxiv](https://arxiv.org/abs/2410.08858)