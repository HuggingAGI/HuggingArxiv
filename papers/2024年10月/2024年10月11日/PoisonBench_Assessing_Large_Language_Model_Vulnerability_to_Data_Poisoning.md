# PoisonBench：检测大型语言模型对数据中毒的抗性

发布时间：2024年10月11日

`LLM理论` `网络安全` `人工智能`

> PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning

# 摘要

> 偏好学习是调整大型语言模型的关键，但易受数据中毒攻击。为此，我们推出了PoisonBench，用于评估模型对数据中毒的敏感性。中毒攻击可操纵模型响应，隐藏恶意内容或偏见，导致有害输出。我们在八种场景中测试了21个模型，发现：（1）参数增加无法提升抗攻击能力；（2）攻击效果与中毒比例呈对数线性关系；（3）中毒影响可泛化至未中毒数据。这些发现揭示了当前技术的脆弱性，强调了加强防御的紧迫性。

> Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 21 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.

[Arxiv](https://arxiv.org/abs/2410.08811)