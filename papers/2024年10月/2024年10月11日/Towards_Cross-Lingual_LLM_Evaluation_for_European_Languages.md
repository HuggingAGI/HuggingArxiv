# 探索欧洲语言的跨语言 LLM 评估

发布时间：2024年10月11日

`LLM应用` `多语言技术`

> Towards Cross-Lingual LLM Evaluation for European Languages

# 摘要

> LLM 的崛起彻底改变了自然语言处理领域。然而，在多种欧洲语言中一致评估 LLM 性能仍具挑战，主要因多语言基准稀缺。我们为此设计了跨语言评估方法，利用五大常用基准的翻译版，评估了 40 个 LLM 在 21 种欧洲语言中的表现。我们的创新包括验证翻译基准的有效性，分析不同翻译服务的影响，并推出包含新数据集的多语言评估框架：EU20-MMLU、EU20-HellaSwag、EU20-ARC、EU20-TruthfulQA 和 EU20-GSM8K。所有基准和结果均已公开，旨在推动多语言 LLM 评估研究。

> The rise of Large Language Models (LLMs) has revolutionized natural language processing across numerous languages and tasks. However, evaluating LLM performance in a consistent and meaningful way across multiple European languages remains challenging, especially due to the scarcity of multilingual benchmarks. We introduce a cross-lingual evaluation approach tailored for European languages. We employ translated versions of five widely-used benchmarks to assess the capabilities of 40 LLMs across 21 European languages. Our contributions include examining the effectiveness of translated benchmarks, assessing the impact of different translation services, and offering a multilingual evaluation framework for LLMs that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly available to encourage further research in multilingual LLM evaluation.

[Arxiv](https://arxiv.org/abs/2410.08928)