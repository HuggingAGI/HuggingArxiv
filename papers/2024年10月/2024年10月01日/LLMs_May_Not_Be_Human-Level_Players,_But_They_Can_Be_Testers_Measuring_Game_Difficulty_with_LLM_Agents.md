# 虽然 LLM 无法达到人类玩家的水平，但它们可以作为游戏难度的测试工具。通过 LLM 代理，我们能够精准测量游戏的挑战性。

发布时间：2024年10月01日

`LLM应用` `人工智能`

> LLMs May Not Be Human-Level Players, But They Can Be Testers: Measuring Game Difficulty with LLM Agents

# 摘要

> 大型语言模型 (LLM) 的进步展示了它们在各种任务中的潜力，其中一个新兴应用是游戏领域。我们探讨了一个实际问题：LLM 能否评估游戏难度？为此，我们设计了一个基于 LLM 的游戏测试框架，并在 Wordle 和 Slay the Spire 这两款热门策略游戏中进行了测试。结果显示，尽管 LLM 的表现不及人类玩家，但在简单提示技术的引导下，其表现与人类感知的难度高度相关。这表明 LLM 在游戏开发中可作为有效的难度评估工具。此外，我们还总结了将 LLM 应用于游戏测试的原则和指南。

> Recent advances in Large Language Models (LLMs) have demonstrated their potential as autonomous agents across various tasks. One emerging application is the use of LLMs in playing games. In this work, we explore a practical problem for the gaming industry: Can LLMs be used to measure game difficulty? We propose a general game-testing framework using LLM agents and test it on two widely played strategy games: Wordle and Slay the Spire. Our results reveal an interesting finding: although LLMs may not perform as well as the average human player, their performance, when guided by simple, generic prompting techniques, shows a statistically significant and strong correlation with difficulty indicated by human players. This suggests that LLMs could serve as effective agents for measuring game difficulty during the development process. Based on our experiments, we also outline general principles and guidelines for incorporating LLMs into the game testing process.

[Arxiv](https://arxiv.org/abs/2410.02829)