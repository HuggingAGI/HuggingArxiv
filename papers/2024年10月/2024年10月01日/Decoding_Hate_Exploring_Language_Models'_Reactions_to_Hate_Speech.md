# 解码仇恨：探究语言模型如何应对仇恨言论

发布时间：2024年10月01日

`LLM应用` `社交媒体` `网络安全`

> Decoding Hate: Exploring Language Models' Reactions to Hate Speech

# 摘要

> 仇恨言论作为网络有害表达的一种，常以贬损性帖子形式出现，成为数字环境中的重大风险。随着大型语言模型（LLM）的普及，其基于大量未经筛选的互联网数据训练，可能助长仇恨言论的传播，引发担忧。因此，深入了解LLM如何应对仇恨言论，对其负责任应用至关重要。本文聚焦于七款顶尖LLM（LLaMA 2、Vicuna、LLaMA 3、Mistral、GPT-3.5、GPT-4及Gemini Pro）对仇恨言论的反应，通过定性分析揭示其应对能力的全貌。同时，探讨通过微调与指南防护等策略，降低LLM生成仇恨言论的风险。此外，还考察了这些模型对以政治正确措辞包装的仇恨言论的识别与应对。

> Hate speech is a harmful form of online expression, often manifesting as derogatory posts. It is a significant risk in digital environments. With the rise of Large Language Models (LLMs), there is concern about their potential to replicate hate speech patterns, given their training on vast amounts of unmoderated internet data. Understanding how LLMs respond to hate speech is crucial for their responsible deployment. However, the behaviour of LLMs towards hate speech has been limited compared. This paper investigates the reactions of seven state-of-the-art LLMs (LLaMA 2, Vicuna, LLaMA 3, Mistral, GPT-3.5, GPT-4, and Gemini Pro) to hate speech. Through qualitative analysis, we aim to reveal the spectrum of responses these models produce, highlighting their capacity to handle hate speech inputs. We also discuss strategies to mitigate hate speech generation by LLMs, particularly through fine-tuning and guideline guardrailing. Finally, we explore the models' responses to hate speech framed in politically correct language.

[Arxiv](https://arxiv.org/abs/2410.00775)