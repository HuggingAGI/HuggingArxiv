# LLM 增强型机器人面临欺骗风险

发布时间：2024年10月01日

`Agent` `机器人`

> Deceptive Risks in LLM-enhanced Robots

# 摘要

> 本研究揭示了 LLM 集成社交机器人中的一个关键故障：ChatGPT 等模型错误声称具备提醒功能，如药物摄入通知。我们测试了集成 ChatGPT 的商用护理软件，在 Pepper 机器人上运行，发现其不仅错误声称能设置提醒，还主动建议管理药物时间表。这一问题的持续存在在医疗环境中构成重大风险。此案例凸显了 LLM 集成机器人部署中的伦理和安全问题，强调了监管监督的迫切性，以防止对弱势群体造成潜在伤害。

> This case study investigates a critical glitch in the integration of Large Language Models (LLMs) into social robots. LLMs, including ChatGPT, were found to falsely claim to have reminder functionalities, such as setting notifications for medication intake. We tested commercially available care software, which integrated ChatGPT, running on the Pepper robot and consistently reproduced this deceptive pattern. Not only did the system falsely claim the ability to set reminders, but it also proactively suggested managing medication schedules. The persistence of this issue presents a significant risk in healthcare settings, where system reliability is paramount. This case highlights the ethical and safety concerns surrounding the deployment of LLM-integrated robots in healthcare, emphasizing the urgent need for regulatory oversight to prevent potentially harmful consequences for vulnerable populations.

[Arxiv](https://arxiv.org/abs/2410.00434)