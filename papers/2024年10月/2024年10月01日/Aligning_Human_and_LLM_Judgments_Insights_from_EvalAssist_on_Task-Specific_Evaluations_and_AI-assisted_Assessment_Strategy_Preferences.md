# 协调人类与 LLM 的判断：EvalAssist 在任务特定评估及 AI 辅助评估策略偏好方面的洞察

发布时间：2024年10月01日

`LLM应用` `机器学习` `用户体验`

> Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences

# 摘要

> 评估 LLM 输出时，用户需在多种配置中挑选最佳结果，这一过程既耗时又费力。为此，LLM 常被用作评估工具，协助过滤数据、评估模型或辅助人类评估。为提升评估效率，前端工具至关重要。我们研究了两种评估方法：直接评估与成对比较。通过对 15 名机器学习从业者的实验（每人完成 6 项任务，共 131 次评估），我们发现，通过使评估标准更贴合任务、调整判断和更换评估模型，用户在直接评估中表现更佳。基于此，我们提出了优化 LLM 辅助评估交互的建议。

> Evaluation of large language model (LLM) outputs requires users to make critical judgments about the best outputs across various configurations. This process is costly and takes time given the large amounts of data. LLMs are increasingly used as evaluators to filter training data, evaluate model performance or assist human evaluators with detailed assessments. To support this process, effective front-end tools are critical for evaluation. Two common approaches for using LLMs as evaluators are direct assessment and pairwise comparison. In our study with machine learning practitioners (n=15), each completing 6 tasks yielding 131 evaluations, we explore how task-related factors and assessment strategies influence criteria refinement and user perceptions. Findings show that users performed more evaluations with direct assessment by making criteria task-specific, modifying judgments, and changing the evaluator model. We conclude with recommendations for how systems can better support interactions in LLM-assisted evaluations.

[Arxiv](https://arxiv.org/abs/2410.00873)