# BabelBench：一个专为代码驱动、多模态与多结构化数据分析而设计的全能基准

发布时间：2024年10月01日

`LLM应用` `人工智能` `数据处理`

> BabelBench: An Omni Benchmark for Code-Driven Analysis of Multimodal and Multistructured Data

# 摘要

> 大型语言模型 (LLM) 在处理复杂数据类型方面日益重要，涵盖结构化数据处理（如 ChartQA 和 ChatGPT-Ada）和多模态非结构化数据处理（如视觉问答 VQA）。尽管这些领域备受关注，但缺乏统一的评估方法。为此，我们推出了 BabelBench，一个创新的基准框架，用于评估 LLM 在多模态多结构化数据处理和代码执行方面的能力。BabelBench 包含 247 个精心设计的问题，涵盖感知、常识推理、逻辑推理等任务，不仅测试基本能力，还要求高级的探索、规划、推理和调试技能。实验显示，即使是 ChatGPT 4 也有显著提升空间。我们的全面分析为未来研究提供了宝贵指导。基准数据可在 https://github.com/FFD8FFE/babelbench 获取。

> Large language models (LLMs) have become increasingly pivotal across various domains, especially in handling complex data types. This includes structured data processing, as exemplified by ChartQA and ChatGPT-Ada, and multimodal unstructured data processing as seen in Visual Question Answering (VQA). These areas have attracted significant attention from both industry and academia. Despite this, there remains a lack of unified evaluation methodologies for these diverse data handling scenarios. In response, we introduce BabelBench, an innovative benchmark framework that evaluates the proficiency of LLMs in managing multimodal multistructured data with code execution. BabelBench incorporates a dataset comprising 247 meticulously curated problems that challenge the models with tasks in perception, commonsense reasoning, logical reasoning, and so on. Besides the basic capabilities of multimodal understanding, structured data processing as well as code generation, these tasks demand advanced capabilities in exploration, planning, reasoning and debugging. Our experimental findings on BabelBench indicate that even cutting-edge models like ChatGPT 4 exhibit substantial room for improvement. The insights derived from our comprehensive analysis offer valuable guidance for future research within the community. The benchmark data can be found at https://github.com/FFD8FFE/babelbench.

[Arxiv](https://arxiv.org/abs/2410.00773)