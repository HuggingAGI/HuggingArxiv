# OCC-MLLM：助力多模态大型语言模型，提升对遮挡物体的理解能力

发布时间：2024年10月02日

`LLM应用` `计算机视觉` `人工智能`

> OCC-MLLM:Empowering Multimodal Large Language Model For the Understanding of Occluded Objects

# 摘要

> 现有的大规模视觉语言多模态模型在理解被遮挡物体方面存在不足。当前最先进的多模态模型在描述被遮挡物体时表现不佳，且缺乏包含大量被遮挡物体的图像-文本对数据集。为此，我们提出了一种新型多模态模型，采用新设计的视觉编码器来解析RGB图像中的被遮挡物体，并引入了一个大规模的视觉语言对数据集。我们的实验从与最先进模型的对比开始。

> There is a gap in the understanding of occluded objects in existing large-scale visual language multi-modal models. Current state-of-the-art multimodal models fail to provide satisfactory results in describing occluded objects for visual-language multimodal models through universal visual encoders. Another challenge is the limited number of datasets containing image-text pairs with a large number of occluded objects. Therefore, we introduce a novel multimodal model that applies a newly designed visual encoder to understand occluded objects in RGB images. We also introduce a large-scale visual-language pair dataset for training large-scale visual-language multimodal models and understanding occluded objects. We start our experiments comparing with the state-of-the-art models.

[Arxiv](https://arxiv.org/abs/2410.01261)