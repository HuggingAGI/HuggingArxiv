# 语言模型中的人性化尝试：随机同理心的语言印记

发布时间：2024年10月02日

`LLM应用` `人工智能` `社会科学`

> Trying to be human: Linguistic traces of stochastic empathy in language models

# 摘要

> 在现代社会，区分机器生成与人类创作的内容至关重要。大型语言模型（LLM）极大地提升了计算机生成内容的质量，以至于人类越来越难以分辨文本的来源。我们的研究聚焦于两个关键因素：同理心和模仿人类的动机，探讨它们在人机竞争中的作用。通过两项实验，我们让参与者（n=530）和最先进的 LLM 分别撰写关系建议或描述性文本（n=610），并指示他们尽可能展现人性化特征。随后，新的人类样本（n=428 和 n=408）对文本来源进行了判断。结果显示，在需要同理心的情境下，人类表现更佳。然而，出乎意料的是，模仿人类的指示仅对 LLM 有效，从而削弱了人类的优势。进一步的文本分析表明，LLM 之所以能更接近人类风格，是因为它们隐含地掌握了使文本显得人性化的要素，并能轻松应用这些技巧。模型通过采用对话式、自我参照的非正式语调和简化词汇，巧妙地模仿了随机同理心。这些发现为我们理解 LLM 的性能提供了新的视角，尤其是在与人类创作相媲美的背景下。

> Differentiating between generated and human-written content is important for navigating the modern world. Large language models (LLMs) are crucial drivers behind the increased quality of computer-generated content. Reportedly, humans find it increasingly difficult to identify whether an AI model generated a piece of text. Our work tests how two important factors contribute to the human vs AI race: empathy and an incentive to appear human. We address both aspects in two experiments: human participants and a state-of-the-art LLM wrote relationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610), either instructed to be as human as possible or not. New samples of humans (n=428 and n=408) then judged the texts' source. Our findings show that when empathy is required, humans excel. Contrary to expectations, instructions to appear human were only effective for the LLM, so the human advantage diminished. Computational text analysis revealed that LLMs become more human because they may have an implicit representation of what makes a text human and effortlessly apply these heuristics. The model resorts to a conversational, self-referential, informal tone with a simpler vocabulary to mimic stochastic empathy. We discuss these findings in light of recent claims on the on-par performance of LLMs.

[Arxiv](https://arxiv.org/abs/2410.01675)