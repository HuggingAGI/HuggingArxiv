# MedQA-CS：通过 AI-SCE 框架，评估大型语言模型在临床技能方面的表现。

发布时间：2024年10月02日

`LLM应用` `人工智能`

> MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an AI-SCE Framework

# 摘要

> 在医疗领域，AI 和 LLMs 需要高级临床技能，但现有评估标准未能全面覆盖。为此，我们推出了 MedQA-CS，一个受医学教育 OSCEs 启发的 AI-SCE 框架。MedQA-CS 通过模拟真实临床场景的任务，如“LLM 扮演医学生”和“LLM 担任临床技能考官”，来评估 LLMs。我们不仅开发了这一综合评估框架，还提供了公开数据和专家注释，以及 LLMs 在临床技能评估中的定量和定性分析。实验显示，MedQA-CS 比传统多项选择题更具挑战性，能更全面地评估 LLMs 的临床能力，无论是开源还是闭源模型。

> Artificial intelligence (AI) and large language models (LLMs) in healthcare require advanced clinical skills (CS), yet current benchmarks fail to evaluate these comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by medical education's Objective Structured Clinical Examinations (OSCEs), to address this gap. MedQA-CS evaluates LLMs through two instruction-following tasks, LLM-as-medical-student and LLM-as-CS-examiner, designed to reflect real clinical scenarios. Our contributions include developing MedQA-CS, a comprehensive evaluation framework with publicly available data and expert annotations, and providing the quantitative and qualitative assessment of LLMs as reliable judges in CS evaluation. Our experiments show that MedQA-CS is a more challenging benchmark for evaluating clinical skills than traditional multiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks, MedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities for both open- and closed-source LLMs.

[Arxiv](https://arxiv.org/abs/2410.01553)