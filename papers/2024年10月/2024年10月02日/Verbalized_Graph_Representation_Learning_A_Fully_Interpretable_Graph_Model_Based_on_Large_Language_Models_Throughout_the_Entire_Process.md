# 语言化图表示学习：基于大型语言模型的全过程完全可解释图模型

发布时间：2024年10月02日

`LLM应用` `人工智能` `图神经网络`

> Verbalized Graph Representation Learning: A Fully Interpretable Graph Model Based on Large Language Models Throughout the Entire Process

# 摘要

> 文本属性图 (TAG) 的表示学习因其实际应用广泛而备受关注，尤其是通过图神经网络 (GNN)。传统 GNN 方法主要编码图的结构信息，常使用浅层文本嵌入表示节点或边属性，这限制了模型对数据丰富语义的理解和复杂任务的推理能力，且缺乏可解释性。随着大型语言模型 (LLM) 的兴起，越来越多的研究将 LLM 与 GNN 结合，有效利用 TAG 数据集的语义信息，但这些方法仅部分可解释，限制了其在关键领域的应用。本文提出了一种完全可解释的图表示学习方法——口头化图表示学习 (VGRL)。与传统图机器学习模型不同，VGRL 将参数空间限制为文本描述，确保整个过程的完全可解释性，使用户更易理解和信任模型决策。我们通过多项研究实证评估 VGRL 的有效性，并认为这些方法可作为图表示学习的基石。

> Representation learning on text-attributed graphs (TAGs) has attracted significant interest due to its wide-ranging real-world applications, particularly through Graph Neural Networks (GNNs). Traditional GNN methods focus on encoding the structural information of graphs, often using shallow text embeddings for node or edge attributes. This limits the model to understand the rich semantic information in the data and its reasoning ability for complex downstream tasks, while also lacking interpretability. With the rise of large language models (LLMs), an increasing number of studies are combining them with GNNs for graph representation learning and downstream tasks. While these approaches effectively leverage the rich semantic information in TAGs datasets, their main drawback is that they are only partially interpretable, which limits their application in critical fields. In this paper, we propose a verbalized graph representation learning (VGRL) method which is fully interpretable. In contrast to traditional graph machine learning models, which are usually optimized within a continuous parameter space, VGRL constrains this parameter space to be text description which ensures complete interpretability throughout the entire process, making it easier for users to understand and trust the decisions of the model. We conduct several studies to empirically evaluate the effectiveness of VGRL and we believe these method can serve as a stepping stone in graph representation learning.

[Arxiv](https://arxiv.org/abs/2410.01457)