# 生成式奖励模型

发布时间：2024年10月02日

`LLM理论` `人工智能` `机器学习`

> Generative Reward Models

# 摘要

> 摘要：基于人类反馈的强化学习 (RLHF) 显著提升了现代大型语言模型 (LLM) 的性能。然而，RLHF 过程既耗资源又具技术挑战，通常需要大量的人类偏好标签来标注模型输出。基于 AI 反馈的强化学习 (RLAIF) 通过利用 LLM 生成的合成偏好来应对这一数据收集难题。但最新研究表明，合成偏好标签可能与人类判断不一致。为此，我们提出了一种结合 RLHF 和 RLAIF 的混合方法。我们引入了 GenRM，一种迭代算法，通过在自生成推理轨迹上训练 LLM，生成与人类判断相符的合成偏好标签。实证结果显示，零-shot LLM 判断在分布内任务上的表现不如 Bradley-Terry 奖励模型 (9-36%)。相比之下，GenRM 在分布内任务上的准确性与 Bradley-Terry 模型相当，并在分布外任务上显著超越它们 (10-45%)。此外，GenRM 在分布内 (9-31%) 和分布外 (2-6%) 任务上的表现均优于直接使用 LLM 判断。我们的研究结果表明，结合 RLHF 和 RLAIF 的优势，为提升合成偏好标签的质量开辟了新的途径。

> 
Abstract:Reinforcement Learning from Human Feedback (RLHF) has greatly improved the performance of modern Large Language Models (LLMs). The RLHF process is resource-intensive and technically challenging, generally requiring a large collection of human preference labels over model-generated outputs. Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection challenge by leveraging synthetic preferences generated by an LLM. However, recent work has shown that synthetic preferences labels may not align well with human preference judgments. To address this, we propose a hybrid approach that unifies RLHF and RLAIF methodologies. We introduce GenRM, an iterative algorithm that trains an LLM on self-generated reasoning traces, leading to synthetic preference labels matching human preference judgments. Empirically, we show that zero-shot LLM-based judgments under-perform compared to Bradley-Terry reward models on in-distribution tasks (between 9-36%). In contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 2- 6%). Our results show that combining the strengths of RLHF and RLAIF offers a promising approach for improving the quality of synthetic preference labels.
    

[Arxiv](https://arxiv.org/pdf/2410.12832)