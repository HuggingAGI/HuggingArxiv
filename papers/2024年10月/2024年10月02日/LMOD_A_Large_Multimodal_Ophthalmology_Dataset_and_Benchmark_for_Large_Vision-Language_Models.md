# LMOD：一个专为大型视觉-语言模型设计的大型多模态眼科数据集及基准

发布时间：2024年10月02日

`LLM应用`

> LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large Vision-Language Models

# 摘要

> 眼科诊断和治疗计划高度依赖于图像分析。尽管大型视觉-语言模型（LVLMs）在理解复杂视觉信息方面表现出色，但在眼科图像上的应用仍需深入研究。为此，我们推出了 LMOD 数据集和基准，用于评估 LVLMs 在眼科图像上的表现，涵盖解剖学、诊断和人口统计学等多个方面。LMOD 包含 21,993 张图像，涉及多种眼科检查技术。通过测试 13 个最先进的 LVLMs，我们发现这些模型在理解眼科图像方面仍有很大提升空间，特别是在诊断分析和人口统计学提取上，同时也暴露了在空间推理、域外查询处理和生物标志物识别等方面的不足。

> Ophthalmology relies heavily on detailed image analysis for diagnosis and treatment planning. While large vision-language models (LVLMs) have shown promise in understanding complex visual information, their performance on ophthalmology images remains underexplored. We introduce LMOD, a dataset and benchmark for evaluating LVLMs on ophthalmology images, covering anatomical understanding, diagnostic analysis, and demographic extraction. LMODincludes 21,993 images spanning optical coherence tomography, scanning laser ophthalmoscopy, eye photos, surgical scenes, and color fundus photographs. We benchmark 13 state-of-the-art LVLMs and find that they are far from perfect for comprehending ophthalmology images. Models struggle with diagnostic analysis and demographic extraction, reveal weaknesses in spatial reasoning, diagnostic analysis, handling out-of-domain queries, and safeguards for handling biomarkers of ophthalmology images.

[Arxiv](https://arxiv.org/abs/2410.01620)