# Pixtral 12B

发布时间：2024年10月09日

`LLM应用` `人工智能` `计算机视觉`

> Pixtral 12B

# 摘要

> 摘要：我们推出了 Pixtral-12B，一个拥有 120 亿参数的多模态语言模型。Pixtral-12B 不仅在理解自然图像和文档方面表现卓越，还在多模态基准测试中超越了许多更大规模的模型。与许多开源模型不同，Pixtral 在文本处理上也表现出色，不因多模态任务而牺牲自然语言性能。其独特的视觉编码器允许以自然分辨率和宽高比处理图像，为用户提供了灵活的令牌使用方式。此外，Pixtral 能在 128K 令牌的长上下文窗口中处理任意数量的图像。Pixtral 12B 不仅在性能上超越了类似大小的开源模型，还以 7 倍小的体积优于 Llama-3.2 90B 等大型模型。我们还推出了开源基准测试 MM-MT-Bench，用于实际场景中的视觉语言模型评估，并提供了标准化评估协议的详细分析和代码。Pixtral-12B 已在 Apache 2.0 许可证下发布。

> 
Abstract:We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.
    

[Arxiv](https://arxiv.org/pdf/2410.07073)