# 在医疗领域的大型语言模型中，检测并消除偏见，提升诊断准确性。

发布时间：2024年10月09日

`LLM应用` `人工智能`

> Detecting Bias and Enhancing Diagnostic Accuracy in Large Language Models for Healthcare

# 摘要

> AI 生成的医疗建议若存在偏见和误诊，可能严重威胁患者安全，因此 AI 在医疗领域的可靠性变得尤为重要。随着大型语言模型 (LLM) 在医疗决策中的应用日益广泛，消除偏见、提高准确性成为确保安全护理的关键。本研究通过引入新资源，直接应对这些挑战，旨在推动医疗 AI 的道德与精确发展。我们推出了两个数据集：BiasMD 包含 6,007 个问答对，用于评估和减少健康相关 LLM 输出的偏见；DiseaseMatcher 则包含 32,000 个临床问答对，覆盖 700 种疾病，用于评估基于症状的诊断准确性。基于这些数据集，我们开发了 EthiClinician 模型，该模型在道德推理和临床判断方面均超越了 GPT-4。通过揭示并修正现有医疗模型中的隐性偏见，我们的研究为实现更安全、更可靠的医疗效果奠定了新标准。

> Biased AI-generated medical advice and misdiagnoses can jeopardize patient safety, making the integrity of AI in healthcare more critical than ever. As Large Language Models (LLMs) take on a growing role in medical decision-making, addressing their biases and enhancing their accuracy is key to delivering safe, reliable care. This study addresses these challenges head-on by introducing new resources designed to promote ethical and precise AI in healthcare. We present two datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate and mitigate biases in health-related LLM outputs, and DiseaseMatcher, with 32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing symptom-based diagnostic accuracy. Using these datasets, we developed the EthiClinician, a fine-tuned model built on the ChatDoctor framework, which outperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing and correcting hidden biases in existing models for healthcare, our work sets a new benchmark for safer, more reliable patient outcomes.

[Arxiv](https://arxiv.org/abs/2410.06566)