# TuringQ：理论计算中的 AI 理解力基准测试

发布时间：2024年10月09日

`LLM理论` `计算机科学` `人工智能`

> TuringQ: Benchmarking AI Comprehension in Theory of Computation

# 摘要

> 我们推出了 TuringQ，首个专注于评估 LLM 在计算理论中推理能力的基准。TuringQ 包含 4,006 道本科及研究生级别的问题，分为四个难度等级，覆盖七大核心理论领域。我们通过 Chain of Thought 提示和专家评估，测试了多个开源 LLM 及 GPT-4。此外，我们设计了一套基于 LLM 的自动化评估系统，其准确性与人工评估不相上下。在 TuringQ 上微调 Llama3-8B 模型，显著提升了其推理能力及代数等跨领域任务的表现。TuringQ 不仅是衡量 LLM 性能的标杆，更是提升其在复杂计算推理任务中表现的重要资源。我们的研究深入剖析了 LLM 的潜力，并揭示了 AI 在理论计算机科学理解上的进步。

> We present TuringQ, the first benchmark designed to evaluate the reasoning capabilities of large language models (LLMs) in the theory of computation. TuringQ consists of 4,006 undergraduate and graduate-level question-answer pairs, categorized into four difficulty levels and covering seven core theoretical areas. We evaluate several open-source LLMs, as well as GPT-4, using Chain of Thought prompting and expert human assessment. Additionally, we propose an automated LLM-based evaluation system that demonstrates competitive accuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on TuringQ shows measurable improvements in reasoning ability and out-of-domain tasks such as algebra. TuringQ serves as both a benchmark and a resource for enhancing LLM performance in complex computational reasoning tasks. Our analysis offers insights into LLM capabilities and advances in AI comprehension of theoretical computer science.

[Arxiv](https://arxiv.org/abs/2410.06547)