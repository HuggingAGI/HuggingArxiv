# PLaMo-100B：专为日语精通而生的全新语言模型

发布时间：2024年10月09日

`LLM应用` `人工智能` `语言模型`

> PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency

# 摘要

> 我们推出了 PLaMo-100B，一款专为日语设计的大型语言模型。该模型从零开始，训练数据量高达 2 万亿个标记，并采用了 QK 归一化和 Z-Loss 等技术，确保训练稳定。随后，通过监督微调和直接偏好优化等后训练技术，进一步提升了模型性能。基准测试显示，PLaMo-100B 在日语任务中表现出色，与 GPT-4 等顶尖模型不相上下。

> We introduce PLaMo-100B, a large-scale language model designed for Japanese proficiency. The model was trained from scratch using 2 trillion tokens, with architecture such as QK Normalization and Z-Loss to ensure training stability during the training process. Post-training techniques, including Supervised Fine-Tuning and Direct Preference Optimization, were applied to refine the model's performance. Benchmark evaluations suggest that PLaMo-100B performs well, particularly in Japanese-specific tasks, achieving results that are competitive with frontier models like GPT-4.

[Arxiv](https://arxiv.org/abs/2410.07563)