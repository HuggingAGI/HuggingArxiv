# 基于成对自我评估的理性答案验证

发布时间：2024年10月07日

`LLM应用` `人工智能`

> Rationale-Aware Answer Verification by Pairwise Self-Evaluation

# 摘要

> 答案验证旨在从 LLM 生成的候选方案中识别正确解决方案。传统方法仅基于最终答案是否匹配黄金答案来标记解决方案，忽略了推理过程中的缺陷，导致验证模型难以区分合理与有缺陷的推理。我们在 StrategyQA 中发现，仅 19% 的 LLM 生成方案具备有效推理，使得验证模型不可靠。通过在有效推理上训练，验证模型能更好地区分推理质量。为避免额外监督，我们提出 REPS，利用 LLM 自我评估迭代选择有效推理。实验表明，REPS 训练的验证模型在 ARC-Challenge、DROP 和 StrategyQA 上表现更优。这表明，确保推理有效性对训练可靠验证模型至关重要，尤其在辅助复杂推理任务时。

> Answer verification identifies correct solutions among candidates generated by large language models (LLMs). Current approaches typically train verifier models by labeling solutions as correct or incorrect based solely on whether the final answer matches the gold answer. However, this approach neglects any flawed rationale in the solution yielding the correct answer, undermining the verifier's ability to distinguish between sound and flawed rationales. We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier. Furthermore, we demonstrate that training a verifier on valid rationales significantly improves its ability to distinguish valid and flawed rationale. To make a better verifier without extra human supervision, we introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions. Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA). Our results suggest that training reliable verifiers requires ensuring the validity of rationales in addition to the correctness of the final answers, which would be critical for models assisting humans in solving complex reasoning tasks.

[Arxiv](https://arxiv.org/abs/2410.04838)