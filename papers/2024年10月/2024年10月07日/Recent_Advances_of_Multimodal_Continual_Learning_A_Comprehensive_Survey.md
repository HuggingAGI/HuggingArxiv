# 多模态持续学习领域迎来新突破，本文为你带来全面解读。

发布时间：2024年10月07日

`LLM理论` `人工智能` `机器学习`

> Recent Advances of Multimodal Continual Learning: A Comprehensive Survey

# 摘要

> 持续学习 (CL) 旨在让机器学习模型不断从新数据中学习，同时不忘旧知识。随着模型从简单到复杂，从单模态到多模态，多模态持续学习 (MMCL) 应运而生。MMCL 的挑战在于，简单堆叠单模态方法往往效果不佳。本文首次全面调查了 MMCL，介绍了其背景、设置及方法分类，包括基于正则化、架构、回放和提示的四类方法，并总结了关键创新。此外，我们还概述了开放数据集和基准，探讨了未来研究方向，并创建了 GitHub 仓库以索引相关论文和资源，网址为 https://github.com/LucyDYu/Awesome-Multimodal-Continual-Learning。

> Continual learning (CL) aims to empower machine learning models to learn continually from new data, while building upon previously acquired knowledge without forgetting. As machine learning models have evolved from small to large pre-trained architectures, and from supporting unimodal to multimodal data, multimodal continual learning (MMCL) methods have recently emerged. The primary challenge of MMCL is that it goes beyond a simple stacking of unimodal CL methods, as such straightforward approaches often yield unsatisfactory performance. In this work, we present the first comprehensive survey on MMCL. We provide essential background knowledge and MMCL settings, as well as a structured taxonomy of MMCL methods. We categorize existing MMCL methods into four categories, i.e., regularization-based, architecture-based, replay-based, and prompt-based methods, explaining their methodologies and highlighting their key innovations. Additionally, to prompt further research in this field, we summarize open MMCL datasets and benchmarks, and discuss several promising future directions for investigation and development. We have also created a GitHub repository for indexing relevant MMCL papers and open resources available at https://github.com/LucyDYu/Awesome-Multimodal-Continual-Learning.

[Arxiv](https://arxiv.org/abs/2410.05352)