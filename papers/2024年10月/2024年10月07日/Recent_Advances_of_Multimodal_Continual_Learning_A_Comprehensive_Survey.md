# 多模态持续学习领域迎来新突破，本文为您带来全面解读。

发布时间：2024年10月07日

`LLM理论` `人工智能` `机器学习`

> Recent Advances of Multimodal Continual Learning: A Comprehensive Survey

# 摘要

> 持续学习 (CL) 旨在让机器学习模型不断从新数据中学习，同时不忘旧知识。随着模型从简单到复杂，从单模态到多模态，多模态持续学习 (MMCL) 应运而生。MMCL 的挑战在于它不仅仅是单模态方法的简单叠加，因为直接的方法往往效果不佳。我们首次对 MMCL 进行了全面调查，提供了背景知识、设置和方法分类。我们将现有方法分为四类：基于正则化的、基于架构的、基于回放的和基于提示的，并分析了它们的关键创新。此外，我们总结了开放数据集和基准，并探讨了未来的研究方向。我们还创建了一个 GitHub 仓库，收录相关论文和资源，网址为 https://github.com/LucyDYu/Awesome-Multimodal-Continual-Learning。

> Continual learning (CL) aims to empower machine learning models to learn continually from new data, while building upon previously acquired knowledge without forgetting. As machine learning models have evolved from small to large pre-trained architectures, and from supporting unimodal to multimodal data, multimodal continual learning (MMCL) methods have recently emerged. The primary challenge of MMCL is that it goes beyond a simple stacking of unimodal CL methods, as such straightforward approaches often yield unsatisfactory performance. In this work, we present the first comprehensive survey on MMCL. We provide essential background knowledge and MMCL settings, as well as a structured taxonomy of MMCL methods. We categorize existing MMCL methods into four categories, i.e., regularization-based, architecture-based, replay-based, and prompt-based methods, explaining their methodologies and highlighting their key innovations. Additionally, to prompt further research in this field, we summarize open MMCL datasets and benchmarks, and discuss several promising future directions for investigation and development. We have also created a GitHub repository for indexing relevant MMCL papers and open resources available at https://github.com/LucyDYu/Awesome-Multimodal-Continual-Learning.

[Arxiv](https://arxiv.org/abs/2410.05352)