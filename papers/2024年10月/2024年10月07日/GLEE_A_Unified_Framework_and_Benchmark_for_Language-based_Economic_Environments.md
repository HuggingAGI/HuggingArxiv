# GLEE：基于语言的经济环境统一框架与基准

发布时间：2024年10月07日

`Agent`

> GLEE: A Unified Framework and Benchmark for Language-based Economic Environments

# 摘要

> 大型语言模型（LLM）在经济与战略互动中潜力巨大，尤其在自然语言沟通普遍的场景中。然而，这引发了一系列关键问题：LLM 是否理性？能否模仿人类？是否能达成高效公平的结果？自然语言在战略互动中扮演何种角色？经济环境如何影响这些动态？这些问题在将 LLM 应用于在线零售和推荐系统等实际数据驱动系统时尤为重要。尽管机器学习界已在多代理设置中探索 LLM 的潜力，但研究间的假设、设计与评估标准差异，使得得出有力结论变得困难。为此，我们推出一个基准，标准化两人、顺序、基于语言的游戏研究。受经济学启发，我们定义了三个基本游戏家族，统一参数化、自由度与经济指标，评估代理的自我收益及游戏结果的效率与公平性。我们开发了开源框架进行交互模拟与分析，收集了大量 LLM 对 LLM 及人类对 LLM 的交互数据。通过广泛实验，我们展示了框架与数据集如何用于：(i) 比较 LLM 代理与人类在不同经济背景下的行为；(ii) 评估代理的个体与集体表现；(iii) 量化经济环境特征对代理行为的影响。

> Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic human behavior? Do they tend to reach an efficient and fair outcome? What is the role of natural language in the strategic interaction? How do characteristics of the economic environment influence these dynamics? These questions become crucial concerning the economic and societal implications of integrating LLM-based agents into real-world data-driven systems, such as online retail platforms and recommender systems. While the ML community has been exploring the potential of LLMs in such multi-agent setups, varying assumptions, design choices and evaluation criteria across studies make it difficult to draw robust and meaningful conclusions. To address this, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of LLM vs. LLM interactions across numerous game configurations and an additional dataset of human vs. LLM interactions. Through extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of LLM-based agents to human players in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents.

[Arxiv](https://arxiv.org/abs/2410.05254)