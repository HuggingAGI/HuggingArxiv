# 聊天机器人的推荐，是刻板印象的产物，还是个性化的体现？用户身份的偏见在其中扮演了关键角色。

发布时间：2024年10月07日

`LLM应用` `推荐系统` `人工智能伦理`

> Stereotype or Personalization? User Identity Biases Chatbot Recommendations

# 摘要

> 我们发现，大型语言模型 (LLMs) 在生成推荐时，不仅能捕捉用户需求，还能反映用户身份。尽管个性化推荐备受青睐，但实践中区分偏见与个性化却颇具挑战。我们观察到，无论用户身份是显性还是隐性透露，模型推荐中都存在种族刻板印象。我们认为，聊天机器人应明确标示推荐是否受用户身份影响，但现状却未能如此。实验表明，尽管用户身份对推荐影响显著 (p < 0.001)，模型回应却对此含糊其辞。这种偏见与透明度缺失，在多个热门 LLMs 及四个美国种族群体中普遍存在。

> We demonstrate that when people use large language models (LLMs) to generate recommendations, the LLMs produce responses that reflect both what the user wants and who the user is. While personalized recommendations are often desired by users, it can be difficult in practice to distinguish cases of bias from cases of personalization: we find that models generate racially stereotypical recommendations regardless of whether the user revealed their identity intentionally through explicit indications or unintentionally through implicit cues. We argue that chatbots ought to transparently indicate when recommendations are influenced by a user's revealed identity characteristics, but observe that they currently fail to do so. Our experiments show that even though a user's revealed identity significantly influences model recommendations (p < 0.001), model responses obfuscate this fact in response to user queries. This bias and lack of transparency occurs consistently across multiple popular consumer LLMs (gpt-4o-mini, gpt-4-turbo, llama-3-70B, and claude-3.5) and for four American racial groups.

[Arxiv](https://arxiv.org/abs/2410.05613)