# 编程数据生成模板：提升 LLM 生成能力的秘籍

发布时间：2024年10月07日

`LLM应用` `人工智能` `软件开发`

> Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates

# 摘要

> 微调大型语言模型（LLM）以提升生成能力，通常依赖于指令数据集。然而，手动构建这些数据集既耗时又昂贵，而由LLM生成的数据虽节省人力，却可能触犯隐私协议或服务条款。为此，我们研发了Cookbook框架，通过编程生成基于随机标记的简单模式训练数据，既规避法律和隐私问题，又经济高效。Cookbook首先利用Python函数模板生成数据，促使模型掌握与任务相关的显式规则，从而提升性能高达52.7个准确度点。此外，Cookbook还能智能混合多模板数据，优化多任务表现。在GPT4ALL多任务评估中，Mistral-7B经Cookbook微调后，平均准确度领先其他7B参数模型，并在三项任务中夺冠。最后，我们深入剖析Cookbook提升性能的原因，并提出验证指标，确保改进源自模型更好地遵循模板规则。

> Fine-tuning large language models (LLMs) on instruction datasets is a common way to improve their generative capabilities. However, instruction datasets can be expensive and time-consuming to manually curate, and while LLM-generated data is less labor-intensive, it may violate user privacy agreements or terms of service of LLM providers. Therefore, we seek a way of constructing instruction datasets with samples that are not generated by humans or LLMs but still improve LLM generative capabilities. In this work, we introduce Cookbook, a framework that programmatically generates training data consisting of simple patterns over random tokens, resulting in a scalable, cost-effective approach that avoids legal and privacy issues. First, Cookbook uses a template -- a data generating Python function -- to produce training data that encourages the model to learn an explicit pattern-based rule that corresponds to a desired task. We find that fine-tuning on Cookbook-generated data is able to improve performance on its corresponding task by up to 52.7 accuracy points. Second, since instruction datasets improve performance on multiple downstream tasks simultaneously, Cookbook algorithmically learns how to mix data from various templates to optimize performance on multiple tasks. On the standard multi-task GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated dataset attains the best accuracy on average compared to other 7B parameter instruction-tuned models and is the best performing model on 3 out of 8 tasks. Finally, we analyze when and why Cookbook improves performance and present a metric that allows us to verify that the improvement is largely explained by the model's generations adhering better to template rules.

[Arxiv](https://arxiv.org/abs/2410.05224)