# 在生成式 AI 时代，学习将迎来新变革：借助大型语言模型，实现自动问题生成与评估。

发布时间：2024年10月12日

`LLM应用` `人工智能`

> The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models

# 摘要

> 近年来，LLM 和生成式 AI 在 NLP 领域带来了革命性变革，尤其在教育方面展现出巨大潜力。本章深入探讨了 LLM 在自动问题生成和答案评估中的应用。首先，分析了 LLM 的核心机制，特别是其生成类人文本的能力。接着，讨论了如何通过多样化、上下文相关的问题设计，利用定制的适应性策略提升学习效果。本章还评估了零-shot 和链式思维提示等关键技术，探讨了其在生成高质量问题中的应用，包括多语言环境下的开放式和多项选择题。尽管存在成本问题，微调和提示调优等高级 NLP 方法在特定任务中的应用也得到了探讨。此外，本章还关注了生成问题的人工评估，指出了不同方法间的质量差异及改进方向。最后，深入探讨了 LLM 在自动答案评估中的应用，展示了其准确评估、提供反馈及识别细微理解差异的能力。通过实例，展示了成功案例及待改进之处。讨论强调了在适当引导下，LLM 有望取代传统人工评估，显著提升教育效率。

> In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.

[Arxiv](https://arxiv.org/abs/2410.09576)