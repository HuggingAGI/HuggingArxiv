# 扩展版日本常识道德数据集，融合掩码标记与标签增强技术

发布时间：2024年10月12日

`LLM应用` `人工智能` `道德伦理`

> Extended Japanese Commonsense Morality Dataset with Masked Token and Label Enhancement

# 摘要

> 随着 AI 技术的迅猛发展，将道德推理融入 AI 系统变得尤为重要。然而，现有模型和数据集往往忽视了地区和文化的多样性。为此，我们扩展了唯一专注于日本道德的公开数据集 JCommonsenseMorality (JCM)，形成了扩展版 eJCM。通过我们提出的掩码标记和标签增强 (MTLE) 方法，eJCM 从 13,975 个句子扩展到 31,184 个。MTLE 方法通过掩码和替换道德判断相关的重要部分，并重新分配标签，显著提升了模型的 F1 分数，达到 0.857，超越了原始 JCM、ChatGPT 单次分类和 AugGPT 增强的数据。特别是在日本文化特有的复杂道德推理任务中，eJCM 训练的模型性能大幅提升，接近 GPT-4 Turbo 的水平。这些成果不仅验证了 eJCM 数据集的有效性，也强调了开发考虑文化背景的 AI 模型和数据集的重要性。

> Rapid advancements in artificial intelligence (AI) have made it crucial to integrate moral reasoning into AI systems. However, existing models and datasets often overlook regional and cultural differences. To address this shortcoming, we have expanded the JCommonsenseMorality (JCM) dataset, the only publicly available dataset focused on Japanese morality. The Extended JCM (eJCM) has grown from the original 13,975 sentences to 31,184 sentences using our proposed sentence expansion method called Masked Token and Label Enhancement (MTLE). MTLE selectively masks important parts of sentences related to moral judgment and replaces them with alternative expressions generated by a large language model (LLM), while re-assigning appropriate labels. The model trained using our eJCM achieved an F1 score of 0.857, higher than the scores for the original JCM (0.837), ChatGPT one-shot classification (0.841), and data augmented using AugGPT, a state-of-the-art augmentation method (0.850). Specifically, in complex moral reasoning tasks unique to Japanese culture, the model trained with eJCM showed a significant improvement in performance (increasing from 0.681 to 0.756) and achieved a performance close to that of GPT-4 Turbo (0.787). These results demonstrate the validity of the eJCM dataset and the importance of developing models and datasets that consider the cultural context.

[Arxiv](https://arxiv.org/abs/2410.09564)