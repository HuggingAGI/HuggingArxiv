# SLAM-AAC：借助 LLM 的改写增强与 CLAP-Refine 技术，提升音频字幕质量

发布时间：2024年10月12日

`LLM应用` `音频处理`

> SLAM-AAC: Enhancing Audio Captioning with Paraphrasing Augmentation and CLAP-Refine through LLMs

# 摘要

> 自动音频字幕生成 (AAC) 旨在为音频信号生成自然文本描述。随着音频预训练模型和大型语言模型 (LLM) 的进步，音频理解和文本推理能力显著提升，为 AAC 的改进创造了条件。本文提出 SLAM-AAC，通过 LLM 的释义增强和 CLAP-Refine 进一步优化 AAC。我们利用自监督 EAT 模型提取音频细粒度特征，并通过轻量级线性层与文本嵌入对齐。字幕生成 LLM 采用 LoRA 适配器进行高效微调。借鉴机器翻译中的回译方法，我们实现释义增强，扩展 Clotho 数据集，缓解音频-文本对稀缺问题，从少量音频片段生成多样化字幕。推理阶段，引入即插即用的 CLAP-Refine 策略，充分利用多个解码输出，类似语音识别中的 n-best 重评分策略。通过 CLAP 模型计算音频-文本相似度，选择与输入音频最匹配的文本描述。实验显示，SLAM-AAC 在 Clotho V2 和 AudioCaps 上表现卓越，超越了主流模型。

> Automated Audio Captioning (AAC) aims to generate natural textual descriptions for input audio signals. Recent progress in audio pre-trained models and large language models (LLMs) has significantly enhanced audio understanding and textual reasoning capabilities, making improvements in AAC possible. In this paper, we propose SLAM-AAC to further enhance AAC with paraphrasing augmentation and CLAP-Refine through LLMs. Our approach uses the self-supervised EAT model to extract fine-grained audio representations, which are then aligned with textual embeddings via lightweight linear layers. The caption generation LLM is efficiently fine-tuned using the LoRA adapter. Drawing inspiration from the back-translation method in machine translation, we implement paraphrasing augmentation to expand the Clotho dataset during pre-training. This strategy helps alleviate the limitation of scarce audio-text pairs and generates more diverse captions from a small set of audio clips. During inference, we introduce the plug-and-play CLAP-Refine strategy to fully exploit multiple decoding outputs, akin to the n-best rescoring strategy in speech recognition. Using the CLAP model for audio-text similarity calculation, we could select the textual descriptions generated by multiple searching beams that best match the input audio. Experimental results show that SLAM-AAC achieves state-of-the-art performance on Clotho V2 and AudioCaps, surpassing previous mainstream models.

[Arxiv](https://arxiv.org/abs/2410.09503)