# 探究大型语言模型的精神价值与偏见

发布时间：2024年10月15日

`LLM理论` `精神健康` `人工智能伦理`

> Measuring Spiritual Values and Bias of Large Language Models

# 摘要

> 大型语言模型（LLM）已成为各行各业用户的得力助手。这些模型在海量数据上训练，反映了其预训练数据中的语言和文化细节。然而，数据中的固有价值观和观点可能影响模型的行为，带来潜在偏见。因此，在涉及精神或道德价值观的场景中使用 LLM 时，必须谨慎考虑这些潜在偏见。我们的研究首先通过测试流行 LLM 的精神价值观来验证假设，发现其价值观相当多元，打破了无神论或世俗主义的刻板印象。接着，我们探讨了不同精神价值观在社会公平场景（如仇恨言论识别）中对 LLM 的影响，发现这些价值观确实导致对不同仇恨目标群体的不同敏感性。最后，我们提出在精神文本上继续预训练 LLM，实证结果显示这种方法能有效减轻精神偏见。

> Large language models (LLMs) have become integral tool for users from various backgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural nuances embedded in their pre-training data. However, the values and perspectives inherent in this data can influence the behavior of LLMs, leading to potential biases. As a result, the use of LLMs in contexts involving spiritual or moral values necessitates careful consideration of these underlying biases. Our work starts with verification of our hypothesis by testing the spiritual values of popular LLMs. Experimental results show that LLMs' spiritual values are quite diverse, as opposed to the stereotype of atheists or secularists. We then investigate how different spiritual values affect LLMs in social-fairness scenarios e.g., hate speech identification). Our findings reveal that different spiritual values indeed lead to different sensitivity to different hate target groups. Furthermore, we propose to continue pre-training LLMs on spiritual texts, and empirical results demonstrate the effectiveness of this approach in mitigating spiritual bias.

[Arxiv](https://arxiv.org/abs/2410.11647)