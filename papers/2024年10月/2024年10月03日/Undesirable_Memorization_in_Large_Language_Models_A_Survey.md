# 大型语言模型中的不良记忆现象：全面调查

发布时间：2024年10月03日

`LLM理论` `人工智能` `隐私保护`

> Undesirable Memorization in Large Language Models: A Survey

# 摘要

> 尽管 LLM 的能力日益显著，但其隐藏的陷阱不容忽视。其中，记忆问题尤为关键，涉及重大伦理和法律风险。本文系统梳理了 LLM 中的记忆问题，指出记忆现象是模型存储并再现训练数据中短语或段落的行为，已成为隐私和安全攻击的根源。  我们从五个维度——意图性、程度、可检索性、抽象性和透明度——全面回顾了记忆现象的研究。随后，探讨了测量记忆的指标与方法，并分析了影响记忆的因素。进一步，我们考察了记忆在不同模型架构中的表现，并提出了缓解策略。最后，我们展望了未来的研究方向，包括如何在 LLM 中平衡性能与隐私，以及在对话代理、检索增强生成、多语言和扩散语言模型等特定场景中的记忆分析。

> While recent research increasingly showcases the remarkable capabilities of Large Language Models (LLMs), it's vital to confront their hidden pitfalls. Among these challenges, the issue of memorization stands out, posing significant ethical and legal risks. In this paper, we presents a Systematization of Knowledge (SoK) on the topic of memorization in LLMs. Memorization is the effect that a model tends to store and reproduce phrases or passages from the training data and has been shown to be the fundamental issue to various privacy and security attacks against LLMs.
  We begin by providing an overview of the literature on the memorization, exploring it across five key dimensions: intentionality, degree, retrievability, abstraction, and transparency. Next, we discuss the metrics and methods used to measure memorization, followed by an analysis of the factors that contribute to memorization phenomenon. We then examine how memorization manifests itself in specific model architectures and explore strategies for mitigating these effects. We conclude our overview by identifying potential research topics for the near future: to develop methods for balancing performance and privacy in LLMs, and the analysis of memorization in specific contexts, including conversational agents, retrieval-augmented generation, multilingual language models, and diffusion language models.

[Arxiv](https://arxiv.org/abs/2410.02650)