# 数学教育中的自动化反馈：有关开放式回答的大型语言模型之比较分析

发布时间：2024年10月29日

`LLM应用`

> Automated Feedback in Math Education: A Comparative Analysis of LLMs for Open-Ended Responses

# 摘要

> 在教育数据挖掘（EDM）领域，反馈对提升学习效果的有效性已有大量文献记载。此前的众多研究探索了增强反馈有效性的方法。大型语言模型（LLMs）的最新进展拓展了其在强化自动反馈系统方面的作用。本研究致力于探究LLMs在促进数学教育自动反馈方面的潜力。我们通过对比Llama、SBERT-Canberra和GPT4这三个不同模型，检验LLMs评估学生回答的有效性。评估要求模型针对学生对开放式数学问题的回答，给出定量分数和定性反馈。我们采用专为数学打造的Llama版本Mistral，并借助中学生数学问题的学生回答和教师编写的反馈数据集对该模型进行微调，以评估学生的回答。SBERT模型的训练也采用了类似方式，而GPT4模型则运用了零样本学习法。我们借助两位教师的判断来评估模型在评分准确性和反馈质量方面的表现。教师使用共同的标准来评定生成反馈的准确性和相关性。我们对模型性能展开了定量和定性的分析。通过对这些方法的详尽比较，本研究旨在进一步推进自动反馈系统的不断发展，并勾勒出利用生成式LLMs打造更具个性化学习体验的潜在未来方向。

> The effectiveness of feedback in enhancing learning outcomes is well documented within Educational Data Mining (EDM). Various prior research has explored methodologies to enhance the effectiveness of feedback. Recent developments in Large Language Models (LLMs) have extended their utility in enhancing automated feedback systems. This study aims to explore the potential of LLMs in facilitating automated feedback in math education. We examine the effectiveness of LLMs in evaluating student responses by comparing 3 different models: Llama, SBERT-Canberra, and GPT4 model. The evaluation requires the model to provide both a quantitative score and qualitative feedback on the student's responses to open-ended math problems. We employ Mistral, a version of Llama catered to math, and fine-tune this model for evaluating student responses by leveraging a dataset of student responses and teacher-written feedback for middle-school math problems. A similar approach was taken for training the SBERT model as well, while the GPT4 model used a zero-shot learning approach. We evaluate the model's performance in scoring accuracy and the quality of feedback by utilizing judgments from 2 teachers. The teachers utilized a shared rubric in assessing the accuracy and relevance of the generated feedback. We conduct both quantitative and qualitative analyses of the model performance. By offering a detailed comparison of these methods, this study aims to further the ongoing development of automated feedback systems and outlines potential future directions for leveraging generative LLMs to create more personalized learning experiences.

[Arxiv](https://arxiv.org/abs/2411.08910)