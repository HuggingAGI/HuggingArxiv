# 自然语言推理增强了视觉语言模型的组合性

发布时间：2024年10月29日

`LLM应用` `视觉语言模型` `自然语言推理`

> Natural Language Inference Improves Compositionality in Vision-Language Models

# 摘要

> 在视觉语言模型（VLMs）中的组合推理颇具挑战，因为此类模型往往难以关联对象、属性和空间关系。近期的方法试图借助文本描述的语义来应对这些局限，利用大型语言模型（LLMs）把它们拆解为问题和答案的子集。然而，这些方法主要停留在表层，未融入更深入的词汇理解，还引入了LLM生成的错误假设。针对这些问题，我们推出了具有矛盾和蕴含的标题扩展（CECE）这一原则性方法，它借助自然语言推理（NLI）从给定前提生成蕴含和矛盾。CECE能生成词汇多样的句子，同时保留核心意义。通过大量实验，我们证明CECE增强了可解释性，降低了对有偏差或表面特征的过度依赖。通过依据原始前提平衡CECE，我们在无需额外微调的情况下，相比之前的方法有了显著进步，在评判图像-文本对齐与人类判断相符程度的基准测试中取得了领先成果，在Winoground上性能提升了+19.2%（组得分），在EqBen上相对于最佳先前成果（用目标数据微调）提升了+12.9%（组得分）。

> Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the textual description, using Large Language Models (LLMs) to break them down into subsets of questions and answers. However, these methods primarily operate on the surface level, failing to incorporate deeper lexical understanding while introducing incorrect assumptions generated by the LLM. In response to these issues, we present Caption Expansion with Contradictions and Entailments (CECE), a principled approach that leverages Natural Language Inference (NLI) to generate entailments and contradictions from a given premise. CECE produces lexically diverse sentences while maintaining their core meaning. Through extensive experiments, we show that CECE enhances interpretability and reduces overreliance on biased or superficial features. By balancing CECE along the original premise, we achieve significant improvements over previous methods without requiring additional fine-tuning, producing state-of-the-art results on benchmarks that score agreement with human judgments for image-text alignment, and achieving an increase in performance on Winoground of +19.2% (group score) and +12.9% on EqBen (group score) over the best prior work (finetuned with targeted data).

[Arxiv](https://arxiv.org/abs/2410.22315)