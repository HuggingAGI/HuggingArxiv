# Flow-DPO：借助在线多智能体学习提升 LLM 数学推理能力

发布时间：2024年10月29日

`LLM应用` `在线学习`

> Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning

# 摘要

> 数学推理对于大型语言模型（LLMs）而言是关键能力，然而生成详尽且精准的推理轨迹仍是重大挑战。此文引入一种新方法，借助在线学习	extbf{流}为LLM微调打造高质量的推理轨迹。我们的方法运用了增量输出产生流，其中的组件LLMs通过反复交流协作构建解决方案。我们利用带有展开的在线直接偏好优化（DPO）学习来训练流，为每个训练实例生成DPO对，并实时更新模型。我们直接对比了我们的方法所生成的推理轨迹和通过直接模型推理生成的推理轨迹的质量，以此证明我们的方法在提升LLM于数学推理任务中的表现方面是有效的。

> Mathematical reasoning is a crucial capability for Large Language Models (LLMs), yet generating detailed and accurate reasoning traces remains a significant challenge. This paper introduces a novel approach to produce high-quality reasoning traces for LLM fine-tuning using online learning \textbf{Flows}. Our method employs an incremental output production Flow, where component LLMs collaboratively construct solutions through iterative communication. We train the Flow using online Direct Preference Optimization (DPO) learning with rollouts, generating DPO pairs for each training example and updating models in real-time. We directly compare the quality of reasoning traces generated by our method with those produced through direct model inference, demonstrating the effectiveness of our approach in improving LLM performance in mathematical reasoning tasks.

[Arxiv](https://arxiv.org/abs/2410.22304)