# 大型语言模型的可废止推理基准测试——初探与展望

发布时间：2024年10月16日

`LLM理论` `人工智能` `逻辑推理`

> Benchmarking Defeasible Reasoning with Large Language Models -- Initial Experiments and Future Directions

# 摘要

> LLM 因其卓越性能在 AI 领域备受瞩目。为了深入了解其能力与局限，尤其是在非单调推理方面，本文提出了一种新的基准，涵盖多种基于可废止规则的推理模式。我们通过将可废止规则转化为适合 LLM 的文本，改进了现有基准。初步实验中，我们使用 ChatGPT 进行非单调规则推理，并与可废止逻辑的推理模式进行了对比。

> Large Language Models (LLMs) have gained prominence in the AI landscape due to their exceptional performance. Thus, it is essential to gain a better understanding of their capabilities and limitations, among others in terms of nonmonotonic reasoning. This paper proposes a benchmark that corresponds to various defeasible rule-based reasoning patterns. We modified an existing benchmark for defeasible logic reasoners by translating defeasible rules into text suitable for LLMs. We conducted preliminary experiments on nonmonotonic rule-based reasoning using ChatGPT and compared it with reasoning patterns defined by defeasible logic.

[Arxiv](https://arxiv.org/abs/2410.12509)