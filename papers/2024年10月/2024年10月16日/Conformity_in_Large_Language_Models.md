# 大型语言模型中的从众现象

发布时间：2024年10月16日

`LLM理论` `人工智能` `心理学`

> Conformity in Large Language Models

# 摘要

> 一致性效应揭示了个体倾向于与多数人保持一致的倾向。在大语言模型 (LLM) 中，这种偏差的研究至关重要，因为 LLM 正越来越多地被用于信息搜索和决策任务中，以提升效率。然而，对错误答案的顺从会削弱其效能。本文通过心理学实验，揭示了最先进 LLM 中一致性的程度。研究发现，无论初始选择或正确性如何，所有测试模型在不同知识领域中均表现出不同程度的一致性。特别地，我们首次发现，LLM 在自身预测不确定时更易顺从。此外，我们探讨了训练范式和输入特征对一致性的影响，发现指令调整模型较不易受影响，而多数声音的自然性增强则会加剧一致性。最后，我们提出了“魔鬼代言人”和“问题蒸馏”两种干预措施，以减少一致性，为构建更稳健的语言模型提供新思路。

> The conformity effect describes the tendency of individuals to align their responses with the majority. Studying this bias in large language models (LLMs) is crucial, as LLMs are increasingly used in various information-seeking and decision-making tasks as conversation partners to improve productivity. Thus, conformity to incorrect responses can compromise their effectiveness. In this paper, we adapt psychological experiments to examine the extent of conformity in state-of-the-art LLMs. Our findings reveal that all models tested exhibit varying levels of conformity toward the majority, regardless of their initial choice or correctness, across different knowledge domains. Notably, we are the first to show that LLMs are more likely to conform when they are more uncertain in their own prediction. We further explore factors that influence conformity, such as training paradigms and input characteristics, finding that instruction-tuned models are less susceptible to conformity, while increasing the naturalness of majority tones amplifies conformity. Finally, we propose two interventions--Devil's Advocate and Question Distillation--to mitigate conformity, providing insights into building more robust language models.

[Arxiv](https://arxiv.org/abs/2410.12428)