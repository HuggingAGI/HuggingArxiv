# 利用语言模型生成的对抗性示例对错误信息检测发起攻击

发布时间：2024年10月28日

`LLM应用` `内容审核` `文本分类`

> Attacking Misinformation Detection Using Adversarial Examples Generated by Language Models

# 摘要

> 我们探究了生成对抗样本以测试检测低可信度内容（涵盖宣传、虚假声明、谣言和超党派新闻）的文本分类算法的稳健性这一挑战。我们着重通过对攻击者被允许尝试的查询数量设定实际限制来模拟内容审核。在我们的解决方案（TREPAT）里，初始改写由大型语言模型生成，其提示受保留意义的 NLP 任务（如文本简化和风格转换）启发。接着，这些修改被分解为小的变动，并通过波束搜索流程应用，直至受害分类器改变其决策。评估表明，我们的方法在受限场景中具有优越性，尤其在长输入文本（新闻文章）的情形下，穷举搜索不可行。

> We investigate the challenge of generating adversarial examples to test the robustness of text classification algorithms detecting low-credibility content, including propaganda, false claims, rumours and hyperpartisan news. We focus on simulation of content moderation by setting realistic limits on the number of queries an attacker is allowed to attempt. Within our solution (TREPAT), initial rephrasings are generated by large language models with prompts inspired by meaning-preserving NLP tasks, e.g. text simplification and style transfer. Subsequently, these modifications are decomposed into small changes, applied through beam search procedure until the victim classifier changes its decision. The evaluation confirms the superiority of our approach in the constrained scenario, especially in case of long input text (news articles), where exhaustive search is not feasible.

[Arxiv](https://arxiv.org/abs/2410.20940)