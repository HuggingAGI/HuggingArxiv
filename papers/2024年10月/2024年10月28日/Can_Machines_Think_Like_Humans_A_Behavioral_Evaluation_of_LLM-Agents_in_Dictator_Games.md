# 机器能否像人类一样思考？对大型语言模型代理在独裁者博弈中的行为评估

发布时间：2024年10月28日

`Agent` `人工智能` `社会行为`

> Can Machines Think Like Humans? A Behavioral Evaluation of LLM-Agents in Dictator Games

# 摘要

> 随着基于大型语言模型（LLM）的智能体在现实世界中承担的任务越来越多，与人类社会的互动愈发频繁，我们对它们的行为究竟了解多少？本研究（1）探究了不同角色如何诱导 LLM 智能体的亲社会行为（这是一种基本的社会规范），并以人类行为作为参照进行衡量；（2）引入了一种行为方法来评估 LLM 智能体在复杂决策场景中的表现。我们研究了不同角色和实验框架如何影响这些人工智能体在独裁者博弈中的利他行为，并对同一 LLM 家族内部、不同家族之间以及与人类行为进行了比较。我们的发现表明，LLM 之间存在诸多变化和不一致，与人类行为相比也有显著差异。仅仅给 LLM 赋予类人的身份，并不能让其产生类人的行为。尽管这些人工智能体是基于大量人类生成的数据进行训练的，但它们无法精准预测人类的决策。LLM 智能体无法捕捉人类决策的内在过程，它们与人类行为的一致性高度可变，且取决于特定的模型架构和提示表述；更糟的是，这种依赖关系并无清晰规律。

> As Large Language Model (LLM)-based agents increasingly undertake real-world tasks and engage with human society, how well do we understand their behaviors? This study (1) investigates how LLM agents' prosocial behaviors -- a fundamental social norm -- can be induced by different personas and benchmarked against human behaviors; and (2) introduces a behavioral approach to evaluate the performance of LLM agents in complex decision-making scenarios. We explored how different personas and experimental framings affect these AI agents' altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. Our findings reveal substantial variations and inconsistencies among LLMs and notable differences compared to human behaviors. Merely assigning a human-like identity to LLMs does not produce human-like behaviors. Despite being trained on extensive human-generated data, these AI agents cannot accurately predict human decisions. LLM agents are not able to capture the internal processes of human decision-making, and their alignment with human behavior is highly variable and dependent on specific model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern.

[Arxiv](https://arxiv.org/abs/2410.21359)