# 对机器的信任：调查语言模型的认识论盲点

发布时间：2024年10月28日

`LLM应用`

> Belief in the Machine: Investigating Epistemological Blind Spots of Language Models

# 摘要

> 随着语言模型（LMs）成为医疗保健、法律和新闻等领域不可或缺的一部分，它们区分事实、信念和知识的能力对于可靠的决策至关重要。未能掌握这些区别可能会在医疗诊断、法律判决和假新闻传播等领域导致重大后果。尽管如此，当前的文献在很大程度上集中在更复杂的问题上，如心理理论，而忽略了更基本的认知挑战。本研究使用一个新的数据集 KaBLE，系统地评估了现代语言模型（包括 GPT-4、Claude-3 和 Llama-3）的认知推理能力，该数据集包含 13 个任务中的 13,000 个问题。我们的结果揭示了关键的局限性。首先，虽然语言模型在事实场景中达到 86％的准确率，但在虚假场景中其表现显著下降，特别是在与信念相关的任务中。其次，语言模型在识别和确认个人信念方面存在困难，尤其是当这些信念与事实数据相矛盾时，这引起了对医疗保健和咨询等应用的关注，在这些应用中与个人的信念互动至关重要。第三，我们发现语言模型在处理第一人称与第三人称信念时存在明显的偏差，在第三人称任务上的表现（80.7％）比第一人称任务（54.4％）更好。第四，语言模型对知识的事实性本质缺乏坚实的理解，即知识本质上需要真实。第五，语言模型依靠语言线索进行事实核查，有时会绕过更深入的推理。这些发现突出了对当前语言模型在真理、信念和知识推理能力方面的重大担忧，同时强调了在关键领域广泛部署之前在这些方面取得进展的必要性。

> As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to significant consequences in areas such as medical diagnosis, legal judgments, and dissemination of fake news. Despite this, current literature has largely focused on more complex issues such as theory of mind, overlooking more fundamental epistemic challenges. This study systematically evaluates the epistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and Llama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13 tasks. Our results reveal key limitations. First, while LMs achieve 86% accuracy on factual scenarios, their performance drops significantly with false scenarios, particularly in belief-related tasks. Second, LMs struggle with recognizing and affirming personal beliefs, especially when those beliefs contradict factual data, which raises concerns for applications in healthcare and counseling, where engaging with a person's beliefs is critical. Third, we identify a salient bias in how LMs process first-person versus third-person beliefs, performing better on third-person tasks (80.7%) compared to first-person tasks (54.4%). Fourth, LMs lack a robust understanding of the factive nature of knowledge, namely, that knowledge inherently requires truth. Fifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the deeper reasoning. These findings highlight significant concerns about current LMs' ability to reason about truth, belief, and knowledge while emphasizing the need for advancements in these areas before broad deployment in critical sectors.

[Arxiv](https://arxiv.org/abs/2410.21195)