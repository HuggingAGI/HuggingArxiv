# CycleResearcher：借助自动审查提升自动化研究水平

发布时间：2024年10月28日

`Agent` `科学研究` `人工智能`

> CycleResearcher: Improving Automated Research via Automated Review

# 摘要

> 科学发现的自动化一直是研究领域的长期追求，其动力源于加速知识创造的潜能。尽管将商业大型语言模型（LLMs）用作研究助手或创意生成器已取得显著进展，但利用开源LLMs实现整个研究流程自动化的可能性在很大程度上尚未被探索。本文探究了将开源后训练的LLMs用作能够完成从文献综述、稿件筹备到同行评审和论文修改的全周期自主代理的可行性。我们的迭代偏好训练框架包含执行研究任务的CycleResearcher和模拟同行评审流程的CycleReviewer，通过强化学习提供迭代反馈。为训练这些模型，我们开发了两个新数据集，即Review-5k和Research-14k，它们反映了真实世界中的机器学习研究和同行评审动态。我们的成果显示，CycleReviewer在预测论文分数时，其平均绝对误差（MAE）比单个人类评审员降低了26.89％，这表明LLMs在研究评估中能够超越专家水平的表现。在研究方面，CycleResearcher模型生成的论文在模拟同行评审中获得了5.36分，超过了人类专家的预印本水平5.24分，接近已接受论文的水平5.69分。此项工作是迈向全自动科学探索的重要一步，提供了道德保障，也增强了人工智能驱动的研究能力。相关代码、数据集和模型权重在 url{http://github/minjun-zhu/Researcher} 发布。

> The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored. This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper revision. Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning. To train these models, we develop two new datasets, Review-5k and Research-14k, reflecting real-world machine learning research and peer review dynamics. Our results demonstrate that CycleReviewer achieves a 26.89\% improvement in mean absolute error (MAE) over individual human reviewers in predicting paper scores, indicating that LLMs can surpass expert-level performance in research evaluation. In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, surpassing the preprint level of 5.24 from human experts and approaching the accepted paper level of 5.69. This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and advancing AI-driven research capabilities. The code, dataset and model weight are released at url{http://github/minjun-zhu/Researcher}.

[Arxiv](https://arxiv.org/abs/2411.00816)