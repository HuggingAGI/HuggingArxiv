# BongLLaMA：用于孟加拉语的 LLaMA

发布时间：2024年10月28日

`LLM应用` `语言模型` `孟加拉语`

> BongLLaMA: LLaMA for Bangla Language

# 摘要

> 孟加拉语（或“孟加拉文”）是一种约有 2.4 亿母语使用者和全球约 3 亿人使用的语言。尽管它是世界上第五大使用语言，但孟加拉语仍然是一种“低资源”语言，现有的预训练语言模型在孟加拉语处理（BLP）任务上往往表现不佳。这项工作通过引入 BongLLaMA（即孟加拉 - LLaMA）来解决这一差距，这是一个仅在大型孟加拉语语料库和指令调整数据集上进行微调的开源大型语言模型。我们展示了我们的方法、数据增强技术、微调细节和综合基准测试结果，展示了 BongLLaMA 在 BLP 任务上的效用。我们相信 BongLLaMA 将成为孟加拉语语言模型的新标准基线，从而促进未来针对这种广泛使用但“低资源”语言的基准研究。所有 BongLLaMA 模型均可在 https://huggingface.co/BanglaLLM 上供公众使用。

> Bangla (or "Bengali") is a language spoken by approximately 240 million native speakers and around 300 million people worldwide. Despite being the 5th largest spoken language in the world, Bangla is still a "low-resource" language, and existing pretrained language models often struggle to perform well on Bangla Language Processing (BLP) tasks. This work addresses this gap by introducing BongLLaMA (i.e., Bangla-LLaMA), an open-source large language model fine-tuned exclusively on large Bangla corpora and instruction-tuning datasets. We present our methodology, data augmentation techniques, fine-tuning details, and comprehensive benchmarking results showcasing the utility of BongLLaMA on BLP tasks. We believe BongLLaMA will serve as the new standard baseline for Bangla Language Models and, thus, facilitate future benchmarking studies focused on this widely-spoken yet "low-resource" language. All BongLLaMA models are available for public use at https://huggingface.co/BanglaLLM.

[Arxiv](https://arxiv.org/abs/2410.21200)