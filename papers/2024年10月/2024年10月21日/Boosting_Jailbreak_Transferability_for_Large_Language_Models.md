# 增强大型语言模型的越狱转移能力

发布时间：2024年10月21日

`LLM应用` `人工智能`

> Boosting Jailbreak Transferability for Large Language Models

# 摘要

> 大型语言模型面临的安全对齐挑战，尤其是越狱攻击，引发了广泛关注。针对现有方法如GCG在单模型攻击中表现优异但可转移性不足的问题，我们提出了一系列改进措施，包括场景诱导模板、优化后缀选择及重新后缀攻击机制，以提升攻击的一致性和成功率。实验结果显示，我们的方法在多个基准测试中表现卓越，攻击执行和可转移性成功率接近100%。值得一提的是，该方法在全球安全与安全LLM挑战赛中荣获在线第一名。

> Large language models have drawn significant attention to the challenge of safe alignment, especially regarding jailbreak attacks that circumvent security measures to produce harmful content. To address the limitations of existing methods like GCG, which perform well in single-model attacks but lack transferability, we propose several enhancements, including a scenario induction template, optimized suffix selection, and the integration of re-suffix attack mechanism to reduce inconsistent outputs. Our approach has shown superior performance in extensive experiments across various benchmarks, achieving nearly 100% success rates in both attack execution and transferability. Notably, our method has won the online first place in the AISG-hosted Global Challenge for Safe and Secure LLMs.

[Arxiv](https://arxiv.org/abs/2410.15645)