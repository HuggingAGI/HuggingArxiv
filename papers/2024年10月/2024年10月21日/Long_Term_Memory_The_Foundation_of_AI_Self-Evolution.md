# 长期记忆，乃 AI 自我进化的基石。

发布时间：2024年10月21日

`LLM理论` `人工智能`

> Long Term Memory: The Foundation of AI Self-Evolution

# 摘要

> 大型语言模型（如 GPT）通过海量数据训练，展现出卓越的语言理解、推理和规划能力，甚至在多项任务中达到人类水平。多数研究致力于通过更大规模的数据训练来提升这些模型的性能。然而，除了强化训练，让模型在推理过程中实现自我进化同样重要，这一过程被称为 AI 自我进化。不同于大规模训练，自我进化可能依赖于有限的数据或交互。受人类大脑皮层结构的启发，我们推测 AI 模型能通过与环境的迭代交互，发展认知能力并构建内部表示。为此，模型需借助长期记忆（LTM）来存储和管理交互数据，LTM 通过多样化的经验表示支持自我进化。本报告探讨了 AI 自我进化及其在推理中的潜力，分析了 LTM 在终身学习中的作用，并概述了 LTM 的结构及所需系统。我们还展示了如何利用 LTM 数据构建个性化模型，并通过交互实现自我进化。我们的多代理框架 OMNE 在 GAIA 基准测试中夺冠，凸显了 LTM 在 AI 自我进化中的潜力。最后，我们提出了未来研究方向，强调 LTM 在推动 AI 技术进步及其应用中的关键作用。

> Large language models (LLMs) like GPTs, trained on vast datasets, have demonstrated impressive capabilities in language understanding, reasoning, and planning, achieving human-level performance in various tasks. Most studies focus on enhancing these models by training on ever-larger datasets to build more powerful foundation models. While training stronger models is important, enabling models to evolve during inference is equally crucial, a process we refer to as AI self-evolution. Unlike large-scale training, self-evolution may rely on limited data or interactions. Inspired by the columnar organization of the human cerebral cortex, we hypothesize that AI models could develop cognitive abilities and build internal representations through iterative interactions with their environment. To achieve this, models need long-term memory (LTM) to store and manage processed interaction data. LTM supports self-evolution by representing diverse experiences across environments and agents. In this report, we explore AI self-evolution and its potential to enhance models during inference. We examine LTM's role in lifelong learning, allowing models to evolve based on accumulated interactions. We outline the structure of LTM and the systems needed for effective data retention and representation. We also classify approaches for building personalized models with LTM data and show how these models achieve self-evolution through interaction. Using LTM, our multi-agent framework OMNE achieved first place on the GAIA benchmark, demonstrating LTM's potential for AI self-evolution. Finally, we present a roadmap for future research, emphasizing the importance of LTM for advancing AI technology and its practical applications.

[Arxiv](https://arxiv.org/abs/2410.15665)