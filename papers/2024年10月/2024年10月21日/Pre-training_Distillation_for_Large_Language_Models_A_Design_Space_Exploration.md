# 探索大型语言模型预训练蒸馏的设计空间

发布时间：2024年10月21日

`LLM理论` `人工智能`

> Pre-training Distillation for Large Language Models: A Design Space Exploration

# 摘要

> 知识蒸馏 (KD) 旨在将大型教师模型的知识传递给较小的学生模型。以往在大型语言模型 (LLM) 领域的 KD 应用主要集中在训练后阶段，学生 LLM 直接从教师模型生成的指令和响应中学习。本文中，我们将 KD 扩展至 LLM 的预训练阶段，称为预训练蒸馏 (PD)。我们首先使用 GLM-4-9B 作为教师 LLM，蒸馏出 1.9B 参数的学生 LLM，验证了 PD 的有效性。针对蒸馏的关键影响因素，我们系统地探索了 PD 的设计空间，包括 logits 处理、损失选择、缩放定律以及离线或在线 logits 四个方面。通过大量实验，我们发现更好的配置和有趣结论，如较大的学生 LLM 通常从 PD 中受益更多，而较大的教师 LLM 并不一定带来更好的结果。我们希望这些探索能为未来的预训练蒸馏实践提供参考。

> Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation.

[Arxiv](https://arxiv.org/abs/2410.16215)