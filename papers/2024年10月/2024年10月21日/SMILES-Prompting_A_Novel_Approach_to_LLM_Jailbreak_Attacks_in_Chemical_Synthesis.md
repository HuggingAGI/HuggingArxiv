# SMILES-Prompting：化学合成领域中，破解 LLM 的新颖策略

发布时间：2024年10月21日

`LLM应用` `网络安全`

> SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis

# 摘要

> 随着 LLM 在各领域的广泛应用，其传播危险信息的风险也日益凸显。本文聚焦于 LLM 在化学领域的安全漏洞，尤其是其合成危险物质的指令能力。我们测试了多种提示注入攻击方法，包括红队测试、显式和隐式提示，并引入了一种新型攻击技术——SMILES-prompting，利用 SMILES 系统引用化学物质。结果显示，SMILES-prompting 能有效绕过现有安全机制。这表明，加强 LLM 在特定领域的安全措施刻不容缓，以防止滥用，并最大化其对社会的积极影响。

> The increasing integration of large language models (LLMs) across various fields has heightened concerns about their potential to propagate dangerous information. This paper specifically explores the security vulnerabilities of LLMs within the field of chemistry, particularly their capacity to provide instructions for synthesizing hazardous substances. We evaluate the effectiveness of several prompt injection attack methods, including red-teaming, explicit prompting, and implicit prompting. Additionally, we introduce a novel attack technique named SMILES-prompting, which uses the Simplified Molecular-Input Line-Entry System (SMILES) to reference chemical substances. Our findings reveal that SMILES-prompting can effectively bypass current safety mechanisms. These findings highlight the urgent need for enhanced domain-specific safeguards in LLMs to prevent misuse and improve their potential for positive social impact.

[Arxiv](https://arxiv.org/abs/2410.15641)