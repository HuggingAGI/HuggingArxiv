# PUMA：为统一的多模态语言模型注入多粒度视觉生成力量

发布时间：2024年10月21日

`LLM应用` `人工智能` `计算机视觉`

> PUMA: Empowering Unified MLLM with Multi-granular Visual Generation

# 摘要

> 多模态基础模型的进步在视觉与语言理解上取得了显著成果。然而，现有研究在处理多模态大型语言模型（MLLM）中不同图像生成任务的粒度需求时，仍显不足。为此，我们推出了 PUMA，一个通过多粒度视觉生成强化统一 MLLM 的系统。PUMA 巧妙地将多粒度视觉特征融入 MLLM 的输入与输出，完美应对了各类图像生成任务的粒度需求。经过多模态预训练与任务导向的指令调优，PUMA 在多模态任务中表现出色。这标志着我们向一个能灵活适应各种视觉任务粒度需求的真正统一 MLLM 迈出了重要一步。相关代码与模型即将在 https://github.com/rongyaofang/PUMA 发布。

> Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.

[Arxiv](https://arxiv.org/abs/2410.13861)