# 多语言模型如何记住事实？探索其背后的记忆机制

发布时间：2024年10月18日

`LLM理论` `语言学` `人工智能`

> How Do Multilingual Models Remember? Investigating Multilingual Factual Recall Mechanisms

# 摘要

> 大型语言模型（LLM）能够存储和检索大量预训练期间获取的事实知识。尽管已有研究揭示了英语单语模型中的知识回忆机制，但这些机制如何应用于其他语言和多语言模型仍未被深入探讨。本文通过分析两个高度多语言的LLM，填补了这一研究空白。我们评估了英语中已知的知识回忆机制在多语言环境中的适用性，并探讨了语言在回忆过程中的作用，揭示了语言无关和语言依赖的机制。

> Large Language Models (LLMs) store and retrieve vast amounts of factual knowledge acquired during pre-training. Prior research has localized and identified mechanisms behind knowledge recall; however, it has primarily focused on English monolingual models. The question of how these processes generalize to other languages and multilingual LLMs remains unexplored. In this paper, we address this gap by conducting a comprehensive analysis of two highly multilingual LLMs. We assess the extent to which previously identified components and mechanisms of factual recall in English apply to a multilingual context. Then, we examine when language plays a role in the recall process, uncovering evidence of language-independent and language-dependent mechanisms.

[Arxiv](https://arxiv.org/abs/2410.14387)