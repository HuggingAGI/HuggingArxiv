# 论文评分背后的逻辑：利用 LLM 生成的理由，提升 S-LLM 在多维度评分中的表现

发布时间：2024年10月18日

`LLM应用` `语言评估`

> Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs

# 摘要

> 现有的自动作文评分 (AES) 仅依赖作文文本，未使用解释性理由，错失了以细粒度方式捕捉评分标准具体评估方面的机会。本文提出基于理由的多特质评分 (RMTS)，结合基于提示工程的 LLM 与微调型 S-LLM 作文评分模型。RMTS 通过独立的 LLM 代理根据评分标准生成特质特定理由，评分模型据此准确预测多特质分数。实验表明，RMTS 在特质特定评分上显著优于最先进模型和普通 S-LLM。通过细粒度定性理由辅助定量评估，RMTS 提升了特质评分的可靠性，为作文提供了部分解释。

> Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with a fine-tuning-based essay scoring model using a smaller large language model (S-LLM). RMTS uses an LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays.

[Arxiv](https://arxiv.org/abs/2410.14202)