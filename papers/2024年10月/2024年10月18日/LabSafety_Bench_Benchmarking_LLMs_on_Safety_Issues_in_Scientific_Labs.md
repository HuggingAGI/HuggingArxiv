# LabSafety Bench：科学实验室安全问题上的 LLM 基准测试

发布时间：2024年10月18日

`LLM应用` `实验室安全` `人工智能`

> LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs

# 摘要

> 实验室事故威胁生命与财产，凸显了安全协议的重要性。尽管安全培训不断进步，实验室人员仍可能无意中采取不安全行为。随着 LLM 在实验室等领域的应用日益广泛，其在关键安全决策中的可靠性引发担忧。LLM 缺乏正式的实验室安全教育，其提供安全指导的能力存疑。现有研究主要关注 LLM 的伦理、真实性和公平性，但未充分涵盖实验室安全等关键应用。为此，我们提出了 LabSafety Bench，一个基于 OSHA 协议的综合评估框架，包含 765 道专家验证的多选题，评估 LLM 和 VLM 在实验室安全中的表现。评估显示，GPT-4o 虽优于人类，但仍易犯关键错误，凸显了在安全环境中依赖 LLM 的风险。研究强调，需专门基准来准确评估 LLM 在安全应用中的可信度。

> Laboratory accidents pose significant risks to human life and property, underscoring the importance of robust safety protocols. Despite advancements in safety training, laboratory personnel may still unknowingly engage in unsafe practices. With the increasing reliance on large language models (LLMs) for guidance in various fields, including laboratory settings, there is a growing concern about their reliability in critical safety-related decision-making. Unlike trained human researchers, LLMs lack formal lab safety education, raising questions about their ability to provide safe and accurate guidance. Existing research on LLM trustworthiness primarily focuses on issues such as ethical compliance, truthfulness, and fairness but fails to fully cover safety-critical real-world applications, like lab safety. To address this gap, we propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive evaluation framework based on a new taxonomy aligned with Occupational Safety and Health Administration (OSHA) protocols. This benchmark includes 765 multiple-choice questions verified by human experts, assessing LLMs and vision language models (VLMs) performance in lab safety contexts. Our evaluations demonstrate that while GPT-4o outperforms human participants, it is still prone to critical errors, highlighting the risks of relying on LLMs in safety-critical environments. Our findings emphasize the need for specialized benchmarks to accurately assess the trustworthiness of LLMs in real-world safety applications.

[Arxiv](https://arxiv.org/abs/2410.14182)