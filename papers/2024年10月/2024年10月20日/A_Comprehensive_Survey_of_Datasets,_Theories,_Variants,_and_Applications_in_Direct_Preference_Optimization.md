# 全面探讨直接偏好优化中的数据集、理论、变体及应用

发布时间：2024年10月20日

`LLM理论` `人工智能` `机器学习`

> A Comprehensive Survey of Datasets, Theories, Variants, and Applications in Direct Preference Optimization

# 摘要

> 随着 LLM 的迅猛发展，将策略模型与人类偏好对齐变得愈发关键。DPO 作为一种无需 RL 的 RLHF 替代方案，展现出巨大潜力。尽管 DPO 已取得诸多进展，但其局限性仍需深入探讨。本文全面回顾了 DPO 的挑战与机遇，涉及理论分析、变体、数据集及应用。我们根据关键研究问题对近期研究进行了分类，以全面把握 DPO 现状。此外，我们还提出了未来研究方向，为模型对齐领域提供新思路。

> With the rapid advancement of large language models (LLMs), aligning policy models with human preferences has become increasingly critical. Direct Preference Optimization (DPO) has emerged as a promising approach for alignment, acting as an RL-free alternative to Reinforcement Learning from Human Feedback (RLHF). Despite DPO's various advancements and inherent limitations, an in-depth review of these aspects is currently lacking in the literature. In this work, we present a comprehensive review of the challenges and opportunities in DPO, covering theoretical analyses, variants, relevant preference datasets, and applications. Specifically, we categorize recent studies on DPO based on key research questions to provide a thorough understanding of DPO's current landscape. Additionally, we propose several future research directions to offer insights on model alignment for the research community.

[Arxiv](https://arxiv.org/abs/2410.15595)