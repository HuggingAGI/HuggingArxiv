# 大型视觉语言模型中的幻觉现象调查

发布时间：2024年10月20日

`LLM应用` `人工智能` `计算机视觉`

> A Survey of Hallucination in Large Visual Language Models

# 摘要

> LVLMs 通过融合视觉模态，不仅提升了用户交互体验，还展现了卓越的信息处理能力。然而，幻觉问题却成为其在实际应用中的绊脚石。尽管已有诸多研究致力于幻觉的缓解与纠正，但系统性总结仍显不足。本文首先概述了 LVLMs 及其幻觉现象的背景，随后剖析了模型结构与幻觉生成的主要诱因。接着，我们梳理了近期在幻觉纠正方面的进展，并从判断与生成两个维度介绍了相关的评估基准。最后，我们展望了未来研究方向，旨在进一步提升 LVLMs 的可靠性与实用性。

> The Large Visual Language Models (LVLMs) enhances user interaction and enriches user experience by integrating visual modality on the basis of the Large Language Models (LLMs). It has demonstrated their powerful information processing and generation capabilities. However, the existence of hallucinations has limited the potential and practical effectiveness of LVLM in various fields. Although lots of work has been devoted to the issue of hallucination mitigation and correction, there are few reviews to summary this issue. In this survey, we first introduce the background of LVLMs and hallucinations. Then, the structure of LVLMs and main causes of hallucination generation are introduced. Further, we summary recent works on hallucination correction and mitigation. In addition, the available hallucination evaluation benchmarks for LVLMs are presented from judgmental and generative perspectives. Finally, we suggest some future research directions to enhance the dependability and utility of LVLMs.

[Arxiv](https://arxiv.org/abs/2410.15359)