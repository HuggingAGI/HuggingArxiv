# LlamaLens：一款专为分析新闻与社交媒体内容而设计的多语言大型语言模型

发布时间：2024年10月20日

`LLM应用` `社交媒体`

> LlamaLens: Specialized Multilingual LLM for Analyzing News and Social Media Content

# 摘要

> LLM 在多个领域展现了卓越的通用任务解决能力，但在处理特定领域问题时，尤其是下游 NLP 任务，仍显不足。研究表明，基于指令微调的模型表现更佳。然而，现有研究多聚焦于英语等资源丰富的语言和广泛领域，对多语言和特定领域的关注不足。为此，我们开发了 LlamaLens，一种专门用于多语言环境下新闻和社交媒体内容分析的 LLM。据我们所知，这是首次尝试同时解决领域特异性和多语言性问题，特别是针对新闻和社交媒体。我们的实验涵盖 19 项任务，涉及阿拉伯语、英语和印地语的 52 个数据集。结果显示，LlamaLens 在 16 个测试集上超越了当前最先进的技术，并在 10 个测试集上表现相当。我们公开了模型和资源，供研究社区使用。(https://huggingface.co/QCRI)

> Large Language Models (LLMs) have demonstrated remarkable success as general-purpose task solvers across various fields, including NLP, healthcare, finance, and law. However, their capabilities remain limited when addressing domain-specific problems, particularly in downstream NLP tasks. Research has shown that models fine-tuned on instruction-based downstream NLP datasets outperform those that are not fine-tuned. While most efforts in this area have primarily focused on resource-rich languages like English and broad domains, little attention has been given to multilingual settings and specific domains. To address this gap, this study focuses on developing a specialized LLM, LlamaLens, for analyzing news and social media content in a multilingual context. To the best of our knowledge, this is the first attempt to tackle both domain specificity and multilinguality, with a particular focus on news and social media. Our experimental setup includes 19 tasks, represented by 52 datasets covering Arabic, English, and Hindi. We demonstrate that LlamaLens outperforms the current state-of-the-art (SOTA) on 16 testing sets, and achieves comparable performance on 10 sets. We make the models and resources publicly available for the research community.(https://huggingface.co/QCRI)

[Arxiv](https://arxiv.org/abs/2410.15308)