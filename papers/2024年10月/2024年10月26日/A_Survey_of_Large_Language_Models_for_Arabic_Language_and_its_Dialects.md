# 关于阿拉伯语及其方言的大型语言模型的一项调研

发布时间：2024年10月26日

`LLM应用` `阿拉伯语`

> A Survey of Large Language Models for Arabic Language and its Dialects

# 摘要

> 本次调查对专为阿拉伯语及其方言打造的大型语言模型（LLMs）进行了全面综述。涵盖了关键架构，像仅编码器、仅解码器、编码器-解码器模型，还有用于预训练的数据集，包括古典阿拉伯语、现代标准阿拉伯语和方言阿拉伯语的相关数据集。研究还探讨了单语、双语和多语的LLMs，分析了它们在诸如情感分析、命名实体识别和问答等下游任务中的架构和性能。另外，依据源代码可用性、训练数据、模型权重和文档等因素评估了阿拉伯语LLMs的开放性。调查突出了对更多样化方言数据集的需求，点明了开放性对于研究可重复性和透明度的重要意义。最后明确了未来研究的关键挑战与机遇，并强调需要更具包容性和代表性的模型。

> This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects. It covers key architectures, including encoder-only, decoder-only, and encoder-decoder models, along with the datasets used for pre-training, spanning Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of Arabic LLMs based on factors, such as source code availability, training data, model weights, and documentation. The survey highlights the need for more diverse dialectal datasets and attributes the importance of openness for research reproducibility and transparency. It concludes by identifying key challenges and opportunities for future research and stressing the need for more inclusive and representative models.

[Arxiv](https://arxiv.org/abs/2410.20238)