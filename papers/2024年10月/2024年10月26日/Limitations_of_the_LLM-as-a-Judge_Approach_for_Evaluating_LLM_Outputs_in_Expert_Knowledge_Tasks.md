# 在专家知识任务中，将 LLM 作为评判者来评估 LLM 输出的方法存在局限性

发布时间：2024年10月26日

`LLM应用` `营养学` `心理健康`

> Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks

# 摘要

> 使用大型语言模型（LLMs）自身来评估其输出具有很大潜力，能为不同情境下的模型性能评估提供有效方法。此前研究显示，在遵循一般指令的情况下，作为评判者的 LLM 与人类评判者有很强的相关性。但对于需要专业知识的指令，用 LLM 作评判者是否有效还不确定。在我们的研究中，采用了混合方法，进行了成对比较，让主题专家（SMEs）和 LLM 都对特定领域任务的输出予以评估。我们聚焦于两个不同领域：营养学，有注册营养师专家；心理健康，有临床心理学家专家。结果表明，在评估总体偏好时，营养学领域中 SMEs 与 LLM 评判者意见一致的比例为 68%，心理健康领域为 64%。另外，结果显示在特定领域的具体问题上，SMEs 与 LLM 的一致性存在差别。我们的发现突出了在评估过程中保留人类专家的重要性，因为仅靠 LLM 或许无法为复杂且具特定知识的任务提供所需的深度理解。我们还探究了 LLM 评估在不同领域的影响，并讨论了这些见解如何为评估工作流程的设计提供参考，以保证在交互式系统中人类专家和 LLM 能更好地协同一致。

> The potential of using Large Language Models (LLMs) themselves to evaluate LLM outputs offers a promising method for assessing model performance across various contexts. Previous research indicates that LLM-as-a-judge exhibits a strong correlation with human judges in the context of general instruction following. However, for instructions that require specialized knowledge, the validity of using LLMs as judges remains uncertain. In our study, we applied a mixed-methods approach, conducting pairwise comparisons in which both subject matter experts (SMEs) and LLMs evaluated outputs from domain-specific tasks. We focused on two distinct fields: dietetics, with registered dietitian experts, and mental health, with clinical psychologist experts. Our results showed that SMEs agreed with LLM judges 68% of the time in the dietetics domain and 64% in mental health when evaluating overall preference. Additionally, the results indicated variations in SME-LLM agreement across domain-specific aspect questions. Our findings emphasize the importance of keeping human experts in the evaluation process, as LLMs alone may not provide the depth of understanding required for complex, knowledge specific tasks. We also explore the implications of LLM evaluations across different domains and discuss how these insights can inform the design of evaluation workflows that ensure better alignment between human experts and LLMs in interactive systems.

[Arxiv](https://arxiv.org/abs/2410.20266)