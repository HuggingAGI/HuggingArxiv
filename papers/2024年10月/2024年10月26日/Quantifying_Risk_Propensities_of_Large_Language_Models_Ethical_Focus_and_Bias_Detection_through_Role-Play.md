# 量化大型语言模型的风险倾向：借由角色扮演实现伦理关注与偏差检测

发布时间：2024年10月26日

`LLM应用` `人工智能`

> Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play

# 摘要

> 随着大型语言模型（LLMs）日益普及，人们对其安全性、道德性和潜在偏差的担忧也随之增多。系统评估LLMs的风险决策倾向和态度，尤其是在道德领域，已变得极为关键。本研究开创性地把认知科学中的特定领域风险承担（DOSPERT）量表应用于LLMs，还提出了全新的伦理决策风险态度量表（EDRAS），用以深度评估LLMs的伦理风险态度。我们进一步提出了一种将风险量表与角色扮演相融合的新办法，来定量评估LLMs中的系统性偏差。通过对多个主流LLMs进行系统评估与分析，我们评估了LLMs在多个领域的“风险个性”，重点聚焦于道德领域，并且揭示和量化了LLMs对不同群体的系统性偏差。此项研究有助于理解LLMs的风险决策，保障其安全可靠地应用。我们的方法为识别和减轻偏差提供了工具，有助于打造更公平、更可信的人工智能系统。代码和数据均有提供。

> As Large Language Models (LLMs) become more prevalent, concerns about their safety, ethics, and potential biases have risen. Systematically evaluating LLMs' risk decision-making tendencies and attitudes, particularly in the ethical domain, has become crucial. This study innovatively applies the Domain-Specific Risk-Taking (DOSPERT) scale from cognitive science to LLMs and proposes a novel Ethical Decision-Making Risk Attitude Scale (EDRAS) to assess LLMs' ethical risk attitudes in depth. We further propose a novel approach integrating risk scales and role-playing to quantitatively evaluate systematic biases in LLMs. Through systematic evaluation and analysis of multiple mainstream LLMs, we assessed the "risk personalities" of LLMs across multiple domains, with a particular focus on the ethical domain, and revealed and quantified LLMs' systematic biases towards different groups. This research helps understand LLMs' risk decision-making and ensure their safe and reliable application. Our approach provides a tool for identifying and mitigating biases, contributing to fairer and more trustworthy AI systems. The code and data are available.

[Arxiv](https://arxiv.org/abs/2411.08884)