# ChitroJera：专为孟加拉语设计的区域性视觉问答数据集

发布时间：2024年10月19日

`LLM应用` `人工智能` `语言处理`

> ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla

# 摘要

> 视觉问答 (VQA) 需要根据视觉内容回答自然语言问题。尽管孟加拉语使用广泛，但由于缺乏基准数据集，在 VQA 领域被视为低资源语言。这种缺失对其他语言中表现优异的模型构成了挑战。现有孟加拉语 VQA 数据集文化相关性不足，大多改编自国外。为此，我们推出了大规模孟加拉语 VQA 数据集 ChitroJera，包含超过 15k 个多样且本地相关的样本。我们测试了文本编码器、图像编码器、多模态模型及创新的双编码器模型。结果显示，预训练双编码器模型表现最佳。我们还用基于提示的技术评估了大型语言模型 (LLM)，LLM 表现最优。鉴于现有数据集的不足，我们期待 ChitroJera 能拓展孟加拉语视觉语言任务的领域。

> Visual Question Answer (VQA) poses the problem of answering a natural language question about a visual context. Bangla, despite being a widely spoken language, is considered low-resource in the realm of VQA due to the lack of a proper benchmark dataset. The absence of such datasets challenges models that are known to be performant in other languages. Furthermore, existing Bangla VQA datasets offer little cultural relevance and are largely adapted from their foreign counterparts. To address these challenges, we introduce a large-scale Bangla VQA dataset titled ChitroJera, totaling over 15k samples where diverse and locally relevant data sources are used. We assess the performance of text encoders, image encoders, multimodal models, and our novel dual-encoder models. The experiments reveal that the pre-trained dual-encoders outperform other models of its scale. We also evaluate the performance of large language models (LLMs) using prompt-based techniques, with LLMs achieving the best performance. Given the underdeveloped state of existing datasets, we envision ChitroJera expanding the scope of Vision-Language tasks in Bangla.

[Arxiv](https://arxiv.org/abs/2410.14991)