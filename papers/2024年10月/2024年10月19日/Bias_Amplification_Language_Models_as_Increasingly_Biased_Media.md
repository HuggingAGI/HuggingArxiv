# 语言模型：日益加剧的偏见放大器

发布时间：2024年10月19日

`LLM理论` `社会科学` `人工智能`

> Bias Amplification: Language Models as Increasingly Biased Media

# 摘要

> 随着 LLM 在社会中的广泛应用，大量在线文本变得合成，引发了偏见放大的担忧。现有文献很少单独讨论这一问题。我们通过四个主要贡献填补了这一理解空白。首先，我们提出了一个理论框架，明确了偏见放大的独立性。通过统计模拟，我们展示了偏见放大如何在无模型崩溃的情况下发生。其次，我们使用 GPT-2 实验，发现其在政治生成任务中表现出右倾偏见，且随迭代微调加剧。第三，我们探讨了三种缓解策略，发现保留和积累策略有效。最后，通过机制解释技术，我们证实了偏见放大与模型崩溃由不同神经元驱动，与理论框架一致。

> As Large Language Models (LLMs) become increasingly integrated into various facets of society, a significant portion of online text consequently become synthetic. This raises concerns about bias amplification, a phenomenon where models trained on synthetic data amplify the pre-existing biases over successive training iterations. Previous literature seldom discusses bias amplification as an independent issue from model collapse. In this work, we address the gap in understanding the bias amplification of LLMs with four main contributions. Firstly, we propose a theoretical framework, defining the necessary and sufficient conditions for its occurrence, and emphasizing that it occurs independently of model collapse. Using statistical simulations with weighted maximum likelihood estimation, we demonstrate the framework and show how bias amplification arises without the sampling and functional form issues that typically drive model collapse. Secondly, we conduct experiments with GPT-2 to empirically demonstrate bias amplification, specifically examining open-ended generational political bias with a benchmark we developed. We observe that GPT-2 exhibits a right-leaning bias in sentence continuation tasks and that the bias progressively increases with iterative fine-tuning on synthetic data generated by previous iterations. Thirdly, we explore three potential mitigation strategies: Overfitting, Preservation, and Accumulation. We find that both Preservation and Accumulation effectively mitigate bias amplification and model collapse. Finally, using novel mechanistic interpretation techniques, we demonstrate that in the GPT-2 experiments, bias amplification and model collapse are driven by distinct sets of neurons, which aligns with our theoretical framework.

[Arxiv](https://arxiv.org/abs/2410.15234)