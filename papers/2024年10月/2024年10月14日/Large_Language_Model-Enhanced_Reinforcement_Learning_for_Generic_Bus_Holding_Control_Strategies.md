# 大型语言模型加持的强化学习，助力通用公交保持控制策略

发布时间：2024年10月14日

`LLM应用` `公共交通` `智能交通`

> Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies

# 摘要

> 公交车保持控制广泛用于提升系统稳定性和运营效率。传统模型方法在状态预测和需求估计上常显不足。而强化学习（RL）作为数据驱动策略，在制定保持策略上潜力巨大。RL通过优化控制策略，最大化累积奖励，反映整体目标。然而，将现实中的稀疏延迟目标转化为RL的密集实时奖励颇具挑战，常需大量手动调试。为此，本研究借助大型语言模型（LLM）的上下文学习和推理能力，提出自动奖励生成范式，即LLM增强RL。该范式包含奖励初始化、修改、性能分析和优化等LLM模块，协同工作，根据反馈迭代优化奖励函数，过滤无效奖励，确保RL性能稳定提升。为验证其可行性，我们将其应用于单线合成和多线现实系统。结果显示，相较于传统RL、LLM控制器和空间车距反馈控制，该范式表现更优且更稳健。这揭示了LLM在智能移动应用中的广阔前景。

> Bus holding control is a widely-adopted strategy for maintaining stability and improving the operational efficiency of bus systems. Traditional model-based methods often face challenges with the low accuracy of bus state prediction and passenger demand estimation. In contrast, Reinforcement Learning (RL), as a data-driven approach, has demonstrated great potential in formulating bus holding strategies. RL determines the optimal control strategies in order to maximize the cumulative reward, which reflects the overall control goals. However, translating sparse and delayed control goals in real-world tasks into dense and real-time rewards for RL is challenging, normally requiring extensive manual trial-and-error. In view of this, this study introduces an automatic reward generation paradigm by leveraging the in-context learning and reasoning capabilities of Large Language Models (LLMs). This new paradigm, termed the LLM-enhanced RL, comprises several LLM-based modules: reward initializer, reward modifier, performance analyzer, and reward refiner. These modules cooperate to initialize and iteratively improve the reward function according to the feedback from training and test results for the specified RL-based task. Ineffective reward functions generated by the LLM are filtered out to ensure the stable evolution of the RL agents' performance over iterations. To evaluate the feasibility of the proposed LLM-enhanced RL paradigm, it is applied to various bus holding control scenarios, including a synthetic single-line system and a real-world multi-line system. The results demonstrate the superiority and robustness of the proposed paradigm compared to vanilla RL strategies, the LLM-based controller, and conventional space headway-based feedback control. This study sheds light on the great potential of utilizing LLMs in various smart mobility applications.

[Arxiv](https://arxiv.org/abs/2410.10212)