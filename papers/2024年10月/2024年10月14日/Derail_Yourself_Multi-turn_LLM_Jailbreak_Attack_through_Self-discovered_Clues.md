# 自我脱轨：利用自我发现的线索进行多轮 LLM 越狱攻击

发布时间：2024年10月14日

`LLM应用` `网络安全` `人工智能`

> Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues

# 摘要

> 本研究揭示了 LLM 在多轮对话中的安全漏洞，恶意用户可通过多个查询掩盖有害意图。我们提出了 ActorAttack，一种受演员网络理论启发的多轮攻击方法，通过建模语义相关的演员网络作为攻击线索，生成多样且有效的攻击路径。ActorAttack 解决了两大难题：(1) 通过创建无害的对话主题掩盖有害意图，(2) 利用 LLM 知识指定相关演员为攻击线索，揭示多样化的攻击路径。实验表明，ActorAttack 在先进的 LLM 中优于现有攻击方法，包括 GPT-o1。我们将发布包含多轮对抗提示和安全对齐数据的 SafeMTData 数据集，证明使用该数据集调优的模型对多轮攻击更具抵抗力。代码已开源，详见 https://github.com/renqibing/ActorAttack。

> This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries. We introduce ActorAttack, a novel multi-turn attack method inspired by actor-network theory, which models a network of semantically linked actors as attack clues to generate diverse and effective attack paths toward harmful targets. ActorAttack addresses two main challenges in multi-turn attacks: (1) concealing harmful intents by creating an innocuous conversation topic about the actor, and (2) uncovering diverse attack paths towards the same harmful target by leveraging LLMs' knowledge to specify the correlated actors as various attack clues. In this way, ActorAttack outperforms existing single-turn and multi-turn attack methods across advanced aligned LLMs, even for GPT-o1. We will publish a dataset called SafeMTData, which includes multi-turn adversarial prompts and safety alignment data, generated by ActorAttack. We demonstrate that models safety-tuned using our safety dataset are more robust to multi-turn attacks. Code is available at https://github.com/renqibing/ActorAttack.

[Arxiv](https://arxiv.org/abs/2410.10700)