# 记录图：通过图增强检索生成，助力长上下文摘要的提升

发布时间：2024年10月14日

`RAG` `信息检索`

> Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs

# 摘要

> RAG 通过引入非参数事实知识，为 LLM 带来了新的生机。与长上下文 LLM 相比，RAG 以更简洁轻便的方式成为有效的总结工具，能够通过多样化的查询与 LLM 多次交互，获取全面响应。然而，现有方法往往忽视了 LLM 生成的历史响应中潜在的有价值信息，导致结果不尽如人意。为此，我们提出了 \textbf{GoR}，利用 LLM 的历史响应来增强 RAG 的长上下文总结能力。借鉴 RAG 的 \textit{retrieve-then-generate} 范式，我们构建了一个图，通过在检索文本块与 LLM 响应之间建立联系。为了深入挖掘它们之间的复杂关系，GoR 还引入了 \textit{graph neural network} 和基于 \textit{BERTScore} 的自监督训练目标，实现了参考总结与节点嵌入之间的无缝监督信号传递。我们在四个长上下文总结数据集上与 12 个基线进行了全面对比，结果显示 GoR 表现最佳，例如在 WCEP 数据集上，相对于 Rouge-L、Rouge-1 和 Rouge-2，检索器的性能分别提升了 15\%、8\% 和 19\%。大量实验进一步验证了 GoR 的有效性。代码已开源，详见 https://github.com/ulab-uiuc/GoR。

> Retrieval-augmented generation (RAG) has revitalized Large Language Models (LLMs) by injecting non-parametric factual knowledge. Compared with long-context LLMs, RAG is considered an effective summarization tool in a more concise and lightweight manner, which can interact with LLMs multiple times using diverse queries to get comprehensive responses. However, the LLM-generated historical responses, which contain potentially insightful information, are largely neglected and discarded by existing approaches, leading to suboptimal results. In this paper, we propose \textit{graph of records} (\textbf{GoR}), which leverages historical responses generated by LLMs to enhance RAG for long-context global summarization. Inspired by the \textit{retrieve-then-generate} paradigm of RAG, we construct a graph by establishing an edge between the retrieved text chunks and the corresponding LLM-generated response. To further uncover the intricate correlations between them, GoR further features a \textit{graph neural network} and an elaborately designed \textit{BERTScore}-based objective for self-supervised model training, enabling seamless supervision signal backpropagation between reference summaries and node embeddings. We comprehensively compare GoR with 12 baselines across four long-context summarization datasets, and the results indicate that our proposed method reaches the best performance e.g., 15\%, 8\%, and 19\% improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP dataset). Extensive experiments further demonstrate the effectiveness of GoR. Code is available at https://github.com/ulab-uiuc/GoR

[Arxiv](https://arxiv.org/abs/2410.11001)