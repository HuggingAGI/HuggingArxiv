# 结构化状态空间模型的隐含偏差，竟能被看似无害的标签所影响。

发布时间：2024年10月14日

`LLM理论` `人工智能` `网络安全`

> The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels

# 摘要

> 神经网络的隐式偏差使其能够通过梯度下降有效泛化未见数据。近期流行的结构化状态空间模型（SSMs）被视为变压器的有效替代品。先前研究表明，SSMs在低维数据生成环境中具有良好的泛化能力。然而，我们发现，某些特殊训练样本会彻底破坏SSMs的隐式偏差，导致泛化失败，即使这些样本标签干净。我们通过实验验证了这一现象，并呼吁在大型语言模型中广泛应用SSMs的背景下，深入研究其对干净标签中毒的易感性，并开发相应对策。

> Neural networks are powered by an implicit bias: a tendency of gradient descent to fit training data in a way that generalizes to unseen data. A recent class of neural network models gaining increasing popularity is structured state space models (SSMs), regarded as an efficient alternative to transformers. Prior work argued that the implicit bias of SSMs leads to generalization in a setting where data is generated by a low dimensional teacher. In this paper, we revisit the latter setting, and formally establish a phenomenon entirely undetected by prior work on the implicit bias of SSMs. Namely, we prove that while implicit bias leads to generalization under many choices of training data, there exist special examples whose inclusion in training completely distorts the implicit bias, to a point where generalization fails. This failure occurs despite the special training examples being labeled by the teacher, i.e. having clean labels! We empirically demonstrate the phenomenon, with SSMs trained independently and as part of non-linear neural networks. In the area of adversarial machine learning, disrupting generalization with cleanly labeled training examples is known as clean-label poisoning. Given the proliferation of SSMs, particularly in large language models, we believe significant efforts should be invested in further delineating their susceptibility to clean-label poisoning, and in developing methods for overcoming this susceptibility.

[Arxiv](https://arxiv.org/abs/2410.10473)