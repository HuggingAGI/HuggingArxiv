# 基于模型的差分隐私技术助力大型语言模型的知识迁移

发布时间：2024年10月14日

`LLM应用` `网络服务` `隐私保护`

> Model-Based Differentially Private Knowledge Transfer for Large Language Models

# 摘要

> 随着 LLM 在网络服务中的广泛应用，如何在保护隐私的同时有效利用领域知识成为关键。现有方法如 RAG 和差分隐私数据合成，常在知识实用性和数据隐私间做出妥协，限制了其在专业领域的应用。为此，我们提出了 Llamdex，一种将隐私保护的领域模型集成到 LLM 中的创新框架。该框架在相同隐私约束下，将领域任务的准确性提升了高达 26%。实验显示，Llamdex 不仅提升了 LLM 的响应准确性，还保持了与原始 LLM 相当的推理效率，展现了其在实际应用中的巨大潜力。

> As large language models (LLMs) become increasingly prevalent in web services, effectively leveraging domain-specific knowledge while ensuring privacy has become critical. Existing methods, such as retrieval-augmented generation (RAG) and differentially private data synthesis, often compromise either the utility of domain knowledge or the privacy of sensitive data, limiting their applicability in specialized domains. To address these challenges, we propose \textit{Llamdex}, a novel framework that integrates privacy-preserving, domain-specific models into LLMs. Our approach significantly enhances the accuracy of domain-specific tasks, achieving up to a 26\% improvement compared to existing methods under the same differential privacy constraints. Experimental results show that Llamdex not only improves the accuracy of LLM responses but also maintains comparable inference efficiency to the original LLM, highlighting its potential for real-world applications.

[Arxiv](https://arxiv.org/abs/2410.10481)