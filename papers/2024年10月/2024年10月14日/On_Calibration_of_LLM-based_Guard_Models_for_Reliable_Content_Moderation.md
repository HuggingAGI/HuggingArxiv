# 校准基于 LLM 的防护模型，确保内容审核的可靠性

发布时间：2024年10月14日

`LLM应用` `网络安全` `人工智能`

> On Calibration of LLM-based Guard Models for Reliable Content Moderation

# 摘要

> 大型语言模型 (LLM) 因可能生成有害内容或用户试图绕过防护措施而存在重大风险。现有研究已开发出基于 LLM 的防护模型，用于调节威胁 LLM 的输入和输出，确保在部署时阻止违反安全协议的内容。然而，这些防护模型的可靠性和校准性尚未得到充分关注。我们针对 9 个现有防护模型在 12 个基准上的置信度校准进行了全面实证调查，发现这些模型存在过度自信、易受攻击和鲁棒性不足等问题。我们还评估了事后校准方法，展示了温度缩放和上下文校准的益处，特别是在缺乏验证集的情况下。我们的研究揭示了当前防护模型的局限性，并为未来开发更可靠的防护模型提供了重要见解。我们建议在发布新模型时，应纳入对置信度校准可靠性的评估。

> Large language models (LLMs) pose significant risks due to the potential for generating harmful content or users attempting to evade guardrails. Existing studies have developed LLM-based guard models designed to moderate the input and output of threat LLMs, ensuring adherence to safety policies by blocking content that violates these protocols upon deployment. However, limited attention has been given to the reliability and calibration of such guard models. In this work, we empirically conduct comprehensive investigations of confidence calibration for 9 existing LLM-based guard models on 12 benchmarks in both user input and model output classification. Our findings reveal that current LLM-based guard models tend to 1) produce overconfident predictions, 2) exhibit significant miscalibration when subjected to jailbreak attacks, and 3) demonstrate limited robustness to the outputs generated by different types of response models. Additionally, we assess the effectiveness of post-hoc calibration methods to mitigate miscalibration. We demonstrate the efficacy of temperature scaling and, for the first time, highlight the benefits of contextual calibration for confidence calibration of guard models, particularly in the absence of validation sets. Our analysis and experiments underscore the limitations of current LLM-based guard models and provide valuable insights for the future development of well-calibrated guard models toward more reliable content moderation. We also advocate for incorporating reliability evaluation of confidence calibration when releasing future LLM-based guard models.

[Arxiv](https://arxiv.org/abs/2410.10414)