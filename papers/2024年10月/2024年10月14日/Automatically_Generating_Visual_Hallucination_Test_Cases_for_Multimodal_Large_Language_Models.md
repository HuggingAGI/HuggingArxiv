# 自动为多模态大型语言模型生成视觉幻觉测试用例

发布时间：2024年10月14日

`LLM应用` `人工智能` `计算机视觉`

> Automatically Generating Visual Hallucination Test Cases for Multimodal Large Language Models

# 摘要

> 当多模态大型语言模型 (MLLM) 为提示生成包含错误视觉细节的响应时，就会发生视觉幻觉 (VH)。现有 VH 测试用例的生成方法主要依赖人工注释，通常以 (图像，问题，答案) 三元组形式呈现。本文中，我们推出了 VHExpansion，首个自动化扩展 MLLM VH 测试用例的方法。给定初始 VH 测试用例，VHExpansion 通过否定问题和答案，并使用常见及对抗性扰动修改图像，实现自动扩展。此外，我们提出了新的评估指标——对称准确率，用于衡量正确回答的 VH 测试用例对的比例。每对包含一个测试用例及其否定对应物。理论分析表明，对称准确率是无偏评估指标，不受 MLLM 随机猜测答案时 VH 测试用例答案不平衡的影响，而传统准确率则易受此影响。我们将 VHExpansion 应用于扩展三个手动注释的 VH 数据集，并使用这些扩展数据集基准测试七个 MLLM。评估显示，VHExpansion 有效识别更多 VH 测试用例。由于对称准确率无偏，它对 MLLM 对 VH 脆弱性的结论与传统准确率不同。最后，我们发现，在 VHExpansion 生成的扩展 VH 数据集上微调 MLLM，比在原始手动注释数据集上微调，更能有效缓解 VH。代码链接：https://github.com/lycheeefish/VHExpansion。

> Visual hallucination (VH) occurs when a multimodal large language model (MLLM) generates responses with incorrect visual details for prompts. Existing methods for generating VH test cases primarily rely on human annotations, typically in the form of triples: (image, question, answer). In this paper, we introduce VHExpansion, the first automated method for expanding VH test cases for MLLMs. Given an initial VH test case, VHExpansion automatically expands it by perturbing the question and answer through negation as well as modifying the image using both common and adversarial perturbations. Additionally, we propose a new evaluation metric, symmetric accuracy, which measures the proportion of correctly answered VH test-case pairs. Each pair consists of a test case and its negated counterpart. Our theoretical analysis shows that symmetric accuracy is an unbiased evaluation metric that remains unaffected by the imbalance of VH testing cases with varying answers when an MLLM is randomly guessing the answers, whereas traditional accuracy is prone to such imbalance. We apply VHExpansion to expand three VH datasets annotated manually and use these expanded datasets to benchmark seven MLLMs. Our evaluation shows that VHExpansion effectively identifies more VH test cases. Moreover, symmetric accuracy, being unbiased, leads to different conclusions about the vulnerability of MLLMs to VH compared to traditional accuracy metric. Finally, we show that fine-tuning MLLMs on the expanded VH dataset generated by VHExpansion mitigates VH more effectively than fine-tuning on the original, manually annotated dataset. Our code is available at: https://github.com/lycheeefish/VHExpansion.

[Arxiv](https://arxiv.org/abs/2410.11242)