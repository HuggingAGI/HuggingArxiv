# 自动为多模态大型语言模型生成视觉幻觉测试用例

发布时间：2024年10月14日

`LLM应用` `人工智能` `计算机视觉`

> Automatically Generating Visual Hallucination Test Cases for Multimodal Large Language Models

# 摘要

> 视觉幻觉 (VH) 是指多模态大型语言模型 (MLLM) 在生成响应时，为提示提供错误的视觉细节。现有的 VH 测试用例生成方法主要依赖人工注释，通常以 (图像，问题，答案) 三元组形式呈现。本文中，我们推出了 VHExpansion，首个自动扩展 MLLM 的 VH 测试用例的方法。给定初始 VH 测试用例，VHExpansion 通过否定问题和答案，以及使用常见和对抗性扰动修改图像，实现自动扩展。我们还提出了新的评估指标——对称准确性，用于衡量正确回答的 VH 测试用例对的比例。理论分析表明，对称准确性是无偏的，不受 MLLM 随机猜测答案时 VH 测试用例答案不平衡的影响，而传统准确性则易受此影响。我们应用 VHExpansion 扩展了三个手动注释的 VH 数据集，并用于基准测试七个 MLLM。评估显示，VHExpansion 有效识别更多 VH 测试用例。对称准确性作为无偏指标，与传统准确性相比，得出关于 MLLM 对 VH 脆弱性的不同结论。最后，我们发现，在 VHExpansion 生成的扩展 VH 数据集上微调 MLLM，比在原始手动注释数据集上微调，更能有效缓解 VH。代码已公开：https://github.com/lycheeefish/VHExpansion。

> Visual hallucination (VH) occurs when a multimodal large language model (MLLM) generates responses with incorrect visual details for prompts. Existing methods for generating VH test cases primarily rely on human annotations, typically in the form of triples: (image, question, answer). In this paper, we introduce VHExpansion, the first automated method for expanding VH test cases for MLLMs. Given an initial VH test case, VHExpansion automatically expands it by perturbing the question and answer through negation as well as modifying the image using both common and adversarial perturbations. Additionally, we propose a new evaluation metric, symmetric accuracy, which measures the proportion of correctly answered VH test-case pairs. Each pair consists of a test case and its negated counterpart. Our theoretical analysis shows that symmetric accuracy is an unbiased evaluation metric that remains unaffected by the imbalance of VH testing cases with varying answers when an MLLM is randomly guessing the answers, whereas traditional accuracy is prone to such imbalance. We apply VHExpansion to expand three VH datasets annotated manually and use these expanded datasets to benchmark seven MLLMs. Our evaluation shows that VHExpansion effectively identifies more VH test cases. Moreover, symmetric accuracy, being unbiased, leads to different conclusions about the vulnerability of MLLMs to VH compared to traditional accuracy metric. Finally, we show that fine-tuning MLLMs on the expanded VH dataset generated by VHExpansion mitigates VH more effectively than fine-tuning on the original, manually annotated dataset. Our code is available at: https://github.com/lycheeefish/VHExpansion.

[Arxiv](https://arxiv.org/abs/2410.11242)