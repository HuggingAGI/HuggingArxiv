# MEGA-Bench：将多模态评估扩展至 500 多个现实任务

发布时间：2024年10月14日

`其他` `人工智能` `多模态`

> MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks

# 摘要

> 我们推出了 MEGA-Bench，这是一个涵盖超过 500 个现实世界任务的多模态评估套件，旨在应对终端用户多样化的日常需求。我们的目标是通过一组高质量、多样化的数据样本，实现高效且准确的模型评估。我们收集了 505 个现实任务，涵盖了 16 位专家注释者的 8,000 多个样本，以全面覆盖多模态任务。我们没有将这些任务简化为标准的多选题，而是采用了多种输出格式，如数字、短语、代码、\LaTeX、坐标、JSON 和自由形式等。为此，我们开发了 40 多种评估指标。MEGA-Bench 不仅提供了多维度的细粒度能力报告，还允许用户深入交互和可视化模型能力。我们在 MEGA-Bench 上评估了多种前沿视觉语言模型，以全面了解它们在不同维度上的表现。

> 
Abstract:We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MMBench, and MMT-Bench), we embrace a wide range of output formats like numbers, phrases, code, \LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. Unlike existing benchmarks, MEGA-Bench offers a fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate a wide variety of frontier vision-language models on MEGA-Bench to understand their capabilities across these dimensions.
    

[Arxiv](https://arxiv.org/pdf/2410.10563)