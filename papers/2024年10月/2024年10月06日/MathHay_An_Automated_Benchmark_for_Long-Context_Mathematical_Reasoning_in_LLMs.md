# MathHay：LLMs 中长上下文数学推理的自动化标杆

发布时间：2024年10月06日

`LLM应用`

> MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs

# 摘要

> 最近，大型语言模型（LLM）在长上下文场景中展现了多样化的能力。然而，现有的基准主要评估信息检索，缺乏对长上下文中数学推理能力的评估，这对实际应用至关重要。为此，我们推出了 MathHay，一个专注于长上下文数学推理的自动化基准。与以往的基准不同，MathHay 不仅要求信息检索，更强调复杂的数学推理。实验结果显示，即使是顶尖的 LLM，如 Gemini-1.5-Pro-002，在长上下文数学推理上也仅达到 51.26% 的准确率，表明在 MathHay 基准上仍有巨大的提升空间。

> Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark.

[Arxiv](https://arxiv.org/abs/2410.04698)