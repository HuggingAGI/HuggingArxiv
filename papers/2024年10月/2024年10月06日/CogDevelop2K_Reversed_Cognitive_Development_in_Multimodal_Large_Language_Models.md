# CogDevelop2K：探索多模态大型语言模型中的逆向认知发展

发布时间：2024年10月06日

`LLM理论` `人工智能` `认知科学`

> CogDevelop2K: Reversed Cognitive Development in Multimodal Large Language Models

# 摘要

> 多模态大型语言模型（MLLM）是否只是随机鹦鹉？它们真的能理解和执行其擅长的任务吗？本文深入探讨了MLLM的核心认知基础，即人类智能用于感知、理解和推理的能力。为此，我们推出了CogDevelop2K，一个涵盖12个子概念的综合基准，从基本的物体恒常性和边界，到高级的意图理解，均按照人类思维的发展轨迹构建。我们对46个MLLM进行了评估，并进一步研究了评估策略和提示技术的影响。令人惊讶的是，我们发现其认知发展轨迹与人类相反。

> Are Multi-modal Large Language Models (MLLMs) stochastic parrots? Do they genuinely understand and are capable of performing the tasks they excel at? This paper aims to explore the fundamental basis of MLLMs, i.e. core cognitive abilities that human intelligence builds upon to perceive, comprehend, and reason. To this end, we propose CogDevelop2K, a comprehensive benchmark that spans 12 sub-concepts from fundamental knowledge like object permanence and boundary to advanced reasoning like intentionality understanding, structured via the developmental trajectory of a human mind. We evaluate 46 MLLMs on our benchmarks. Comprehensively, we further evaluate the influence of evaluation strategies and prompting techniques. Surprisingly, we observe a reversed cognitive developmental trajectory compared to humans.

[Arxiv](https://arxiv.org/abs/2410.10855)