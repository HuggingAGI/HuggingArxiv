# 精准预测未来：多轮 RLHF 策略的高效优化

发布时间：2024年10月06日

`LLM应用` `人工智能` `对话系统`

> Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF

# 摘要

> 大型语言模型 (LLM) 在单次交互任务（如摘要）中表现出色，但在需要长期规划的多轮对话任务中仍显不足。先前的多轮对话研究通过将所有先前对话视为长上下文，扩展了单轮 RLHF 方法，但存在协变量偏移问题。为此，我们提出了 REFUEL，一种高效的策略优化方法，通过单一模型估计 $Q$-值并使用自生成数据训练，解决了协变量偏移问题。REFUEL 将多轮 RLHF 问题转化为一系列回归任务，简化了实现。理论证明 REFUEL 能匹配训练集中任何策略的性能。实验中，REFUEL 在各种设置下均优于 DPO 和 REBEL 等先进方法。此外，仅 80 亿参数的 Llama-3-8B-it 经 REFUEL 微调后，在长多轮对话中超越了 Llama-3.1-70B-it。REFUEL 实现代码和训练模型分别可在 https://github.com/ZhaolinGao/REFUEL/ 和 https://huggingface.co/Cornell-AGI 获取。

> Large Language Models (LLMs) have achieved remarkable success at tasks like summarization that involve a single turn of interaction. However, they can still struggle with multi-turn tasks like dialogue that require long-term planning. Previous works on multi-turn dialogue extend single-turn reinforcement learning from human feedback (RLHF) methods to the multi-turn setting by treating all prior dialogue turns as a long context. Such approaches suffer from covariate shift: the conversations in the training set have previous turns generated by some reference policy, which means that low training error may not necessarily correspond to good performance when the learner is actually in the conversation loop. In response, we introduce REgressing the RELative FUture (REFUEL), an efficient policy optimization approach designed to address multi-turn RLHF in LLMs. REFUEL employs a single model to estimate $Q$-values and trains on self-generated data, addressing the covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence of regression tasks on iteratively collected datasets, enabling ease of implementation. Theoretically, we prove that REFUEL can match the performance of any policy covered by the training set. Empirically, we evaluate our algorithm by using Llama-3.1-70B-it to simulate a user in conversation with our model. REFUEL consistently outperforms state-of-the-art methods such as DPO and REBEL across various settings. Furthermore, despite having only 8 billion parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it on long multi-turn dialogues. Implementation of REFUEL can be found at https://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be found at https://huggingface.co/Cornell-AGI.

[Arxiv](https://arxiv.org/abs/2410.04612)