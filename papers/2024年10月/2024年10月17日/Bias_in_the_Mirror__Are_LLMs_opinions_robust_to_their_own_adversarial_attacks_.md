# 镜中偏见：LLM 的观点能否经受住自身的对抗性攻击？

发布时间：2024年10月17日

`LLM理论` `人工智能` `社会科学`

> Bias in the Mirror : Are LLMs opinions robust to their own adversarial attacks ?

# 摘要

> LLM 从训练数据中继承了偏见，这些偏见在交互中以微妙的方式影响其行为。尽管已有研究探讨了这些偏见，但对其在交互中的鲁棒性了解甚少。本文提出了一种创新方法：两个 LLM 实例进行自我辩论，通过争论相反观点来影响模型的中性版本。借此，我们评估了偏见的牢固性，并探讨模型是否易受错误信息强化或转向有害观点。实验涵盖了多种 LLM，揭示了语言和文化背景下偏见的持久性与灵活性。

> Large language models (LLMs) inherit biases from their training data and alignment processes, influencing their responses in subtle ways. While many studies have examined these biases, little work has explored their robustness during interactions. In this paper, we introduce a novel approach where two instances of an LLM engage in self-debate, arguing opposing viewpoints to persuade a neutral version of the model. Through this, we evaluate how firmly biases hold and whether models are susceptible to reinforcing misinformation or shifting to harmful viewpoints. Our experiments span multiple LLMs of varying sizes, origins, and languages, providing deeper insights into bias persistence and flexibility across linguistic and cultural contexts.

[Arxiv](https://arxiv.org/abs/2410.13517)