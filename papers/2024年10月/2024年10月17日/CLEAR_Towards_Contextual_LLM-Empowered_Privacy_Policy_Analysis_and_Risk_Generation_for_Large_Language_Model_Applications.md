# CLEAR：借助上下文 LLM 赋能，助力隐私政策分析与大型语言模型应用的风险生成

发布时间：2024年10月17日

`LLM应用` `隐私保护` `用户体验`

> CLEAR: Towards Contextual LLM-Empowered Privacy Policy Analysis and Risk Generation for Large Language Model Applications

# 摘要

> 随着大型语言模型（LLM）驱动的终端用户应用（包括对话界面和现有图形用户界面的插件）的兴起，新的隐私挑战也随之而来。然而，许多用户对此类风险仍不自知。本文通过五次共同设计研讨会，深入探讨了提升用户对LLM相关隐私风险意识的方法，并基于研讨会成果开发了CLEAR——一个实时上下文助手，旨在帮助用户识别敏感信息、总结隐私政策并提示潜在风险。我们在ChatGPT和Gmail的Gemini插件中测试了CLEAR，结果显示其不仅易于使用，还能显著提升用户对数据处理和隐私风险的理解。此外，本文还探讨了LLM在隐私风险中的双重角色，为相关设计和政策提供了有益的启示。

> The rise of end-user applications powered by large language models (LLMs), including both conversational interfaces and add-ons to existing graphical user interfaces (GUIs), introduces new privacy challenges. However, many users remain unaware of the risks. This paper explores methods to increase user awareness of privacy risks associated with LLMs in end-user applications. We conducted five co-design workshops to uncover user privacy concerns and their demand for contextual privacy information within LLMs. Based on these insights, we developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk Generation), a just-in-time contextual assistant designed to help users identify sensitive information, summarize relevant privacy policies, and highlight potential risks when sharing information with LLMs. We evaluated the usability and usefulness of CLEAR across in two example domains: ChatGPT and the Gemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use and improves user understanding of data practices and privacy risks. We also discussed LLM's duality in posing and mitigating privacy risks, offering design and policy implications.

[Arxiv](https://arxiv.org/abs/2410.13387)