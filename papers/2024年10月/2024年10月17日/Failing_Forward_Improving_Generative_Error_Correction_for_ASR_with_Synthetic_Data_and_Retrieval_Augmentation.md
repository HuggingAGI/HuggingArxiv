# 向前失败：利用合成数据和检索增强技术提升 ASR 的生成错误校正能力

发布时间：2024年10月17日

`LLM应用` `语音识别`

> Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation

# 摘要

> 生成错误纠正 (GEC) 是提升自动语音识别 (ASR) 系统性能的有效手段。然而，GEC 模型在处理训练中未见的新错误时表现不佳，尤其是在域外 (OOD) 场景中。命名实体 (NEs) 问题尤为突出，新实体的不断涌现使得模型难以应对。为此，我们提出 DARAG (数据和检索增强生成错误纠正)，通过生成合成数据和检索增强输入，显著提升 GEC 在域内 (ID) 和 OOD 场景中的表现。实验结果显示，DARAG 在 ID 和 OOD 设置中分别实现了 8% -- 30% 和 10% -- 33% 的相对 WER 改进，超越了所有基线方法。

> Generative Error Correction (GEC) has emerged as a powerful post-processing method to enhance the performance of Automatic Speech Recognition (ASR) systems. However, we show that GEC models struggle to generalize beyond the specific types of errors encountered during training, limiting their ability to correct new, unseen errors at test time, particularly in out-of-domain (OOD) scenarios. This phenomenon amplifies with named entities (NEs), where, in addition to insufficient contextual information or knowledge about the NEs, novel NEs keep emerging. To address these issues, we propose DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach designed to improve GEC for ASR in in-domain (ID) and OOD scenarios. We augment the GEC training dataset with synthetic data generated by prompting LLMs and text-to-speech models, thereby simulating additional errors from which the model can learn. For OOD scenarios, we simulate test-time errors from new domains similarly and in an unsupervised fashion. Additionally, to better handle named entities, we introduce retrieval-augmented correction by augmenting the input with entities retrieved from a database. Our approach is simple, scalable, and both domain- and language-agnostic. We experiment on multiple datasets and settings, showing that DARAG outperforms all our baselines, achieving 8\% -- 30\% relative WER improvements in ID and 10\% -- 33\% improvements in OOD settings.

[Arxiv](https://arxiv.org/abs/2410.13198)