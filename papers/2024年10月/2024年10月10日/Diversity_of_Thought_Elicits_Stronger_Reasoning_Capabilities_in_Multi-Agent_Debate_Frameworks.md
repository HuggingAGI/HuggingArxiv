# 思维的多样性在多代理辩论框架中能激发出更强的推理能力。

发布时间：2024年10月10日

`Agent` `人工智能`

> Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks

# 摘要

> 大型语言模型（LLM）在自然语言生成方面表现优异，但在数学推理等任务中常自信地给出错误答案。思维链提示、自我验证和多智能体辩论等策略被提出以提升 LLM 的推理和事实准确性。基于 Du 等人的多智能体辩论框架，我们发现无论模型规模如何，多智能体辩论均有助于提升性能，且思想的多样性能增强辩论中的推理能力。在各种模型规模中，使用多样化训练模型时，数学推理任务的性能提升最为显著。经过 4 轮辩论，一组中等容量的多样化模型（如 Gemini-Pro、Mixtral 7BX8 和 PaLM 2-M）在 GSM-8K 基准测试中超越了 GPT-4，准确率达到 91%。而仅使用 3 个 Gemini-Pro 实例时，性能仅为 82%。此外，这组模型在 ASDiv 基准测试中创下了 94% 的新纪录。这些结果表明，未来 AI 的发展将依赖于多样化合作智能体，它们将展现出超越单一强大模型的涌现能力。

> Large language models (LLMs) excel in natural language generation but often confidently produce incorrect responses, especially in tasks like mathematical reasoning. Chain-of-thought prompting, self-verification, and multi-agent debate are among the strategies proposed to improve the reasoning and factual accuracy of LLMs. Building on Du et al.'s multi-agent debate framework, we find that multi-agent debate helps at any model scale, and that diversity of thought elicits stronger reasoning in debating LLMs. Across various model sizes, performance on mathematical reasoning tasks benefits most when diverse trained models are used. Remarkably, after 4 rounds of debate, a diverse set of medium-capacity models (Gemini-Pro, Mixtral 7BX8, and PaLM 2-M) outperforms GPT-4 on the GSM-8K benchmark, scoring 91% accuracy. By comparison, when 3 instances of Gemini-Pro are used, performance only reaches 82%. Finally, this diverse set of medium-capacity models sets a new state-of-the-art performance on the ASDiv benchmark (94%). These results underscore the idea that the future of AI is agentic, with diverse cooperating agents yielding emergent capabilities beyond even the most powerful individual models.

[Arxiv](https://arxiv.org/abs/2410.12853)