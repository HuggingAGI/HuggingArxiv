# 通过本体驱动论证，迈向确保 LLM 对抗鲁棒性的道路

发布时间：2024年10月10日

`LLM理论` `人工智能` `网络安全`

> Towards Assurance of LLM Adversarial Robustness using Ontology-Driven Argumentation

# 摘要

> 尽管 LLM 适应性强，但其安全性、透明度和可解释性仍面临挑战。由于易受攻击，LLM 需通过不断更新的对抗训练和防护措施来保护。然而，管理隐性和异构知识以持续确保鲁棒性颇具难度。我们提出了一种基于形式论证的 LLM 对抗性鲁棒性保证新方法。通过本体形式化，我们构建了最先进的攻击和防御结构，便于创建人类可读的保证案例和机器可读的表示。我们通过英语和代码翻译任务的示例展示了其应用，并为工程师、数据科学家、用户和审计员提供了理论和实践的启示。

> Despite the impressive adaptability of large language models (LLMs), challenges remain in ensuring their security, transparency, and interpretability. Given their susceptibility to adversarial attacks, LLMs need to be defended with an evolving combination of adversarial training and guardrails. However, managing the implicit and heterogeneous knowledge for continuously assuring robustness is difficult. We introduce a novel approach for assurance of the adversarial robustness of LLMs based on formal argumentation. Using ontologies for formalization, we structure state-of-the-art attacks and defenses, facilitating the creation of a human-readable assurance case, and a machine-readable representation. We demonstrate its application with examples in English language and code translation tasks, and provide implications for theory and practice, by targeting engineers, data scientists, users, and auditors.

[Arxiv](https://arxiv.org/abs/2410.07962)