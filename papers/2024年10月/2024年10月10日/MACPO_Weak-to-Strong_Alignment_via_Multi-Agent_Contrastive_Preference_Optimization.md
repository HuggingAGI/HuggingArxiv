# MACPO：借助多智能体对比偏好优化，实现从弱到强的对齐。

发布时间：2024年10月10日

`Agent` `人工智能`

> MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization

# 摘要

> 随着 LLM 的飞速发展并接近人类水平，将它们与人类价值观对齐变得愈发紧迫。在 LLM 超越人类的场景中，我们面临一个从弱到强的对齐难题，即通过弱教师的弱监督来有效对齐强学生 LLM。现有方法主要针对强到弱对齐和自对齐，难以适应更复杂的弱到强对齐。为此，我们提出了多代理对比偏好优化 (MACPO) 框架。MACPO 通过迭代强化不熟悉的积极行为并惩罚熟悉的消极行为，促进弱教师和强学生相互学习。我们设计了相互积极行为增强策略，鼓励双方从彼此的积极行为中学习，并为下一轮提供更高质量的积极行为。此外，我们提出了硬消极行为构建策略，通过在消极行为数据上微调，诱导双方生成熟悉的消极行为。实验结果表明，MACPO 同时提升了强学生和弱教师的对齐性能。随着弱教师数量的增加，MACPO 通过更多轮迭代优化实现了更好的弱到强对齐效果。

> As large language models (LLMs) are rapidly advancing and achieving near-human capabilities, aligning them with human values is becoming more urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong alignment problem where we need to effectively align strong student LLMs through weak supervision generated by weak teachers. Existing alignment methods mainly focus on strong-to-weak alignment and self-alignment settings, and it is impractical to adapt them to the much harder weak-to-strong alignment setting. To fill this gap, we propose a multi-agent contrastive preference optimization (MACPO) framework. MACPO facilitates weak teachers and strong students to learn from each other by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar negative ones. To get this, we devise a mutual positive behavior augmentation strategy to encourage weak teachers and strong students to learn from each other's positive behavior and further provide higher quality positive behavior for the next iteration. Additionally, we propose a hard negative behavior construction strategy to induce weak teachers and strong students to generate familiar negative behavior by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human judgments, demonstrate that MACPO simultaneously improves the alignment performance of strong students and weak teachers. Moreover, as the number of weak teachers increases, MACPO achieves better weak-to-strong alignment performance through more iteration optimization rounds.

[Arxiv](https://arxiv.org/abs/2410.07672)