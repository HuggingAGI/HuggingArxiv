# Omni-MATH：专为大型语言模型设计的全能奥林匹克数学基准

发布时间：2024年10月10日

`LLM应用`

> Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models

# 摘要

> 近期，大型语言模型（LLM）在数学推理方面取得了显著进展。然而，现有基准如 GSM8K 或 MATH 已被高精度解决（如 OpenAI o1 在 MATH 数据集上达到 94.8%），表明它们已不足以真正挑战这些模型。为此，我们提出了一项专为评估 LLM 奥林匹克级别数学推理能力而设计的全面且具挑战性的基准。与现有奥林匹克相关基准不同，我们的数据集聚焦于数学，包含 4428 道竞赛级别问题，并经过严格人工标注。这些问题被细分为 33 个子领域，跨越 10 多个难度级别，全面评估模型在奥林匹克数学推理中的表现。实验结果显示，即使是最先进的模型，如 OpenAI o1-mini 和 OpenAI o1-preview，在奥林匹克级别问题上准确率也仅为 60.54% 和 52.55%，凸显了奥林匹克级别数学推理的重大挑战。

> Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.

[Arxiv](https://arxiv.org/abs/2410.07985)