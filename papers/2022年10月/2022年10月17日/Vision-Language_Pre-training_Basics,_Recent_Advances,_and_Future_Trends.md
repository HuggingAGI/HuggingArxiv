# 视觉-语言预训练：从基础到前沿，展望未来

发布时间：2022年10月17日

`其他` `计算机视觉` `人工智能`

> Vision-Language Pre-training: Basics, Recent Advances, and Future Trends

# 摘要

> 本文综述了近年来多模态智能领域的视觉-语言预训练（VLP）方法。我们将这些方法归纳为三类：图像-文本任务（如图像描述、检索、问答和定位）、核心计算机视觉任务（如图像分类、检测和分割）以及视频-文本任务（如视频描述、检索和问答）。针对每类任务，我们详细介绍了当前最先进的技术，并结合具体案例探讨了已取得的进展和面临的挑战。此外，我们还探讨了研究前沿的热点话题，如大型基础模型、统一建模、上下文少样本学习、知识整合、鲁棒性及真实环境下的计算机视觉等。

> 
Abstract:This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: ($i$) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and ($iii$) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.
    

[Arxiv](https://arxiv.org/pdf/2210.09263)