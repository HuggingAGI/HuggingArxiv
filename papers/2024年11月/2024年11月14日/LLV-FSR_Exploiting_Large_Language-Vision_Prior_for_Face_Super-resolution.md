# LLV-FSR：借助大型语言视觉先验实现人脸超分辨率

发布时间：2024年11月14日

`LLM应用` `计算机视觉` `图像处理`

> LLV-FSR: Exploiting Large Language-Vision Prior for Face Super-resolution

# 摘要

> 现有的人脸超分辨率（FSR）方法进步显著，然而它们主要针对视觉信息有限的人脸，尤其是原始像素级空间进行超分辨率处理，往往忽视了诸如高阶深度、语义以及非视觉输入（文本标题和描述）等多元线索。所以，这些方法难以从输入人脸生成统一且有意义的表示。我们认为，将语言-视觉多元表示引入未探索的潜在嵌入空间，通过对语言-视觉先验的编码和利用，能够增强 FSR。这促使我们提出了名为 LLV-FSR 的新框架，它将大型视觉语言模型和高阶视觉先验的优势与具有挑战性的 FSR 任务相结合。具体而言，除了直接从原始输入中获取知识，我们还引入了预训练的视觉语言模型来生成多元先验，包括图像标题、描述、人脸语义掩码和深度。这些先验随后被用于引导更关键的特征表示，有助于实现逼真且高质量的人脸超分辨率。实验结果表明，我们提出的框架在重建质量和感知质量上均有显著提升，在 MMCelebA-HQ 数据集上的 PSNR 指标超过了 SOTA 达 0.43dB。

> Existing face super-resolution (FSR) methods have made significant advancements, but they primarily super-resolve face with limited visual information, original pixel-wise space in particular, commonly overlooking the pluralistic clues, like the higher-order depth and semantics, as well as non-visual inputs (text caption and description). Consequently, these methods struggle to produce a unified and meaningful representation from the input face. We suppose that introducing the language-vision pluralistic representation into unexplored potential embedding space could enhance FSR by encoding and exploiting the complementarity across language-vision prior. This motivates us to propose a new framework called LLV-FSR, which marries the power of large vision-language model and higher-order visual prior with the challenging task of FSR. Specifically, besides directly absorbing knowledge from original input, we introduce the pre-trained vision-language model to generate pluralistic priors, involving the image caption, descriptions, face semantic mask and depths. These priors are then employed to guide the more critical feature representation, facilitating realistic and high-quality face super-resolution. Experimental results demonstrate that our proposed framework significantly improves both the reconstruction quality and perceptual quality, surpassing the SOTA by 0.43dB in terms of PSNR on the MMCelebA-HQ dataset.

[Arxiv](https://arxiv.org/abs/2411.09293)