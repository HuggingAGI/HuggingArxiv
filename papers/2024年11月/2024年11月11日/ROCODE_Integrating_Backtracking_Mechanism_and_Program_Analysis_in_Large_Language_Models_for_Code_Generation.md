# ROCODE：在大型语言模型中集成回溯机制和程序分析以用于代码生成

发布时间：2024年11月11日

`LLM应用` `软件开发` `代码生成`

> ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation

# 摘要

> 大型语言模型（LLMs）最近在代码生成方面取得了令人瞩目的成绩，为程序员在软件开发中提供了革命性的帮助。然而，由于 LLMs 的自回归性质，它们在代码生成过程中容易出现错误积累。一旦产生错误，鉴于 LLMs 无法调整之前的输出，它们只能继续基于该错误生成后续代码。现有的基于 LLMs 的方法通常考虑在代码生成后进行后期修订，导致积累错误的解决具有挑战性，并造成资源的大量浪费。理想情况下，LLMs 应该在代码生成期间及时回滚并解决出现的错误，而不是基于错误继续进行并等待生成后的后期修订。在本文中，我们提出了 ROCODE，它将回溯机制和程序分析集成到 LLMs 中用于代码生成。具体来说，我们使用程序分析在生成过程中进行增量错误检测。当检测到错误时，触发回溯机制以启动回滚策略和约束重新生成，从而尽早消除错误并确保在正确的基础上继续生成。在多个代码生成基准上的实验表明，ROCODE 可以显著减少 LLMs 生成的错误，编译通过率为 99.1%。与最佳基线方法相比，测试通过率提高了多达 23.8%。与后期修订基线相比，令牌成本降低了 19.3%。此外，我们的方法与模型无关，在九个具有代表性的 LLMs 中均实现了一致的改进。

> Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are susceptible to error accumulation during code generation. Once an error is produced, LLMs can merely continue to generate the subsequent code conditioned on it, given their inability to adjust previous outputs. Existing LLM-based approaches typically consider post-revising after code generation, leading to the challenging resolution of accumulated errors and the significant wastage of resources. Ideally, LLMs should rollback and resolve the occurred error in time during code generation, rather than proceed on the basis of the error and wait for post-revising after generation. In this paper, we propose ROCODE, which integrates the backtracking mechanism and program analysis into LLMs for code generation. Specifically, we employ program analysis to perform incremental error detection during the generation process. When an error is detected, the backtracking mechanism is triggered to priming rollback strategies and constraint regeneration, thereby eliminating the error early and ensuring continued generation on the correct basis. Experiments on multiple code generation benchmarks show that ROCODE can significantly reduce the errors generated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is improved by up to 23.8% compared to the best baseline approach. Compared to the post-revising baseline, the token cost is reduced by 19.3%. Moreover, our approach is model-agnostic and achieves consistent improvements across nine representative LLMs.

[Arxiv](https://arxiv.org/abs/2411.07112)