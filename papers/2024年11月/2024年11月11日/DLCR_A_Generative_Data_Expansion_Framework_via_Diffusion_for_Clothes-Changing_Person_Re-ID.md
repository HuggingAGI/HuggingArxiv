# DLCR：通过扩散进行生成式数据扩展的框架，用于换装人物重识别

发布时间：2024年11月11日

`LLM应用` `计算机视觉` `人物识别`

> DLCR: A Generative Data Expansion Framework via Diffusion for Clothes-Changing Person Re-ID

# 摘要

> 随着生成扩散模型最近展现出的强大实力，一个开放的研究问题是	extit{这些模型生成的图像是否可用于学习更好的视觉表示}。虽然这种生成数据扩展可能足以应对较简单的视觉任务，但我们探索了其在更困难的判别任务上的效果：换衣人物再识别（CC-ReID）。CC-ReID 旨在匹配出现在不重叠摄像头中的人物，即使他们在不同摄像头中更换了衣服。当前的 CC-ReID 模型不仅受到当前 CC-ReID 数据集中服装多样性有限的限制，而且生成保留重要个人特征以进行准确识别的额外数据是当前的挑战。为了解决这个问题，我们提出了 DLCR，这是一种新颖的数据扩展框架，利用预训练的扩散和大型语言模型（LLM）来准确生成穿着不同服装的个人的多样化图像。我们为五个基准 CC-ReID 数据集（PRCC、CCVID、LaST、VC-Clothes 和 LTCC）生成了额外的数据，并	extbf{将它们的服装多样性增加了oldmath{$10$}倍，总共生成了超过oldmath{$2.1$}M 张图像}。DLCR 采用基于扩散的文本引导修复，以使用 LLM 构建的服装提示为条件，生成仅修改主体服装同时保留其个人可识别特征的合成数据。随着数据的大量增加，我们引入了两种新策略 - 渐进式学习和测试时预测改进 - 分别减少了训练时间并进一步提高了 CC-ReID 性能。在 PRCC 数据集上，通过使用 DLCR 生成的数据训练 CAL（一种先前的最先进（SOTA）方法），我们获得了 11.3\%的 top-1 准确率大幅提升。我们在此处公开了每个数据集的代码和生成的数据：url{https://github.com/CroitoruAlin/dlcr}。

> With the recent exhibited strength of generative diffusion models, an open research question is \textit{if images generated by these models can be used to learn better visual representations}. While this generative data expansion may suffice for easier visual tasks, we explore its efficacy on a more difficult discriminative task: clothes-changing person re-identification (CC-ReID). CC-ReID aims to match people appearing in non-overlapping cameras, even when they change their clothes across cameras. Not only are current CC-ReID models constrained by the limited diversity of clothing in current CC-ReID datasets, but generating additional data that retains important personal features for accurate identification is a current challenge. To address this issue we propose DLCR, a novel data expansion framework that leverages pre-trained diffusion and large language models (LLMs) to accurately generate diverse images of individuals in varied attire. We generate additional data for five benchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and \textbf{increase their clothing diversity by \boldmath{$10$}x, totaling over \boldmath{$2.1$}M images generated}. DLCR employs diffusion-based text-guided inpainting, conditioned on clothing prompts constructed using LLMs, to generate synthetic data that only modifies a subject's clothes while preserving their personally identifiable features. With this massive increase in data, we introduce two novel strategies - progressive learning and test-time prediction refinement - that respectively reduce training time and further boosts CC-ReID performance. On the PRCC dataset, we obtain a large top-1 accuracy improvement of $11.3\%$ by training CAL, a previous state of the art (SOTA) method, with DLCR-generated data. We publicly release our code and generated data for each dataset here: \url{https://github.com/CroitoruAlin/dlcr}.

[Arxiv](https://arxiv.org/abs/2411.07205)