# 多模态可解释的自动视频字幕

发布时间：2024年11月11日

`LLM应用`

> Multi-Modal interpretable automatic video captioning

# 摘要

> 视频字幕旨在使用自然语言格式描述视频内容，这涉及到对视图上同时发生的场景、动作和事件的理解和解释。当前的方法主要集中在视觉线索上，往往忽略了从其他重要的音频信息模态中获得的丰富信息，包括它们之间的相互依赖关系。在这项工作中，我们引入了一种新的视频字幕方法，该方法使用多模态对比损失进行训练，强调多模态集成和可解释性。我们的方法旨在捕捉这些模态之间的依赖关系，从而产生更准确、因此更相关的字幕。此外，我们强调了可解释性的重要性，采用了多种注意力机制，为模型的决策过程提供了解释。我们的实验结果表明，我们提出的方法在常用的 MSR-VTT 和 VATEX 基准数据集上相对于最先进的模型表现良好。

> Video captioning aims to describe video contents using natural language format that involves understanding and interpreting scenes, actions and events that occurs simultaneously on the view. Current approaches have mainly concentrated on visual cues, often neglecting the rich information available from other important modality of audio information, including their inter-dependencies. In this work, we introduce a novel video captioning method trained with multi-modal contrastive loss that emphasizes both multi-modal integration and interpretability. Our approach is designed to capture the dependency between these modalities, resulting in more accurate, thus pertinent captions. Furthermore, we highlight the importance of interpretability, employing multiple attention mechanisms that provide explanation into the model's decision-making process. Our experimental results demonstrate that our proposed method performs favorably against the state-of the-art models on commonly used benchmark datasets of MSR-VTT and VATEX.

[Arxiv](https://arxiv.org/abs/2411.06872)