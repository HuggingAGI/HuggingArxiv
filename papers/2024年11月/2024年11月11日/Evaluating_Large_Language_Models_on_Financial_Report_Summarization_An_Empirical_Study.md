# 在财务报告总结方面评估大型语言模型：一项实证研究

发布时间：2024年11月11日

`LLM应用` `财务报告`

> Evaluating Large Language Models on Financial Report Summarization: An Empirical Study

# 摘要

> 近年来，大型语言模型（LLMs）在包括自然语言理解、特定领域知识任务等各种应用中展现出了显著的多功能性。然而，将 LLMs 应用于像金融这样复杂、高风险的领域需要严格的评估，以确保可靠性、准确性和符合行业标准。为了满足这一需求，我们对三个最先进的 LLMs，即 GLM-4、Mistral-NeMo 和 LLaMA3.1 进行了全面和比较研究，重点关注它们在生成自动化财务报告方面的有效性。我们的主要动机是探索如何在金融领域利用这些模型，这个领域需要精确性、上下文相关性和对错误或误导性信息的稳健性。通过检查每个模型的能力，我们旨在对它们的优势和局限性提供有洞察力的评估。我们的论文为财务报告分析提供了基准，包括提出的指标，如 ROUGE-1、BERT 分数和 LLM 分数。我们引入了一个创新的评估框架，它整合了定量指标（例如，精度、召回率）和定性分析（例如，上下文适应性、一致性），以提供每个模型输出质量的整体视图。此外，我们将我们的财务数据集公开可用，邀请研究人员和从业者通过更广泛的社区参与和协作改进来利用、审查和增强我们的发现。我们的数据集可在 huggingface 上获取。

> In recent years, Large Language Models (LLMs) have demonstrated remarkable versatility across various applications, including natural language understanding, domain-specific knowledge tasks, etc. However, applying LLMs to complex, high-stakes domains like finance requires rigorous evaluation to ensure reliability, accuracy, and compliance with industry standards. To address this need, we conduct a comprehensive and comparative study on three state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their effectiveness in generating automated financial reports. Our primary motivation is to explore how these models can be harnessed within finance, a field demanding precision, contextual relevance, and robustness against erroneous or misleading information. By examining each model's capabilities, we aim to provide an insightful assessment of their strengths and limitations. Our paper offers benchmarks for financial report analysis, encompassing proposed metrics such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative evaluation framework that integrates both quantitative metrics (e.g., precision, recall) and qualitative analyses (e.g., contextual fit, consistency) to provide a holistic view of each model's output quality. Additionally, we make our financial dataset publicly available, inviting researchers and practitioners to leverage, scrutinize, and enhance our findings through broader community engagement and collaborative improvement. Our dataset is available on huggingface.

[Arxiv](https://arxiv.org/abs/2411.06852)