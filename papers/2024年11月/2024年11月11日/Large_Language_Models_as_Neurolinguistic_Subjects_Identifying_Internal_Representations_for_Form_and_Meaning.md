# 大型语言模型作为神经语言学研究对象：明确形式与意义的内部表征

发布时间：2024年11月11日

`LLM理论` `语言学` `语言模型评估`

> Large Language Models as Neurolinguistic Subjects: Identifying Internal Representations for Form and Meaning

# 摘要

> 本研究通过区分心理语言学和神经语言学这两种LLM评估范式，对大型语言模型（LLMs）在能指（形式）和所指（意义）方面的语言理解展开探究。传统的心理语言学评估通常会反映出可能误表LLMs真实语言能力的统计偏差。我们引入了一种神经语言学方法，运用将最小对和诊断探测相结合的全新方法来剖析模型层的激活模式。此方法能够细致研究LLMs怎样表示形式和意义，以及这些表示在不同语言中是否一致。我们有三方面的贡献：（1）对神经语言学和心理语言学方法进行比较，揭示LLM评估中的不同模式；（2）证明LLMs在形式方面的能力高于意义方面，且后者在很大程度上与前者相关；（3）为中文（COMPS-ZH）和德语（COMPS-DE）提供新的概念性最小对数据集，对现有的英语数据集加以补充。

> This study investigates the linguistic understanding of Large Language Models (LLMs) regarding signifier (form) and signified (meaning) by distinguishing two LLM evaluation paradigms: psycholinguistic and neurolinguistic. Traditional psycholinguistic evaluations often reflect statistical biases that may misrepresent LLMs' true linguistic capabilities. We introduce a neurolinguistic approach, utilizing a novel method that combines minimal pair and diagnostic probing to analyze activation patterns across model layers. This method allows for a detailed examination of how LLMs represent form and meaning, and whether these representations are consistent across languages. Our contributions are three-fold: (1) We compare neurolinguistic and psycholinguistic methods, revealing distinct patterns in LLM assessment; (2) We demonstrate that LLMs exhibit higher competence in form compared to meaning, with the latter largely correlated to the former; (3) We present new conceptual minimal pair datasets for Chinese (COMPS-ZH) and German (COMPS-DE), complementing existing English datasets.

[Arxiv](https://arxiv.org/abs/2411.07533)