# LLMs 能否理解文本中的歧义？以开放世界问答为例展开研究

发布时间：2024年11月19日

`LLM应用` `问答系统`

> Do LLMs Understand Ambiguity in Text? A Case Study in Open-world Question Answering

# 摘要

> 自然语言中的歧义给用于开放领域问答的大型语言模型（LLMs）带来了严峻挑战。LLMs 常常在人类交流固有的不确定性面前举步维艰，从而引发误解、沟通障碍、幻觉以及有偏差的回答。这严重削弱了它们在事实核查、问答、特征提取和情感分析等任务中的应用能力。以开放领域问答为例，我们对现成的和少样本的 LLM 性能进行了比较，重点衡量明确消歧策略的影响。我们展示了简单、无需训练的标记级消歧方法如何能有效提升 LLM 在歧义问答任务中的表现。我们通过实践经验呈现了研究发现，并探讨了有关 LLMs 中歧义的最佳实践和更广泛的影响。

> Ambiguity in natural language poses significant challenges to Large Language Models (LLMs) used for open-domain question answering. LLMs often struggle with the inherent uncertainties of human communication, leading to misinterpretations, miscommunications, hallucinations, and biased responses. This significantly weakens their ability to be used for tasks like fact-checking, question answering, feature extraction, and sentiment analysis. Using open-domain question answering as a test case, we compare off-the-shelf and few-shot LLM performance, focusing on measuring the impact of explicit disambiguation strategies. We demonstrate how simple, training-free, token-level disambiguation methods may be effectively used to improve LLM performance for ambiguous question answering tasks. We empirically show our findings and discuss best practices and broader impacts regarding ambiguity in LLMs.

[Arxiv](https://arxiv.org/abs/2411.12395)