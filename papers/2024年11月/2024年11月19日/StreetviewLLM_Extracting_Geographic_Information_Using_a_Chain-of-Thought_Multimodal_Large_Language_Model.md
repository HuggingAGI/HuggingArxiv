# StreetviewLLM：借助思维链多模态大型语言模型来提取地理信息

发布时间：2024年11月19日

`LLM应用` `城市规划` `灾害管理`

> StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model

# 摘要

> 地理空间预测在灾害管理、城市规划和公共卫生等众多领域都极为关键。传统机器学习方法在处理像街景图像这样的非结构化或多模态数据时，常常遭遇限制。为解决这些难题，我们推出了 StreetViewLLM，这是一个把大型语言模型与思维链推理及多模态数据源相融合的全新框架。通过将街景图像与地理坐标和文本数据相结合，StreetViewLLM 提升了地理空间预测的精度与粒度。借助检索增强生成技术，我们的方法强化了地理信息提取，能够对城市环境展开细致分析。该模型已在包括香港、东京、新加坡、洛杉矶、纽约、伦敦和巴黎在内的七个全球城市得到应用，在预测城市指标（如人口密度、医疗保健可达性、归一化植被指数、建筑物高度和不透水表面）方面展现出卓越性能。结果显示，StreetViewLLM 始终优于基线模型，提供了更优的预测精准度，并对建筑环境有更深刻的洞察。此项研究为将大型语言模型融入城市分析、城市规划决策、基础设施管理和环境监测开辟了新契机。

> Geospatial predictions are crucial for diverse fields such as disaster management, urban planning, and public health. Traditional machine learning methods often face limitations when handling unstructured or multi-modal data like street view imagery. To address these challenges, we propose StreetViewLLM, a novel framework that integrates a large language model with the chain-of-thought reasoning and multimodal data sources. By combining street view imagery with geographic coordinates and textual data, StreetViewLLM improves the precision and granularity of geospatial predictions. Using retrieval-augmented generation techniques, our approach enhances geographic information extraction, enabling a detailed analysis of urban environments. The model has been applied to seven global cities, including Hong Kong, Tokyo, Singapore, Los Angeles, New York, London, and Paris, demonstrating superior performance in predicting urban indicators, including population density, accessibility to healthcare, normalized difference vegetation index, building height, and impervious surface. The results show that StreetViewLLM consistently outperforms baseline models, offering improved predictive accuracy and deeper insights into the built environment. This research opens new opportunities for integrating the large language model into urban analytics, decision-making in urban planning, infrastructure management, and environmental monitoring.

[Arxiv](https://arxiv.org/abs/2411.14476)