# Smart-LLaMA：用于智能合约漏洞检测和解释的大型语言模型的两阶段后训练

发布时间：2024年11月09日

`LLM应用` `区块链` `智能合约`

> Smart-LLaMA: Two-Stage Post-Training of Large Language Models for Smart Contract Vulnerability Detection and Explanation

# 摘要

> 随着区块链技术的快速发展，智能合约安全已成为一个关键挑战。现有的智能合约漏洞检测方法面临三个主要问题：（1）数据集质量不足，缺乏详细的解释和精确的漏洞位置。（2）大型语言模型（LLM）对智能合约领域的适应性有限，因为大多数LLM是在一般文本数据上进行预训练的，但针对智能合约的特定数据极少。（3）对于检测到的漏洞缺乏高质量的解释，因为现有方法仅专注于检测而没有清晰的解释。这些限制阻碍了检测性能，并使开发人员更难快速理解和修复漏洞，可能导致严重的财务损失。为了解决这些问题，我们提出了Smart-LLaMA，一种基于LLaMA语言模型的先进检测方法。首先，我们构建了一个涵盖四种漏洞类型的综合数据集，带有标签、详细解释和精确的漏洞位置。其次，我们引入了智能合约特定的持续预训练，使用原始智能合约数据使LLM能够学习智能合约的语法和语义，增强其领域适应性。此外，我们提出了解释引导的微调，使用成对的漏洞代码和解释对LLM进行微调，实现漏洞检测和合理的解释。我们通过LLM和人工评估来评估解释质量，重点关注正确性、完整性和简洁性。实验结果表明，Smart-LLaMA优于最先进的基线，F1分数平均提高了6.49％，准确率平均提高了3.78％，同时提供了可靠的解释。

> With the rapid development of blockchain technology, smart contract security has become a critical challenge. Existing smart contract vulnerability detection methods face three main issues: (1) Insufficient quality of datasets, lacking detailed explanations and precise vulnerability locations. (2) Limited adaptability of large language models (LLMs) to the smart contract domain, as most LLMs are pre-trained on general text data but minimal smart contract-specific data. (3) Lack of high-quality explanations for detected vulnerabilities, as existing methods focus solely on detection without clear explanations. These limitations hinder detection performance and make it harder for developers to understand and fix vulnerabilities quickly, potentially leading to severe financial losses. To address these problems, we propose Smart-LLaMA, an advanced detection method based on the LLaMA language model. First, we construct a comprehensive dataset covering four vulnerability types with labels, detailed explanations, and precise vulnerability locations. Second, we introduce Smart Contract-Specific Continual Pre-Training, using raw smart contract data to enable the LLM to learn smart contract syntax and semantics, enhancing their domain adaptability. Furthermore, we propose Explanation-Guided Fine-Tuning, which fine-tunes the LLM using paired vulnerable code and explanations, enabling both vulnerability detection and reasoned explanations. We evaluate explanation quality through LLM and human evaluation, focusing on Correctness, Completeness, and Conciseness. Experimental results show that Smart-LLaMA outperforms state-of-the-art baselines, with average improvements of 6.49% in F1 score and 3.78% in accuracy, while providing reliable explanations.

[Arxiv](https://arxiv.org/abs/2411.06221)