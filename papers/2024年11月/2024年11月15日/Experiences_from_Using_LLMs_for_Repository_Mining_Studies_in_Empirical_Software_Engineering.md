# 在经验软件工程领域运用大型语言模型开展存储库挖掘研究的经验

发布时间：2024年11月15日

`LLM应用` `软件工程` `软件库挖掘`

> Experiences from Using LLMs for Repository Mining Studies in Empirical Software Engineering

# 摘要

> 上下文：大型语言模型（LLMs）的出现为分析软件库提供了创新手段，极大地改变了软件工程（SE）。目标：我们旨在为未来从事软件工程研究的人员构建一个实用框架，以便他们在运用 LLMs 开展软件库挖掘研究时，能够强化数据收集和数据集。方法：此经验报告分享了此前两项库挖掘研究的心得，着重于创建、优化和验证能提升 LLMs 输出的提示的方法，尤其是在实证研究中的数据收集方面。结果：我们的研究整合出一个框架，名为挖掘实证软件库的提示优化与见解（PRIMES），其中包含一份检查表，通过迭代流程以及不同 LLMs 之间的对比，能够提升 LLM 的使用性能、优化输出质量并减少错误。我们还通过实施追踪模型结果的机制，强调了可重复性的重要意义。结论：我们的研究结果表明，规范提示工程并使用 PRIMES 能够增强运用 LLMs 开展研究的可靠性和可重复性。最终，此项工作呼吁开展进一步研究，以应对将 LLMs 融入工作流时出现的幻觉、模型偏差和成本效益等挑战。

> Context: The emergence of Large Language Models (LLMs) has significantly transformed Software Engineering (SE) by providing innovative methods for analyzing software repositories. Objectives: Our objective is to establish a practical framework for future SE researchers needing to enhance the data collection and dataset while conducting software repository mining studies using LLMs. Method: This experience report shares insights from two previous repository mining studies, focusing on the methodologies used for creating, refining, and validating prompts that enhance the output of LLMs, particularly in the context of data collection in empirical studies. Results: Our research packages a framework, coined Prompt Refinement and Insights for Mining Empirical Software repositories (PRIMES), consisting of a checklist that can improve LLM usage performance, enhance output quality, and minimize errors through iterative processes and comparisons among different LLMs. We also emphasize the significance of reproducibility by implementing mechanisms for tracking model results. Conclusion: Our findings indicate that standardizing prompt engineering and using PRIMES can enhance the reliability and reproducibility of studies utilizing LLMs. Ultimately, this work calls for further research to address challenges like hallucinations, model biases, and cost-effectiveness in integrating LLMs into workflows.

[Arxiv](https://arxiv.org/abs/2411.09974)