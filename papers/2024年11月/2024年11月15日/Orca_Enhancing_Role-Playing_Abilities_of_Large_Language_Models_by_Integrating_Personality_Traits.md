# Orca：借由整合个性特征提升大型语言模型的角色扮演本领

发布时间：2024年11月15日

`LLM应用` `对话系统` `社交平台`

> Orca: Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits

# 摘要

> 大型语言模型推动了个性化对话系统的发展，众多角色扮演对话代理纷纷涌现。此前的研究主要致力于通过设计角色配置文件来增强模型遵循指令的能力，却忽视了驱动人类对话的心理因素。在本文中，我们提出了 Orca 这一框架，用于通过融合个性特征来进行数据处理和训练定制角色的大型语言模型。Orca 涵盖四个阶段：（1）个性特征推断，借助大型语言模型推断用户的大五人格特征报告及分数。（2）数据增强，模拟用户的个人资料、背景故事和心理活动。（3）数据集构建，采用基于个性条件的指令提示（PCIP）来激发大型语言模型。（4）建模与训练，进行基于个性条件的指令调整（PTIT 和 PSIT），利用生成的数据增强现有的开源大型语言模型。我们引入了 OrcaBench，这是首个用于评估大型语言模型在多规模社交平台上生成内容质量的基准。我们的实验表明，我们所提出的模型在该基准上表现出色，展现出其在感知个性特征方面的卓越性和有效性，显著提升了角色扮演能力。我们的代码可在 https://github.com/Aipura/Orca 获取。

> Large language models has catalyzed the development of personalized dialogue systems, numerous role-playing conversational agents have emerged. While previous research predominantly focused on enhancing the model's capability to follow instructions by designing character profiles, neglecting the psychological factors that drive human conversations. In this paper, we propose Orca, a framework for data processing and training LLMs of custom characters by integrating personality traits. Orca comprises four stages: (1) Personality traits inferring, leverage LLMs to infer user's BigFive personality trait reports and scores. (2) Data Augment, simulate user's profile, background story, and psychological activities. (3) Dataset construction, personality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4) Modeling and Training, personality-conditioned instruction tuning (PTIT and PSIT), using the generated data to enhance existing open-source LLMs. We introduce OrcaBench, the first benchmark for evaluating the quality of content generated by LLMs on social platforms across multiple scales. Our experiments demonstrate that our proposed model achieves superior performance on this benchmark, demonstrating its excellence and effectiveness in perceiving personality traits that significantly improve role-playing abilities. Our Code is available at https://github.com/Aipura/Orca.

[Arxiv](https://arxiv.org/abs/2411.10006)