# 静态网络结构难以稳固大型语言模型代理间的合作

发布时间：2024年11月15日

`LLM应用` `社会科学` `人工智能`

> Static network structure cannot stabilize cooperation among Large Language Model agents

# 摘要

> 大型语言模型（LLMs）愈发用于人类社会行为的建模，近期研究探索了其模拟社会动态的能力。在此，我们检验在社会困境（个人与集体利益冲突时）中，LLMs 是否能映照人类行为。在实验室环境里，人类通常比预想的更合作，在充分混合的人群中合作较少，在固定网络中合作较多。相较而言，LLMs 在充分混合的环境里往往合作更甚。这引出关键问题：LLMs 会否在网络合作困境中效仿人类行为？本研究考察了网络化互动，其中代理在充分混合和结构化的网络配置中反复陷入“囚徒困境”，旨在找出 LLMs 与人类合作行为的相似点。研究结果揭示了关键差异：人类在结构化网络中通常更合作，而 LLMs 主要在充分混合环境中合作增多，对网络环境的适应有限。值得注意的是，不同模型类型的 LLM 合作情况也有别，体现了复制类人社会适应能力的复杂性。这些结果凸显出关键差距：LLMs 难以仿效人类在固定网络中运用的精妙、适应型社会策略。和人类参与者不同，LLMs 不会依据网络结构或变化的社会背景改变合作行为，缺失人类适应性采用的互惠规范。这一局限指明了未来 LLM 设计的根本需求——融入对社会规范的更深刻理解，从而在网络环境中实现对类人合作和适应性更逼真的建模。

> Large language models (LLMs) are increasingly used to model human social behavior, with recent research exploring their ability to simulate social dynamics. Here, we test whether LLMs mirror human behavior in social dilemmas, where individual and collective interests conflict. Humans generally cooperate more than expected in laboratory settings, showing less cooperation in well-mixed populations but more in fixed networks. In contrast, LLMs tend to exhibit greater cooperation in well-mixed settings. This raises a key question: Are LLMs about to emulate human behavior in cooperative dilemmas on networks? In this study, we examine networked interactions where agents repeatedly engage in the Prisoner's Dilemma within both well-mixed and structured network configurations, aiming to identify parallels in cooperative behavior between LLMs and humans. Our findings indicate critical distinctions: while humans tend to cooperate more within structured networks, LLMs display increased cooperation mainly in well-mixed environments, with limited adjustment to networked contexts. Notably, LLM cooperation also varies across model types, illustrating the complexities of replicating human-like social adaptability in artificial agents. These results highlight a crucial gap: LLMs struggle to emulate the nuanced, adaptive social strategies humans deploy in fixed networks. Unlike human participants, LLMs do not alter their cooperative behavior in response to network structures or evolving social contexts, missing the reciprocity norms that humans adaptively employ. This limitation points to a fundamental need in future LLM design -- to integrate a deeper comprehension of social norms, enabling more authentic modeling of human-like cooperation and adaptability in networked environments.

[Arxiv](https://arxiv.org/abs/2411.10294)