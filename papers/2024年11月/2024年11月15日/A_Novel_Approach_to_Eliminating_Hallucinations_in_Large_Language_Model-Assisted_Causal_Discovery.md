# 一种用于消除大型语言模型辅助因果发现中幻觉现象的新颖方法

发布时间：2024年11月15日

`LLM应用` `因果发现` `语言模型`

> A Novel Approach to Eliminating Hallucinations in Large Language Model-Assisted Causal Discovery

# 摘要

> 大型语言模型（LLMs）在因果发现中愈发多地替代人类领域专家，这凸显出选择最优模型的必要性。本文首次对用于因果发现的热门LLMs的幻觉情况展开调查。我们发现，在因果发现中运用LLMs时存在幻觉，所以LLM的选择至关重要。我们提议，在有优质数据可用时，采用检索增强生成（RAG）来降低幻觉。另外，我们引入了一种新办法，即在辩论中使用多个配有仲裁者的LLMs来审查因果图中的边，达到了与RAG相当的幻觉减少成效。

> The increasing use of large language models (LLMs) in causal discovery as a substitute for human domain experts highlights the need for optimal model selection. This paper presents the first hallucination survey of popular LLMs for causal discovery. We show that hallucinations exist when using LLMs in causal discovery so the choice of LLM is important. We propose using Retrieval Augmented Generation (RAG) to reduce hallucinations when quality data is available. Additionally, we introduce a novel method employing multiple LLMs with an arbiter in a debate to audit edges in causal graphs, achieving a comparable reduction in hallucinations to RAG.

[Arxiv](https://arxiv.org/abs/2411.12759)