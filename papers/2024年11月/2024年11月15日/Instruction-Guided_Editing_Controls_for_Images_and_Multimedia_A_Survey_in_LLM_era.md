# 《指令引导的图像和多媒体编辑控制：LLM 时代的调研》

发布时间：2024年11月15日

`LLM应用` `视觉编辑` `多模态学习`

> Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era

# 摘要

> 大型语言模型（LLMs）与多模态学习的迅猛发展，变革了数字内容的创作与处理方式。传统视觉编辑工具对专业知识要求颇高，限制了其普及程度。而基于指令的编辑技术的最新突破，让人们能够以自然语言为桥梁，在用户意图和复杂编辑操作之间实现直观交互，从而与视觉内容进行互动。本次调研对这些技术进行了综述，重点探讨了LLMs和多模态模型如何助力用户在无需深厚技术知识的情况下，实现精准的视觉修改。通过整合100多篇文献，我们探索了从生成对抗网络到扩散模型的诸多方法，研究了用于细粒度内容控制的多模态集成。我们论述了其在时尚、3D场景操控和视频合成等领域的实际应用，凸显了可访问性的提升以及与人类直觉的契合。我们的调研对现有文献进行了比较，着重强调了LLM赋能的编辑，并指出了关键挑战，以推动进一步的研究。我们致力于让强大的视觉编辑在从娱乐到教育的各行各业中得到广泛应用。感兴趣的读者可访问我们的存储库https://github.com/tamlhp/awesome-instruction-editing。

> The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.

[Arxiv](https://arxiv.org/abs/2411.09955)