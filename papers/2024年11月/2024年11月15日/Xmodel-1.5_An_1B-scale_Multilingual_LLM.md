# Xmodel-1.5：一款 10 亿规模的多语言大语言模型

发布时间：2024年11月15日

`LLM应用` `多语言研究`

> Xmodel-1.5: An 1B-scale Multilingual LLM

# 摘要

> 我们推出了 Xmodel-1.5，这是一款全新的 10 亿参数多语言大型模型，在约 2 万亿个标记上完成了预训练。此模型在多种语言中表现强劲，在泰语、阿拉伯语和法语方面尤为突出，在中文和英语方面也颇具成效。另外，我们为研究界贡献了一个泰语评估数据集，其中包含数百个由朱拉隆功大学综合创新学院学生标注的问题。尽管成果可喜，但我们也深知仍有提升空间。我们期望这项工作能助力多语言人工智能研究的不断推进，并在各类自然语言处理任务中增进更好的跨语言理解。我们的模型和代码在 GitHub 上公开可获取，链接为 https://github.com/XiaoduoAILab/XmodelLM。

> We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model pretrained on approximately 2 trillion tokens. The model demonstrates strong performance across several languages, with particularly notable results in Thai, Arabic, and French, alongside its effectiveness in Chinese and English. In addition, we contribute to the research community by releasing a Thai evaluation dataset, which includes hundreds of questions annotated by students from Chulalongkorn University's School of Integrated Innovation. While the results are promising, we acknowledge that there is still room for improvement. We hope this work advances ongoing efforts in multilingual AI research and promotes better cross-linguistic understanding in various natural language processing tasks. Our models and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelLM.

[Arxiv](https://arxiv.org/abs/2411.10083)