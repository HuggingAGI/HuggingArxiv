# 一个助力大型语言模型应对涉及数字的长上下文任务的有效框架

发布时间：2024年11月15日

`LLM应用` `数值计算`

> An Effective Framework to Help Large Language Models Handle Numeric-involved Long-context Tasks

# 摘要

> 大型语言模型（LLMs）处理长文本的能力十分出色，在传统检索任务中表现近乎完美。然而，在长上下文的数值计算上，其表现大幅下滑。由于当前 LLMs 本身存在局限，难以同时处理复杂且海量的信息，所以在常规设置下，涉及数字的长上下文任务通常难以解决。像思维链（CoT）这类提示方法虽能提高准确性，但需要大量输出标记，既费钱又耗时。为应对此问题，我们提出了一种工作流程，将涉及数字的长上下文任务分解为 4 个低层级子任务：判断、提取、用代码处理以及得出结论。前两个子任务相对简单，因此我们可以使用较小的模型来高效处理长上下文。当需要进行数值计算时，我们使用 LLMs 生成的代码，以规避 LLM 不擅计算的短板。在 2 个涉及数字的长上下文基准测试中的结果显示，我们的工作流程不仅能提升准确性，还能大幅降低 API 调用的成本。

> Large Language Models (LLMs) have demonstrated remarkable capabilities in handling long texts and have almost perfect performance in traditional retrieval tasks. However, their performance significantly degrades when it comes to numerical calculations in the long-context. Numeric-involved long-context tasks typically cannot be addressed by current LLMs in normal settings due to their inherent limitations in simultaneously handling complex and massive information. Some CoT like prompting methods can improve accuracy but demands massive output tokens, which is costly and slow. To address this issue, we propose a workflow, which decompose a numeric-involved long-context task into 4 low-level subtasks: judging, extracting and processing with code and conclusion. The former 2 subtasks is relatively simple, which allows us to use smaller models for efficiently processing long context. When numerical calculations are required, we use code generated by LLMs to avoid the disadvantage of LLM not being good at calculations. The results in 2 numeric-involved long-context benchmarks demonstrate our workflow can not only improve accuracy, but also significantly reduce the cost of API calls.

[Arxiv](https://arxiv.org/abs/2411.10145)