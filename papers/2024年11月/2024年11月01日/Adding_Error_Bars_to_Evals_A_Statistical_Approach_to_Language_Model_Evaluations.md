# 为评估添加误差条：语言模型评估的一种统计学方法

发布时间：2024年11月01日

`LLM理论` `语言模型` `统计学`

> Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations

# 摘要

> 评估对于了解大型语言模型（LLMs）的能力极为关键。本质上，评估就是实验；然而，有关评估的文献很大程度上忽视了其他科学领域中关于实验分析和规划的内容。本文为接受过一定统计学训练的研究人员展示了应如何思考和分析语言模型评估的数据。把评估问题设想为从一个未知的超级总体中抽取，我们给出了分析评估数据、衡量两个模型的差异以及规划评估实验的公式。我们针对开展语言模型评估以及以能最小化统计噪声和最大化信息量的方式报告实验结果，提出了若干具体建议。

> Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness.

[Arxiv](https://arxiv.org/abs/2411.00640)