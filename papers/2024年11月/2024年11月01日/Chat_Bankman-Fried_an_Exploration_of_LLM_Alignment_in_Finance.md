# Chat Bankman-Fried：探索金融领域中的 LLM 对齐

发布时间：2024年11月01日

`LLM应用` `人工智能`

> Chat Bankman-Fried: an Exploration of LLM Alignment in Finance

# 摘要

> 大型语言模型（LLMs）的发展再度引发了对人工智能对齐问题的关注，也就是人类与人工智能的目标和价值观的一致性问题。随着各个司法管辖区针对人工智能安全制定立法，对齐这一概念在不同领域都得加以定义和衡量。本文提出了一个实验框架，用于评估LLMs在相对未被充分探索的金融情境中是否遵循道德和法律标准。我们让九个LLMs扮演金融机构的首席执行官，测试他们是否愿意挪用客户资产来偿还公司未偿债务。从基准配置出发，我们调整偏好、激励和约束条件，并通过逻辑回归分析每次调整的影响。我们的发现表明，LLMs在不道德行为的基准倾向方面存在显著差异。像风险规避、利润预期和监管环境等因素，始终以经济理论所预测的方式影响着不对齐情况，不过这些影响的程度在不同的LLMs中有所不同。本文突出了基于模拟的事后安全测试的优点和局限性。虽然它能为致力于确保LLMs安全的金融当局和机构提供参考，但在通用性和成本之间存在明显的权衡。

> Advancements in large language models (LLMs) have renewed concerns about AI alignment - the consistency between human and AI goals and values. As various jurisdictions enact legislation on AI safety, the concept of alignment must be defined and measured across different domains. This paper proposes an experimental framework to assess whether LLMs adhere to ethical and legal standards in the relatively unexplored context of finance. We prompt nine LLMs to impersonate the CEO of a financial institution and test their willingness to misuse customer assets to repay outstanding corporate debt. Beginning with a baseline configuration, we adjust preferences, incentives and constraints, analyzing the impact of each adjustment with logistic regression. Our findings reveal significant heterogeneity in the baseline propensity for unethical behavior of LLMs. Factors such as risk aversion, profit expectations, and regulatory environment consistently influence misalignment in ways predicted by economic theory, although the magnitude of these effects varies across LLMs. This paper highlights both the benefits and limitations of simulation-based, ex post safety testing. While it can inform financial authorities and institutions aiming to ensure LLM safety, there is a clear trade-off between generality and cost.

[Arxiv](https://arxiv.org/abs/2411.11853)