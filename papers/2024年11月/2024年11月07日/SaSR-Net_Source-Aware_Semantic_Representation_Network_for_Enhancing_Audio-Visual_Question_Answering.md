# SaSR-Net：用于增强视听问答的源感知语义表示网络

发布时间：2024年11月07日

`其他` `音频-视觉` `问答系统`

> SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering

# 摘要

> 音频-视觉问答（AVQA）是一项具有挑战性的任务，涉及根据视频中的听觉和视觉信息回答问题。一个重大的挑战是解释复杂的多模态场景，其中包括视觉对象和声源，并将它们与给定的问题联系起来。在本文中，我们介绍了源感知语义表示网络（SaSR-Net），这是一种为 AVQA 设计的新型模型。SaSR-Net 利用源方向可学习的标记来有效地捕获和对齐音频-视觉元素与相应的问题。它使用空间和时间注意力机制简化了音频和视觉信息的融合，以在多模态场景中识别答案。在 Music-AVQA 和 AVQA-Yang 数据集上进行的大量实验表明，SaSR-Net 优于最先进的 AVQA 方法。

> Audio-Visual Question Answering (AVQA) is a challenging task that involves answering questions based on both auditory and visual information in videos. A significant challenge is interpreting complex multi-modal scenes, which include both visual objects and sound sources, and connecting them to the given question. In this paper, we introduce the Source-aware Semantic Representation Network (SaSR-Net), a novel model designed for AVQA. SaSR-Net utilizes source-wise learnable tokens to efficiently capture and align audio-visual elements with the corresponding question. It streamlines the fusion of audio and visual information using spatial and temporal attention mechanisms to identify answers in multi-modal scenes. Extensive experiments on the Music-AVQA and AVQA-Yang datasets show that SaSR-Net outperforms state-of-the-art AVQA methods.

[Arxiv](https://arxiv.org/abs/2411.04933)