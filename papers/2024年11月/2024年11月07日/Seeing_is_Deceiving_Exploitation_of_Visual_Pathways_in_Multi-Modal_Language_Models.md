# “看到即欺骗”：多模态语言模型中视觉通路的运用

发布时间：2024年11月07日

`LLM应用` `医疗保健` `多模态人工智能`

> Seeing is Deceiving: Exploitation of Visual Pathways in Multi-Modal Language Models

# 摘要

> 多模态语言模型（MLLMs）将视觉与文本数据相结合，改变了人工智能的发展态势，让图像字幕生成、视觉问答、多模态内容创作等应用得以实现。其处理复杂信息的能力，在医疗保健、自治系统、数字内容等领域大显身手。然而，多种类型数据的融合也带来了安全隐患。攻击者能够操纵视觉或文本输入，甚至双管齐下，致使模型给出意料之外甚至有害的回应。本文探讨了 MLLMs 中的视觉输入是如何被各类攻击策略所利用的。我们把这些攻击分为几类：简单的视觉微调与跨模态操纵，以及诸如 VLATTACK、HADES 和协同多模态对抗攻击（Co-Attack）之类的高级策略。这些攻击即便与原始视觉几乎无异，也能误导最为强大的模型，且难以察觉。我们还论述了更广泛的安全风险，涵盖重要应用中对隐私和安全的威胁。为应对这些风险，我们回顾了当下的防御手段，如 SmoothVLM 框架、像素级随机化和 MirrorCheck，并分析了它们的优缺点。我们还探讨了让 MLLMs 更安全的新方法，包括自适应防御、更优的评估工具以及能同时保护视觉和文本数据的安全策略。通过整合近期的进展并明确关键的改进方向，本综述旨在助力创建更安全、更可靠的多模态人工智能系统，以投入实际应用。

> Multi-Modal Language Models (MLLMs) have transformed artificial intelligence by combining visual and text data, making applications like image captioning, visual question answering, and multi-modal content creation possible. This ability to understand and work with complex information has made MLLMs useful in areas such as healthcare, autonomous systems, and digital content. However, integrating multiple types of data also creates security risks. Attackers can manipulate either the visual or text inputs, or both, to make the model produce unintended or even harmful responses. This paper reviews how visual inputs in MLLMs can be exploited by various attack strategies. We break down these attacks into categories: simple visual tweaks and cross-modal manipulations, as well as advanced strategies like VLATTACK, HADES, and Collaborative Multimodal Adversarial Attack (Co-Attack). These attacks can mislead even the most robust models while looking nearly identical to the original visuals, making them hard to detect. We also discuss the broader security risks, including threats to privacy and safety in important applications. To counter these risks, we review current defense methods like the SmoothVLM framework, pixel-wise randomization, and MirrorCheck, looking at their strengths and limitations. We also discuss new methods to make MLLMs more secure, including adaptive defenses, better evaluation tools, and security approaches that protect both visual and text data. By bringing together recent developments and identifying key areas for improvement, this review aims to support the creation of more secure and reliable multi-modal AI systems for real-world use.

[Arxiv](https://arxiv.org/abs/2411.05056)