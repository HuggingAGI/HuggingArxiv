# 事实还是虚构？大型语言模型能否成为政治真相的可靠标注者？

发布时间：2024年11月08日

`LLM应用`

> Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?

# 摘要

> 政治错误信息对民主进程构成重大挑战，影响公众舆论和对媒体的信任。手动事实核查方法面临可扩展性和标注员偏差的问题，而机器学习模型需要大量昂贵的标注数据集。本研究调查了使用最先进的大型语言模型（LLM）作为可靠的标注员来检测新闻文章中的政治真实性。使用开源的 LLM，我们创建了一个政治多样化的数据集，通过 LLM 生成的标注进行偏差标注。这些标注由人类专家验证，并由基于 LLM 的评委进一步评估，以评估标注的准确性和可靠性。我们的方法为传统的事实核查提供了一种可扩展且强大的替代方案，增强了媒体的透明度和公众信任。

> Political misinformation poses significant challenges to democratic processes, shaping public opinion and trust in media. Manual fact-checking methods face issues of scalability and annotator bias, while machine learning models require large, costly labelled datasets. This study investigates the use of state-of-the-art large language models (LLMs) as reliable annotators for detecting political factuality in news articles. Using open-source LLMs, we create a politically diverse dataset, labelled for bias through LLM-generated annotations. These annotations are validated by human experts and further evaluated by LLM-based judges to assess the accuracy and reliability of the annotations. Our approach offers a scalable and robust alternative to traditional fact-checking, enhancing transparency and public trust in media.

[Arxiv](https://arxiv.org/abs/2411.05775)