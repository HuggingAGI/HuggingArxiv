# 对大型语言模型的分布对齐进行基准测试

发布时间：2024年11月08日

`LLM应用` `语言模型` `模拟系统`

> Benchmarking Distributional Alignment of Large Language Models

# 摘要

> 语言模型（LMs）越来越多地被用作人类的模拟物，然而它们匹配特定人口群体观点分布并实现	extit{分布对齐}的能力仍然不确定。这种分布对齐的概念是复杂的，因为所模拟的属性类型存在显著差异。先前的工作对三个关键变量——问题领域、引导方法和分布表达方法——的作用研究不足，这促使我们贡献了一个明确针对这些维度的基准。我们构建了一个超越政治价值观的数据集，为这项任务创建了人类基线，并评估了语言模型与特定群体意见分布对齐的程度，以告知此类模拟系统的设计选择。我们的分析揭示了关于语言模型是否以及如何用于模拟人类的未解决问题，并且大型语言模型（LLMs）能够更准确地描述意见分布，而非模拟此类分布。

> Language models (LMs) are increasingly used as simulacra for people, yet their ability to match the distribution of views of a specific demographic group and be \textit{distributionally aligned} remains uncertain. This notion of distributional alignment is complex, as there is significant variation in the types of attributes that are simulated. Prior works have underexplored the role of three critical variables -- the question domain, steering method, and distribution expression method -- which motivates our contribution of a benchmark explicitly addressing these dimensions. We construct a dataset expanding beyond political values, create human baselines for this task, and evaluate the extent to which an LM can align with a particular group's opinion distribution to inform design choices of such simulation systems. Our analysis reveals open problems regarding if, and how, LMs can be used to simulate humans, and that LLMs can more accurately describe the opinion distribution than simulate such distributions.

[Arxiv](https://arxiv.org/abs/2411.05403)