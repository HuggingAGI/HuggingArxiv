# 测量 ChatGPT 参与的语言游戏中的胡言乱语情况

发布时间：2024年11月22日

`LLM理论` `语言研究` `社会科学`

> Measuring Bullshit in the Language Games played by ChatGPT

# 摘要

> 生成式大型语言模型（LLMs）生成的文本与真值无直接对应，这被普遍认为类似法兰克福热门专著《论胡说》中所描述的语言运用。本文对这一主题展开了严谨的探究，明确了此现象的产生方式及分析方法。文中我们详细阐述了这一观点，提出基于LLM的聊天机器人在玩“胡说的语言游戏”。我们通过统计文本分析来探究这种维特根斯坦式语言游戏的特征，基于构建的数据集，将1000篇科学出版物的语言与ChatGPT生成的典型伪科学文本作对比。接着，我们探讨在乔治·奥威尔对政治和语言的批判以及大卫·格雷伯对胡说工作的描述这两个著名的社会功能失调情境中，是否能检测到相同的语言特征。运用简单的假设检验方法，我们证明了胡说语言的统计模型能够可靠地将ChatGPT的法兰克福式人造胡说与自然人类语言中观察到的胡说的政治和工作场所功能关联起来。

> Generative large language models (LLMs), which create text without direct correspondence to truth value, are widely understood to resemble the uses of language described in Frankfurt's popular monograph On Bullshit. In this paper, we offer a rigorous investigation of this topic, identifying how the phenomenon has arisen, and how it might be analysed. In this paper, we elaborate on this argument to propose that LLM-based chatbots play the 'language game of bullshit'. We use statistical text analysis to investigate the features of this Wittgensteinian language game, based on a dataset constructed to contrast the language of 1,000 scientific publications with typical pseudo-scientific text generated by ChatGPT. We then explore whether the same language features can be detected in two well-known contexts of social dysfunction: George Orwell's critique of politics and language, and David Graeber's characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a statistical model of the language of bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language.

[Arxiv](https://arxiv.org/abs/2411.15129)