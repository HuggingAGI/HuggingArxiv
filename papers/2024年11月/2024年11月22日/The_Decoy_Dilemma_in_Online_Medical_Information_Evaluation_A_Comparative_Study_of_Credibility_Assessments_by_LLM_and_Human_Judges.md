# 在线医疗信息评估中的“诱饵困境”：关于 LLM 和人类评委可信度评估的比较研究

发布时间：2024年11月22日

`LLM应用` `信息检索` `人工智能`

> The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges

# 摘要

> 在自动化信息判断任务中，AI 会有认知偏差吗？尽管在测量和减轻 AI 及大型语言模型（LLMs）中的社会和算法偏差方面近来有所进展，但 LLMs 到底在多大程度上表现“理性”，或者它们是否也易受人类认知偏差触发因素影响，这还不清楚。为解决此开放问题，我们的研究涵盖一个众包用户实验和一个启用 LLM 的模拟实验，在信息检索（IR）场景的潜在诱饵效应下，比较了 LLM 与人类评判者的可信度评估，并通过实验考查了 LLMs 在 COVID-19 医疗（错误）信息评估任务中相较于传统人类评估者（作为基线）在认知上的偏差程度。从受试者间用户实验和启用 LLM 的复制实验所收集的结果显示：1）更大且更新的 LLMs 在区分可信信息与错误信息时往往呈现出更高的一致性和准确性。然而，由于存在更显著的诱饵错误信息结果，它们更可能给错误信息更高的评级；2）虽然人类和 LLM 评估中都存在诱饵效应，但与人类的可信度评级相比，该效应在 LLM 判断的不同条件和主题中更为常见。与通常所假定的 AI 工具的“理性”不同，我们的研究通过实证确认了嵌入在 LLM 代理中的认知偏差风险，评估了诱饵对 LLMs 与人类可信度评估的影响，进而凸显了为自动化判断任务及其他任务消除 AI 代理偏差以及开发基于心理学的 AI 审计技术和政策的复杂性与重要性。

> Can AI be cognitively biased in automated information judgment tasks? Despite recent progresses in measuring and mitigating social and algorithmic biases in AI and large language models (LLMs), it is not clear to what extent LLMs behave "rationally", or if they are also vulnerable to human cognitive bias triggers. To address this open problem, our study, consisting of a crowdsourcing user experiment and a LLM-enabled simulation experiment, compared the credibility assessments by LLM and human judges under potential decoy effects in an information retrieval (IR) setting, and empirically examined the extent to which LLMs are cognitively biased in COVID-19 medical (mis)information assessment tasks compared to traditional human assessors as a baseline. The results, collected from a between-subject user experiment and a LLM-enabled replicate experiment, demonstrate that 1) Larger and more recent LLMs tend to show a higher level of consistency and accuracy in distinguishing credible information from misinformation. However, they are more likely to give higher ratings for misinformation due to the presence of a more salient, decoy misinformation result; 2) While decoy effect occurred in both human and LLM assessments, the effect is more prevalent across different conditions and topics in LLM judgments compared to human credibility ratings. In contrast to the generally assumed "rationality" of AI tools, our study empirically confirms the cognitive bias risks embedded in LLM agents, evaluates the decoy impact on LLMs against human credibility assessments, and thereby highlights the complexity and importance of debiasing AI agents and developing psychology-informed AI audit techniques and policies for automated judgment tasks and beyond.

[Arxiv](https://arxiv.org/abs/2411.15396)