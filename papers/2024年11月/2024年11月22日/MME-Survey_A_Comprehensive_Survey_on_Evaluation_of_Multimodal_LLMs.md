# MME-Survey：有关多模态 LLM 评估的全面调研

发布时间：2024年11月22日

`LLM应用` `人工智能` `模型评估`

> MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs

# 摘要

> 作为通用人工智能（AGI）的一个显著方向，多模态大型语言模型（MLLMs）在工业界和学术界备受关注。基于预训练的大型语言模型（LLMs），这一系列模型进一步具备了令人瞩目的多模态感知与推理能力，比如依据流程图编写代码或者基于图像创作故事。在开发进程中，评估极为关键，因其能为模型的改进提供直观的反馈与指引。和仅适用于像图像分类这类单一任务的传统训练-评估-测试范式不同，MLLMs 的多功能特性催生了各种新的基准和评估方法。在本文里，我们意在对 MLLM 评估展开全面调研，探讨四个关键要点：1）按评估能力划分的汇总基准类型，涵盖基础能力、模型自我分析和扩展应用；2）基准构建的典型流程，包含数据收集、标注和注意事项；3）由评判、指标和工具包构成的系统评估方式；4）下一个基准的展望。此工作旨在让研究人员轻松知晓如何依据不同需求有效评估 MLLMs，并催生更优的评估方法，进而推动 MLLM 研究的进步。

> As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia. Building upon pre-trained LLMs, this family of models further develops multimodal perception and reasoning capabilities that are impressive, such as writing code given a flow chart or creating stories based on an image. In the development process, evaluation is critical since it provides intuitive feedback and guidance on improving models. Distinct from the traditional train-eval-test paradigm that only favors a single task like image classification, the versatility of MLLMs has spurred the rise of various new benchmarks and evaluation methods. In this paper, we aim to present a comprehensive survey of MLLM evaluation, discussing four key aspects: 1) the summarised benchmarks types divided by the evaluation capabilities, including foundation capabilities, model self-analysis, and extented applications; 2) the typical process of benchmark counstruction, consisting of data collection, annotation, and precautions; 3) the systematic evaluation manner composed of judge, metric, and toolkit; 4) the outlook for the next benchmark. This work aims to offer researchers an easy grasp of how to effectively evaluate MLLMs according to different needs and to inspire better evaluation methods, thereby driving the progress of MLLM research.

[Arxiv](https://arxiv.org/abs/2411.15296)