# 合成查询变体与生成式大型语言模型的数据融合

发布时间：2024年11月06日

`LLM应用` `信息检索` `数据融合`

> Data Fusion of Synthetic Query Variants With Generative Large Language Models

# 摘要

> 在信息检索（IR）实验中考虑查询方差对检索效果是有益的。特别是基于不同主题相关查询的排名集成比基于单个查询的排名能检索到更好的结果。最近，生成式指令调整的大型语言模型（LLMs）在捕捉人类语言的各种不同任务上有所改进。为此，这项工作探索了在数据融合实验中使用由指令调整的 LLMs 生成的合成查询变体的可行性。更具体地说，我们引入了一种轻量级、无监督且成本效益高的方法，该方法利用了原则性的提示和数据融合技术。在我们的实验中，当为 LLMs 提供关于主题的额外上下文信息时，它们会生成更有效的查询。此外，我们基于四个 TREC 新闻专线基准的分析表明，基于合成查询变体的数据融合明显优于具有单个查询的基线，并且也优于伪相关反馈方法。我们向社区公开分享代码和查询数据集，作为后续研究的资源。

> Considering query variance in information retrieval (IR) experiments is beneficial for retrieval effectiveness. Especially ranking ensembles based on different topically related queries retrieve better results than rankings based on a single query alone. Recently, generative instruction-tuned Large Language Models (LLMs) improved on a variety of different tasks in capturing human language. To this end, this work explores the feasibility of using synthetic query variants generated by instruction-tuned LLMs in data fusion experiments. More specifically, we introduce a lightweight, unsupervised, and cost-efficient approach that exploits principled prompting and data fusion techniques. In our experiments, LLMs produce more effective queries when provided with additional context information on the topic. Furthermore, our analysis based on four TREC newswire benchmarks shows that data fusion based on synthetic query variants is significantly better than baselines with single queries and also outperforms pseudo-relevance feedback methods. We publicly share the code and query datasets with the community as resources for follow-up studies.

[Arxiv](https://arxiv.org/abs/2411.03881)