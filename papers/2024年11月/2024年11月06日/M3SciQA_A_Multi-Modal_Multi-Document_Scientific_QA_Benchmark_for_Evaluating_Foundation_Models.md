# M3SciQA：用于评估基础模型的多模态多文档科学问答基准

发布时间：2024年11月06日

`LLM应用` `科学文献`

> M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models

# 摘要

> 现有的用于评估基础模型的基准主要集中在单文档、纯文本任务上。然而，它们往往不能完全捕捉研究工作流程的复杂性，这些工作流程通常涉及解释非文本数据和跨多个文档收集信息。为了解决这一差距，我们引入了 M3SciQA，这是一个多模态、多文档的科学问答基准，旨在对基础模型进行更全面的评估。M3SciQA 由 1452 个专家注释的问题组成，涵盖 70 个自然语言处理论文集群，每个集群代表一篇主要论文及其所有引用的文档，通过要求多模态和多文档数据来反映理解一篇论文的工作流程。利用 M3SciQA，我们对 18 个基础模型进行了全面评估。我们的结果表明，与人类专家相比，当前的基础模型在多模态信息检索和跨多个科学文档的推理方面仍表现不佳。此外，我们探讨了这些发现对未来在多模态科学文献分析中应用基础模型的发展的影响。

> Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address this gap, we introduce M3SciQA, a multi-modal, multi-document scientific question answering benchmark designed for a more comprehensive evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, where each cluster represents a primary paper along with all its cited documents, mirroring the workflow of comprehending a single paper by requiring multi-modal and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of 18 foundation models. Our results indicate that current foundation models still significantly underperform compared to human experts in multi-modal information retrieval and in reasoning across multiple scientific documents. Additionally, we explore the implications of these findings for the future advancement of applying foundation models in multi-modal scientific literature analysis.

[Arxiv](https://arxiv.org/abs/2411.04075)