# 大型语言模型的训练数据

发布时间：2024年11月12日

`LLM应用` `人工智能` `数据集`

> Training Data for Large Language Model

# 摘要

> 在 2022 年，随着 ChatGPT 的发布，大规模语言模型受到了广泛关注。ChatGPT 不仅在参数和预训练语料库的规模方面超过了以前的模型，而且通过在大量高质量的、人工标注的数据上进行微调实现了革命性的性能提升。这一进展使企业和研究机构认识到，构建更智能、更强大的模型依赖于丰富和高质量的数据集。因此，数据集的构建和优化已成为人工智能领域的关键焦点。本文总结了用于训练大规模语言模型的预训练和微调数据的现状，涵盖了数据规模、收集方法、数据类型和特征、处理工作流程等方面，并提供了可用的开源数据集的概述。

> In 2022, with the release of ChatGPT, large-scale language models gained widespread attention. ChatGPT not only surpassed previous models in terms of parameters and the scale of its pretraining corpus but also achieved revolutionary performance improvements through fine-tuning on a vast amount of high-quality, human-annotated data. This progress has led enterprises and research institutions to recognize that building smarter and more powerful models relies on rich and high-quality datasets. Consequently, the construction and optimization of datasets have become a critical focus in the field of artificial intelligence. This paper summarizes the current state of pretraining and fine-tuning data for training large-scale language models, covering aspects such as data scale, collection methods, data types and characteristics, processing workflows, and provides an overview of available open-source datasets.

[Arxiv](https://arxiv.org/abs/2411.07715)