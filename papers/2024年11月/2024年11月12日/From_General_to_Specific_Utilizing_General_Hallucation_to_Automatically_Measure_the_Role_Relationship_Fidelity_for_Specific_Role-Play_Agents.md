# 从一般到具体：利用一般幻觉来自动测量特定角色扮演代理的角色关系保真度

发布时间：2024年11月12日

`LLM应用` `语言模型` `知识图谱`

> From General to Specific: Utilizing General Hallucation to Automatically Measure the Role Relationship Fidelity for Specific Role-Play Agents

# 摘要

> 大型语言模型（LLMs）的先进角色扮演能力为开发角色扮演代理（RPAs）铺平了道路。然而，现有的基准，如 HPD，它将手动评分的角色关系纳入 LLMs 的上下文中以对连贯性进行排序，以及 SocialBench，它在多项选择任务的上下文中使用 LLMs 生成的特定配置文件来评估角色偏好，面临着诸如泛化能力差、判断隐含和不准确以及上下文长度过长等限制。为了解决上述问题，我们提出了一种自动、可扩展和通用的范例。具体来说，我们通过从通用知识图谱中提取关系来构建基准，并利用 RPA 的固有幻觉特性促使其跨角色交互，使用 ChatGPT 进行立场检测，并定义关系幻觉以及三个相关指标。大量实验验证了我们指标的有效性和稳定性。我们的研究结果进一步探讨了影响这些指标的因素，并讨论了关系幻觉和真实性之间的权衡。

> The advanced role-playing capabilities of Large Language Models (LLMs) have paved the way for developing Role-Playing Agents (RPAs). However, existing benchmarks, such as HPD, which incorporates manually scored character relationships into the context for LLMs to sort coherence, and SocialBench, which uses specific profiles generated by LLMs in the context of multiple-choice tasks to assess character preferences, face limitations like poor generalizability, implicit and inaccurate judgments, and excessive context length. To address the above issues, we propose an automatic, scalable, and generalizable paradigm. Specifically, we construct a benchmark by extracting relations from a general knowledge graph and leverage RPA's inherent hallucination properties to prompt it to interact across roles, employing ChatGPT for stance detection and defining relationship hallucination along with three related metrics. Extensive experiments validate the effectiveness and stability of our metrics. Our findings further explore factors influencing these metrics and discuss the trade-off between relationship hallucination and factuality.

[Arxiv](https://arxiv.org/abs/2411.07965)