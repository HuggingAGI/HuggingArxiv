# Howzat？通过呼吁专家判断来评估针对新手程序员的人类和人工智能的下一步提示

发布时间：2024年11月27日

`LLM应用`

> Howzat? Appealing to Expert Judgement for Evaluating Human and AI Next-Step Hints for Novice Programmers

# 摘要

> 动机：学生在学习编程时，常常会陷入停滞不前的状态。自动生成的下一步提示能助力他们继续前进，促进学习。弄清楚什么样的提示是好的，什么样的是坏的，以及如何在新手编程工具中（比如借助大型语言模型（LLMs））自动生成优质提示，这至关重要。
  方法与参与者：我们从全球招募了 44 位 Java 教育工作者参与在线研究。以一组真实的学生代码状态作为提示生成的场景。参与者运用比较判断的技术，对由大型语言模型（LLMs）和五位经验丰富的人类教育工作者生成的一系列候选下一步 Java 提示进行排序。参与者在不知提示生成方式的情况下对其进行排名。
  发现：我们发现，LLMs 在为编程新手生成高质量下一步提示方面差异较大，GPT-4 表现优于其他测试模型。当搭配精心设计的提示时，GPT-4 在生成具有教学价值的提示方面胜过人类专家。多阶段提示是最有效的 LLM 提示。我们发现好提示的两个关键因素是长度（80 - 160 个单词为宜）和阅读水平（美国 9 年级或更低为佳）。提供解决问题的替代方法被视为不佳，且未发现情感的影响。
  结论：鉴于 LLMs 表现优于人类——即便学生的任务未知，自动生成这些提示也是立即可行的。只有最优的提示才能达成这一结果，这意味着学生自己不太可能产生同样的效果。因此，提示任务应嵌入专家设计的工具之中。

> Motivation: Students learning to program often reach states where they are stuck and can make no forward progress. An automatically generated next-step hint can help them make forward progress and support their learning. It is important to know what makes a good hint or a bad hint, and how to generate good hints automatically in novice programming tools, for example using Large Language Models (LLMs).
  Method and participants: We recruited 44 Java educators from around the world to participate in an online study. We used a set of real student code states as hint-generation scenarios. Participants used a technique known as comparative judgement to rank a set of candidate next-step Java hints, which were generated by Large Language Models (LLMs) and by five human experienced educators. Participants ranked the hints without being told how they were generated.
  Findings: We found that LLMs had considerable variation in generating high quality next-step hints for programming novices, with GPT-4 outperforming other models tested. When used with a well-designed prompt, GPT-4 outperformed human experts in generating pedagogically valuable hints. A multi-stage prompt was the most effective LLM prompt. We found that the two most important factors of a good hint were length (80--160 words being best), and reading level (US grade 9 or below being best). Offering alternative approaches to solving the problem was considered bad, and we found no effect of sentiment.
  Conclusions: Automatic generation of these hints is immediately viable, given that LLMs outperformed humans -- even when the students' task is unknown. The fact that only the best prompts achieve this outcome suggests that students on their own are unlikely to be able to produce the same benefit. The prompting task, therefore, should be embedded in an expert-designed tool.

[Arxiv](https://arxiv.org/abs/2411.18151)