# 我的会议总结好不好？借助多语言模型评估器来评估其质量

发布时间：2024年11月27日

`LLM应用` `会议摘要`

> Is my Meeting Summary Good? Estimating Quality with a Multi-LLM Evaluator

# 摘要

> 自然语言生成（NLG）系统生成的会议摘要质量难以自动衡量。像 ROUGE 和 BERTScore 这类既定指标与人类判断的相关性较低，难以捕捉细微错误。近期研究表明，大型语言模型（LLM）无需基于大量人类偏好判断的训练，就具备更好的上下文理解和错误定义适应能力。不过，当前基于 LLM 的评估器可能掩盖错误，只能充当较弱的替代方案，因此，尽管人类评估成本高且难以在不同研究中比较，但仍是金标准。在本项工作中，我们推出了 MESA，这是一个基于 LLM 的框架，它采用了对单个错误类型的三步评估、多智能体讨论以优化决策，以及基于反馈的自我训练，从而优化对错误定义的理解并与人类判断保持一致。我们发现，MESA 的组件能够进行彻底的错误检测、给出一致的评级，并适应自定义错误指南。以 GPT-4o 为核心，MESA 在错误检测方面与人类判断实现了中等到较高的点二列相关，在反映错误对摘要质量的影响方面实现了中等的斯皮尔曼和肯德尔相关，平均比以往方法高出 0.25。该框架适应自定义错误指南的灵活性使其适用于人类标记数据有限的各类任务。

> The quality of meeting summaries generated by natural language generation (NLG) systems is hard to measure automatically. Established metrics such as ROUGE and BERTScore have a relatively low correlation with human judgments and fail to capture nuanced errors. Recent studies suggest using large language models (LLMs), which have the benefit of better context understanding and adaption of error definitions without training on a large number of human preference judgments. However, current LLM-based evaluators risk masking errors and can only serve as a weak proxy, leaving human evaluation the gold standard despite being costly and hard to compare across studies. In this work, we present MESA, an LLM-based framework employing a three-step assessment of individual error types, multi-agent discussion for decision refinement, and feedback-based self-training to refine error definition understanding and alignment with human judgment. We show that MESA's components enable thorough error detection, consistent rating, and adaptability to custom error guidelines. Using GPT-4o as its backbone, MESA achieves mid to high Point-Biserial correlation with human judgment in error detection and mid Spearman and Kendall correlation in reflecting error impact on summary quality, on average 0.25 higher than previous methods. The framework's flexibility in adapting to custom error guidelines makes it suitable for various tasks with limited human-labeled data.

[Arxiv](https://arxiv.org/abs/2411.18444)