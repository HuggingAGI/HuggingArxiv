# Safe + Safe = Unsafe？探究安全图像怎样被用于破解大型视觉语言模型

发布时间：2024年11月18日

`Agent` `多模态` `安全防护`

> Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models

# 摘要

> 近期，大型视觉语言模型（LVLMs）取得了显著进展，在多模态场景中展现出强大的推理能力，在各类现实应用中实现重大突破。尽管成果斐然，LVLMs 的安全防护却可能难以覆盖视觉模态带来的未知领域。现有研究多侧重于通过精心设计基于图像的越狱手段，诱导 LVLMs 生成有害内容，以绕过对齐防御。本研究发现，当与其他安全图像和提示相结合时，一张安全图像也能被用以达成相同的越狱效果。这归因于 LVLMs 的两个基本特性：通用推理能力和安全滚雪球效应。基于此，我们提出了安全滚雪球代理（SSA），这是一种创新的基于代理的框架，借助代理的自主和工具使用能力来突破 LVLMs 的限制。SSA 主要通过两个阶段运行：（1）初始响应生成，工具依据潜在有害意图生成或检索越狱图像；（2）有害滚雪球，经优化的后续提示逐步诱导有害输出。我们的实验表明，\ours 几乎能用任何图像诱导 LVLMs 生成不安全内容，对最新的 LVLMs 有着很高的越狱成功率。与以往利用对齐缺陷的工作不同，\ours 利用了 LVLMs 的固有属性，给生成多模态系统的安全性保障带来了严峻挑战。我们的代码可在 url{https://github.com/gzcch/Safety_Snowball_Agent}获取。

> Recent advances in Large Vision-Language Models (LVLMs) have showcased strong reasoning abilities across multiple modalities, achieving significant breakthroughs in various real-world applications. Despite this great success, the safety guardrail of LVLMs may not cover the unforeseen domains introduced by the visual modality. Existing studies primarily focus on eliciting LVLMs to generate harmful responses via carefully crafted image-based jailbreaks designed to bypass alignment defenses. In this study, we reveal that a safe image can be exploited to achieve the same jailbreak consequence when combined with additional safe images and prompts. This stems from two fundamental properties of LVLMs: universal reasoning capabilities and safety snowball effect. Building on these insights, we propose Safety Snowball Agent (SSA), a novel agent-based framework leveraging agents' autonomous and tool-using abilities to jailbreak LVLMs. SSA operates through two principal stages: (1) initial response generation, where tools generate or retrieve jailbreak images based on potential harmful intents, and (2) harmful snowballing, where refined subsequent prompts induce progressively harmful outputs. Our experiments demonstrate that \ours can use nearly any image to induce LVLMs to produce unsafe content, achieving high success jailbreaking rates against the latest LVLMs. Unlike prior works that exploit alignment flaws, \ours leverages the inherent properties of LVLMs, presenting a profound challenge for enforcing safety in generative multimodal systems. Our code is avaliable at url{https://github.com/gzcch/Safety_Snowball_Agent}.

[Arxiv](https://arxiv.org/abs/2411.11496)