# 关于医学视觉和语言应用及其技术的一项调研

发布时间：2024年11月18日

`LLM应用`

> A Survey of Medical Vision-and-Language Applications and Their Techniques

# 摘要

> 医疗视觉和语言模型（MVLMs）因能为解读复杂医疗数据提供自然语言界面而备受关注。其应用广泛，既能提升个体患者的诊断精准度和决策水平，又能通过更高效地分析大数据集，助力强化公共卫生监测、疾病监测及政策制定。MVLMs将自然语言处理与医疗图像相融合，能对医疗图像及其对应的文本信息实现更全面且基于上下文的理解。不同于在各类非专业数据集上训练的普通视觉和语言模型，MVLMs专为医疗领域打造，可自动从医疗图像和文本报告中提取并解读关键信息，以支持临床决策。MVLMs热门的临床应用涵盖自动生成医疗报告、医疗视觉问答、医疗多模态分割、诊断与预后以及医疗图像-文本检索。在此，我们对MVLMs及其所应用的各类医疗任务予以全面综述。我们针对各种视觉和语言模型架构展开了细致分析，着重关注其在跨模态整合/利用医疗视觉和文本特征方面的独特策略。我们还探究了用于这些任务的数据集，并依据标准化评估指标比较了不同模型的性能。另外，我们突出了潜在挑战，并总结了未来的研究趋势和方向。完整的论文和代码集合可在：https://github.com/YtongXie/Medical-Vision-and-Language-Tasks-and-Methodologies-A-Survey 获取。

> Medical vision-and-language models (MVLMs) have attracted substantial interest due to their capability to offer a natural language interface for interpreting complex medical data. Their applications are versatile and have the potential to improve diagnostic accuracy and decision-making for individual patients while also contributing to enhanced public health monitoring, disease surveillance, and policy-making through more efficient analysis of large data sets. MVLMS integrate natural language processing with medical images to enable a more comprehensive and contextual understanding of medical images alongside their corresponding textual information. Unlike general vision-and-language models trained on diverse, non-specialized datasets, MVLMs are purpose-built for the medical domain, automatically extracting and interpreting critical information from medical images and textual reports to support clinical decision-making. Popular clinical applications of MVLMs include automated medical report generation, medical visual question answering, medical multimodal segmentation, diagnosis and prognosis and medical image-text retrieval. Here, we provide a comprehensive overview of MVLMs and the various medical tasks to which they have been applied. We conduct a detailed analysis of various vision-and-language model architectures, focusing on their distinct strategies for cross-modal integration/exploitation of medical visual and textual features. We also examine the datasets used for these tasks and compare the performance of different models based on standardized evaluation metrics. Furthermore, we highlight potential challenges and summarize future research trends and directions. The full collection of papers and codes is available at: https://github.com/YtongXie/Medical-Vision-and-Language-Tasks-and-Methodologies-A-Survey.

[Arxiv](https://arxiv.org/abs/2411.12195)