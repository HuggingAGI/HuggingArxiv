# 针对长上下文大型语言模型的会员资格推理攻击

发布时间：2024年11月18日

`LLM应用` `隐私保护` `问答系统`

> Membership Inference Attack against Long-Context Large Language Models

# 摘要

> 近期，大型语言模型（LLMs）取得了进展，得以克服其上下文窗口的限制，在更长的上下文中展现出出色的检索和推理能力。搭载长上下文语言模型（LCLMs）的问答系统能够自动搜索海量外部数据并融入其上下文，实现精准预测，减少诸如幻觉和知识陈旧等问题。现有的针对 LCLMs 的研究主要聚焦于解决所谓的中间丢失问题或提升推理效率，其隐私风险却基本未被探究。本文旨在填补这一空白，指出将所有信息整合至长上下文会使之成为敏感信息的库藏，当中常包含诸如医疗记录或个人身份之类的私人数据。我们进一步探究了 LCLMs 外部上下文中的成员隐私，旨在判定给定的文档或序列是否在 LCLMs 的上下文中。我们的基本思路是，若一份文档处于上下文中，它会呈现出低生成损失或与 LCLMs 生成的内容有高度语义相似性。我们首次为 LCLMs 提出了六种成员推理攻击（MIA）策略，并在各类流行模型上开展了大量实验。实证结果表明，在多数情况下，我们的攻击能够精准推断成员身份，比如在具有 LongChat-7b-v1.5-32k 的多文档问答数据集中，攻击 F1 分数达 90.66％，凸显了 LCLMs 输入上下文中成员信息泄露的重大风险。此外，我们还探讨了 LCLMs 容易泄露此类成员信息的根本缘由。

> Recent advances in Large Language Models (LLMs) have enabled them to overcome their context window limitations, and demonstrate exceptional retrieval and reasoning capacities on longer context. Quesion-answering systems augmented with Long-Context Language Models (LCLMs) can automatically search massive external data and incorporate it into their contexts, enabling faithful predictions and reducing issues such as hallucinations and knowledge staleness. Existing studies targeting LCLMs mainly concentrate on addressing the so-called lost-in-the-middle problem or improving the inference effiencicy, leaving their privacy risks largely unexplored. In this paper, we aim to bridge this gap and argue that integrating all information into the long context makes it a repository of sensitive information, which often contains private data such as medical records or personal identities. We further investigate the membership privacy within LCLMs external context, with the aim of determining whether a given document or sequence is included in the LCLMs context. Our basic idea is that if a document lies in the context, it will exhibit a low generation loss or a high degree of semantic similarity to the contents generated by LCLMs. We for the first time propose six membership inference attack (MIA) strategies tailored for LCLMs and conduct extensive experiments on various popular models. Empirical results demonstrate that our attacks can accurately infer membership status in most cases, e.g., 90.66% attack F1-score on Multi-document QA datasets with LongChat-7b-v1.5-32k, highlighting significant risks of membership leakage within LCLMs input contexts. Furthermore, we examine the underlying reasons why LCLMs are susceptible to revealing such membership information.

[Arxiv](https://arxiv.org/abs/2411.11424)