# 借助 MLLM 嵌入与属性平滑来实现组合式零样本学习

发布时间：2024年11月18日

`LLM应用` `计算机视觉` `零样本学习`

> Leveraging MLLM Embeddings and Attribute Smoothing for Compositional Zero-Shot Learning

# 摘要

> 组合式零样本学习（CZSL）旨在识别从已见组合中学到的属性和对象的新组合。以往的研究通过提取具有相同属性（对象）的图像对之间的共享和独有部分，并将其与预训练的词嵌入对齐，以提升对未见属性-对象的识别能力，从而实现属性和对象的分离。尽管已有成果显著，但仍存在三个局限：（1）受背景影响以及同一部分中属性与对象的复杂交织，分离效果不佳。（2）现有的词嵌入难以捕捉复杂的多模态语义信息。（3）现有模型在已见组合中表现出的过度自信，阻碍了其对新组合的泛化能力。鉴于此，我们为 CZSL 提出了一个新的框架，名为多模态大型语言模型（MLLM）嵌入和属性平滑引导解缠（TRIDENT）。首先，我们借助特征自适应聚合模块来降低背景的影响，并使用可学习的条件掩码来捕获多粒度特征以实现解缠。其次，MLLM 的最后隐藏状态因其出色的表示能力而被用作词嵌入。此外，针对已见组合，我们提出了由大型语言模型（LLM）生成辅助属性的属性平滑方法，通过鼓励模型在给定组合中学习更多属性，解决过度自信的问题。大量实验表明，TRIDENT 在三个基准测试中达到了最先进的性能。

> Compositional zero-shot learning (CZSL) aims to recognize novel compositions of attributes and objects learned from seen compositions. Previous works disentangle attribute and object by extracting shared and exclusive parts between image pairs sharing the same attribute (object), as well as aligning them with pretrained word embeddings to improve unseen attribute-object recognition. Despite the significant achievements of existing efforts, they are hampered by three limitations: (1) the efficacy of disentanglement is compromised due to the influence of the background and the intricate entanglement of attribute with object in the same parts. (2) existing word embeddings fail to capture complex multimodal semantic information. (3) overconfidence exhibited by existing models in seen compositions hinders their generalization to novel compositions. Being aware of these, we propose a novel framework named Multimodal Large Language Model (MLLM) embeddings and attribute smoothing guided disentanglement (TRIDENT) for CZSL. First, we leverage feature adaptive aggregation modules to mitigate the impact of background, and utilize learnable condition masks to capture multigranularity features for disentanglement. Then, the last hidden states of MLLM are employed as word embeddings for their superior representation capabilities. Moreover, we propose attribute smoothing with auxiliary attributes generated by Large Language Model (LLM) for seen compositions, addressing the issue of overconfidence by encouraging the model to learn more attributes in one given composition. Extensive experiments demonstrate that TRIDENT achieves state-of-the-art performance on three benchmarks.

[Arxiv](https://arxiv.org/abs/2411.12584)