# 在大型语言模型中的道德劝说：评估其敏感性与伦理一致性

发布时间：2024年11月18日

`LLM应用` `语言模型`

> Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment

# 摘要

> 我们探索如何通过提示大型语言模型（LLMs）来改变其初始决策，使之与既定的道德框架相符。我们的研究基于两项实验，旨在评估LLMs对道德劝服的敏感度。在第一项实验里，我们在道德模糊的情境中评估基础代理LLM，并观察劝说代理怎样试图更改基础代理的初始决策，以此检验其对道德模糊的敏感度。第二项实验通过提示LLMs采纳基于既定哲学理论的特定价值取向，来评估它们与预设道德框架一致的敏感性。结果显示，在有道德因素的场景中，LLMs确实能够被劝服，劝服的成功取决于像所用模型、场景复杂程度以及对话长度之类的因素。特别要指出的是，来自同一家公司但规模不同的LLMs产生了显著不同的结果，凸显了它们对道德劝服敏感度的差异。

> We explore how large language models (LLMs) can be influenced by prompting them to alter their initial decisions and align them with established ethical frameworks. Our study is based on two experiments designed to assess the susceptibility of LLMs to moral persuasion. In the first experiment, we examine the susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally ambiguous scenarios and observing how a Persuader Agent attempts to modify the Base Agent's initial decisions. The second experiment evaluates the susceptibility of LLMs to align with predefined ethical frameworks by prompting them to adopt specific value alignments rooted in established philosophical theories. The results demonstrate that LLMs can indeed be persuaded in morally charged scenarios, with the success of persuasion depending on factors such as the model used, the complexity of the scenario, and the conversation length. Notably, LLMs of distinct sizes but from the same company produced markedly different outcomes, highlighting the variability in their susceptibility to ethical persuasion.

[Arxiv](https://arxiv.org/abs/2411.11731)