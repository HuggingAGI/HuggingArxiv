# 跨越语言的藩篱：借助大型语言模型实现低资源语言的翻译

发布时间：2024年11月18日

`LLM应用` `语言翻译` `低资源语言`

> Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation

# 摘要

> 大型语言模型（LLMs）在众多任务和领域中成绩斐然。然而，它们在低资源语言翻译，尤其是译成此类语言方面的表现，尚未得到充分研究。这一缺口带来了严峻挑战，因为语言障碍阻碍了少数群体社区的文化保存与发展。为解决此问题，本文推出一种新颖的基于检索的方法，通过聚焦关键术语来提升低资源语言的翻译质量，包括翻译关键词并从现有数据中检索相应示例。为评估该方法的有效性，我们开展了从英语到三种低资源语言的实验：切罗基语，北美一种濒危至极的土著语言；藏语，亚洲具有重要历史文化意义的语言；满语，使用者稀少的语言。我们将其与 GPT-4o 和 LLaMA 3.1 405B 的零样本性能作比较，凸显了这些模型在译成低资源语言时遭遇的重大难题。相较而言，我们基于检索的方法有望通过更高效地利用现有资源，提高单词层面的准确性和整体语义理解。

> Large Language Models (LLMs) have demonstrated remarkable success across a wide range of tasks and domains. However, their performance in low-resource language translation, particularly when translating into these languages, remains underexplored. This gap poses significant challenges, as linguistic barriers hinder the cultural preservation and development of minority communities. To address this issue, this paper introduces a novel retrieval-based method that enhances translation quality for low-resource languages by focusing on key terms, which involves translating keywords and retrieving corresponding examples from existing data. To evaluate the effectiveness of this method, we conducted experiments translating from English into three low-resource languages: Cherokee, a critically endangered indigenous language of North America; Tibetan, a historically and culturally significant language in Asia; and Manchu, a language with few remaining speakers. Our comparison with the zero-shot performance of GPT-4o and LLaMA 3.1 405B, highlights the significant challenges these models face when translating into low-resource languages. In contrast, our retrieval-based method shows promise in improving both word-level accuracy and overall semantic understanding by leveraging existing resources more effectively.

[Arxiv](https://arxiv.org/abs/2411.11295)