# InstruGen：借助大型多模态模型为视觉与语言导航自动生成指令

发布时间：2024年11月18日

`Agent` `多模态模型`

> InstruGen: Automatic Instruction Generation for Vision-and-Language Navigation Via Large Multimodal Models

# 摘要

> 近期关于视觉和语言导航（VLN）的研究显示，由于缺少真实的训练环境和优质的路径-指令对，智能体在陌生环境中的泛化能力欠佳。现有的多数构建真实导航场景的方法成本颇高，且指令的拓展主要依赖预设模板或规则，适应性不足。为解决此问题，我们提出了InstruGen，这是一种VLN路径-指令对的生成模式。具体而言，我们把YouTube房屋游览视频当作真实的导航场景，借助大型多模态模型（LMMs）强大的视觉理解与生成能力，自动生成多样且高质量的VLN路径-指令对。我们的方法能生成不同粒度的导航指令，实现指令与视觉观察之间的精细对齐，这是以往方法难以做到的。另外，我们还设计了一个多阶段验证机制，以减少LMMs的幻觉和不一致情况。实验结果表明，用InstruGen生成的路径-指令对训练的智能体在R2R和RxR基准测试中表现出色，尤其是在陌生环境中。代码可在https://github.com/yanyu0526/InstruGen获取。

> Recent research on Vision-and-Language Navigation (VLN) indicates that agents suffer from poor generalization in unseen environments due to the lack of realistic training environments and high-quality path-instruction pairs. Most existing methods for constructing realistic navigation scenes have high costs, and the extension of instructions mainly relies on predefined templates or rules, lacking adaptability. To alleviate the issue, we propose InstruGen, a VLN path-instruction pairs generation paradigm. Specifically, we use YouTube house tour videos as realistic navigation scenes and leverage the powerful visual understanding and generation abilities of large multimodal models (LMMs) to automatically generate diverse and high-quality VLN path-instruction pairs. Our method generates navigation instructions with different granularities and achieves fine-grained alignment between instructions and visual observations, which was difficult to achieve with previous methods. Additionally, we design a multi-stage verification mechanism to reduce hallucinations and inconsistency of LMMs. Experimental results demonstrate that agents trained with path-instruction pairs generated by InstruGen achieves state-of-the-art performance on the R2R and RxR benchmarks, particularly in unseen environments. Code is available at https://github.com/yanyu0526/InstruGen.

[Arxiv](https://arxiv.org/abs/2411.11394)