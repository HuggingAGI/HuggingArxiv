# 《信任的阴暗面：由权威引用引发的大型语言模型越狱式攻击》

发布时间：2024年11月18日

`LLM应用` `语言模型` `安全防护`

> The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models

# 摘要

> 大型语言模型（LLMs）在众多领域的广泛应用，既展现出了其巨大潜力，又暴露出了严重的安全漏洞。其中一个关键问题在于，要保证 LLM 生成的内容符合人类价值观。现有的越狱技术表明，通过特定提示或对抗性后缀，会破坏这种一致性。本研究引入了新的威胁：LLMs 对权威的偏向。这种固有偏向虽能提升 LLM 的输出质量，却也带来潜在漏洞，加大了产生有害内容的风险。值得注意的是，LLMs 的偏差体现为在有害查询中对不同类型权威信息的信任程度各异。比如，恶意软件开发往往更信赖 GitHub。为更清晰地揭示 LLM 的风险，我们推出了 DarkCite，这是一款针对黑盒环境的自适应权威引文匹配器和生成器。DarkCite 能将最优引文类型与特定风险类型匹配，并生成与有害指令相关的权威引文，从而对已对齐的 LLM 实施更有效的越狱攻击。实验表明，DarkCite 的攻击成功率更高（如 LLama-2 达到 76%，而之前为 68%）。为应对此风险，我们提出了真实性和危害验证防御策略，将平均防御通过率（DPR）从 11%提升至 74%。更重要的是，将引文与所涵盖内容相链接的能力已成为 LLM 的一项基础功能，放大了 LLM 对权威的偏向所产生的影响。

> The widespread deployment of large language models (LLMs) across various domains has showcased their immense potential while exposing significant safety vulnerabilities. A major concern is ensuring that LLM-generated content aligns with human values. Existing jailbreak techniques reveal how this alignment can be compromised through specific prompts or adversarial suffixes. In this study, we introduce a new threat: LLMs' bias toward authority. While this inherent bias can improve the quality of outputs generated by LLMs, it also introduces a potential vulnerability, increasing the risk of producing harmful content. Notably, the biases in LLMs is the varying levels of trust given to different types of authoritative information in harmful queries. For example, malware development often favors trust GitHub. To better reveal the risks with LLM, we propose DarkCite, an adaptive authority citation matcher and generator designed for a black-box setting. DarkCite matches optimal citation types to specific risk types and generates authoritative citations relevant to harmful instructions, enabling more effective jailbreak attacks on aligned LLMs.Our experiments show that DarkCite achieves a higher attack success rate (e.g., LLama-2 at 76% versus 68%) than previous methods. To counter this risk, we propose an authenticity and harm verification defense strategy, raising the average defense pass rate (DPR) from 11% to 74%. More importantly, the ability to link citations to the content they encompass has become a foundational function in LLMs, amplifying the influence of LLMs' bias toward authority.

[Arxiv](https://arxiv.org/abs/2411.11407)