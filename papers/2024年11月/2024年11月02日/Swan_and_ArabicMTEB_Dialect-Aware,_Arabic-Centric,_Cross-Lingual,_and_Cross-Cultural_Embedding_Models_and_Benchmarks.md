# Swan 和 ArabicMTEB：具备方言感知能力、以阿拉伯语为核心、跨语言及跨文化的嵌入模型与基准

发布时间：2024年11月02日

`LLM应用` `阿拉伯语`

> Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks

# 摘要

> 我们推出了 Swan 系列嵌入模型，其围绕阿拉伯语展开，涵盖了小规模和大规模的应用场景。Swan 包含两个变体：基于 ARBERTv2 的 Swan-Small 以及基于预训练的阿拉伯大型语言模型 ArMistral 的 Swan-Large 。为评估这些模型，我们提出了 ArabicMTEB 这一综合基准套件，用于测评跨语言、多方言、多领域和多文化的阿拉伯语文本嵌入表现，涵盖了八项不同任务和 94 个数据集。Swan-Large 取得了前沿成果，在多数阿拉伯任务中胜过 Multilingual-E5-large ，而 Swan-Small 始终优于 Multilingual-E5 base 。我们的大量评估显示，Swan 模型兼具方言和文化感知能力，在各类阿拉伯领域表现卓越，同时具备显著的经济效率。此项工作有力推动了阿拉伯语语言建模领域的发展，并为阿拉伯自然语言处理的未来研究和应用提供了宝贵资源。我们的模型和基准将公开供研究使用。

> We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research.

[Arxiv](https://arxiv.org/abs/2411.01192)