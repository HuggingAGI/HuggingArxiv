# 黑箱对抗攻击的缩放定律

发布时间：2024年11月25日

`LLM应用` `图像分类`

> Scaling Laws for Black box Adversarial Attacks

# 摘要

> 深度学习模型一直存在的一个难题是易受对抗样本的侵害，这些对抗样本往往是给自然样本施加不易察觉的扰动而产生的。对抗样本具有跨模型转移性，能在对黑盒模型的架构和参数信息知之甚少的情况下发起攻击。模型集成是提升转移性的有效策略，即同时攻击多个替代模型。但此前的研究在集成中采用的模型数量较少，所以扩大模型数量能否进一步优化黑盒攻击仍是个未知数。受大型基础模型的成果启发，我们在本次研究中探索了黑盒对抗攻击的缩放规律。通过剖析替代模型数量与对抗样本转移性的关系，我们得出了清晰的缩放规律，突出了运用更多替代模型增强对抗转移性的潜力。大量实验在标准图像分类器、多模态大型语言模型，甚至像 GPT-4o 这样的专有模型上验证了这些观点，表明更多替代模型带来了一致的缩放效果和出色的攻击成功率。通过可视化的进一步研究显示，缩放攻击在语义上具备更好的可解释性，意味着模型的共同特征被捕捉到了。

> A longstanding problem of deep learning models is their vulnerability to adversarial examples, which are often generated by applying imperceptible perturbations to natural examples. Adversarial examples exhibit cross-model transferability, enabling to attack black-box models with limited information about their architectures and parameters. Model ensembling is an effective strategy to improve the transferability by attacking multiple surrogate models simultaneously. However, as prior studies usually adopt few models in the ensemble, there remains an open question of whether scaling the number of models can further improve black-box attacks. Inspired by the findings in large foundation models, we investigate the scaling laws of black-box adversarial attacks in this work. By analyzing the relationship between the number of surrogate models and transferability of adversarial examples, we conclude with clear scaling laws, emphasizing the potential of using more surrogate models to enhance adversarial transferability. Extensive experiments verify the claims on standard image classifiers, multimodal large language models, and even proprietary models like GPT-4o, demonstrating consistent scaling effects and impressive attack success rates with more surrogate models. Further studies by visualization indicate that scaled attacks bring better interpretability in semantics, indicating that the common features of models are captured.

[Arxiv](https://arxiv.org/abs/2411.16782)