# 利用伪反馈进行推理的偏好优化

发布时间：2024年11月25日

`LLM应用`

> Preference Optimization for Reasoning with Pseudo Feedback

# 摘要

> 偏好优化技术，像直接偏好优化（DPO）这类，常被用于提升大型语言模型（LLMs）在数学推理和编码等领域的推理能力，通常是在有监督微调之后。这些方法得依靠高质量的推理任务标签来生成偏好对；然而，有人类验证标签的推理数据集数量有限。在本研究中，我们引入了一种新办法，把推理问题解决方案的标注视作针对相关测试用例的评估，从而为推理任务生成伪反馈。我们基于测试用例探索了两种伪反馈形式：一种由前沿的LLMs生成，另一种通过将自洽性拓展到多测试用例生成。我们针对数学推理和编码任务，使用伪反馈进行偏好优化实验，发现两个任务都有改进。具体而言，以Mathstral-7B为基础模型，我们把MATH的结果从58.3提升至68.6，超越了NuminaMath-72B和GPT-4-Turbo-1106-Preview。在GSM8K和College Math中，分数分别从85.6提升到90.3、从34.3提升到42.3。基于Deepseek-coder-7B-v1.5，我们在LiveCodeBench上的得分从21.1提高到24.6，超过了Claude-3-Haiku。

> Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited. In this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated test cases. We explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case. We conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.

[Arxiv](https://arxiv.org/abs/2411.16345)