# 大型语言模型能否在不借助捷径的情况下进行潜在的多跳推理？

发布时间：2024年11月25日

`LLM应用` `语言模型` `推理评估`

> Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?

# 摘要

> 我们来评估大型语言模型（LLMs）在潜在地回忆与组合事实以回答像“斯嘉丽·约翰逊出生那年，夏季奥运会在哪个国家举办”这类多跳查询时的表现。评估此能力的一大难题在于，LLMs 可能因在相同训练序列中碰到头部实体“斯嘉丽·约翰逊”和答案实体“美国”而形成捷径，或者仅仅依据基于频率的先验来猜测答案。为避免捷径，我们排除了预训练语料库中头部和答案实体同时出现的测试查询。通过精心挑选关系和事实，并系统地剔除模型可能猜答案或利用部分匹配的情况，我们构建了评估数据集 SOCRATES（无捷径潜在推理）。我们发现，LLMs 在不利用捷径时展现出了不错的潜在多跳推理能力，但只适用于某些类型的查询。对于需要潜在回忆国家作为中间答案的查询，最优模型能达到 80%的潜在可组合性，可对于年份的回忆，这一比例仅为 5%。与思维链可组合性的对比凸显出模型潜在推理能力和显式推理能力之间的巨大差距。分析表明，在潜在可组合性较高的查询中，中间答案的潜在表示更常被构建，并且预训练期间出现了潜在的多跳推理。

> We evaluate how well Large Language Models (LLMs) latently recall and compose facts to answer multi-hop queries like "In the year Scarlett Johansson was born, the Summer Olympics were hosted in the country of". One major challenge in evaluating this ability is that LLMs may have developed shortcuts by encounters of the head entity "Scarlett Johansson" and the answer entity "United States" in the same training sequences or merely guess the answer based on frequency-based priors. To prevent shortcuts, we exclude test queries where the head and answer entities co-appear in pretraining corpora. Through careful selection of relations and facts and systematic removal of cases where models might guess answers or exploit partial matches, we construct an evaluation dataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs demonstrate promising latent multi-hop reasoning abilities without exploiting shortcuts, but only for certain types of queries. For queries requiring latent recall of countries as the intermediate answer, the best models achieve 80% latent composability, but this drops to just 5% for the recall of years. Comparisons with Chain-of-Thought composability highlight a significant gap between the ability of models to reason latently versus explicitly. Analysis reveals that latent representations of the intermediate answer are constructed more often in queries with higher latent composability, and shows the emergence of latent multi-hop reasoning during pretraining.

[Arxiv](https://arxiv.org/abs/2411.16679)