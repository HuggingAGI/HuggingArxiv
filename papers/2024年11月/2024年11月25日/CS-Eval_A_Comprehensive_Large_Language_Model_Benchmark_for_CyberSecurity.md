# CS-Eval：一个针对网络安全的综合性大型语言模型基准

发布时间：2024年11月25日

`LLM应用` `网络安全` `基准评估`

> CS-Eval: A Comprehensive Large Language Model Benchmark for CyberSecurity

# 摘要

> 在过去一年，网络安全领域中大型语言模型（LLMs）于学术研究和工业实践的运用显著增多。但评估 LLMs 在网络安全任务上表现的全面且公开可用的基准仍缺失。为填补这一空缺，我们推出了 CS-Eval，这是一个公开、全面且双语的 LLMs 基准，专为网络安全而设。CS-Eval 融合了学术界的研究热点和工业界的实际应用，精心挑选了网络安全中 42 个类别的一系列优质问题，并系统地划分为知识、能力和应用这三个认知层级。借由使用 CS-Eval 对众多 LLMs 展开广泛评估，我们收获了宝贵的发现。比如，尽管 GPT-4 整体表现出众，但其他模型在某些特定子类别中可能更胜一筹。另外，历经数月的评估，我们留意到许多 LLMs 解决网络安全任务的能力有了显著提升。该基准现可于 https://github.com/CS-EVAL/CS-Eval 公开获取。

> Over the past year, there has been a notable rise in the use of large language models (LLMs) for academic research and industrial practices within the cybersecurity field. However, it remains a lack of comprehensive and publicly accessible benchmarks to evaluate the performance of LLMs on cybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly accessible, comprehensive and bilingual LLM benchmark specifically designed for cybersecurity. CS-Eval synthesizes the research hotspots from academia and practical applications from industry, curating a diverse set of high-quality questions across 42 categories within cybersecurity, systematically organized into three cognitive levels: knowledge, ability, and application. Through an extensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered valuable insights. For instance, while GPT-4 generally excels overall, other models may outperform it in certain specific subcategories. Additionally, by conducting evaluations over several months, we observed significant improvements in many LLMs' abilities to solve cybersecurity tasks. The benchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval.

[Arxiv](https://arxiv.org/abs/2411.16239)