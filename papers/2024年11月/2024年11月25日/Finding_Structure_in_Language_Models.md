# 探寻语言模型里的结构

发布时间：2024年11月25日

`LLM理论` `语言处理` `语言学`

> Finding Structure in Language Models

# 摘要

> 当我们说话、写作或者倾听时，会依据对语言语法的认知持续进行预测。令人瞩目的是，儿童短短几年就能掌握这种语法知识，从而能够理解并推广到此前从未说过的新结构。语言模型是强大的工具，通过逐步预测句子中的下一个词来构建语言表征，近年对社会产生了巨大影响。本论文的核心研究问题在于，这些模型对语法结构的理解是否与人类相似。此问题处于自然语言处理、语言学和可解释性的交叉领域。为解决该问题，我们将开发新的可解释性技术，以增进对大规模语言模型复杂特性的理解。我们从三个方向展开研究。首先，通过结构启动来探寻抽象语言信息的存在，这是心理语言学中揭示人类语言处理中语法结构的关键范式。接着，研究诸如形容词顺序和否定极性项等各种语言现象，并将模型对这些现象的理解与它所训练的数据分布相联系。最后，引入一个受控测试平台，利用各种复杂度渐增的合成语言来研究语言模型中的层次结构，并考察特征交互在构建此结构中的作用。我们的发现详细阐述了语言模型表征中嵌入的语法知识，并为运用计算方法研究基础语言问题提供了若干方向。

> When we speak, write or listen, we continuously make predictions based on our knowledge of a language's grammar. Remarkably, children acquire this grammatical knowledge within just a few years, enabling them to understand and generalise to novel constructions that have never been uttered before. Language models are powerful tools that create representations of language by incrementally predicting the next word in a sentence, and they have had a tremendous societal impact in recent years. The central research question of this thesis is whether these models possess a deep understanding of grammatical structure similar to that of humans. This question lies at the intersection of natural language processing, linguistics, and interpretability. To address it, we will develop novel interpretability techniques that enhance our understanding of the complex nature of large-scale language models. We approach our research question from three directions. First, we explore the presence of abstract linguistic information through structural priming, a key paradigm in psycholinguistics for uncovering grammatical structure in human language processing. Next, we examine various linguistic phenomena, such as adjective order and negative polarity items, and connect a model's comprehension of these phenomena to the data distribution on which it was trained. Finally, we introduce a controlled testbed for studying hierarchical structure in language models using various synthetic languages of increasing complexity and examine the role of feature interactions in modelling this structure. Our findings offer a detailed account of the grammatical knowledge embedded in language model representations and provide several directions for investigating fundamental linguistic questions using computational methods.

[Arxiv](https://arxiv.org/abs/2411.16433)