# 所有语言皆重要：对涵盖多元文化的 100 种语言中的 LMMs 进行评估

发布时间：2024年11月25日

`LLM应用` `多模态模型` `语言文化`

> All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages

# 摘要

> 现有的大型多模态模型（LMMs）往往只聚焦于少数区域和语言。随着 LMMs 持续优化，确保它们理解文化背景、尊重当地敏感性、支持低资源语言，并有效整合相应视觉线索，变得愈发关键。为追求文化多元的全球多模态模型，我们推出的“所有语言都重要基准”（ALM-bench）是迄今评估 100 种语言的 LMMs 规模最大、最全面的一次尝试。ALM-bench 测试现有模型对多种语言（包括众多在 LMM 研究中向来代表性不足的低资源语言）的文化多元图像与文本配对的理解和推理能力，以此对现有模型发起挑战。该基准提供了强大且细致的评估框架，涵盖各种问题格式，如真/假、多项选择和开放式问题，这些又进一步分为短答案和长答案类别。ALM-bench 的设计能全面评估模型处理视觉和语言推理不同难度水平的能力。为展现全球文化的绚丽多彩，ALM-bench 精心选取了来自 13 个不同文化方面的内容，从传统习俗到名人庆典皆有涉及。由此，ALM-bench 不仅为顶尖的开源和闭源 LMMs 提供了严格的测试平台，还凸显了文化和语言包容性的重要性，激励开发能有效服务全球多元人群的模型。我们的基准可公开获取。

> Existing Large Multimodal Models (LMMs) generally focus on only a few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. In pursuit of culturally diverse global multimodal models, our proposed All Languages Matter Benchmark (ALM-bench) represents the largest and most comprehensive effort to date for evaluating LMMs across 100 languages. ALM-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in LMM research. The benchmark offers a robust and nuanced evaluation framework featuring various question formats, including true/false, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. ALM-bench design ensures a comprehensive assessment of a model's ability to handle varied levels of difficulty in visual and linguistic reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. Through this, ALM-bench not only provides a rigorous testing ground for state-of-the-art open and closed-source LMMs but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. Our benchmark is publicly available.

[Arxiv](https://arxiv.org/abs/2411.16508)