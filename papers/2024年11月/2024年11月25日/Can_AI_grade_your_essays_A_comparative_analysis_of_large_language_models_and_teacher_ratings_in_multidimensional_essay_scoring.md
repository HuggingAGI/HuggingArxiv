# 人工智能能为你的文章打分吗？对大型语言模型和教师在多维文章评分中的比较分析

发布时间：2024年11月25日

`LLM应用` `语言模型`

> Can AI grade your essays? A comparative analysis of large language models and teacher ratings in multidimensional essay scoring

# 摘要

> 学生写作的人工评估与评分，于教师而言，既耗时又关键。生成式人工智能（像大型语言模型）的最新进展，为助力教师的作文评分工作提供了潜在解法。在我们的研究里，我们对开源和闭源的大型语言模型评估德国学生作文的表现及可靠性进行了评估，并将其与 37 位教师依据 10 个预设标准（比如情节逻辑、表达）做出的评估作比较。运用五个大型语言模型（GPT-3.5、GPT-4、o1、LLaMA 3-70B 和 Mixtral 8x7B），对来自 7 年级和 8 年级学生的 20 篇真实作文组成的语料库展开分析，旨在深度洞悉大型语言模型的评分能力。闭源的 GPT 模型在内部一致性以及与人类评分的契合度上，都胜过开源模型，尤其在语言相关标准方面表现卓越。新型的 o1 模型超越了其他所有大型语言模型，在总分上与人类评估的斯皮尔曼相关系数达 r =.74，内部一致性 ICC 为.80。这些发现表明，基于大型语言模型的评估能够成为减轻教师工作量的有效工具，辅助作文评估，特别是在语言相关标准方面。不过，由于这些模型倾向给出较高分数，所以需要进一步完善，以更好地把控内容质量方面的情况。

> The manual assessment and grading of student writing is a time-consuming yet critical task for teachers. Recent developments in generative AI, such as large language models, offer potential solutions to facilitate essay-scoring tasks for teachers. In our study, we evaluate the performance and reliability of both open-source and closed-source LLMs in assessing German student essays, comparing their evaluations to those of 37 teachers across 10 pre-defined criteria (i.e., plot logic, expression). A corpus of 20 real-world essays from Year 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA 3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring capabilities. Closed-source GPT models outperform open-source models in both internal consistency and alignment with human ratings, particularly excelling in language-related criteria. The novel o1 model outperforms all other LLMs, achieving Spearman's $r = .74$ with human assessments in the overall score, and an internal consistency of $ICC=.80$. These findings indicate that LLM-based assessment can be a useful tool to reduce teacher workload by supporting the evaluation of essays, especially with regard to language-related criteria. However, due to their tendency for higher scores, the models require further refinement to better capture aspects of content quality.

[Arxiv](https://arxiv.org/abs/2411.16337)