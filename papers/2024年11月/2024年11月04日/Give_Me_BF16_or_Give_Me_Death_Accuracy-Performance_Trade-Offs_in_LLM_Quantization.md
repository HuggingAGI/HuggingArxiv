# “给我 BF16 否则给我死”？大型语言模型量化中的精度-性能权衡

发布时间：2024年11月04日

`LLM应用` `语言模型` `量化技术`

> "Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization

# 摘要

> 尽管大型语言模型（LLM）量化用于推理加速很受欢迎，但对于与各种量化格式相关的准确性-性能权衡仍存在很大的不确定性。我们对量化准确性进行了全面的实证研究，在整个 Llama-3.1 模型系列中，评估了流行的量化格式（FP8、INT8、INT4）在学术基准和实际任务中的表现。此外，我们的研究还考察了量化模型生成的文本与未压缩模型生成的文本之间的差异。除了基准测试，我们还提出了一些量化改进，使我们能够获得最先进的准确性恢复结果。我们的调查包括超过 50 万次单独评估，得出了几个关键发现：（1）FP8 权重和激活量化（W8A8-FP）在所有模型规模上都是无损的，（2）INT8 权重和激活量化（W8A8-INT），如果适当调整，会令人惊讶地仅导致 1 - 3%的准确性下降，（3）仅 INT4 权重量化（W4A16-INT）与 8 位整数权重和激活量化具有竞争力。为了解决给定部署环境中“最佳”格式的问题，我们使用流行的开源 vLLM 框架在各种 GPU 架构上进行推理性能分析。我们发现，W4A16 对于同步部署以及在中端 GPU 上的异步部署提供了最佳的成本效率。同时，W8A8 格式在高端 GPU 上的中大型模型的异步“连续批处理”部署中表现出色。我们的结果为在不同规模和性能要求下部署量化的 LLM 提供了一套实用指南。

> Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the "best" format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous "continuous batching" deployment of mid- and large-size models on high-end GPUs. Our results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements.

[Arxiv](https://arxiv.org/abs/2411.02355)