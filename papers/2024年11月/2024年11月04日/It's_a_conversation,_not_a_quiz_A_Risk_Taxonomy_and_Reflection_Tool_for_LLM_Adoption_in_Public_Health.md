# “这是一场对话，不是一次测验”：公共卫生中采用大型语言模型的风险分类和反思工具

发布时间：2024年11月04日

`LLM应用` `公共卫生` `语言模型`

> "It's a conversation, not a quiz": A Risk Taxonomy and Reflection Tool for LLM Adoption in Public Health

# 摘要

> 大型语言模型（LLM）近期的突破既引发了人们对其作为不同领域可获取的信息源或通信工具的潜在采用的兴趣，也引发了担忧。在公共卫生领域——风险很高且影响波及人群——采用 LLM 带来了独特的挑战，需要进行全面评估。然而，评估公共卫生潜在风险的结构化方法仍未得到充分探索。为了弥补这一差距，我们与卫生专业人员和健康问题经历者进行了焦点小组讨论，以揭示他们的担忧，这些担忧涉及三个不同且关键的需要高质量信息的公共卫生问题：疫苗、阿片类药物使用障碍和亲密伴侣暴力。我们将参与者的观点综合成一个风险分类法，区分并结合背景阐述了 LLM 在与传统健康传播并列时可能带来的潜在危害。这个分类法突出了个人行为、以人为本的护理、信息生态系统和技术责任这四个风险维度。对于每个维度，我们讨论了具体的风险和示例反思问题，以帮助从业者采用风险反思方法。这项工作为计算和公共卫生领域的专家提供了一个共同的词汇和反思工具，以共同预测、评估和减轻在决定是否采用 LLM 能力以及在使用时如何减轻危害方面的风险。

> Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as accessible information sources or communication tools across different domains. In public health -- where stakes are high and impacts extend across populations -- adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with health professionals and health issue experiencers to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: vaccines, opioid use disorder, and intimate partner violence. We synthesize participants' perspectives into a risk taxonomy, distinguishing and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk in individual behaviors, human-centered care, information ecosystem, and technology accountability. For each dimension, we discuss specific risks and example reflection questions to help practitioners adopt a risk-reflexive approach. This work offers a shared vocabulary and reflection tool for experts in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm when they are used.

[Arxiv](https://arxiv.org/abs/2411.02594)