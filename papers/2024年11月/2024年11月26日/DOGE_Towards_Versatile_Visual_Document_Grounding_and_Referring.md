# DOGE：致力于实现通用的视觉文档定位与引用

发布时间：2024年11月26日

`LLM应用` `文档处理` `多模态语言模型`

> DOGE: Towards Versatile Visual Document Grounding and Referring

# 摘要

> 近年来，多模态大型语言模型（MLLMs）愈发注重基础和引用能力，旨在达成详尽的理解以及灵活的用户交互。然而，在视觉文档理解范畴，因细粒度数据集和全面基准的匮乏，这些能力稍显落后。为弥补此空缺，我们推出了文档基础与引用数据引擎（DOGE-Engine），其产出了两类高质量的细粒度文档数据：用于强化基础文本定位和识别能力的多粒度解析数据；以及在对话和推理中激活 MLLM 基础和引用能力的指令调优数据。另外，借助我们的引擎，我们构建了 DOGE-Bench，它涵盖了 3 种文档类型（图表、海报、PDF 文档）的 7 项基础和引用任务，为细粒度文档理解提供了全面评估。再者，利用我们引擎生成的数据，我们研发了一个强劲的基线模型——DOGE。这款开创性的 MLLM 能够在文档图像内的多个粒度上精准地引用和定位文本。我们的代码、数据和模型将会开源，以推动社区发展。

> In recent years, Multimodal Large Language Models (MLLMs) have increasingly emphasized grounding and referring capabilities to achieve detailed understanding and flexible user interaction. However, in the realm of visual document understanding, these capabilities lag behind due to the scarcity of fine-grained datasets and comprehensive benchmarks. To fill this gap, we propose the DOcument Grounding and Eferring data engine (DOGE-Engine), which produces two types of high-quality fine-grained document data: multi-granular parsing data for enhancing fundamental text localization and recognition capabilities; and instruction-tuning data to activate MLLM's grounding and referring capabilities during dialogue and reasoning. Additionally, using our engine, we construct DOGE-Bench, which encompasses 7 grounding and referring tasks across 3 document types (chart, poster, PDF document), providing comprehensive evaluations for fine-grained document understanding. Furthermore, leveraging the data generated by our engine, we develop a strong baseline model, DOGE. This pioneering MLLM is capable of accurately referring and grounding texts at multiple granularities within document images. Our code, data, and model will be open-sourced for community development.

[Arxiv](https://arxiv.org/abs/2411.17125)