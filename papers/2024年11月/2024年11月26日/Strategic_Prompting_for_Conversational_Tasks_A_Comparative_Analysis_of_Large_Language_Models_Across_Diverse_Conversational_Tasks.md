# 关于会话任务的战略提示：对不同会话任务中大型语言模型的比较分析

发布时间：2024年11月26日

`LLM应用` `会话式人工智能`

> Strategic Prompting for Conversational Tasks: A Comparative Analysis of Large Language Models Across Diverse Conversational Tasks

# 摘要

> 鉴于会话式人工智能的不断发展，对大型语言模型（LLMs）的评估与衡量在保障各类会话任务的最优性能方面起着至关重要的作用。在本文中，我们展开了一项全面的研究，深入评估了五个常见的LLMs（Llama、OPT、Falcon、Alpaca和MPT）的能力与局限。此项研究涵盖了众多会话任务，诸如预订、共情回应生成、心理健康与法律咨询、劝说以及谈判。为进行评估，我们采用了广泛的测试设置，运用了从自动到人工的多种评估标准。这涵盖了使用通用及特定任务的指标来精准衡量LMs的性能。从我们的评估结果来看，没有任何一个模型在所有任务中都堪称完美。相反，它们的表现会因每个任务的具体要求而大相径庭。有些模型在某些任务中表现卓越，但在其他任务中可能表现欠佳。这些发现凸显了在为会话应用选择最合适的LM时，考虑任务特定要求和特征的重要性。

> Given the advancements in conversational artificial intelligence, the evaluation and assessment of Large Language Models (LLMs) play a crucial role in ensuring optimal performance across various conversational tasks. In this paper, we present a comprehensive study that thoroughly evaluates the capabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon, Alpaca, and MPT. The study encompasses various conversational tasks, including reservation, empathetic response generation, mental health and legal counseling, persuasion, and negotiation. To conduct the evaluation, an extensive test setup is employed, utilizing multiple evaluation criteria that span from automatic to human evaluation. This includes using generic and task-specific metrics to gauge the LMs' performance accurately. From our evaluation, no single model emerges as universally optimal for all tasks. Instead, their performance varies significantly depending on the specific requirements of each task. While some models excel in certain tasks, they may demonstrate comparatively poorer performance in others. These findings emphasize the importance of considering task-specific requirements and characteristics when selecting the most suitable LM for conversational applications.

[Arxiv](https://arxiv.org/abs/2411.17204)