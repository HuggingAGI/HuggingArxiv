# 一种主题层面的自我校正方法来缓解大型多语言模型中的幻觉

发布时间：2024年11月26日

`LLM应用` `人工智能` `多模态语言模型`

> A Topic-level Self-Correctional Approach to Mitigate Hallucinations in MLLMs

# 摘要

> 让多模态大型语言模型（MLLMs）的行为契合人类偏好，这对于开发强大且可靠的人工智能系统极为关键。尽管近期的尝试借助人类专家或强大的辅助人工智能系统来提供更精准的偏好反馈，比如确定 MLLMs 中更优的响应，或者直接重写无幻觉的响应，然而大量的资源耗费影响了反馈收集的可扩展性。在本研究中，我们引入了主题级偏好覆盖（TPO）这一自我校正的方法，引导模型自身在主题层面减轻自身的幻觉。通过一种解混淆策略，把响应中的每个主题替换为模型自身生成的最佳或最差替代方案，TPO 创造出更具对比性的成对偏好反馈，在无需人类或专有模型干预的情况下提升了反馈质量。值得一提的是，实验结果显示，所提出的 TPO 在可信度方面达到了前沿水平，显著减少了 92％的对象幻觉和 38％的总体幻觉。代码、模型和数据将会发布。

> Aligning the behaviors of Multimodal Large Language Models (MLLMs) with human preferences is crucial for developing robust and trustworthy AI systems. While recent attempts have employed human experts or powerful auxiliary AI systems to provide more accurate preference feedback, such as determining the preferable responses from MLLMs or directly rewriting hallucination-free responses, extensive resource overhead compromise the scalability of the feedback collection. In this work, we introduce Topic-level Preference Overwriting (TPO), a self-correctional approach that guide the model itself to mitigate its own hallucination at the topic level. Through a deconfounded strategy that replaces each topic within the response with the best or worst alternatives generated by the model itself, TPO creates more contrasting pairwise preference feedback, enhancing the feedback quality without human or proprietary model intervention. Notably, the experimental results demonstrate proposed TPO achieves state-of-the-art performance in trustworthiness, significantly reducing the object hallucinations by 92% and overall hallucinations by 38%. Code, model and data will be released.

[Arxiv](https://arxiv.org/abs/2411.17265)