# CleanVul：利用 LLM 启发式实现代码提交中的自动函数级漏洞检测

发布时间：2024年11月26日

`LLM应用` `软件安全` `漏洞检测`

> CleanVul: Automatic Function-Level Vulnerability Detection in Code Commits Using LLM Heuristics

# 摘要

> 准确识别软件漏洞对于保障系统的完整性极为关键。漏洞数据集通常来源于国家漏洞数据库（NVD）或者直接取自 GitHub，它们对于训练机器学习模型来检测这些安全漏洞至关重要。然而，这些数据集往往存在大量噪声，通常在 40%至 75%之间，主要原因是漏洞修复提交（VFCs）中的所有变更都被自动且不加区分地标记为与漏洞相关。之所以会出现这种错误分类，是因为并非所有旨在修复漏洞的提交中的变更都涉及安全威胁，其中很多只是常规更新，比如错误修复或者测试改进。
  本文引入了首个运用具有启发式增强功能的大型语言模型（LLM），自动从 VFCs 中识别漏洞修复变更的方法，实现了 0.82 的 F1 分数。VulSifter 应用于大规模研究中，我们对 GitHub 上的 127,063 个存储库进行了爬取，获取了 5,352,105 次提交。VulSifter 利用 LLM 来理解代码语义和上下文信息，同时运用启发式方法过滤掉不相关的变更。随后，我们开发了 CleanVul，这是一个运用我们的 LLM 启发式增强方法、包含 11,632 个函数的高质量数据集，其正确性（90.6%）可与诸如 SVEN 和 PrimeVul 等已有的数据集相媲美。
  为评估 CleanVul 数据集，我们开展了实验，重点是在 CleanVul 和其他高质量数据集上对各类 LLM 进行微调。评估结果显示，在 CleanVul 上微调的 LLM 不仅准确性更高，而且泛化能力也优于在未清理的数据集中训练的模型。具体来说，在 CleanVul 上训练并在 PrimeVul 上测试的模型，其准确性高于仅在 PrimeVul 上训练和测试的模型。

> Accurate identification of software vulnerabilities is crucial for system integrity. Vulnerability datasets, often derived from the National Vulnerability Database (NVD) or directly from GitHub, are essential for training machine learning models to detect these security flaws. However, these datasets frequently suffer from significant noise, typically 40% to 75%, due primarily to the automatic and indiscriminate labeling of all changes in vulnerability-fixing commits (VFCs) as vulnerability-related. This misclassification occurs because not all changes in a commit aimed at fixing vulnerabilities pertain to security threats; many are routine updates like bug fixes or test improvements.
  This paper introduces the first methodology that uses the Large Language Model (LLM) with a heuristic enhancement to automatically identify vulnerability-fixing changes from VFCs, achieving an F1-score of 0.82. VulSifter was applied to a large-scale study, where we conducted a crawl of 127,063 repositories on GitHub, resulting in the acquisition of 5,352,105 commits. VulSifter involves utilizing an LLM to comprehend code semantics and contextual information, while applying heuristics to filter out unrelated changes. We then developed CleanVul, a high-quality dataset comprising 11,632 functions using our LLM heuristic enhancement approach, demonstrating Correctness (90.6%) comparable to established datasets such as SVEN and PrimeVul.
  To evaluate the CleanVul dataset, we conducted experiments focusing on fine-tuning various LLMs on CleanVul and other high-quality datasets. Evaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit enhanced accuracy but also superior generalization capabilities compared to those trained on uncleaned datasets. Specifically, models trained on CleanVul and tested on PrimeVul achieve accuracy higher than those trained and tested exclusively on PrimeVul.

[Arxiv](https://arxiv.org/abs/2411.17274)