# 关于在视觉问答中运用 MLLM 进行自然语言理解和推理的调查

发布时间：2024年11月26日

`LLM应用` `计算机视觉`

> Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey

# 摘要

> 视觉问答（VQA）是一项融合自然语言处理与计算机视觉技术的挑战性任务，已逐渐成为多模态大型语言模型（MLLMs）里的基准测试任务。我们此次调研旨在对 VQA 的发展予以概述，并对时效性强的最新模型进行详尽描述。本次调研对核心 VQA 任务中有关图像和文本的自然语言理解，以及基于图像 - 问题信息的知识推理模块做了最新综合。另外，我们详述了在 VQA 中借助视觉语言预训练模型和多模态大型语言模型来提取和融合模态信息的最新进展。我们还通过细致讲解内部知识的提取和外部知识的引入，全面回顾了 VQA 中知识推理的进程。最后，我们展示了 VQA 的数据集和不同的评估指标，并探讨了未来工作的可能方向。

> Visual Question Answering (VQA) is a challenge task that combines natural language processing and computer vision techniques and gradually becomes a benchmark test task in multimodal large language models (MLLMs). The goal of our survey is to provide an overview of the development of VQA and a detailed description of the latest models with high timeliness. This survey gives an up-to-date synthesis of natural language understanding of images and text, as well as the knowledge reasoning module based on image-question information on the core VQA tasks. In addition, we elaborate on recent advances in extracting and fusing modal information with vision-language pretraining models and multimodal large language models in VQA. We also exhaustively review the progress of knowledge reasoning in VQA by detailing the extraction of internal knowledge and the introduction of external knowledge. Finally, we present the datasets of VQA and different evaluation metrics and discuss possible directions for future work.

[Arxiv](https://arxiv.org/abs/2411.17558)