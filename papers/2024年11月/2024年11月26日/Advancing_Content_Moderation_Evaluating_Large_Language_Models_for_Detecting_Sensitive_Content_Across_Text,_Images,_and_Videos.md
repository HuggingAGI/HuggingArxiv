# 推进内容审核：评估大型语言模型检测文本、图像和视频中敏感内容的能力

发布时间：2024年11月26日

`LLM应用` `内容审核`

> Advancing Content Moderation: Evaluating Large Language Models for Detecting Sensitive Content Across Text, Images, and Videos

# 摘要

> 仇恨言论、骚扰、有害及性相关内容、暴力等在网站和媒体平台的广泛传播，带来了重大挑战，引发了社会各界的广泛忧虑。政府、教育者和家长在如何监管、控制及限制这类内容的传播上，常与媒体平台意见相左。检测和审查媒体内容的技术是解决这些挑战的关键。自然语言处理和计算机视觉技术已被广泛运用，能自动识别并过滤文本、图像和视频中的敏感内容，比如冒犯性语言、暴力、裸体和成瘾相关内容，让平台得以大规模施行内容政策。然而，现有的方法在达到高检测准确率、减少误报和漏报方面仍有局限。所以，用于理解文本和图像上下文的更复杂算法，或许能为内容审查的改进创造条件，构建更高效的审查系统。在本文中，我们评估了现有的基于LLM的内容审核方案，像OpenAI审核模型和Llama-Guard3，并探究了它们检测敏感内容的能力。另外，我们研究了近期的LLM，如GPT、Gemini和Llama在识别媒体渠道中不当内容方面的表现。各种文本和视觉数据集，像X推文、亚马逊评论、新闻文章、人物照片、卡通、草图和暴力视频等，都被用于评估和比较。结果显示，LLM凭借更高的准确率和更低的误报、漏报率，胜过传统技术。这凸显了将LLM融入网站、社交媒体平台和视频共享服务，用于监管和内容审核的潜力。

> The widespread dissemination of hate speech, harassment, harmful and sexual content, and violence across websites and media platforms presents substantial challenges and provokes widespread concern among different sectors of society. Governments, educators, and parents are often at odds with media platforms about how to regulate, control, and limit the spread of such content. Technologies for detecting and censoring the media contents are a key solution to addressing these challenges. Techniques from natural language processing and computer vision have been used widely to automatically identify and filter out sensitive content such as offensive languages, violence, nudity, and addiction in both text, images, and videos, enabling platforms to enforce content policies at scale. However, existing methods still have limitations in achieving high detection accuracy with fewer false positives and false negatives. Therefore, more sophisticated algorithms for understanding the context of both text and image may open rooms for improvement in content censorship to build a more efficient censorship system. In this paper, we evaluate existing LLM-based content moderation solutions such as OpenAI moderation model and Llama-Guard3 and study their capabilities to detect sensitive contents. Additionally, we explore recent LLMs such as GPT, Gemini, and Llama in identifying inappropriate contents across media outlets. Various textual and visual datasets like X tweets, Amazon reviews, news articles, human photos, cartoons, sketches, and violence videos have been utilized for evaluation and comparison. The results demonstrate that LLMs outperform traditional techniques by achieving higher accuracy and lower false positive and false negative rates. This highlights the potential to integrate LLMs into websites, social media platforms, and video-sharing services for regulatory and content moderation purposes.

[Arxiv](https://arxiv.org/abs/2411.17123)