# 使用大型语言模型生成可执行的预言来检查客户端代码是否符合 JDK Javadocs 的要求

发布时间：2024年11月03日

`LLM应用` `软件测试` `Java 开发`

> Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs

# 摘要

> 软件测试仍然是验证代码质量最广泛使用的方法。然而，测试的有效性关键取决于所使用的测试套件的质量。测试套件中的测试用例由两个基本部分组成：（1）被测试代码的输入值，以及（2）对其产生的输出的正确检查。这些检查通常被写为断言，并称为测试神谕。在过去的几十年中，自动化测试输入生成方面取得了很大进展，例如使用模糊测试和符号执行。然而，自动化测试神谕仍然是一个相对较少探索的问题领域。实际上，测试神谕本质上需要对预期行为的了解，这可能只有开发人员知道，并且可能不存在支持自动推理的正式语言中。

我们在本文中的重点是为广泛使用的 Java 库（例如 java.lang 和 java.util 包）的客户端自动化测试神谕。我们的关键见解是提供丰富信息源的 Javadocs 能够实现测试神谕的自动化生成。核心 Java 库的 Javadocs 是相当详细的文档，不仅包含库的行为方式的自然语言描述，还包含客户端必须（不）如何使用它们的描述。我们使用大型语言模型作为使能技术，将我们的见解体现在测试神谕自动化的框架中，并进行实验评估。我们的实验表明，LLM 可以从 Javadocs 生成用于检查正常和异常行为的神谕，其中 98.8％的神谕是可编译的，96.4％准确反映了预期的属性。即使对于少数不正确的神谕，错误也很小，并且在 LLM 生成的附加注释信息的帮助下可以轻松纠正。

> Software testing remains the most widely used methodology for validating quality of code. However, effectiveness of testing critically depends on the quality of test suites used. Test cases in a test suite consist of two fundamental parts: (1) input values for the code under test, and (2) correct checks for the outputs it produces. These checks are commonly written as assertions, and termed test oracles. The last couple of decades have seen much progress in automated test input generation, e.g., using fuzzing and symbolic execution. However, automating test oracles remains a relatively less explored problem area. Indeed, a test oracle by its nature requires knowledge of expected behavior, which may only be known to the developer and may not not exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely used Java libraries, e.g., java.lang and java.util packages. Our key insight is that Javadocs that provide a rich source of information can enable automated generation of test oracles. Javadocs of the core Java libraries are fairly detailed documents that contain natural language descriptions of not only how the libraries behave but also how the clients must (not) use them. We use large language models as an enabling technology to embody our insight into a framework for test oracle automation, and evaluate it experimentally. Our experiments demonstrate that LLMs can generate oracles for checking normal and exceptional behaviors from Javadocs, with 98.8% of these oracles being compilable and 96.4% accurately reflecting intended properties. Even for the few incorrect oracles, errors are minor and can be easily corrected with the help of additional comment information generated by the LLMs.

[Arxiv](https://arxiv.org/abs/2411.01789)