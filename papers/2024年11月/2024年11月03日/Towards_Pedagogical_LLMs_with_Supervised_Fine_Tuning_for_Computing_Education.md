# 朝着用于计算教育、具备监督微调功能的教学型大型语言模型迈进

发布时间：2024年11月03日

`LLM应用` `计算机`

> Towards Pedagogical LLMs with Supervised Fine Tuning for Computing Education

# 摘要

> 这篇论文探究了对大型语言模型（LLMs）进行有监督微调，以增强其在计算机教育中的教学适配性，化解了人们对于LLMs可能影响学习效果的顾虑。此项目运用了一个源自编程课程论坛的包含 2500 个高质量问答对的专有数据集，并探索了两个研究问题：大学课程论坛在助力微调数据集方面的适宜性，以及有监督微调怎样提升 LLMs 与建构主义等教育原则的契合度。初步发现显示 LLMs 在教学适配方面存在益处，但仍需更深入的评估。

> This paper investigates supervised fine-tuning of large language models (LLMs) to improve their pedagogical alignment in computing education, addressing concerns that LLMs may hinder learning outcomes. The project utilised a proprietary dataset of 2,500 high quality question/answer pairs from programming course forums, and explores two research questions: the suitability of university course forums in contributing to fine-tuning datasets, and how supervised fine-tuning can improve LLMs' alignment with educational principles such as constructivism. Initial findings suggest benefits in pedagogical alignment of LLMs, with deeper evaluations required.

[Arxiv](https://arxiv.org/abs/2411.01765)