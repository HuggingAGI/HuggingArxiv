# GMAI-VL 与 GMAI-VL-5.5M：一个大型的视觉语言模型以及一个致力于通用医疗 AI 的综合多模态数据集

发布时间：2024年11月21日

`LLM应用` `多模态`

> GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI

# 摘要

> 尽管像 GPT-4 这类通用人工智能进步显著，但其在医疗领域（通用医疗人工智能，GMAI）的成效仍受限于专业医学知识的缺失。为应对此挑战，我们推出了 GMAI-VL-5.5M，这是通过将数百个专业医疗数据集转化为精心构建的图像-文本对而形成的综合多模态医疗数据集。该数据集任务覆盖全面、模态多样、图像-文本数据质量高。基于此多模态数据集，我们提出了 GMAI-VL，这是一个采用逐步三阶段训练策略的通用医疗视觉语言模型。此方法通过融合视觉和文本信息，显著提升了模型能力，进而增强了其处理多模态数据以及支持精准诊断和临床决策的能力。实验评估显示，GMAI-VL 在诸如视觉问答和医学图像诊断等众多多模态医疗任务中达到了领先水平。我们的贡献涵盖了 GMAI-VL-5.5M 数据集的开发、GMAI-VL 模型的引入以及在多个医疗领域新基准的建立。代码和数据集将在 https://github.com/uni-medical/GMAI-VL 发布。

> Despite significant advancements in general artificial intelligence, such as GPT-4, their effectiveness in the medical domain (general medical AI, GMAI) remains constrained due to the absence of specialized medical knowledge. To address this challenge, we present GMAI-VL-5.5M, a comprehensive multimodal medical dataset created by converting hundreds of specialized medical datasets into meticulously constructed image-text pairs. This dataset features comprehensive task coverage, diverse modalities, and high-quality image-text data. Building upon this multimodal dataset, we propose GMAI-VL, a general medical vision-language model with a progressively three-stage training strategy. This approach significantly enhances the model's ability by integrating visual and textual information, thereby improving its ability to process multimodal data and support accurate diagnosis and clinical decision-making. Experimental evaluations demonstrate that GMAI-VL achieves state-of-the-art results across a wide range of multimodal medical tasks, such as visual question answering and medical image diagnosis. Our contributions include the development of the GMAI-VL-5.5M dataset, the introduction of the GMAI-VL model, and the establishment of new benchmarks in multiple medical domains. Code and dataset will be released at https://github.com/uni-medical/GMAI-VL.

[Arxiv](https://arxiv.org/abs/2411.14522)