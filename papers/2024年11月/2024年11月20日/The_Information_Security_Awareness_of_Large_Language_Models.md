# 大型语言模型的信息安全认知

发布时间：2024年11月20日

`LLM应用` `信息安全` `语言模型`

> The Information Security Awareness of Large Language Models

# 摘要

> 大型语言模型（LLMs）越来越受欢迎，基于 LLM 的助手随处可见，在生活的诸多方面为不同背景的人群提供协助。大量资源已投入到 LLMs 的安全性以及其与社会规范的适配性方面。然而，从信息安全意识（ISA）视角对其行为展开的研究却十分稀缺。聊天机器人和基于 LLM 的助手可能会因助长不安全行为，让不知情的用户陷入危险境地。我们发现，当下一些最流行的 LLMs 所固有的 ISA 差异明显，多数模型需要有明确安全背景的用户提示，才能运用其安全知识为用户提供安全回应。基于此，我们构建了一套涵盖 30 个场景的综合体系来评估 LLMs 的 ISA。这些场景依据移动 ISA 分类法中所定义的所有重点领域，对评估模型进行了基准测试。我们的研究结果表明，ISA 受模型温度变化的影响较小，而调整系统提示则会产生较大影响。这凸显了设置恰当系统提示以减轻 ISA 缺陷的必要性。我们的发现也突出了 ISA 评估对于未来基于 LLM 助手开发的重要性。

> The popularity of large language models (LLMs) continues to increase, and LLM-based assistants have become ubiquitous, assisting people of diverse backgrounds in many aspects of life. Significant resources have been invested in the safety of LLMs and their alignment with social norms. However, research examining their behavior from the information security awareness (ISA) perspective is lacking. Chatbots and LLM-based assistants may put unwitting users in harm's way by facilitating unsafe behavior. We observe that the ISA inherent in some of today's most popular LLMs varies significantly, with most models requiring user prompts with a clear security context to utilize their security knowledge and provide safe responses to users. Based on this observation, we created a comprehensive set of 30 scenarios to assess the ISA of LLMs. These scenarios benchmark the evaluated models with respect to all focus areas defined in a mobile ISA taxonomy. Among our findings is that ISA is mildly affected by changing the model's temperature, whereas adjusting the system prompt can substantially impact it. This underscores the necessity of setting the right system prompt to mitigate ISA weaknesses. Our findings also highlight the importance of ISA assessment for the development of future LLM-based assistants.

[Arxiv](https://arxiv.org/abs/2411.13207)