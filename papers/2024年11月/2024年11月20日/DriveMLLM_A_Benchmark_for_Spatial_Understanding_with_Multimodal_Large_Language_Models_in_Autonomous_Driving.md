# DriveMLLM：自动驾驶中借助多模态大型语言模型实现空间理解的基准

发布时间：2024年11月20日

`LLM应用` `自动驾驶` `空间推理`

> DriveMLLM: A Benchmark for Spatial Understanding with Multimodal Large Language Models in Autonomous Driving

# 摘要

> 自动驾驶需要全面理解 3D 环境，以助力运动预测、规划和地图绘制等高级任务。在本文中，我们推出了 DriveMLLM，这一专门用于评测自动驾驶中多模态大型语言模型（MLLMs）空间理解能力的基准。DriveMLLM 涵盖 2734 个前置摄像头图像，引入了绝对和相对空间推理任务，还伴有语言丰富多样的自然语言问题。为衡量 MLLMs 的表现，我们提出了侧重于空间理解的新型评估指标。我们在 DriveMLLM 上对几个前沿的 MLLMs 进行了评估，结果表明当前模型在理解驾驶场景中的复杂空间关系时存在局限。我们认为，这些发现凸显了对更先进的基于 MLLM 的空间推理方法的需求，也展现了 DriveMLLM 在推动自动驾驶深入研究方面的潜力。代码可在 url{https://github.com/XiandaGuo/Drive-MLLM}获取。

> Autonomous driving requires a comprehensive understanding of 3D environments to facilitate high-level tasks such as motion prediction, planning, and mapping. In this paper, we introduce DriveMLLM, a benchmark specifically designed to evaluate the spatial understanding capabilities of multimodal large language models (MLLMs) in autonomous driving. DriveMLLM includes 2,734 front-facing camera images and introduces both absolute and relative spatial reasoning tasks, accompanied by linguistically diverse natural language questions. To measure MLLMs' performance, we propose novel evaluation metrics focusing on spatial understanding. We evaluate several state-of-the-art MLLMs on DriveMLLM, and our results reveal the limitations of current models in understanding complex spatial relationships in driving contexts. We believe these findings underscore the need for more advanced MLLM-based spatial reasoning methods and highlight the potential for DriveMLLM to drive further research in autonomous driving. Code will be available at \url{https://github.com/XiandaGuo/Drive-MLLM}.

[Arxiv](https://arxiv.org/abs/2411.13112)