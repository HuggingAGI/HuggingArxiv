# 自洽框架：为语言模型精准调校，旨在多元观点间寻求共识

发布时间：2023年05月19日

`分类：Agent` `人工智能` `多智能体系统`

> Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions

# 摘要

> 在多智能体系统中达成共识是一项挑战，但大型语言模型（LLM）凭借其理解人类观点和生成类人文本的非凡能力，为这一挑战提供了解决之道。尽管如此，LLM通常依赖大量人工标注数据。本文提出了“自共识”框架，一种创新方法，旨在通过LLM自身产生的数据，自主寻找共识，并对LLM进行微调。具体而言，我们利用生成预训练变换器（GPT-3）为问题集的每个问题生成多种观点，从而形成多个潜在的共识选项。接着，基于双向编码器表示的变换器（BERT）模型对每个潜在共识进行评分，挑选出共识度最高的选项。通过这种方式，我们构建了包含问题、观点和共识的数据集，用以微调预训练的LLM，以便在多元意见中寻找共识。引人注目的是，经过自共识框架微调的预训练LLM，在参数量仅为GPT-3的1/25的情况下，实现了与GPT-3相媲美的性能，这证明了它在无需人工标注数据的环境中，依然能够有效地识别出不同观点之间的共识。

> Finding an agreement among diverse opinions is a challenging topic in multiagent systems. Recently, large language models (LLMs) have shown great potential in addressing this challenge due to their remarkable capabilities in comprehending human opinions and generating human-like text. However, they typically rely on extensive human-annotated data. In this paper, we propose Self-Agreement, a novel framework for fine-tuning LLMs to autonomously find agreement using data generated by LLM itself. Specifically, our approach employs the generative pre-trained transformer-3 (GPT-3) to generate multiple opinions for each question in a question dataset and create several agreement candidates among these opinions. Then, a bidirectional encoder representations from transformers (BERT)-based model evaluates the agreement score of each agreement candidate and selects the one with the highest agreement score. This process yields a dataset of question-opinion-agreements, which we use to fine-tune a pre-trained LLM for discovering agreements among diverse opinions. Remarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework achieves comparable performance to GPT-3 with only 1/25 of its parameters, showcasing its ability to identify agreement among various opinions without the need for human-annotated data.

[Arxiv](https://arxiv.org/abs/2305.11460)