# UnifiedVisionGPT：采用广义多模态框架，优化视觉导向的人工智能流程

发布时间：2023年11月16日

`LLM应用` `计算机视觉` `人工智能`

> UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework

# 摘要

> 在当今人工智能的版图中，基础模型是推动语言和视觉技术发展的核心。OpenAI GPT-4 已在大型语言模型（LLMs）中独占鳌头，而计算机视觉（CV）领域则有 Meta 的 SAM、DINO 和 YOLOS 等众多前沿模型。尽管如此，从头开始训练新模型所需的财务和计算成本仍是技术进步的一大障碍。为解决这一难题，我们推出了 UnifiedVisionGPT，这是一个创新框架，旨在整合并自动化集成顶尖视觉模型，以加速视觉导向的人工智能开发。UnifiedVisionGPT 以其四项核心优势而著称：（1）构建于多模态基础模型之上，提供灵活的多模态框架，适用于多种应用场景；（2）无缝融合多种前沿视觉模型，打造全面的多模态平台，汇聚各个模型的精华；（3）专注于视觉人工智能，力求在 CV 领域实现超越现有 LLMs 发展轨迹的快速进步；（4）引入自动化选择前沿视觉模型，根据不同的多模态输入如文本和图像，产生最佳结果。本文详细介绍了 UnifiedVisionGPT 的架构和功能，证明了它在提升计算机视觉领域的效率、多样性、泛化能力和性能方面的革命性潜力。我们的实现，包括统一的多模态框架和全面的数据集，已在 https://github.com/LHBuilder/SA-Segment-Anything 向公众开放。

> In the current landscape of artificial intelligence, foundation models serve as the bedrock for advancements in both language and vision domains. OpenAI GPT-4 has emerged as the pinnacle in large language models (LLMs), while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models such as Meta's SAM and DINO, and YOLOS. However, the financial and computational burdens of training new models from scratch remain a significant barrier to progress. In response to this challenge, we introduce UnifiedVisionGPT, a novel framework designed to consolidate and automate the integration of SOTA vision models, thereby facilitating the development of vision-oriented AI. UnifiedVisionGPT distinguishes itself through four key features: (1) provides a versatile multimodal framework adaptable to a wide range of applications, building upon the strengths of multimodal foundation models; (2) seamlessly integrates various SOTA vision models to create a comprehensive multimodal platform, capitalizing on the best components of each model; (3) prioritizes vision-oriented AI, ensuring a more rapid progression in the CV domain compared to the current trajectory of LLMs; and (4) introduces automation in the selection of SOTA vision models, generating optimal results based on diverse multimodal inputs such as text prompts and images. This paper outlines the architecture and capabilities of UnifiedVisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, generalization, and performance. Our implementation, along with the unified multimodal framework and comprehensive dataset, is made publicly available at https://github.com/LHBuilder/SA-Segment-Anything.

[Arxiv](https://arxiv.org/abs/2311.10125)