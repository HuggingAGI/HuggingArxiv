# AI 领域的炒作、可持续性问题，以及“越大越好”理念背后的代价

发布时间：2024年09月21日

`LLM理论` `人工智能`

> Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI

# 摘要

> 随着大型语言模型等AI技术的兴起，“越大越好”的观念逐渐深入人心。然而，这种假设的依据何在？我们如何衡量AI的价值、力量和表现？这种不断追求更大规模的竞赛又带来了哪些潜在问题？本文深入探讨了当前的扩展趋势及其权衡，并质疑了两个常见观点：一是性能提升源于规模扩大，二是所有有趣的人工智能问题都需要大规模模型。我们认为，这种做法不仅在科学上存在脆弱性，还伴随着一系列不良后果。首先，其计算需求增长速度远超性能提升，导致经济负担加重和环境影响不均衡。其次，它导致资源过度集中于某些领域，忽视了健康、教育、气候等重要应用。最后，这种趋势加剧了权力集中，使得少数人掌握决策权，同时削弱了其他人在AI研究和应用中的影响力。

> With the growing attention and investment in recent AI approaches such as large language models, the narrative that the larger the AI system the more valuable, powerful and interesting it is is increasingly seen as common sense. But what is this assumption based on, and how are we measuring value, power, and performance? And what are the collateral consequences of this race to ever-increasing scale? Here, we scrutinize the current scaling trends and trade-offs across multiple axes and refute two common assumptions underlying the 'bigger-is-better' AI paradigm: 1) that improved performance is a product of increased scale, and 2) that all interesting problems addressed by AI require large-scale models. Rather, we argue that this approach is not only fragile scientifically, but comes with undesirable consequences. First, it is not sustainable, as its compute demands increase faster than model performance, leading to unreasonable economic requirements and a disproportionate environmental footprint. Second, it implies focusing on certain problems at the expense of others, leaving aside important applications, e.g. health, education, or the climate. Finally, it exacerbates a concentration of power, which centralizes decision-making in the hands of a few actors while threatening to disempower others in the context of shaping both AI research and its applications throughout society.

[Arxiv](https://arxiv.org/abs/2409.14160)