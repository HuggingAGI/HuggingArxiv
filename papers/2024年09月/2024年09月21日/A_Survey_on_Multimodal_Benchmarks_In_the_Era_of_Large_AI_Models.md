# 在大规模 AI 模型时代，多模态基准的调查

发布时间：2024年09月21日

`LLM应用` `人工智能` `多模态内容生成`

> A Survey on Multimodal Benchmarks: In the Era of Large AI Models

# 摘要

> 多模态大型语言模型 (MLLM) 的迅猛发展，极大地提升了人工智能理解和生成多模态内容的能力。尽管已有研究多聚焦于模型架构和训练方法，但用于评估这些模型的基准分析仍显不足。本调查系统回顾了 211 个评估 MLLM 的基准，涵盖理解、推理、生成和应用四大领域，详细剖析了任务设计、评估指标及数据集构建。我们希望通过这份调查，为 MLLM 研究提供全面的基准实践概览，并指明未来研究方向，推动该领域持续进步。相关 GitHub 仓库已收录最新论文。

> The rapid evolution of Multimodal Large Language Models (MLLMs) has brought substantial advancements in artificial intelligence, significantly enhancing the capability to understand and generate multimodal content. While prior studies have largely concentrated on model architectures and training methodologies, a thorough analysis of the benchmarks used for evaluating these models remains underexplored. This survey addresses this gap by systematically reviewing 211 benchmarks that assess MLLMs across four core domains: understanding, reasoning, generation, and application. We provide a detailed analysis of task designs, evaluation metrics, and dataset constructions, across diverse modalities. We hope that this survey will contribute to the ongoing advancement of MLLM research by offering a comprehensive overview of benchmarking practices and identifying promising directions for future work. An associated GitHub repository collecting the latest papers is available.

[Arxiv](https://arxiv.org/abs/2409.18142)