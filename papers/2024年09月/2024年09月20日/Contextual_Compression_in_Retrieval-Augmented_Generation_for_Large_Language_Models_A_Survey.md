# 大型语言模型中检索增强生成的上下文压缩研究

发布时间：2024年09月20日

`RAG` `人工智能`

> Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey

# 摘要

> 尽管大型语言模型 (LLM) 展示了卓越的能力，但它们在幻觉、过时知识、不透明性和无法解释的推理等方面仍面临挑战。为此，检索增强生成 (RAG) 应运而生，通过利用外部数据库，显著提升了生成内容的一致性和连贯性，尤其在复杂、知识密集型任务中表现突出，并能通过特定领域见解的融入实现持续改进。RAG 结合了 LLM 的内在知识与外部数据库的动态资源，实现了协同效应。然而，RAG 也存在局限，如上下文窗口有限、无关信息干扰以及处理大量上下文数据的高开销。本文深入探讨了上下文压缩范式的演变，并指出了当前的挑战，为未来的研究和发展方向提供了建议，助力该领域的进一步突破。

> Large Language Models (LLMs) showcase remarkable abilities, yet they struggle with limitations such as hallucinations, outdated knowledge, opacity, and inexplicable reasoning. To address these challenges, Retrieval-Augmented Generation (RAG) has proven to be a viable solution, leveraging external databases to improve the consistency and coherence of generated content, especially valuable for complex, knowledge-rich tasks, and facilitates continuous improvement by leveraging domain-specific insights. By combining the intrinsic knowledge of LLMs with the vast, dynamic repositories of external databases, RAG achieves a synergistic effect. However, RAG is not without its limitations, including a limited context window, irrelevant information, and the high processing overhead for extensive contextual data. In this comprehensive work, we explore the evolution of Contextual Compression paradigms, providing an in-depth examination of the field. Finally, we outline the current challenges and suggest potential research and development directions, paving the way for future advancements in this area.

[Arxiv](https://arxiv.org/abs/2409.13385)