# 探索机器人操作中视觉-语言-动作模型的测试与评估：一项实证研究

发布时间：2024年09月19日

`Agent` `机器人` `人工智能`

> Towards Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation: An Empirical Study

# 摘要

> 多模态基础模型和生成式AI在各领域应用中展现出巨大潜力。近期，视觉-语言-动作（VLA）模型因其在机器人操作中的应用前景备受瞩目。尽管VLA模型提供了端到端的感知-控制循环，但其能力和在不同操作场景中的鲁棒性仍待深入探究。为此，我们推出了VLATest框架，自动生成多样化的操作场景，全面评估VLA模型性能。实验涵盖八种VLA模型、四种操作任务及18,604个测试场景，结果显示现有模型在实际应用中鲁棒性不足，易受相机姿态、光照条件等因素影响。我们的研究成果将为更可靠的VLA机器人操作系统奠定基础。

> Multi-modal foundation models and generative AI have demonstrated promising capabilities in applications across various domains. Recently, Vision-language-action (VLA) models have attracted much attention regarding their potential to advance robotic manipulation. Despite the end-to-end perception-control loop offered by the VLA models, there is a lack of comprehensive understanding of the capabilities of such models and an automated testing platform to reveal their robustness and reliability across different robotic manipulation scenarios. To address these challenges, in this work, we present VLATest, a testing framework that automatically generates diverse robotic manipulation scenes to assess the performance of VLA models from various perspectives. Large-scale experiments are considered, including eight VLA models, four types of manipulation tasks, and over 18,604 testing scenes. The experimental results show that existing VAL models still lack imperative robustness for practical applications. Specifically, the performance of VLA models can be significantly affected by several factors from the operation environments, such as camera poses, lighting conditions, and unseen objects. Our framework and the insights derived from the study are expected to pave the way for more advanced and reliable VLA-enabled robotic manipulation systems in practice.

[Arxiv](https://arxiv.org/abs/2409.12894)