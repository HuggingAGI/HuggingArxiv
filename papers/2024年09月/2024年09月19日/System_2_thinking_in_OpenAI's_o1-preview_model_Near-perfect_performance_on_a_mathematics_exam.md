# OpenAI 的 o1-preview 模型展现了系统 2 思维，在数学考试中几乎取得了满分。

发布时间：2024年09月19日

`LLM应用` `人工智能`

> System 2 thinking in OpenAI's o1-preview model: Near-perfect performance on a mathematics exam

# 摘要

> 人类认知分为系统1（快速直观）和系统2（缓慢深思）。过去，大型语言模型因缺乏系统2的深度分析能力而受诟病。2024年9月，OpenAI推出O1系列，专攻系统2式推理。尽管OpenAI的基准测试令人振奋，独立验证仍不可或缺。我们两次测试O1-preview在荷兰“数学B”考试中的表现，分别获得76分和73分（满分76分）。相比之下，GPT-4o得分66分和61分，远超荷兰平均40.63分。O1-preview用时约10分钟，GPT-4o仅3分钟，且均未接触考试数据。尽管O1-preview有满分潜力，但其表现波动，偶尔在重复提示下犯错。自一致性方法或能提升准确性。结论：OpenAI新模型潜力巨大，但风险亦需考量。

> The processes underlying human cognition are often divided into two systems: System 1, which involves fast, intuitive thinking, and System 2, which involves slow, deliberate reasoning. Previously, large language models were criticized for lacking the deeper, more analytical capabilities of System 2. In September 2024, OpenAI introduced the O1 model series, specifically designed to handle System 2-like reasoning. While OpenAI's benchmarks are promising, independent validation is still needed. In this study, we tested the O1-preview model twice on the Dutch 'Mathematics B' final exam. It scored a near-perfect 76 and 73 out of 76 points. For context, only 24 out of 16,414 students in the Netherlands achieved a perfect score. By comparison, the GPT-4o model scored 66 and 61 out of 76, well above the Dutch average of 40.63 points. The O1-preview model completed the exam in around 10 minutes, while GPT-4o took 3 minutes, and neither model had access to the exam figures. Although O1-preview had the ability to achieve a perfect score, its performance showed some variability, as it made occasional mistakes with repeated prompting. This suggests that the self-consistency method, where the consensus output is selected, could improve accuracy. We conclude that while OpenAI's new model series holds great potential, certain risks must be considered.

[Arxiv](https://arxiv.org/abs/2410.07114)