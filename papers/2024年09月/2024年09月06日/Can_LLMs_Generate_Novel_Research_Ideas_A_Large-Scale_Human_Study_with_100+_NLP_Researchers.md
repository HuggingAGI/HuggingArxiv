# 大型语言模型能否孕育创新研究思路？这项大规模研究邀请了 100 多位 NLP 专家，共同探讨这一问题。

发布时间：2024年09月06日

`LLM应用` `科学研究`

> Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers

# 摘要

> 大型语言模型 (LLM) 的最新进展让人们对其加速科学发现的潜力充满期待，越来越多的研究代理能够自主生成和验证新想法。然而，目前尚无评估表明 LLM 系统能够产生新颖的专家级想法，更不用说完成整个研究过程。为此，我们设计了一个实验，首次在专家 NLP 研究人员和 LLM 构思代理之间进行直接比较，评估研究想法的生成，并控制混杂因素。通过招募 100 多名 NLP 研究人员撰写新颖想法并进行盲评，我们得出首个统计上显著的结论：LLM 生成的想法在新颖性上优于人类专家想法 (p < 0.05)，但在可行性上稍逊一筹。深入研究后，我们发现 LLM 在自我评估和生成多样性方面存在不足，这是构建和评估研究代理时需要解决的问题。此外，我们意识到即使是专家，对新颖性的判断也可能存在偏差，因此提出一个端到端的研究设计，让研究人员将这些想法转化为完整项目，以验证新颖性和可行性判断是否会影响最终研究成果。

> Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.

[Arxiv](https://arxiv.org/abs/2409.04109)