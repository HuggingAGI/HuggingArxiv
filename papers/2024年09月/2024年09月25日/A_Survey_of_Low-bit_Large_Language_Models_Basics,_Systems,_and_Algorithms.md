# 探索低比特大型语言模型：从基础到系统与算法的全面解析

发布时间：2024年09月25日

`LLM理论` `人工智能` `云计算`

> A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms

# 摘要

> 大型语言模型（LLM）在自然语言处理领域取得了显著成就，但在实际应用中，高昂的内存和计算成本成为一大障碍。低比特量化技术应运而生，通过压缩模型参数、激活和梯度的比特宽度，有效降低资源消耗。本文深入探讨了针对 LLM 的低比特量化方法，从基本原理到系统实现，再到算法策略，全面覆盖。我们首先介绍了低比特 LLM 的核心概念和新数据格式，随后回顾了支持跨平台应用的框架和系统。接着，我们对 LLM 的低比特训练和推理技术进行了分类分析。最后，展望了低比特 LLM 的未来发展方向。本文的系统性综述，旨在为未来研究提供参考，助力通过低比特量化技术提升 LLM 的效率和实用性。

> Large language models (LLMs) have achieved remarkable advancements in natural language processing, showcasing exceptional performance across various tasks. However, the expensive memory and computational requirements present significant challenges for their practical deployment. Low-bit quantization has emerged as a critical approach to mitigate these challenges by reducing the bit-width of model parameters, activations, and gradients, thus decreasing memory usage and computational demands. This paper presents a comprehensive survey of low-bit quantization methods tailored for LLMs, covering the fundamental principles, system implementations, and algorithmic strategies. An overview of basic concepts and new data formats specific to low-bit LLMs is first introduced, followed by a review of frameworks and systems that facilitate low-bit LLMs across various hardware platforms. Then, we categorize and analyze techniques and toolkits for efficient low-bit training and inference of LLMs. Finally, we conclude with a discussion of future trends and potential advancements of low-bit LLMs. Our systematic overview from basic, system, and algorithm perspectives can offer valuable insights and guidelines for future works to enhance the efficiency and applicability of LLMs through low-bit quantization.

[Arxiv](https://arxiv.org/abs/2409.16694)