# 探索与提升大型语言模型在学术出版物中评估新颖性的能力

发布时间：2024年09月25日

`RAG` `学术出版` `人工智能`

> Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications

# 摘要

> 近期研究主要从语义角度评估了 LLM 的创造力，但学术出版物中的新颖性评估仍未被充分探索。为此，我们推出了 SchNovel 基准，用于测试 LLM 在学术论文中识别新颖性的能力。SchNovel 包含 15000 对论文，涵盖六个领域，出版日期相隔 2 至 10 年。每对中，较新的论文被视为更具新颖性。我们还提出了 RAG-Novelty，通过模拟人类评审过程来评估新颖性。实验结果显示，RAG-Novelty 在评估新颖性方面优于现有模型。

> Recent studies have evaluated the creativity/novelty of large language models (LLMs) primarily from a semantic perspective, using benchmarks from cognitive science. However, accessing the novelty in scholarly publications is a largely unexplored area in evaluating LLMs. In this paper, we introduce a scholarly novelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in scholarly papers. SchNovel consists of 15000 pairs of papers across six fields sampled from the arXiv dataset with publication dates spanning 2 to 10 years apart. In each pair, the more recently published paper is assumed to be more novel. Additionally, we propose RAG-Novelty, which simulates the review process taken by human reviewers by leveraging the retrieval of similar papers to assess novelty. Extensive experiments provide insights into the capabilities of different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms recent baseline models.

[Arxiv](https://arxiv.org/abs/2409.16605)