# 大型语言模型中的反事实词元生成

发布时间：2024年09月25日

`LLM理论` `人工智能`

> Counterfactual Token Generation in Large Language Models

# 摘要

> 当然，我很乐意为你编写一个故事：Lyra 船长站在她可靠的船只“旋风之怒”的舵轮旁，凝视着无边无际的大海。[...] 当 Lyra 意识到这个苦涩的真相时，她的眼睛充满了泪水——她为了短暂的财富牺牲了一切，失去了船员的爱、家人的爱，甚至失去了自己。尽管这个由大型语言模型生成的故事引人入胜，但人们可能会想——如果模型选择了“Maeve 船长”作为主角，故事会如何展开？我们无从得知。最先进的语言模型是无状态的——它们不保持任何内部记忆或状态。给定一个提示，它们使用自回归过程生成一系列令牌作为输出。因此，它们无法对过去生成的令牌的反事实替代方案进行推理。在这项工作中，我们的目标是赋予它们这种功能。为此，我们开发了一个基于 Gumbel-Max 结构因果模型的令牌生成因果模型。我们的模型允许任何大型语言模型以几乎不增加成本的方式进行反事实令牌生成，实现起来非常简单，并且不需要任何微调或提示工程。我们在 Llama 3 8B-instruct 上实现了我们的模型，并对反事实生成的文本进行了定性和定量分析。我们最后展示了一个反事实令牌生成的示范应用，用于偏见检测，揭示了大型语言模型构建的世界模型中的有趣见解。

> "Sure, I am happy to generate a story for you: Captain Lyra stood at the helm of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...] Lyra's eyes welled up with tears as she realized the bitter truth - she had sacrificed everything for fleeting riches, and lost the love of her crew, her family, and herself." Although this story, generated by a large language model, is captivating, one may wonder -- how would the story have unfolded if the model had chosen "Captain Maeve" as the protagonist instead? We cannot know. State-of-the-art large language models are stateless -- they maintain no internal memory or state. Given a prompt, they generate a sequence of tokens as an output using an autoregressive process. As a consequence, they cannot reason about counterfactual alternatives to tokens they have generated in the past. In this work, our goal is to enhance them with this functionality. To this end, we develop a causal model of token generation that builds upon the Gumbel-Max structural causal model. Our model allows any large language model to perform counterfactual token generation at almost no cost in comparison with vanilla token generation, it is embarrassingly simple to implement, and it does not require any fine-tuning nor prompt engineering. We implement our model on Llama 3 8B-instruct and conduct both qualitative and quantitative analyses of counterfactually generated text. We conclude with a demonstrative application of counterfactual token generation for bias detection, unveiling interesting insights about the model of the world constructed by large language models.

[Arxiv](https://arxiv.org/abs/2409.17027)