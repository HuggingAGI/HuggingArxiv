# ChatGPT 如何评估期刊文章质量？解析 REF2021 结果

发布时间：2024年09月25日

`LLM应用` `学术研究` `质量评估`

> In which fields can ChatGPT detect journal article quality? An evaluation of REF2021 results

# 摘要

> 如果自动化方法能够提供帮助，学者们在研究质量评估上所花费的时间可能会减少。尽管基于引用的指标已经得到了广泛的发展和评估，但它们存在显著的局限性，而像 ChatGPT 这样的大型语言模型 (LLM) 提供了一种替代方法。本文评估了 ChatGPT 4o-mini 是否可以用于估计学术期刊文章的质量。它从英国研究卓越框架 (REF) 2021 中的所有 34 个评估单元 (UoA) 中抽取了最多 200 篇文章，比较了 ChatGPT 评分与部门平均评分。ChatGPT 评分与部门平均评分之间几乎普遍存在正相关的 Spearman 相关性，范围从 0.08 (哲学) 到 0.78 (心理学、精神病学和神经科学)，除了临床医学 (rho=-0.12)。尽管其他解释是可能的，特别是由于 REF 评分概况是公开的，但结果表明，LLM 可以在大多数科学领域提供合理研究质量估计，特别是在物理和健康科学以及工程领域，甚至在引用数据可用之前。然而，ChatGPT 评估似乎对大多数健康和物理科学比其他领域更为积极，这对多学科评估是一个关注点，而且 ChatGPT 评分仅基于标题和摘要，因此不能作为研究评估。

> Time spent by academics on research quality assessment might be reduced if automated approaches can help. Whilst citation-based indicators have been extensively developed and evaluated for this, they have substantial limitations and Large Language Models (LLMs) like ChatGPT provide an alternative approach. This article assesses whether ChatGPT 4o-mini can be used to estimate the quality of journal articles across academia. It samples up to 200 articles from all 34 Units of Assessment (UoAs) in the UK's Research Excellence Framework (REF) 2021, comparing ChatGPT scores with departmental average scores. There was an almost universally positive Spearman correlation between ChatGPT scores and departmental averages, varying between 0.08 (Philosophy) and 0.78 (Psychology, Psychiatry and Neuroscience), except for Clinical Medicine (rho=-0.12). Although other explanations are possible, especially because REF score profiles are public, the results suggest that LLMs can provide reasonable research quality estimates in most areas of science, and particularly the physical and health sciences and engineering, even before citation data is available. Nevertheless, ChatGPT assessments seem to be more positive for most health and physical sciences than for other fields, a concern for multidisciplinary assessments, and the ChatGPT scores are only based on titles and abstracts, so cannot be research evaluations.

[Arxiv](https://arxiv.org/abs/2409.16695)