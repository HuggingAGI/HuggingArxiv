# LMGT：利用语言模型优化强化学习中的探索与利用平衡

发布时间：2024年09月07日

`Agent` `推荐系统`

> LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs

# 摘要

> 在强化学习中，环境模型的不确定性要求我们在探索与利用之间找到平衡，以优化计算资源，准确估算代理的预期奖励。特别是在奖励稀少的场景中，这种平衡尤为困难。然而，许多环境中已有丰富的先验知识，从头开始学习显得多余。为此，我们提出了 LMGT 框架，它利用大型语言模型中的先验知识，以及处理非标准数据的能力，如维基教程。LMGT 通过 LLM 引导的奖励调整，巧妙地平衡了探索与利用，提升了样本效率。我们在多种 RL 任务和工业级推荐系统中测试了 LMGT，结果显示其性能始终优于传统方法，显著缩短了训练时间。

> The uncertainty inherent in the environmental transition model of Reinforcement Learning (RL) necessitates a careful balance between exploration and exploitation to optimize the use of computational resources for accurately estimating an agent's expected reward. Achieving balance in control systems is particularly challenging in scenarios with sparse rewards. However, given the extensive prior knowledge available for many environments, it is redundant to begin learning from scratch in such settings. To address this, we introduce \textbf{L}anguage \textbf{M}odel \textbf{G}uided \textbf{T}rade-offs (i.e., \textbf{LMGT}), a novel, sample-efficient framework that leverages the comprehensive prior knowledge embedded in Large Language Models (LLMs) and their adeptness at processing non-standard data forms, such as wiki tutorials. LMGT proficiently manages the exploration-exploitation trade-off by employing reward shifts guided by LLMs, which direct agents' exploration endeavors, thereby improving sample efficiency. We have thoroughly tested LMGT across various RL tasks and deployed it in industrial-grade RL recommendation systems, where it consistently outperforms baseline methods. The results indicate that our framework can significantly reduce the time cost required during the training phase in RL.

[Arxiv](https://arxiv.org/abs/2409.04744)