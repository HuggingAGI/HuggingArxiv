# IDGen：通过项目辨别诱导提示生成，助力 LLM 评估

发布时间：2024年09月27日

`LLM应用` `人工智能`

> IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation

# 摘要

> 随着 LLM 在处理复杂任务上的能力不断提升，评估集也需同步进化，以保持其区分性。借鉴教育评估中的项目区分 (ID) 理论，我们提出了一个 ID 引导的提示合成框架，用于评估 LLM，确保评估集能根据模型能力持续优化。我们的框架兼顾广度和特异性，生成的提示不仅能全面评估 LLM 的能力，还能有效区分模型在不同任务和领域中的优劣。为确保数据质量，我们引入了自校正机制，并开发了预测提示区分度和难度分数的模型，为评估数据合成研究提供了有力工具。我们将生成的数据应用于五个 SOTA 模型，结果显示，我们的数据平均得分为 51.92，方差为 10.06，相比之前的工作更具挑战性和区分性。我们将发布一个包含 3,000 多个精心设计的提示的数据集，以推动 LLM 评估研究的发展。

> As Large Language Models (LLMs) grow increasingly adept at managing complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative. Item Discrimination (ID) theory, which is widely used in educational assessment, measures the ability of individual test items to differentiate between high and low performers. Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs to ensure the evaluation set can continually update and refine according to model abilities. Our data synthesis framework prioritizes both breadth and specificity. It can generate prompts that comprehensively evaluate the capabilities of LLMs while revealing meaningful performance differences between models, allowing for effective discrimination of their relative strengths and weaknesses across various tasks and domains. To produce high-quality data, we incorporate a self-correct mechanism into our generalization framework, and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research. We apply our generated data to evaluate five SOTA models. Our data achieves an average score of 51.92, accompanied by a variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2. The results demonstrate that the data generated by our framework is more challenging and discriminative compared to previous works. We will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs.

[Arxiv](https://arxiv.org/abs/2409.18892)