# 无需人类反馈，基于模型的偏好优化在抽象摘要中大显身手。

发布时间：2024年09月27日

`LLM应用` `摘要生成`

> Model-based Preference Optimization in Abstractive Summarization without Human Feedback

# 摘要

> 在抽象摘要任务中，源文档的庞大信息量使得生成简洁准确的摘要充满挑战。尽管大型语言模型 (LLM) 能生成流畅文本，但常因“幻觉”内容而失准。传统的监督微调方法虽有帮助，却难以持续提升摘要的忠实度。基于偏好的优化方法如 DPO 虽能改进模型以符合人类偏好，但仍依赖昂贵的人类反馈。为此，我们提出了一种新颖的基于模型的偏好优化 (MPO) 方法，无需人类反馈即可微调 LLM 以增强摘要能力。通过利用模型自身的摘要能力，我们生成了一个完全由模型驱动的偏好数据集。实验结果显示，MPO 显著提升了摘要质量，且无需人类介入。

> In abstractive summarization, the challenge of producing concise and accurate summaries arises from the vast amount of information contained in the source document. Consequently, although Large Language Models (LLMs) can generate fluent text, they often introduce inaccuracies by hallucinating content not found in the original source. While supervised fine-tuning methods that maximize likelihood contribute to this issue, they do not consistently enhance the faithfulness of the summaries. Preference-based optimization methods, such as Direct Preference Optimization (DPO), can further refine the model to align with human preferences. However, these methods still heavily depend on costly human feedback. In this work, we introduce a novel and straightforward approach called Model-based Preference Optimization (MPO) to fine-tune LLMs for improved summarization abilities without any human feedback. By leveraging the model's inherent summarization capabilities, we create a preference dataset that is fully generated by the model using different decoding strategies. Our experiments on standard summarization datasets and various metrics demonstrate that our proposed MPO significantly enhances the quality of generated summaries without relying on human feedback.

[Arxiv](https://arxiv.org/abs/2409.18618)