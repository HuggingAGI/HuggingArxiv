# 大型语言模型的隐秘应用

发布时间：2024年09月28日

`LLM应用` `人工智能` `社会科学`

> Secret Use of Large Language Models

# 摘要

> 随着大型语言模型（LLM）的发展，AI 使用的透明度责任已逐渐分散。如今，LLM 用户被鼓励或要求公开其在各类实际任务中使用 LLM 生成内容的情况。然而，用户秘密使用 LLM 的现象却给确保透明度带来了新挑战。我们通过混合方法，结合探索性调查（涵盖 125 个真实秘密使用案例）和 300 名用户的控制实验，深入探究了这一现象的背景与成因。研究发现，某些特定任务往往引发用户的秘密行为，且这种行为不受用户群体和个性差异的影响。此外，任务类型通过影响用户对 LLM 使用的外部评价感知，进而影响其使用秘密行为的意愿。这些发现为未来设计干预措施，促进 LLM 及其他 AI 技术的透明使用提供了宝贵启示。

> The advancements of Large Language Models (LLMs) have decentralized the responsibility for the transparency of AI usage. Specifically, LLM users are now encouraged or required to disclose the use of LLM-generated content for varied types of real-world tasks. However, an emerging phenomenon, users' secret use of LLM, raises challenges in ensuring end users adhere to the transparency requirement. Our study used mixed-methods with an exploratory survey (125 real-world secret use cases reported) and a controlled experiment among 300 users to investigate the contexts and causes behind the secret use of LLMs. We found that such secretive behavior is often triggered by certain tasks, transcending demographic and personality differences among users. Task types were found to affect users' intentions to use secretive behavior, primarily through influencing perceived external judgment regarding LLM usage. Our results yield important insights for future work on designing interventions to encourage more transparent disclosure of the use of LLMs or other AI technologies.

[Arxiv](https://arxiv.org/abs/2409.19450)