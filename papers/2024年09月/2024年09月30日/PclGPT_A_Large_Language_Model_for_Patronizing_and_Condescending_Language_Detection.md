# PclGPT：专为检测居高临下与傲慢言辞而设计的大型语言模型

发布时间：2024年09月30日

`LLM应用` `社交媒体` `人工智能`

> PclGPT: A Large Language Model for Patronizing and Condescending Language Detection

# 摘要

> **警告：本文内容可能引发不适，请谨慎阅读！**居高临下的语言（PCL）常针对弱势群体，作为有害言论的一种，它加剧了网络冲突，对弱势群体造成伤害。传统语言模型在检测PCL时表现不佳，因其隐含的虚伪和虚假同情。随着大型语言模型（LLMs）的发展，我们得以利用其丰富的情感语义，探索隐性毒性。本文介绍了PclGPT，一个专为PCL设计的LLM基准。我们整合了Pcl-PT/SFT数据集，并通过预训练和微调，开发了双语PclGPT-EN/CN模型，以提升隐性毒性检测。研究显示，PCL对不同弱势群体的偏见程度各异，呼吁社会更多关注以保护他们。

> Disclaimer: Samples in this paper may be harmful and cause discomfort!
  Patronizing and condescending language (PCL) is a form of speech directed at vulnerable groups. As an essential branch of toxic language, this type of language exacerbates conflicts and confrontations among Internet communities and detrimentally impacts disadvantaged groups. Traditional pre-trained language models (PLMs) perform poorly in detecting PCL due to its implicit toxicity traits like hypocrisy and false sympathy. With the rise of large language models (LLMs), we can harness their rich emotional semantics to establish a paradigm for exploring implicit toxicity. In this paper, we introduce PclGPT, a comprehensive LLM benchmark designed specifically for PCL. We collect, annotate, and integrate the Pcl-PT/SFT dataset, and then develop a bilingual PclGPT-EN/CN model group through a comprehensive pre-training and supervised fine-tuning staircase process to facilitate implicit toxic detection. Group detection results and fine-grained detection from PclGPT and other models reveal significant variations in the degree of bias in PCL towards different vulnerable groups, necessitating increased societal attention to protect them.

[Arxiv](https://arxiv.org/abs/2410.00361)