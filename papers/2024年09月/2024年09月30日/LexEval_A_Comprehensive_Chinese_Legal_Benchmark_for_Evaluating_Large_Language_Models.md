# LexEval：一款专为评估大型语言模型而设计的中文法律综合基准

发布时间：2024年09月30日

`LLM应用` `人工智能`

> LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models

# 摘要

> 大型语言模型（LLM）在自然语言处理和法律领域展现了巨大潜力，但法律应用对准确性、可靠性和公平性要求极高。不经仔细评估就应用 LLM 于法律系统，可能带来重大风险。为此，我们推出了标准化的中国法律基准 LexEval，该基准在能力建模、规模和数据三个方面独具特色：我们提出新的法律认知能力分类法，LexEval 是目前最大的中国法律评估数据集，包含 23 个任务和 14,150 个问题，并综合利用现有数据集、考试数据集和法律专家新标注的数据集。LexEval 不仅评估 LLM 的基础法律知识应用能力，还关注其应用中的伦理问题。我们评估了 38 个开源和商业 LLM，并获得了一些有趣的发现，为开发中国法律系统和 LLM 评估流程提供了宝贵见解。LexEval 数据集和排行榜已在 \url{https://github.com/CSHaitao/LexEval} 公开，并将持续更新。

> Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain. However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice. To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval. This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application. We evaluated 38 open-source and commercial LLMs and obtained some interesting findings. The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at \url{https://github.com/CSHaitao/LexEval} and will be continuously updated.

[Arxiv](https://arxiv.org/abs/2409.20288)