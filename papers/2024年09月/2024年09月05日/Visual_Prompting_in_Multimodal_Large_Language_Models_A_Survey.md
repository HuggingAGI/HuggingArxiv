# 多模态大型语言模型中的视觉提示：全面调查

发布时间：2024年09月05日

`LLM应用` `人工智能` `计算机视觉`

> Visual Prompting in Multimodal Large Language Models: A Survey

# 摘要

> 多模态大型语言模型（MLLM）为预训练的LLM增添了视觉能力。尽管文本提示在LLM中已被广泛研究，但视觉提示的出现为更精细和自由的视觉指令提供了可能。本文首次全面调查了MLLM中的视觉提示方法，涵盖视觉提示、提示生成、组合推理和提示学习。我们分类了现有的视觉提示，并探讨了图像自动注释的生成方法。此外，我们还研究了如何通过视觉提示方法，更好地将视觉编码器与LLM对齐，提升MLLM在视觉基础、对象引用和组合推理方面的能力。最后，我们总结了模型训练和上下文学习方法，以增强MLLM对视觉提示的感知和理解。本文不仅探讨了MLLM中的视觉提示技术，还展望了这些技术的未来发展。

> Multimodal large language models (MLLMs) equip pre-trained large-language models (LLMs) with visual capabilities. While textual prompting in LLMs has been widely studied, visual prompting has emerged for more fine-grained and free-form visual instructions. This paper presents the first comprehensive survey on visual prompting methods in MLLMs, focusing on visual prompting, prompt generation, compositional reasoning, and prompt learning. We categorize existing visual prompts and discuss generative methods for automatic prompt annotations on the images. We also examine visual prompting methods that enable better alignment between visual encoders and backbone LLMs, concerning MLLM's visual grounding, object referring, and compositional reasoning abilities. In addition, we provide a summary of model training and in-context learning methods to improve MLLM's perception and understanding of visual prompts. This paper examines visual prompting methods developed in MLLMs and provides a vision of the future of these methods.

[Arxiv](https://arxiv.org/abs/2409.15310)