# 尽管 LLM 检测器在识别 LLM 生成的短新闻类帖子方面有所尝试，但在现实应用中仍显不足。

发布时间：2024年09月05日

`LLM应用` `信息安全` `新闻媒体`

> LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts

# 摘要

> 随着强大 LLM 的普及，由这些模型生成的虚假信息已成为一大隐患。尽管 LLM 检测器曾被视为解决方案，但其现实效果尚未得到验证。本文聚焦于信息操作中的关键场景——由中等复杂攻击者生成的简短新闻式帖子。我们发现，现有的 LLM 检测器，无论是零-shot 还是专门训练的，在实际应用中均表现不佳。所有测试的零-shot 检测器在先前基准测试中表现不稳定，且极易受到采样温度增加的微小攻击。虽然可以开发出跨 LLM 和未见攻击泛化的专门训练检测器，但其对新人类文本的泛化能力不足。我们认为，这表明需要领域特定的基准测试，并揭示了对抗逃避弹性和过度拟合参考人类文本之间的权衡，两者均需在基准测试中得到评估。因此，我们建议重新审视当前的 LLM 检测器基准测试方法，并提供一个动态可扩展的基准测试（https://github.com/Reliable-Information-Lab-HEVS/dynamic_llm_detector_benchmark）。

> With the emergence of widely available powerful LLMs, disinformation generated by large Language Models (LLMs) has become a major concern. Historically, LLM detectors have been touted as a solution, but their effectiveness in the real world is still to be proven. In this paper, we focus on an important setting in information operations -- short news-like posts generated by moderately sophisticated attackers.
  We demonstrate that existing LLM detectors, whether zero-shot or purpose-trained, are not ready for real-world use in that setting. All tested zero-shot detectors perform inconsistently with prior benchmarks and are highly vulnerable to sampling temperature increase, a trivial attack absent from recent benchmarks. A purpose-trained detector generalizing across LLMs and unseen attacks can be developed, but it fails to generalize to new human-written texts.
  We argue that the former indicates domain-specific benchmarking is needed, while the latter suggests a trade-off between the adversarial evasion resilience and overfitting to the reference human text, with both needing evaluation in benchmarks and currently absent. We believe this suggests a re-consideration of current LLM detector benchmarking approaches and provides a dynamically extensible benchmark to allow it (https://github.com/Reliable-Information-Lab-HEVS/dynamic_llm_detector_benchmark).

[Arxiv](https://arxiv.org/abs/2409.03291)