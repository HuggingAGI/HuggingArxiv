# 大型语言模型正引领数据库问答的革新，本文将提供全面基准与评估。

发布时间：2024年09月05日

`LLM应用` `数据库` `问答系统`

> Revolutionizing Database Q&A with Large Language Models: Comprehensive Benchmark and Evaluation

# 摘要

> 大型语言模型 (LLM) 的进步彻底改变了数据库等领域的问答系统。然而，评估这些模型在数据库问答中的能力仍缺乏全面基准。为此，我们推出了 DQA，首个全面的数据库问答基准。DQA 利用 LLM 创新方法自动生成、清理和重写问答，产出超过 24 万对中英文问答，涵盖数据库知识的各个方面。这不仅评估了 LLM 的 RAG 和 TIG 能力，还构建了高度模块化、可扩展的测试平台，包含 QCR、RAG、TIG 等组件。DQA 还提供全面、准确、公平的评估流程。通过 DQA，我们评估了九种 LLM 问答机器人的优缺点及各组件的性能影响和改进潜力。我们期待 DQA 能引领未来 LLM 数据库问答研究的发展。

> The development of Large Language Models (LLMs) has revolutionized Q&A across various industries, including the database domain. However, there is still a lack of a comprehensive benchmark to evaluate the capabilities of different LLMs and their modular components in database Q&A. To this end, we introduce DQA, the first comprehensive database Q&A benchmark. DQA features an innovative LLM-based method for automating the generation, cleaning, and rewriting of database Q&A, resulting in over 240,000 Q&A pairs in English and Chinese. These Q&A pairs cover nearly all aspects of database knowledge, including database manuals, database blogs, and database tools. This inclusion allows for additional assessment of LLMs' Retrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG) capabilities in the database Q&A task. Furthermore, we propose a comprehensive LLM-based database Q&A testbed on DQA. This testbed is highly modular and scalable, with both basic and advanced components like Question Classification Routing (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Besides, DQA provides a complete evaluation pipeline, featuring diverse metrics and a standardized evaluation process to ensure comprehensiveness, accuracy, and fairness. We use DQA to evaluate the database Q&A capabilities under the proposed testbed comprehensively. The evaluation reveals findings like (i) the strengths and limitations of nine different LLM-based Q&A bots and (ii) the performance impact and potential improvements of various service components (e.g., QCR, RAG, TIG). We hope our benchmark and findings will better guide the future development of LLM-based database Q&A research.

[Arxiv](https://arxiv.org/abs/2409.04475)