# 大型语言模型中的角色设定陷阱：社会身份采纳引发的持续外群体偏见

发布时间：2024年09月05日

`LLM理论` `人工智能` `社会科学`

> Persona Setting Pitfall: Persistent Outgroup Biases in Large Language Models Arising from Social Identity Adoption

# 摘要

> 通过对比人类认知与人工智能，我们研究了大型语言模型 (LLM) 如何内化由特定提示赋予的身份。借鉴社会身份理论，这些身份设定使 LLM 能够区分“我们”（内群体）和“他们”（外群体），从而产生内群体偏好和外群体偏见。然而，现有研究多聚焦于内群体偏好，忽视了外群体偏见这一群体间偏见和歧视的根源。我们的实验表明，外群体偏见同样显著。此外，通过引导 LLM 采纳最初被忽视群体的视角，我们成功减轻了其固有的亲自由主义、反保守主义偏见。这一发现也在性别偏见研究中得到验证。我们的研究揭示了开发更公平、平衡语言模型的可能性。

> Drawing parallels between human cognition and artificial intelligence, we explored how large language models (LLMs) internalize identities imposed by targeted prompts. Informed by Social Identity Theory, these identity assignments lead LLMs to distinguish between "we" (the ingroup) and "they" (the outgroup). This self-categorization generates both ingroup favoritism and outgroup bias. Nonetheless, existing literature has predominantly focused on ingroup favoritism, often overlooking outgroup bias, which is a fundamental source of intergroup prejudice and discrimination. Our experiment addresses this gap by demonstrating that outgroup bias manifests as strongly as ingroup favoritism. Furthermore, we successfully mitigated the inherent pro-liberal, anti-conservative bias in LLMs by guiding them to adopt the perspectives of the initially disfavored group. These results were replicated in the context of gender bias. Our findings highlight the potential to develop more equitable and balanced language models.

[Arxiv](https://arxiv.org/abs/2409.03843)