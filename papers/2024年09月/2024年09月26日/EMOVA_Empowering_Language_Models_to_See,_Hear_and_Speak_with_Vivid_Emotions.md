# EMOVA：让语言模型以鲜活的情感去感知、聆听和表达

发布时间：2024年09月26日

`LLM应用` `人工智能` `语音识别`

> EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions

# 摘要

> GPT-4o，一个能进行多情感和语调语音对话的全模态模型，是全模态基础模型的重要里程碑。然而，在开源社区中，让大型语言模型端到端地感知和生成图像、文本和语音仍具挑战。现有视觉-语言模型依赖外部工具处理语音，而语音-语言模型缺乏视觉理解能力。为此，我们提出EMOVA，赋予大型语言模型端到端语音能力，同时保持顶尖视觉-语言性能。通过语义-声学解耦的语音标记器，我们发现全模态对齐能显著提升视觉-语言和语音能力。此外，轻量级风格模块实现灵活语音风格控制。EMOVA首次在视觉-语言和语音基准测试中达到顶尖性能，并支持生动情感的全模态口语对话。

> GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging in the open-source community. Existing vision-language models rely on external tools for the speech processing, while speech-language models still suffer from limited or even without vision-understanding abilities. To address this gap, we propose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech capabilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we notice surprisingly that omni-modal alignment can further enhance vision-language and speech abilities compared with the corresponding bi-modal aligned counterparts. Moreover, a lightweight style module is proposed for flexible speech style controls (e.g., emotions and pitches). For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.

[Arxiv](https://arxiv.org/abs/2409.18042)