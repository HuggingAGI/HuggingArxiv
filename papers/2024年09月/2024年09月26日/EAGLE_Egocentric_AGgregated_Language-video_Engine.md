# EAGLE：一款以自我为中心的语言与视频聚合引擎

发布时间：2024年09月26日

`LLM应用` `视频分析` `人工智能`

> EAGLE: Egocentric AGgregated Language-video Engine

# 摘要

> 以自我为中心的视频分析飞速发展，为我们从第一人称视角理解人类行为和意图提供了新视角。然而，动作识别、程序学习等任务的碎片化，以及注释不一致和模型开发的孤立性，阻碍了对视频内容的全面理解。为此，我们推出了 EAGLE 模型和 EAGLE-400K 数据集，旨在整合各种以自我为中心的视频理解任务。EAGLE-400K 作为首个专为以自我为中心视频设计的大规模指令调整数据集，包含 400K 多样样本，覆盖从活动识别到程序知识学习的广泛任务。EAGLE 模型则是一款强大的视频多模态大语言模型，能有效捕捉时空信息。我们还提出了一套评估指标，以全面评估 MLLM 在以自我为中心视频理解中的表现。实验结果显示，EAGLE 在性能上超越了现有模型，展现了其在任务特定理解与整体视频解释间的平衡能力。通过 EAGLE，我们希望为现实场景中的研究和应用开辟新道路。

> The rapid evolution of egocentric video analysis brings new insights into understanding human activities and intentions from a first-person perspective. Despite this progress, the fragmentation in tasks like action recognition, procedure learning, and moment retrieval, \etc, coupled with inconsistent annotations and isolated model development, hinders a holistic interpretation of video content. In response, we introduce the EAGLE (Egocentric AGgregated Language-video Engine) model and the EAGLE-400K dataset to provide a unified framework that integrates various egocentric video understanding tasks. EAGLE-400K, the \textit{first} large-scale instruction-tuning dataset tailored for egocentric video, features 400K diverse samples to enhance a broad spectrum of tasks from activity recognition to procedure knowledge learning. Moreover, EAGLE, a strong video multimodal large language model (MLLM), is designed to effectively capture both spatial and temporal information. In addition, we propose a set of evaluation metrics designed to facilitate a thorough assessment of MLLM for egocentric video understanding. Our extensive experiments demonstrate EAGLE's superior performance over existing models, highlighting its ability to balance task-specific understanding with holistic video interpretation. With EAGLE, we aim to pave the way for research opportunities and practical applications in real-world scenarios.

[Arxiv](https://arxiv.org/abs/2409.17523)