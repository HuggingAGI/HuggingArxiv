# 在数学评估中，学会关注边缘案例：借助 AMMORE 数据集和思维链提示，提升评分精准度。

发布时间：2024年09月26日

`LLM应用` `人工智能`

> Learning to Love Edge Cases in Formative Math Assessment: Using the AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy

# 摘要

> 本文推出 AMMORE 数据集，包含 53,000 个来自非洲学生使用的 Rori 平台的数学开放式问题与答案，并进行两项实验，评估 LLM 在评分难题中的应用。实验 1 使用零-shot、少-shot 和链式思维提示，发现链式思维提示在 92% 的难题中表现最佳，将评分准确率从 98.7% 提升至 99.9%。实验 2 通过贝叶斯知识追踪模型，发现个体问题评分准确性的提升显著影响学生掌握程度的估计，将错误分类率从 6.9% 降至 2.6%。这些结果表明，LLM 在 K-12 数学教育中评分开放式问题具有巨大潜力，可能推动形成性评估中更广泛采用开放式问题。

> This paper introduces AMMORE, a new dataset of 53,000 math open-response question-answer pairs from Rori, a learning platform used by students in several African countries and conducts two experiments to evaluate the use of large language models (LLM) for grading particularly challenging student answers. The AMMORE dataset enables various potential analyses and provides an important resource for researching student math acquisition in understudied, real-world, educational contexts. In experiment 1 we use a variety of LLM-driven approaches, including zero-shot, few-shot, and chain-of-thought prompting, to grade the 1% of student answers that a rule-based classifier fails to grade accurately. We find that the best-performing approach -- chain-of-thought prompting -- accurately scored 92% of these edge cases, effectively boosting the overall accuracy of the grading from 98.7% to 99.9%. In experiment 2, we aim to better understand the consequential validity of the improved grading accuracy, by passing grades generated by the best-performing LLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated student mastery of specific lessons. We find that relatively modest improvements in model accuracy at the individual question level can lead to significant changes in the estimation of student mastery. Where the rules-based classifier currently used to grade student, answers misclassified the mastery status of 6.9% of students across their completed lessons, using the LLM chain-of-thought approach this misclassification rate was reduced to 2.6% of students. Taken together, these findings suggest that LLMs could be a valuable tool for grading open-response questions in K-12 mathematics education, potentially enabling encouraging wider adoption of open-ended questions in formative assessment.

[Arxiv](https://arxiv.org/abs/2409.17904)