# 大型语言模型面临的有害微调攻击及其防御策略：全面调查

发布时间：2024年09月26日

`LLM应用` `人工智能`

> Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey

# 摘要

> 最新研究表明，微调即服务的商业模式潜藏严重安全风险——用户上传的有害数据可能导致模型安全受损。这种“有害微调”攻击引发广泛关注，但因其新近，研究界普遍存在误解。本文旨在澄清疑虑，正式定义研究问题。首先，我们描述威胁模型，介绍有害微调及其变种。接着，系统梳理现有攻击、防御及机制分析文献。最后，展望未来研究方向，并列出评审时可能用到的现实性问题清单。相关论文精选列表已整理并公开于：\url{https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers.}

> Recent research demonstrates that the nascent fine-tuning-as-a-service business model exposes serious safety concerns -- fine-tuning over a few harmful data uploaded by the users can compromise the safety alignment of the model. The attack, known as harmful fine-tuning, has raised a broad research interest among the community. However, as the attack is still new, \textbf{we observe from our miserable submission experience that there are general misunderstandings within the research community.} We in this paper aim to clear some common concerns for the attack setting, and formally establish the research problem. Specifically, we first present the threat model of the problem, and introduce the harmful fine-tuning attack and its variants. Then we systematically survey the existing literature on attacks/defenses/mechanical analysis of the problem. Finally, we outline future research directions that might contribute to the development of the field. Additionally, we present a list of questions of interest, which might be useful to refer to when reviewers in the peer review process question the realism of the experiment/attack/defense setting. A curated list of relevant papers is maintained and made accessible at: \url{https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers.}

[Arxiv](https://arxiv.org/abs/2409.18169)