# SKT：融合状态感知的关键点轨迹与视觉-语言模型，助力机器人服装操作

发布时间：2024年09月26日

`Agent` `机器人`

> SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation

# 摘要

> 自动化服装操作对辅助机器人来说是一大难题，因为服装种类繁多且易变形。传统方法需为每种服装单独建模，限制了灵活性。本文提出了一种基于视觉语言模型（VLM）的统一方法，通过融合视觉与语义信息，使机器人能用单一模型处理各种服装状态。我们利用先进模拟技术生成了大规模合成数据集，无需大量真实数据即可进行高效训练。实验显示，VLM 方法显著提升了关键点检测的准确性和任务成功率，为机器人服装操作提供了更灵活的解决方案。此外，这项研究还展示了 VLM 在单一框架内整合多种服装操作任务的潜力，为未来家居自动化和辅助机器人应用开辟了新路径。

> Automating garment manipulation poses a significant challenge for assistive robotics due to the diverse and deformable nature of garments. Traditional approaches typically require separate models for each garment type, which limits scalability and adaptability. In contrast, this paper presents a unified approach using vision-language models (VLMs) to improve keypoint prediction across various garment categories. By interpreting both visual and semantic information, our model enables robots to manage different garment states with a single model. We created a large-scale synthetic dataset using advanced simulation techniques, allowing scalable training without extensive real-world data. Experimental results indicate that the VLM-based method significantly enhances keypoint detection accuracy and task success rates, providing a more flexible and general solution for robotic garment manipulation. In addition, this research also underscores the potential of VLMs to unify various garment manipulation tasks within a single framework, paving the way for broader applications in home automation and assistive robotics for future.

[Arxiv](https://arxiv.org/abs/2409.18082)