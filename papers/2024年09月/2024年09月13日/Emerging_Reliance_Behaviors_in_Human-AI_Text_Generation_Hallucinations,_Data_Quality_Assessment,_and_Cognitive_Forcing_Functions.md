# 人机文本生成中的新兴依赖行为：幻觉现象、数据质量评估及认知强制功能

发布时间：2024年09月13日

`LLM应用` `客户支持` `人工智能`

> Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data Quality Assessment, and Cognitive Forcing Functions

# 摘要

> 本文探讨了幻觉和认知强制功能在人机协作文本生成中的作用，特别是利用大型语言模型 (LLM) 生成高质量对话数据。LLM 的微调需要数据支持，这是提升性能的关键。在客户支持对话中，数据表现为客户与代理的对话，可由 AI 助手生成。我们通过对 11 名用户完成 88 项任务的调查发现，幻觉的存在降低了数据质量。同时，认知强制功能虽不能完全消除幻觉的负面影响，但与幻觉共同作用，影响数据质量和用户对 AI 响应的利用方式。用户行为分析显示，对 AI 生成响应的依赖程度各异，凸显了在对话 AI 中管理幻觉的重要性。

> In this paper, we investigate the impact of hallucinations and cognitive forcing functions in human-AI collaborative text generation tasks, focusing on the use of Large Language Models (LLMs) to assist in generating high-quality conversational data. LLMs require data for fine-tuning, a crucial step in enhancing their performance. In the context of conversational customer support, the data takes the form of a conversation between a human customer and an agent and can be generated with an AI assistant. In our inquiry, involving 11 users who each completed 8 tasks, resulting in a total of 88 tasks, we found that the presence of hallucinations negatively impacts the quality of data. We also find that, although the cognitive forcing function does not always mitigate the detrimental effects of hallucinations on data quality, the presence of cognitive forcing functions and hallucinations together impacts data quality and influences how users leverage the AI responses presented to them. Our analysis of user behavior reveals distinct patterns of reliance on AI-generated responses, highlighting the importance of managing hallucinations in AI-generated content within conversational AI contexts.

[Arxiv](https://arxiv.org/abs/2409.08937)