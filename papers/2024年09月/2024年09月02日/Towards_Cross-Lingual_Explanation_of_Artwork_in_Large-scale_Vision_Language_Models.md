# 探索大型视觉语言模型中艺术品的跨语言阐释

发布时间：2024年09月02日

`LLM应用` `人工智能` `语言处理`

> Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models

# 摘要

> 随着 LVLMs 性能的提升，它们在多语言响应方面展现出越来越强的能力，预计对这些模型生成的解释的需求也将增加。然而，由于预训练和集成训练主要基于英语数据，LVLMs 在非英语语言解释生成方面的潜力尚不确定。此外，现有的多语言 QA 基准因使用机器翻译而存在文化偏见，限制了其作为评估工具的有效性。为此，本研究开发了一个不依赖机器翻译的多语言数据集，并利用该数据集评估了 LVLMs 的解释生成能力。研究还探讨了在英语环境中进行的指令调优是否能提升其他语言的性能。结果显示，LVLMs 在非英语语言中的表现不及英语，且在处理从英语数据中获取的知识方面存在困难。

> As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow. However, pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data, leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English. In addition, multilingual QA benchmarks that create datasets using machine translation have cultural differences and biases, remaining issues for use as evaluation tasks. To address these challenges, this study created an extended dataset in multiple languages without relying on machine translation. This dataset that takes into account nuances and country-specific phrases was then used to evaluate the generation explanation abilities of LVLMs. Furthermore, this study examined whether Instruction-Tuning in resource-rich English improves performance in other languages. Our findings indicate that LVLMs perform worse in languages other than English compared to English. In addition, it was observed that LVLMs struggle to effectively manage the knowledge learned from English data.

[Arxiv](https://arxiv.org/abs/2409.01584)