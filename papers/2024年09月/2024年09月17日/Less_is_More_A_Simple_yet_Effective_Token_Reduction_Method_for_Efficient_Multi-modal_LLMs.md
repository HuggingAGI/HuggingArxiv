# 少即是多：一种简洁高效的标记减少方法，助力多模态大型语言模型更上一层楼。

发布时间：2024年09月17日

`LLM应用` `人工智能` `计算机视觉`

> Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs

# 摘要

> 多模态大型语言模型 (MLLM) 的迅猛发展在各领域表现出色，但也带来了资源消耗的激增。为此，我们提出了一种新方法——基于 CLIP 度量的 Token 减少 (TRIM)，旨在提升 MLLM 的效率而不损性能。受 VQA 任务中人类注意力模式的启发，TRIM 为图像 Token 的选择与减少提供了新思路。经过 12 个数据集的广泛测试，TRIM 显著降低了计算开销，同时保持了稳定的性能。这项研究为高效 MLLM 的发展迈出了关键一步，推动了高性能模型的普及与可持续性。

> The rapid advancement of Multimodal Large Language Models (MLLMs) has led to remarkable performances across various domains. However, this progress is accompanied by a substantial surge in the resource consumption of these models. We address this pressing issue by introducing a new approach, Token Reduction using CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without sacrificing their performance. Inspired by human attention patterns in Visual Question Answering (VQA) tasks, TRIM presents a fresh perspective on the selection and reduction of image tokens. The TRIM method has been extensively tested across 12 datasets, and the results demonstrate a significant reduction in computational overhead while maintaining a consistent level of performance. This research marks a critical stride in efficient MLLM development, promoting greater accessibility and sustainability of high-performing models.

[Arxiv](https://arxiv.org/abs/2409.10994)