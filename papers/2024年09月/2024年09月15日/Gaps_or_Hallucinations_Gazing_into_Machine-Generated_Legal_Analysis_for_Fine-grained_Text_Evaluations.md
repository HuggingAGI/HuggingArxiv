# 机器生成的法律分析：是差距还是幻觉？细探文本评估的微妙之处。

发布时间：2024年09月15日

`LLM应用` `人工智能`

> Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for Fine-grained Text Evaluations

# 摘要

> 大型语言模型 (LLM) 在法律分析写作辅助方面展现出潜力，但在此环境中常出现难以识别的幻觉。我们提出问题：何时机器生成的法律分析可被视为可接受？我们引入“差距”概念，指人类与机器生成内容间的差异，这并不总是错误。与法律专家合作，我们研究了 CLERC 生成任务，构建了分类法、细粒度检测器和注释数据集。最佳检测器在测试集上达到 67% F1 分数和 80% 精确度。应用于 SOTA LLM 生成的法律分析，发现约 80% 包含不同类型的幻觉。

> Large Language Models (LLMs) show promise as a writing aid for professionals performing legal analyses. However, LLMs can often hallucinate in this setting, in ways difficult to recognize by non-professionals and existing text evaluation metrics. In this work, we pose the question: when can machine-generated legal analysis be evaluated as acceptable? We introduce the neutral notion of gaps, as opposed to hallucinations in a strict erroneous sense, to refer to the difference between human-written and machine-generated legal analysis. Gaps do not always equate to invalid generation. Working with legal experts, we consider the CLERC generation task proposed in Hou et al. (2024b), leading to a taxonomy, a fine-grained detector for predicting gap categories, and an annotated dataset for automatic evaluation. Our best detector achieves 67% F1 score and 80% precision on the test set. Employing this detector as an automated metric on legal analysis generated by SOTA LLMs, we find around 80% contain hallucinations of different kinds.

[Arxiv](https://arxiv.org/abs/2409.09947)