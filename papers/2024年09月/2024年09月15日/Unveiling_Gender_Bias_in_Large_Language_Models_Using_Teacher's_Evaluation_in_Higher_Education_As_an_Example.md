# 揭秘大型语言模型中的性别偏见：以高等教育教师评价为例

发布时间：2024年09月15日

`LLM应用` `性别研究`

> Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example

# 摘要

> 本文探讨了 LLM 在高等教育教师评价中生成的性别偏见，特别是 GPT-4 在六个学科中的评价。通过 Odds Ratio 分析、WEAT、情感分析和上下文分析，我们发现了一些反映社会刻板印象的语言模式：女性教师更多被描述为亲和和支持，而男性教师则更多被描述为娱乐性。此外，男性形容词与男性名字有较强关联，但职业和家庭词汇并未明显反映性别偏见。这些发现与现有研究一致，表明 LLM 生成的文本确实反映了社会中的既有偏见。

> This paper investigates gender bias in Large Language Model (LLM)-generated teacher evaluations in higher education setting, focusing on evaluations produced by GPT-4 across six academic subjects. By applying a comprehensive analytical framework that includes Odds Ratio (OR) analysis, Word Embedding Association Test (WEAT), sentiment analysis, and contextual analysis, this paper identified patterns of gender-associated language reflecting societal stereotypes. Specifically, words related to approachability and support were used more frequently for female instructors, while words related to entertainment were predominantly used for male instructors, aligning with the concepts of communal and agentic behaviors. The study also found moderate to strong associations between male salient adjectives and male names, though career and family words did not distinctly capture gender biases. These findings align with prior research on societal norms and stereotypes, reinforcing the notion that LLM-generated text reflects existing biases.

[Arxiv](https://arxiv.org/abs/2409.09652)