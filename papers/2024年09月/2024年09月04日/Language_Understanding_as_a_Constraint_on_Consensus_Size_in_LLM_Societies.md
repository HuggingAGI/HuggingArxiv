# 语言理解：LLM 社会共识规模的制约因素

发布时间：2024年09月04日

`LLM理论` `人工智能` `社会科学`

> Language Understanding as a Constraint on Consensus Size in LLM Societies

# 摘要

> 大型语言模型（LLM）的应用正向协作任务迈进，其中多个代理如同在LLM社会中相互交流。在这种情境下，大量LLM能够就无明确信息支持的任意规范达成共识，并以自组织方式规范自身行为。在人类社会中，无机构介入的共识能力受限于人类认知。为探究LLM是否也存在类似现象，我们融合复杂科学方法与行为科学原则，开创AI人类学新途径。研究发现，LLM能在群体中达成共识，其意见动态可由多数力量系数参数化的函数解释，该系数决定共识可能性。多数力量在语言理解能力强的模型中更强，随群体扩大而减弱，导致存在一个临界群体大小，超过此大小，共识难以实现。此临界大小随模型语言理解能力指数增长，对于顶尖模型，可达非正式人类群体典型大小的数量级之外。

> The applications of Large Language Models (LLMs) are going towards collaborative tasks where several agents interact with each other like in an LLM society. In such a setting, large groups of LLMs could reach consensus about arbitrary norms for which there is no information supporting one option over another, regulating their own behavior in a self-organized way. In human societies, the ability to reach consensus without institutions has a limit in the cognitive capacities of humans. To understand if a similar phenomenon characterizes also LLMs, we apply methods from complexity science and principles from behavioral sciences in a new approach of AI anthropology. We find that LLMs are able to reach consensus in groups and that the opinion dynamics of LLMs can be understood with a function parametrized by a majority force coefficient that determines whether consensus is possible. This majority force is stronger for models with higher language understanding capabilities and decreases for larger groups, leading to a critical group size beyond which, for a given LLM, consensus is unfeasible. This critical group size grows exponentially with the language understanding capabilities of models and for the most advanced models, it can reach an order of magnitude beyond the typical size of informal human groups.

[Arxiv](https://arxiv.org/abs/2409.02822)