# MaterialBENCH：评估大型语言模型在大学水平材料科学问题解决能力的表现

发布时间：2024年09月04日

`LLM应用` `材料科学`

> MaterialBENCH: Evaluating College-Level Materials Science Problem-Solving Abilities of Large Language Models

# 摘要

> 我们构建了名为 MaterialBENCH 的大学水平基准数据集，专门用于材料科学领域的大型语言模型 (LLM)。该数据集包含基于大学教科书的问题-答案对，分为自由回答和选择题两种类型。选择题通过添加三个错误选项来构成，以便 LLM 进行选择。大部分问题内容相同，仅答案格式不同。我们使用 MaterialBENCH 对 ChatGPT-3.5、ChatGPT-4、Bard 以及 GPT-3.5 和 GPT-4 进行了测试，分析了它们在不同类型问题上的表现差异。此外，还探讨了系统消息对选择题的影响。我们期待 MaterialBENCH 能推动 LLM 推理能力的提升，助力解决更复杂的材料科学问题，最终促进材料研究和发现。

> A college-level benchmark dataset for large language models (LLMs) in the materials science field, MaterialBENCH, is constructed. This dataset consists of problem-answer pairs, based on university textbooks. There are two types of problems: one is the free-response answer type, and the other is the multiple-choice type. Multiple-choice problems are constructed by adding three incorrect answers as choices to a correct answer, so that LLMs can choose one of the four as a response. Most of the problems for free-response answer and multiple-choice types overlap except for the format of the answers. We also conduct experiments using the MaterialBENCH on LLMs, including ChatGPT-3.5, ChatGPT-4, Bard (at the time of the experiments), and GPT-3.5 and GPT-4 with the OpenAI API. The differences and similarities in the performance of LLMs measured by the MaterialBENCH are analyzed and discussed. Performance differences between the free-response type and multiple-choice type in the same models and the influence of using system massages on multiple-choice problems are also studied. We anticipate that MaterialBENCH will encourage further developments of LLMs in reasoning abilities to solve more complicated problems and eventually contribute to materials research and discovery.

[Arxiv](https://arxiv.org/abs/2409.03161)