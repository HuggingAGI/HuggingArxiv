# 探索大型语言模型偏好学习的统一视角：一份调查报告

发布时间：2024年09月04日

`LLM理论` `人工智能` `机器学习`

> Towards a Unified View of Preference Learning for Large Language Models: A Survey

# 摘要

> 大型语言模型（LLM）能力非凡，其中关键在于将输出与人类偏好精准对齐。这一过程仅需少量数据即可显著提升性能，尽管涉及多领域且方法复杂。我们通过分解现有策略并构建统一框架，深化了对这些策略的理解，并揭示了它们之间的联系。本调查将偏好学习策略细分为模型、数据、反馈和算法四大部分，不仅加深了对现有算法的理解，还为策略整合提供了新思路。此外，我们详细展示了现有算法的应用实例，助力读者全面掌握。最后，基于这一统一视角，我们探讨了未来在偏好对齐领域的挑战与研究方向。

> Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to efficiently enhance the LLM's performance. While effective, research in this area spans multiple domains, and the methods involved are relatively complex to understand. The relationships between different methods have been under-explored, limiting the development of the preference alignment. In light of this, we break down the existing popular alignment strategies into different components and provide a unified framework to study the current alignment strategies, thereby establishing connections among them. In this survey, we decompose all the strategies in preference learning into four components: model, data, feedback, and algorithm. This unified view offers an in-depth understanding of existing alignment algorithms and also opens up possibilities to synergize the strengths of different strategies. Furthermore, we present detailed working examples of prevalent existing algorithms to facilitate a comprehensive understanding for the readers. Finally, based on our unified perspective, we explore the challenges and future research directions for aligning large language models with human preferences.

[Arxiv](https://arxiv.org/abs/2409.02795)