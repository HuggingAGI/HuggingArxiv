# 治疗作为 NLP 任务：心理学家在认知行为疗法中比较 LLM 与人类同行的表现

发布时间：2024年09月03日

`LLM应用` `心理健康` `人工智能`

> Therapy as an NLP Task: Psychologists' Comparison of LLMs and Human Peers in CBT

# 摘要

> 心理健康治疗中的一大难题是难以广泛获得治疗护理。由于机构障碍，一些寻求心理支持的人转向未经批准和测试的大型语言模型（LLM）寻求个性化治疗。我们通过混合方法临床指标探讨了LLM作为基于证据的治疗提供者的潜力与局限。利用HELPERT，一个在大语言模型上运行的提示，我们复制了基于认知行为疗法（CBT）的公开心理健康对话，比较了原始同行支持会话与重建的HELPERT会话之间的会话动态和咨询师的CBT行为。两位持有执照、接受过CBT训练的临床心理学家使用认知治疗评分量表评估了这些会话，并提供了定性反馈。同行会话的特点是共情、闲聊、治疗联盟和共享经验，但常出现治疗师漂移。HELPERT重建的会话则表现出最小的治疗师漂移和更高的CBT方法依从性，但缺乏合作、共情和文化理解。通过CTRS评分和心理学家的反馈，我们强调了人机协作对于可扩展心理健康的重要性。我们的工作揭示了在治疗环境中赋予LLM类似人类的特性的伦理影响，特别是欺骗性共情的风险，这可能导致不切实际的患者期望和潜在伤害。

> Wider access to therapeutic care is one of the biggest challenges in mental health treatment. Due to institutional barriers, some people seeking mental health support have turned to large language models (LLMs) for personalized therapy, even though these models are largely unsanctioned and untested. We investigate the potential and limitations of using LLMs as providers of evidence-based therapy by using mixed methods clinical metrics. Using HELPERT, a prompt run on a large language model using the same process and training as a comparative group of peer counselors, we replicated publicly accessible mental health conversations rooted in Cognitive Behavioral Therapy (CBT) to compare session dynamics and counselor's CBT-based behaviors between original peer support sessions and their reconstructed HELPERT sessions. Two licensed, CBT-trained clinical psychologists evaluated the sessions using the Cognitive Therapy Rating Scale and provided qualitative feedback. Our findings show that the peer sessions are characterized by empathy, small talk, therapeutic alliance, and shared experiences but often exhibit therapist drift. Conversely, HELPERT reconstructed sessions exhibit minimal therapist drift and higher adherence to CBT methods but display a lack of collaboration, empathy, and cultural understanding. Through CTRS ratings and psychologists' feedback, we highlight the importance of human-AI collaboration for scalable mental health. Our work outlines the ethical implication of imparting human-like subjective qualities to LLMs in therapeutic settings, particularly the risk of deceptive empathy, which may lead to unrealistic patient expectations and potential harm.

[Arxiv](https://arxiv.org/abs/2409.02244)