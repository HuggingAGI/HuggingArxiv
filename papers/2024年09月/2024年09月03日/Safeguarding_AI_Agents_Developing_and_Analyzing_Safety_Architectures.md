# 守护AI代理：构建与评估安全架构

发布时间：2024年09月03日

`Agent` `自动化`

> Safeguarding AI Agents: Developing and Analyzing Safety Architectures

# 摘要

> 由大型语言模型驱动的 AI 代理在需要精确性和有效性的应用中表现出色，但也存在潜在风险，如不安全行为、易受攻击、缺乏透明度和产生幻觉。随着 AI 代理在关键行业的普及，实施有效的安全协议变得尤为重要。本文探讨了 AI 系统中安全措施的必要性，特别是与人类团队合作的系统。我们提出了三种增强安全协议的框架：LLM 驱动的输入输出过滤器、集成安全代理和嵌入安全检查的分层委托系统。通过实施这些框架并测试其应对不安全使用案例的能力，我们全面评估了它们的有效性。结果表明，这些框架能显著提升 AI 代理系统的安全性和可靠性，减少潜在危害。我们的研究为创建安全可靠的 AI 应用，特别是在自动化操作中，奠定了基础，并为确保 AI 代理在现实世界中的负责任使用提供了防护措施。

> AI agents, specifically powered by large language models, have demonstrated exceptional capabilities in various applications where precision and efficacy are necessary. However, these agents come with inherent risks, including the potential for unsafe or biased actions, vulnerability to adversarial attacks, lack of transparency, and tendency to generate hallucinations. As AI agents become more prevalent in critical sectors of the industry, the implementation of effective safety protocols becomes increasingly important. This paper addresses the critical need for safety measures in AI systems, especially ones that collaborate with human teams. We propose and evaluate three frameworks to enhance safety protocols in AI agent systems: an LLM-powered input-output filter, a safety agent integrated within the system, and a hierarchical delegation-based system with embedded safety checks. Our methodology involves implementing these frameworks and testing them against a set of unsafe agentic use cases, providing a comprehensive evaluation of their effectiveness in mitigating risks associated with AI agent deployment. We conclude that these frameworks can significantly strengthen the safety and security of AI agent systems, minimizing potential harmful actions or outputs. Our work contributes to the ongoing effort to create safe and reliable AI applications, particularly in automated operations, and provides a foundation for developing robust guardrails to ensure the responsible use of AI agents in real-world applications.

[Arxiv](https://arxiv.org/abs/2409.03793)