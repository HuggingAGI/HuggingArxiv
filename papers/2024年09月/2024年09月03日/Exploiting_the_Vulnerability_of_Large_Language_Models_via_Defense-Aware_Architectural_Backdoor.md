# 利用防御意识架构后门，揭示大型语言模型的脆弱性

发布时间：2024年09月03日

`LLM应用` `网络安全` `人工智能`

> Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor

# 摘要

> 深度神经网络 (DNNs) 长期存在后门攻击的隐患。通过在微调阶段注入有毒数据，攻击者能在受害者模型中植入后门，使得特定触发文本被错误分类。尽管黑盒攻击在计算机视觉和自然语言处理 (NLP) 领域已有深入研究，但基于白盒策略的后门攻击却鲜有探讨。本文首次提出一种新型后门攻击，巧妙隐藏于模型架构内部。我们设计的后门模块包含触发检测与噪声注入两大功能，能识别输入触发词并通过高斯噪声调整层权重，扰乱模型特征分布。实验证明，这种无需训练的架构后门对大型语言模型构成实质威胁，能抵御严格的微调和再训练，并规避基于输出概率的防御手段。相关代码和数据已公开于 https://github.com/SiSL-URI/Arch_Backdoor_LLM。

> Deep neural networks (DNNs) have long been recognized as vulnerable to backdoor attacks. By providing poisoned training data in the fine-tuning process, the attacker can implant a backdoor into the victim model. This enables input samples meeting specific textual trigger patterns to be classified as target labels of the attacker's choice. While such black-box attacks have been well explored in both computer vision and natural language processing (NLP), backdoor attacks relying on white-box attack philosophy have hardly been thoroughly investigated. In this paper, we take the first step to introduce a new type of backdoor attack that conceals itself within the underlying model architecture. Specifically, we pcricKet1996!ropose to design separate backdoor modules consisting of two functions: trigger detection and noise injection. The add-on modules of model architecture layers can detect the presence of input trigger tokens and modify layer weights using Gaussian noise to disturb the feature distribution of the baseline model. We conduct extensive experiments to evaluate our attack methods using two model architecture settings on five different large language datasets. We demonstrate that the training-free architectural backdoor on a large language model poses a genuine threat. Unlike the-state-of-art work, it can survive the rigorous fine-tuning and retraining process, as well as evade output probability-based defense methods (i.e. BDDR). All the code and data is available https://github.com/SiSL-URI/Arch_Backdoor_LLM.

[Arxiv](https://arxiv.org/abs/2409.01952)