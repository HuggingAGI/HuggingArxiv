# MedUnA：通过语言引导，实现视觉-语言模型在医学图像分类中的无监督适应

发布时间：2024年09月03日

`LLM应用` `图像处理`

> MedUnA: Language guided Unsupervised Adaptation of Vision-Language Models for Medical Image Classification

# 摘要

> 在医学图像分类领域，由于标记医学图像的稀缺，监督学习颇具挑战。本研究一改传统预训练后微调的模式，转而利用视觉-语言模型（VLMs）中的视觉-文本对齐技术，推动无监督学习的发展。我们提出的医疗无监督适应（MedUnA）方案，分为适配器预训练和无监督学习两个阶段。首阶段，借助大型语言模型（LLM）生成详尽的类别描述，经BioBERT文本编码器处理后，通过轻量适配器实现文本嵌入与类别标签的精准对齐。次阶段，将训练有素的适配器与MedCLIP视觉编码器结合，运用对比熵损失与提示调优技术，确保视觉嵌入的高度对齐。此外，我们引入自熵最小化策略，以生成更自信的嵌入，这对无监督学习至关重要。在胸部X光片、眼底和皮肤病变图像三种数据模态上的实验表明，MedUnA相较于基线方法，平均准确率显著提升，充分验证了其有效性。

> In medical image classification, supervised learning is challenging due to the lack of labeled medical images. Contrary to the traditional \textit{modus operandi} of pre-training followed by fine-tuning, this work leverages the visual-textual alignment within Vision-Language models (\texttt{VLMs}) to facilitate the unsupervised learning.
  Specifically, we propose \underline{Med}ical \underline{Un}supervised \underline{A}daptation (\texttt{MedUnA}), constituting two-stage training: Adapter Pre-training, and Unsupervised Learning. In the first stage, we use descriptions generated by a Large Language Model (\texttt{LLM}) corresponding to class labels, which are passed through the text encoder \texttt{BioBERT}. The resulting text embeddings are then aligned with the class labels by training a lightweight \texttt{adapter}. We choose \texttt{\texttt{LLMs}} because of their capability to generate detailed, contextually relevant descriptions to obtain enhanced text embeddings.
  In the second stage, the trained \texttt{adapter} is integrated with the visual encoder of \texttt{MedCLIP}. This stage employs a contrastive entropy-based loss and prompt tuning to align visual embeddings. We incorporate self-entropy minimization into the overall training objective to ensure more confident embeddings, which are crucial for effective unsupervised learning and alignment. We evaluate the performance of \texttt{MedUnA} on three different kinds of data modalities - chest X-rays, eye fundus and skin lesion images. The results demonstrate significant accuracy gain on average compared to the baselines across different datasets, highlighting the efficacy of our approach.

[Arxiv](https://arxiv.org/abs/2409.02729)