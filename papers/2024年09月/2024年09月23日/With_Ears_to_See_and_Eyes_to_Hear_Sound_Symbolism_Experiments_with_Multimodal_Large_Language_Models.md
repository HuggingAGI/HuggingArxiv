# 以耳为眼，以眼为耳：探索多模态大型语言模型的声音象征实验

发布时间：2024年09月23日

`LLM应用` `心理学` `人工智能`

> With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models

# 摘要

> 最近，LLM 和 VLM 展示了在心理语言学实验中替代人类的能力。然而，一个未解之谜是，仅依赖视觉和文本的模型能否通过抽象推理隐式理解声音现象。为此，我们研究了 VLM 和 LLM 展示声音象征主义的能力，以及它们如何通过多模态模型的语言和视觉模块“听”。我们进行了多项实验，包括复制经典的声音象征任务，并比较了人类与 LLM 的判断。结果显示，VLM 与人类的一致性不一，且可能需要更多任务信息。此外，我们发现大小象征主义比形状象征主义更容易识别，且模型大小对理解语言象似性至关重要。

> Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to ``hear'' via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks, and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for in silico experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size.

[Arxiv](https://arxiv.org/abs/2409.14917)