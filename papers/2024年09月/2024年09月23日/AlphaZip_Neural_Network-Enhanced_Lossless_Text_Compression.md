# AlphaZip：借助神经网络提升的无损文本压缩技术

发布时间：2024年09月23日

`LLM应用` `信息技术` `数据压缩`

> AlphaZip: Neural Network-Enhanced Lossless Text Compression

# 摘要

> 数据压缩技术不断进步，传统信息论方法广泛应用于文本、图像和视频的压缩。近期，生成式AI在预测性压缩技术中的应用备受关注。本文提出了一种基于大型语言模型（LLM）的无损文本压缩方法，分为两个关键步骤：首先，利用密集神经网络架构（如transformer块）进行预测；然后，使用自适应哈夫曼、LZ77或Gzip等标准算法压缩预测结果。通过广泛的分析和基准测试，证明神经压缩技术相比传统方法表现更优。

> Data compression continues to evolve, with traditional information theory methods being widely used for compressing text, images, and videos. Recently, there has been growing interest in leveraging Generative AI for predictive compression techniques. This paper introduces a lossless text compression approach using a Large Language Model (LLM). The method involves two key steps: first, prediction using a dense neural network architecture, such as a transformer block; second, compressing the predicted ranks with standard compression algorithms like Adaptive Huffman, LZ77, or Gzip. Extensive analysis and benchmarking against conventional information-theoretic baselines demonstrate that neural compression offers improved performance.

[Arxiv](https://arxiv.org/abs/2409.15046)