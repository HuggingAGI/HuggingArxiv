# HW-TSC 向 CCMT 2024 机器翻译任务提交了作品

发布时间：2024年09月23日

`LLM应用` `机器翻译` `语言处理`

> HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks

# 摘要

> 本文展示了华为翻译服务中心 (HW-TSC) 在第 20 届中国机器翻译会议 (CCMT 2024) 上的机器翻译任务提交。我们参与了双语和多领域机器翻译任务，采用正则化 dropout、双向训练、数据多样化等策略，基于深度 Transformer-big 架构训练神经机器翻译 (NMT) 模型。此外，我们探索了大型语言模型 (LLM) 对 NMT 系统翻译质量的提升作用，通过监督微调训练 llama2-13b 作为自动后编辑 (APE) 模型，改进多领域任务中的翻译结果。这些策略使我们在最终评估中取得了优异成绩。

> This paper presents the submission of Huawei Translation Services Center (HW-TSC) to machine translation tasks of the 20th China Conference on Machine Translation (CCMT 2024). We participate in the bilingual machine translation task and multi-domain machine translation task. For these two translation tasks, we use training strategies such as regularized dropout, bidirectional training, data diversification, forward translation, back translation, alternated training, curriculum learning, and transductive ensemble learning to train neural machine translation (NMT) models based on the deep Transformer-big architecture. Furthermore, to explore whether large language model (LLM) can help improve the translation quality of NMT systems, we use supervised fine-tuning to train llama2-13b as an Automatic post-editing (APE) model to improve the translation results of the NMT model on the multi-domain machine translation task. By using these plyometric strategies, our submission achieves a competitive result in the final evaluation.

[Arxiv](https://arxiv.org/abs/2409.14842)