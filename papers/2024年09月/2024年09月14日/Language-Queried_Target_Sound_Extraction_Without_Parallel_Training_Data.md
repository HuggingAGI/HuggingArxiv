# 无平行训练数据的语言查询目标声音提取

发布时间：2024年09月14日

`LLM应用` `音频处理` `人工智能`

> Language-Queried Target Sound Extraction Without Parallel Training Data

# 摘要

> 语言查询的目标声音提取 (TSE) 旨在根据语言查询从混合物中提取特定声音。传统的全监督训练方案需要大量注释的平行音频-文本数据，这些数据劳动密集。我们引入了一种无需语言的训练方案，仅需要未标记的音频剪辑进行 TSE 模型训练，通过利用对比语言-音频预训练模型 (CLAP) 的多模态表示对齐特性。在普通的无需语言训练阶段，目标音频使用预训练的 CLAP 音频编码器进行编码，以形成 TSE 模型的条件嵌入，而在推理过程中，用户语言查询由 CLAP 文本编码器编码。这种直接的方法面临挑战，因为训练和推理查询之间的模态差距以及训练期间直接暴露于目标音频的信息泄露。为了解决这个问题，我们提出了一种检索增强策略。具体来说，我们使用大型语言模型 (LLM) 生成的音频字幕创建一个嵌入缓存。在训练期间，目标音频嵌入从该缓存中检索文本嵌入以用作条件嵌入，确保训练和推理之间的一致模态并消除信息泄露。广泛的实验结果表明，我们的检索增强方法在现有最先进技术的基础上实现了持续且显著的性能提升，并具有更好的泛化能力。

> Language-queried target sound extraction (TSE) aims to extract specific sounds from mixtures based on language queries. Traditional fully-supervised training schemes require extensively annotated parallel audio-text data, which are labor-intensive. We introduce a language-free training scheme, requiring only unlabelled audio clips for TSE model training by utilizing the multi-modal representation alignment nature of the contrastive language-audio pre-trained model (CLAP). In a vanilla language-free training stage, target audio is encoded using the pre-trained CLAP audio encoder to form a condition embedding for the TSE model, while during inference, user language queries are encoded by CLAP text encoder. This straightforward approach faces challenges due to the modality gap between training and inference queries and information leakage from direct exposure to target audio during training. To address this, we propose a retrieval-augmented strategy. Specifically, we create an embedding cache using audio captions generated by a large language model (LLM). During training, target audio embeddings retrieve text embeddings from this cache to use as condition embeddings, ensuring consistent modalities between training and inference and eliminating information leakage. Extensive experiment results show that our retrieval-augmented approach achieves consistent and notable performance improvements over existing state-of-the-art with better generalizability.

[Arxiv](https://arxiv.org/abs/2409.09398)