# 基础模型之谜：我们是否回到了1924年的起点？

发布时间：2024年09月11日

`LLM理论` `人工智能` `神经科学`

> Understanding Foundation Models: Are We Back in 1924?

# 摘要

> 本文探讨了AI领域基础模型（FMs）的迅猛发展及其对智能与推理的影响。FMs通过大规模数据训练和嵌入空间捕捉语义关系，展现出独特特性。近期，FMs在推理能力上的突破，并非单纯依赖模型扩大，而是得益于创新训练技术，如引发grokking现象。同时，FMs的基准测试充满挑战，其结构与人类大脑的对比也引发深思。尽管FMs在推理和知识表示上前景光明，但其内部机制的理解仍如神经科学探索大脑般艰巨。FMs与大脑虽有相似，但本质差异告诫我们，不宜直接对比，也不应期待神经科学立即揭示FMs的奥秘。

> This position paper explores the rapid development of Foundation Models (FMs) in AI and their implications for intelligence and reasoning. It examines the characteristics of FMs, including their training on vast datasets and use of embedding spaces to capture semantic relationships. The paper discusses recent advancements in FMs' reasoning abilities which we argue cannot be attributed to increased model size but to novel training techniques which yield learning phenomena like grokking. It also addresses the challenges in benchmarking FMs and compares their structure to the human brain. We argue that while FMs show promising developments in reasoning and knowledge representation, understanding their inner workings remains a significant challenge, similar to ongoing efforts in neuroscience to comprehend human brain function. Despite having some similarities, fundamental differences between FMs and the structure of human brain warn us against making direct comparisons or expecting neuroscience to provide immediate insights into FM function.

[Arxiv](https://arxiv.org/abs/2409.07618)