# DSBench：数据科学代理与专家之间，还有多少差距？

发布时间：2024年09月11日

`Agent` `数据科学`

> DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?

# 摘要

> LLM 和 LVLM 展示了强大的语言和视觉推理能力，推动了构建购物助手或 AI 软件工程师等应用代理的趋势。尽管已有多个数据科学基准测试，但它们在复杂性上仍无法与真实应用相媲美。为此，我们推出了 DSBench，一个包含 466 个数据分析和 74 个数据建模任务的综合基准，源自 Eloquence 和 Kaggle 竞赛。DSBench 通过模拟长上下文、多模态背景和复杂数据处理，提供了一个真实的测试环境。评估显示，最先进的模型和代理在多数任务中表现不佳，最佳代理仅能解决 34.12% 的任务，存在 34.74% 的性能差距。这凸显了开发更智能、更自主的数据科学代理的迫切需求。

> Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.

[Arxiv](https://arxiv.org/abs/2409.07703)