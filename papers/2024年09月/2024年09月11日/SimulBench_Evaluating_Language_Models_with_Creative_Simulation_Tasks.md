# SimulBench：通过创意模拟任务评估语言模型

发布时间：2024年09月11日

`LLM应用` `人工智能` `软件开发`

> SimulBench: Evaluating Language Models with Creative Simulation Tasks

# 摘要

> 我们推出了 SimulBench，一个专为评估大型语言模型（LLM）在多样创意模拟场景中的表现而设计的基准测试，如模拟 Linux 终端或与用户玩文字游戏。尽管这些模拟任务能有效衡量 LLM 的智能水平，但它们在现有基准测试中鲜少出现。主要难点在于开发一个既能公平测试不同 LLM，又能保留用户与 AI 间多轮互动特性的评估框架。为此，我们提议先用固定 LLM 作为用户代理，在不同任务下与 LLM 互动收集对话，再提取挑战性对话脚本评估目标 LLM。为实现自动化评估，GPT-4 被指定为评审员，负责评判目标 LLM 在多轮对话后生成回复的质量。实验结果显示，这些独特模拟任务仍具挑战性，并揭示了专有模型与顶尖开源 LLM 间的差距，例如 GPT-4-turbo 在 18.55% 的案例中超越了 LLaMA-3-70b-Chat。

> We introduce SimulBench, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation scenarios, such as acting as a Linux terminal or playing text games with users. While these simulation tasks serve as effective measures of an LLM's general intelligence, they are seldom incorporated into existing benchmarks. A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI. To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks. Then, challenging dialogue scripts are extracted for evaluating different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts. Our comprehensive experiments indicate that these simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs. For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55\% more cases.

[Arxiv](https://arxiv.org/abs/2409.07641)