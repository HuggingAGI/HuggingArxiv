# 母语与非母语提示：一场对比分析

发布时间：2024年09月11日

`LLM应用` `语言学`

> Native vs Non-Native Language Prompting: A Comparative Analysis

# 摘要

> 大型语言模型（LLM）在多个领域表现出色，包括标准自然语言处理（NLP）任务。提示语作为引出模型知识的关键，由自然语言指令构成。大多数 LLM 基于丰富的数字内容（如文本、图像、音频和视频）进行训练，因此对高资源语言掌握较好，但对低资源语言则表现欠佳。提示语在理解模型能力中至关重要，其语言选择仍是一个重要研究课题。尽管已有不少研究，但针对中低资源语言的探索仍显不足。本研究针对 11 个 NLP 任务，涉及 12 个阿拉伯语数据集（共 9.7K 数据点），探讨了母语与非母语提示策略的差异。我们进行了 197 次实验，涵盖 3 个 LLM、12 个数据集和 3 种提示策略。结果显示，非母语提示平均表现最佳，其次是混合提示和母语提示。

> Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks. To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions. Most open and closed source LLMs are trained on available labeled and unlabeled resources--digital content such as text, images, audio, and videos. Hence, these models have better knowledge for high-resourced languages but struggle with low-resourced languages. Since prompts play a crucial role in understanding their capabilities, the language used for prompts remains an important research question. Although there has been significant research in this area, it is still limited, and less has been explored for medium to low-resourced languages. In this study, we investigate different prompting strategies (native vs. non-native) on 11 different NLP tasks associated with 12 different Arabic datasets (9.7K data points). In total, we conducted 197 experiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our findings suggest that, on average, the non-native prompt performs the best, followed by mixed and native prompts.

[Arxiv](https://arxiv.org/abs/2409.07054)