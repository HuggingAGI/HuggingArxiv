# 真实还是机器人？探讨 LLM 在对话中模拟人类反应特质的能力。

发布时间：2024年09月12日

`LLM应用` `人工智能`

> Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue

# 摘要

> 构建对话任务的数据集既费钱又费时，因为需要招募、培训并从参与者那里收集数据。因此，近期许多研究转向使用大型语言模型 (LLM) 来模拟人-人及人-LLM 的互动，因为这些模型能生成逼真的人类文本。然而，LLM 模拟究竟在多大程度上反映了真实的人类对话？我们通过生成 100,000 对 LLM-LLM 和人类-LLM 对话的数据集，并量化模拟与真实对话的匹配度来解答这一问题。结果显示，模拟与真实对话的匹配度较低，尤其在风格和内容上存在系统性差异。此外，在不同语言（如英语、中文和俄语）的对话比较中，模型表现相似。我们的研究指出，当人类写作风格更接近 LLM 时，LLM 的表现通常更佳。

> Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.

[Arxiv](https://arxiv.org/abs/2409.08330)