# 利用大型语言模型实现中文语音识别的全文纠错

发布时间：2024年09月12日

`LLM应用` `语音识别` `媒体制作`

> Full-text Error Correction for Chinese Speech Recognition with Large Language Model

# 摘要

> 大型语言模型（LLM）在自动语音识别（ASR）的错误纠正方面潜力巨大，但多数研究仅限于短时语音数据。本文探讨了LLM在长录音（如播客、新闻广播和会议转录）生成的全文中的错误纠正效果。首先，我们创建了名为ChFT的中文全文错误纠正数据集，涵盖文本到语音合成、ASR及错误纠正对提取。该数据集不仅支持全文和段落级别的错误纠正，还能处理标点恢复和逆文本规范化等多样错误类型。其次，我们通过多样提示和目标格式对预训练LLM进行微调，并评估其在全文错误纠正中的表现。实验结果显示，微调后的LLM在不同提示下均表现出色，各有优劣。这为未来研究奠定了坚实基础。数据集已公开。

> Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic Speech Recognition (ASR). However, most research focuses on utterances from short-duration speech recordings, which are the predominant form of speech data for supervised ASR training. This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer speech recordings, such as transcripts from podcasts, news broadcasts, and meetings. First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-speech synthesis, ASR, and error-correction pair extractor. This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive. Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction. Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. This establishes a promising baseline for further research. The dataset is available on the website.

[Arxiv](https://arxiv.org/abs/2409.07790)