# 超链接

发布时间：2024年09月29日

`LLM理论` `人工智能`

> Hyper-Connections

# 摘要

> 我们引入了超连接，一种既简单又高效的方法，可替代传统的残差连接。它巧妙解决了残差连接中梯度消失与表示崩溃之间的矛盾。理论上，超连接让网络能灵活调整不同深度特征的连接强度，并动态重组层级。在大型语言模型的预训练中，无论是密集还是稀疏模型，超连接都带来了显著的性能提升。视觉任务的实验同样验证了其优势。我们期待这一方法能在众多AI领域大放异彩。

> We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.

[Arxiv](https://arxiv.org/abs/2409.19606)