# 利用多语言变换器为低资源尼泊尔语生成摘要

发布时间：2024年09月29日

`LLM应用`

> Abstractive Summarization of Low resourced Nepali language using Multilingual Transformers

# 摘要

> 尼泊尔语的自动文本摘要是一个尚未被充分探索的领域。尽管抽取式摘要已有大量研究，但抽象式摘要，尤其是针对尼泊尔语这样的低资源语言，仍处于起步阶段。本研究利用多语言变压器模型 mBART 和 mT5，通过抽象摘要技术为尼泊尔新闻生成标题。首先，通过网络抓取创建了一个摘要数据集，解决了尼泊尔语文本摘要的难题。随后，对这些模型进行了微调，并使用 ROUGE 分数和人工评估来确保生成的摘要既连贯又忠实于原文。在人工评估中，参与者根据相关性、流畅性、简洁性等标准，从多个模型生成的摘要中挑选最佳。结果显示，4 位量化的 mBART 与 LoRA 模型在生成高质量尼泊尔新闻标题方面表现突出，被选中的次数高达 34.05%，远超其他微调模型。

> Automatic text summarization in Nepali language is an unexplored area in natural language processing (NLP). Although considerable research has been dedicated to extractive summarization, the area of abstractive summarization, especially for low-resource languages such as Nepali, remains largely unexplored. This study explores the use of multilingual transformer models, specifically mBART and mT5, for generating headlines for Nepali news articles through abstractive summarization. The research addresses key challenges associated with summarizing texts in Nepali by first creating a summarization dataset through web scraping from various Nepali news portals. These multilingual models were then fine-tuned using different strategies. The performance of the fine-tuned models were then assessed using ROUGE scores and human evaluation to ensure the generated summaries were coherent and conveyed the original meaning. During the human evaluation, the participants were asked to select the best summary among those generated by the models, based on criteria such as relevance, fluency, conciseness, informativeness, factual accuracy, and coverage. During the evaluation with ROUGE scores, the 4-bit quantized mBART with LoRA model was found to be effective in generating better Nepali news headlines in comparison to other models and also it was selected 34.05% of the time during the human evaluation, outperforming all other fine-tuned models created for Nepali News headline generation.

[Arxiv](https://arxiv.org/abs/2409.19566)