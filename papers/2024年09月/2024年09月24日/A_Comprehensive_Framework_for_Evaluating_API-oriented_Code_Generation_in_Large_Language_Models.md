# AutoAPIEval：自动化评估 LLM 在 API 导向代码生成中的框架

发布时间：2024年09月24日

`LLM应用` `软件开发` `API`

> A Comprehensive Framework for Evaluating API-oriented Code Generation in Large Language Models

# 摘要

> GitHub Copilot 和 ChatGPT 等大型语言模型（LLMs）已成为代码生成的利器，大幅提升开发效率。然而，现有基准多聚焦于通用代码生成，忽略了面向 API 的代码生成。随着对 API 代码生成需求的增加，我们急需一种系统化、自动化的评估方法。为此，我们推出了 AutoAPIEval，一个轻量级自动化框架，专门用于评估 LLM 在 API 代码生成方面的能力。该框架兼容任何提供 API 文档的库，并聚焦于 API 推荐和代码示例生成两项任务，通过四个指标评估生成结果，如错误 API 推荐比例和不可编译代码比例。我们还通过 ChatGPT、MagiCoder 和 DeepSeek Coder 的案例研究，验证了框架的有效性。研究发现，LLM 在不同任务中的表现差异显著，ChatGPT 更符合指令，但在代码示例生成上与同行效果相当。此外，我们识别了影响代码质量的关键因素，并构建了高精度分类器。检索增强生成虽提升了代码质量，但效果因 LLM 而异。

> Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as powerful tools for code generation, significantly enhancing productivity and accelerating software development. However, existing benchmarks primarily focus on general code generation without considering API-oriented code generation, i.e., generating code that invokes APIs from specific libraries. Given the growing demand for API-oriented code generation, there is a pressing need for a systematic and automated approach to evaluate LLM on API-oriented code generation. To address this gap, we propose AutoAPIEval, a lightweight and automated framework designed to evaluate the capabilities of LLMs in API-oriented code generation. Our framework works with any library that provides API documentation and focuses on two unit tasks: API recommendation and code example generation, along with four metrics to evaluate the generated APIs and code examples, such as the proportion of incorrect API recommendations for Task 1, and the proportion of code examples where no specific API is invoked and uncompilable/unexecutable code examples for Task 2. In addition, we conducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder) and Java Runtime Environment 8 to demonstrate the framework's effectiveness. Our findings reveal substantial variability in LLM performance across tasks, with ChatGPT adhering better to instructions, while sharing similar effectiveness in code example generation with its counterparts (i.e., MagiCoder and DeekSeek Coder). We also identify key factors associated with code quality, such as API popularity and model confidence, and build classifiers that achieve high accuracy in detecting incorrect API recommendations and erroneous code examples. Retrieval-augmented generation enhances the quality of code generated by LLMs, though its effectiveness varies across different LLMs.

[Arxiv](https://arxiv.org/abs/2409.15228)