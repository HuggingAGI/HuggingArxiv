# 联邦大型语言模型：现状与未来展望

发布时间：2024年09月24日

`LLM应用` `隐私保护` `机器学习`

> Federated Large Language Models: Current Progress and Future Directions

# 摘要

> 大型语言模型在实际应用中迅速普及，但数据收集中的隐私问题不容忽视。联邦学习通过允许多个客户端协作训练 LLM 而不共享本地数据，提供了一种解决方案。然而，这也带来了数据异质性和高通信成本等新挑战。本文综述了 FedLLM 的最新进展，重点关注联邦设置中的微调和提示学习，并探讨了未来的研究方向，包括预训练和 LLM 如何进一步增强联邦学习。

> Large language models are rapidly gaining popularity and have been widely adopted in real-world applications. While the quality of training data is essential, privacy concerns arise during data collection. Federated learning offers a solution by allowing multiple clients to collaboratively train LLMs without sharing local data. However, FL introduces new challenges, such as model convergence issues due to heterogeneous data and high communication costs. A comprehensive study is required to address these challenges and guide future research. This paper surveys Federated learning for LLMs (FedLLM), highlighting recent advances and future directions. We focus on two key aspects: fine-tuning and prompt learning in a federated setting, discussing existing work and associated research challenges. We finally propose potential research directions for federated LLMs, including pre-training and how LLMs can further enhance federated learning.

[Arxiv](https://arxiv.org/abs/2409.15723)