# 利用弱标签微调轻量级 LLM，提升放射学报告中疾病检测的精准度

发布时间：2024年09月24日

`LLM应用` `人工智能`

> Enhancing disease detection in radiology reports through fine-tuning lightweight LLM on weak labels

# 摘要

> 尽管 LLM 在医疗领域的应用取得了显著进展，但模型大小和缺乏特定队列的标记数据集等限制仍阻碍了其实际应用。本研究探讨了通过使用合成标签数据集微调轻量级 LLM（如 Llama 3.1-8B）来提升其性能的可能性。当任务特定的合成标签质量较高时（例如，由 GPT4-o 生成），Llama 3.1-8B 在开放式疾病检测任务中表现出色，微 F1 得分为 0.91。而当合成标签质量较低时（例如，来自 MIMIC-CXR 数据集），经过微调的 Llama 3.1-8B 在与精选标签校准后，仍能超越其嘈杂的教师标签（微 F1 得分为 0.67 对 0.63），显示出其强大的内在能力。这些发现为使用合成标签微调 LLM 提供了新的研究方向，尤其是在医疗领域的专业化应用中。

> Despite significant progress in applying large language models (LLMs) to the medical domain, several limitations still prevent them from practical applications. Among these are the constraints on model size and the lack of cohort-specific labeled datasets. In this work, we investigated the potential of improving a lightweight LLM, such as Llama 3.1-8B, through fine-tuning with datasets using synthetic labels. Two tasks are jointly trained by combining their respective instruction datasets. When the quality of the task-specific synthetic labels is relatively high (e.g., generated by GPT4- o), Llama 3.1-8B achieves satisfactory performance on the open-ended disease detection task, with a micro F1 score of 0.91. Conversely, when the quality of the task-relevant synthetic labels is relatively low (e.g., from the MIMIC-CXR dataset), fine-tuned Llama 3.1-8B is able to surpass its noisy teacher labels (micro F1 score of 0.67 v.s. 0.63) when calibrated against curated labels, indicating the strong inherent underlying capability of the model. These findings demonstrate the potential of fine-tuning LLMs with synthetic labels, offering a promising direction for future research on LLM specialization in the medical domain.

[Arxiv](https://arxiv.org/abs/2409.16563)