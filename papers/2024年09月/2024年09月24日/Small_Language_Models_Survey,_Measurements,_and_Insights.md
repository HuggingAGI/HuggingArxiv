# 小型语言模型：探索、评估与洞察

发布时间：2024年09月24日

`LLM理论` `智能设备` `人工智能`

> Small Language Models: Survey, Measurements, and Insights

# 摘要

> 尽管小型语言模型（SLM）在智能设备中广泛应用，但其学术关注度远不及大型语言模型（LLM）。研究人员不断增强 LLM 的能力，而 SLM 研究则致力于让机器智能更亲民、更高效。我们聚焦于 100M-5B 参数的 Transformer 解码器模型，调查了 59 个顶尖开源 SLM，从架构、数据集和算法三方面剖析其创新。此外，我们评估了它们在常识推理、上下文学习、数学和编程等领域的性能，并测试了其在设备上的推理速度和内存消耗。通过深入分析，我们为该领域研究提供了重要洞见。

> Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data centers and cloud environments. While researchers continue to improve the capabilities of LLMs in the pursuit of artificial general intelligence, SLM research aims to make machine intelligence more accessible, affordable, and efficient for everyday tasks. Focusing on transformer-based, decoder-only language models with 100M-5B parameters, we survey 59 state-of-the-art open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms. In addition, we evaluate their capabilities in various domains, including commonsense reasoning, in-context learning, mathematics, and coding. To gain further insight into their on-device runtime costs, we benchmark their inference latency and memory footprints. Through in-depth analysis of our benchmarking data, we offer valuable insights to advance research in this field.

[Arxiv](https://arxiv.org/abs/2409.15790)