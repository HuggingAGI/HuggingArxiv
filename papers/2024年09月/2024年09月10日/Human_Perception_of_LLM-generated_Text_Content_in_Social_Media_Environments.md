# 社交媒体环境下，人类如何感知由大型语言模型生成的文本内容

发布时间：2024年09月10日

`LLM应用` `社交媒体` `网络安全`

> Human Perception of LLM-generated Text Content in Social Media Environments

# 摘要

> 新兴技术，尤其是AI和LLMs，为恶意行为者提供了操纵数字话语的强大工具。LLMs不仅影响选民选择、政府调查等传统民主参与形式，还能在与监管机构的在线沟通中发挥作用。我们招募了1,000多名参与者，测试他们在社交媒体中区分机器人和人类发帖的能力，结果显示人类在这方面表现不佳。我们还发现了人类识别LLM生成文本的模式，并在文本对话中观察到了“恐怖谷”效应，表明尽管识别能力有限，人类仍能感知到LLM生成内容的不适感。

> Emerging technologies, particularly artificial intelligence (AI), and more specifically Large Language Models (LLMs) have provided malicious actors with powerful tools for manipulating digital discourse. LLMs have the potential to affect traditional forms of democratic engagements, such as voter choice, government surveys, or even online communication with regulators; since bots are capable of producing large quantities of credible text. To investigate the human perception of LLM-generated content, we recruited over 1,000 participants who then tried to differentiate bot from human posts in social media discussion threads. We found that humans perform poorly at identifying the true nature of user posts on social media. We also found patterns in how humans identify LLM-generated text content in social media discourse. Finally, we observed the Uncanny Valley effect in text dialogue in both user perception and identification. This indicates that despite humans being poor at the identification process, they can still sense discomfort when reading LLM-generated content.

[Arxiv](https://arxiv.org/abs/2409.06653)