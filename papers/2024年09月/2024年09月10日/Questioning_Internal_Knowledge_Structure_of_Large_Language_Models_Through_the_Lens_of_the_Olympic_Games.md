# 透过奥运会的镜头，探寻大型语言模型内部知识结构的奥秘

发布时间：2024年09月10日

`LLM理论` `人工智能`

> Questioning Internal Knowledge Structure of Large Language Models Through the Lens of the Olympic Games

# 摘要

> 尽管 LLM 在 NLP 领域占据主导地位，但其内部知识结构仍鲜为人知。本文通过分析奥运会历史奖牌榜，揭示了 LLM 的内部知识结构。虽然 LLM 在报告奖牌数方面表现优异，但在处理排名问题上却显得力不从心。这表明 LLM 的知识结构与人类截然不同，人类能轻松从奖牌数推断排名。为推动研究，我们公开了代码、数据集及模型输出。

> Large language models (LLMs) have become a dominant approach in natural language processing, yet their internal knowledge structures remain largely unexplored. In this paper, we analyze the internal knowledge structures of LLMs using historical medal tallies from the Olympic Games. We task the models with providing the medal counts for each team and identifying which teams achieved specific rankings. Our results reveal that while state-of-the-art LLMs perform remarkably well in reporting medal counts for individual teams, they struggle significantly with questions about specific rankings. This suggests that the internal knowledge structures of LLMs are fundamentally different from those of humans, who can easily infer rankings from known medal counts. To support further research, we publicly release our code, dataset, and model outputs.

[Arxiv](https://arxiv.org/abs/2409.06518)