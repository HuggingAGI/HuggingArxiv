# GroUSE：基于事实问答中评估者表现的基准测试

发布时间：2024年09月10日

`RAG` `人工智能`

> GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering

# 摘要

> RAG 已成为结合 LLM 与最新知识库的常用方法。我们探讨了使用 LLM 评估 RAG 生成答案的难题。通过识别 7 种生成失败模式，我们推出了 GroUSE 基准，揭示现有评估框架常忽略关键失败模式，即便使用 GPT-4 也如此。为优化评估框架，我们设计了新流程，发现封闭模型在 GroUSE 上表现优异，但开源评判者未能推广至我们标准，尽管与 GPT-4 判断相关性强。这表明，仅依赖 GPT-4 相关性不足以全面评估评判模型，需结合单元测试以精准检测失败模式。此外，微调 Llama-3 基于 GPT-4 推理轨迹，显著提升其评估能力，不仅增强与 GPT-4 判断的相关性，还在参考情境下改善校准。

> Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge.
  To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4's judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection.
  We further show that finetuning Llama-3 on GPT-4's reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4's evaluations and calibration on reference situations.

[Arxiv](https://arxiv.org/abs/2409.06595)