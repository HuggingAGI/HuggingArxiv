# 跟随我的指引，揭开谜底：在检索增强型生成系统中实现大规模数据抽取

发布时间：2024年02月27日

`RAG`

> Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems

> RAG 技术通过在运行时融入外部知识以支持个性化调整，显著提升了预训练模型的表现。然而，我们针对运用了基于上下文检索技术的 RAG 语言模型（如Llama2等多款现代LMs），揭示了其潜在的数据存储泄露风险。攻击者能利用这些模型遵循指令的特点，通过巧妙构造提示语句，轻松从采用指令调优构建的RAG系统数据存储中直接复制文本信息，而且这一安全隐患随模型规模扩大而愈发严重。在对实际应用中的GPT系列RAG模型进行深入探究时，我们设计了一种攻击策略，该策略只需最多两次查询，便能在随机挑选的25个定制版GPTs上达到100%的数据存储泄露成功率。更令人惊讶的是，仅仅通过向GPTs提供自身生成的100个查询提示，我们就能够在一本7.7万词的书籍中以41%的比例精准抽取原文内容，甚至在156.9万词的庞大语料库中也实现了3%的原文摘取率。

> Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves.

[Arxiv](https://arxiv.org/abs/2402.17840)