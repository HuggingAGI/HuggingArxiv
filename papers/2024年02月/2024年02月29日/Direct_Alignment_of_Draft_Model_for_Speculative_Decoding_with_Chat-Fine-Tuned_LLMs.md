# [研究者尝试将草稿模型直接与经聊天数据优化后的大型语言模型对齐，以提升推测解码性能。](https://arxiv.org/abs/2403.00858)

发布时间：2024年02月29日

`LLM应用`

> Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs

> 众所周知，大型语言模型（LLMs）在文本生成时因自身自回归特性和巨量参数而受到内存限制，加上有限的内存带宽，通常产生较低的令牌处理速率。为了解决这一问题并加速LLMs的推理过程，提出了推测性解码技术。但在当前流行的开源LLM家族中，例如Llama 2 7B等模型，往往缺乏现成的草稿模型，这就要求首先训练出一个高品质的草稿模型以支持推测性解码的加速推理。本论文创新地提出了一个简洁易用的草稿模型训练框架，可直接与具备聊天能力的目标模型精准对齐。在此框架下，我们仅使用相当于原始规模1.64%的资源，成功训练出针对Llama 2 Chat 7B及以上规模模型的草稿模型——Llama 2 Chat Drafter 115M。该训练流程精简高效，只包含预训练、蒸馏数据集生成及结合知识蒸馏的微调环节，无需其他复杂的对齐步骤。在微调过程中，我们创造性地运用目标模型在合理数据分布下生成的指令-回复配对进行知识蒸馏，并引入了一个全新设计的总变差距离++（TVD++）损失函数，灵感来源于强化学习中的策略梯度方法，旨在降低方差影响。实验结果显示，在未经进一步特定任务微调的前提下，采用推测性解码的Llama 2 Chat Drafter 115M在各类任务上的表现显著优于自回归解码，其块效率最高提升了2.3倍，速度更是提高了2.4倍。

> Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional alignment procedure. For the finetuning step, we use instruction-response pairs generated by target model for distillation in plausible data distribution, and propose a new Total Variation Distance++ (TVD++) loss that incorporates variance reduction techniques inspired from the policy gradient method in reinforcement learning. Our empirical results show that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3 block efficiency and 2.4$\times$ speed-up relative to autoregressive decoding on various tasks with no further task-specific fine-tuning.

[Arxiv](https://arxiv.org/abs/2403.00858)