# [PeLLE 是一款以开放数据为基础、专为巴西葡萄牙语设计的编码器式语言模型。](https://arxiv.org/abs/2402.19204)

发布时间：2024年02月29日

`LLM应用`

> PeLLE: Encoder-based language models for Brazilian Portuguese based on open data

> 本文推出了PeLLE——一款以RoBERTa架构为基础、专门面向巴西葡萄牙语的大规模语言模型系列，其训练素材来自精心编纂的Carolina语料库公开数据。为保证实验结果的可复现性，我们详尽阐述了模型预训练的细节。我们还对PeLLE模型和一众现有多语言以及针对PT-BR精炼优化的预训练Transformer-based大型LLM编码器展开了对比评测，在多项下游任务中考察了大型预训练模型与虽小却精心筛选预训练模型之间的性能差异。最终发现，尽管许多任务在大模型上表现出色，但在预训练阶段，部分任务确实能从使用小而精选的数据集中获益。

> In this paper we present PeLLE, a family of large language models based on the RoBERTa architecture, for Brazilian Portuguese, trained on curated, open data from the Carolina corpus. Aiming at reproducible results, we describe details of the pretraining of the models. We also evaluate PeLLE models against a set of existing multilingual and PT-BR refined pretrained Transformer-based LLM encoders, contrasting performance of large versus smaller-but-curated pretrained models in several downstream tasks. We conclude that several tasks perform better with larger models, but some tasks benefit from smaller-but-curated data in its pretraining.

[Arxiv](https://arxiv.org/abs/2402.19204)