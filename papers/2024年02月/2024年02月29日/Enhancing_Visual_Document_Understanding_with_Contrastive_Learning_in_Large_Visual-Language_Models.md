# [针对大型视觉-语言模型，本研究采用对比学习方法以增强对视觉文档的深入理解和解析能力。](https://arxiv.org/abs/2402.19014)

发布时间：2024年02月29日

`LLM应用`

> Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models

> 近期，LVLM在诸多领域崭露头角，尤其在VDU领域备受瞩目。不同于常规视觉-语言任务，VDU专注于富含文本及多样文档元素的情景。然而，对于LVLM而言，细粒度特征的重要价值尚未被充分挖掘，导致其在文本密集场景的表现不尽人意，我们称之为“细粒度特征坍缩问题”。为此，本研究提出了名为DoCo的文档对象对比学习框架，专门针对VDU的下游任务优化设计。DoCo借助辅助的多模态编码器捕获文档对象特征，并将其与LVLM内视觉编码器生成的视觉特征对齐，进而强化了文本丰富的场景下的视觉表达力。换言之，通过对比文档对象的视觉整体表征与其多模态细粒度特征，视觉编码器能更好地捕捉有效视觉提示，从而提升LVLM对文本丰富文档的理解力。值得一提的是，DoCo作为一种即插即用的预训练方法，可在不影响推理过程计算复杂性的前提下，应用于各类LVLM的预训练阶段。大量实验证明，在多个VDU基准测试上，搭载DoCo的LVLM展现出了卓越性能，有效缩小了VDU与一般视觉-语言任务之间的表现鸿沟。

> Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks.