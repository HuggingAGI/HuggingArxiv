# [在多种语言环境下，大规模语言模型普遍存在性别偏见问题。]

发布时间：2024年02月29日

`LLM应用`

> Gender Bias in Large Language Models across Multiple Languages

> 随着 LLMS 在各领域的广泛应用，探究其中蕴含的性别偏见的影响日益重要，尤其在英语环境下，NLP 中性别偏见的研究已备受瞩目。然而，对非英语语言中性别偏见的探讨尚不充分。本文聚焦于不同语言中 LLMs 生成内容的性别偏见问题，通过三种衡量手段进行研究：一是在性别相关情境下选取描述性词语的性别倾向；二是在给定描述性词语后选用性别代词（她/他）的偏好；三是LLMs生成对话话题的性别偏向性。我们采用这些方法对GPT系列LLMs在多种语言环境下的输出进行了深入研究，发现无论何种语言，性别偏见现象普遍存在且较为显著。

> With the growing deployment of large language models (LLMs) across various applications, assessing the influence of gender biases embedded in LLMs becomes crucial. The topic of gender bias within the realm of natural language processing (NLP) has gained considerable focus, particularly in the context of English. Nonetheless, the investigation of gender bias in languages other than English is still relatively under-explored and insufficiently analyzed. In this work, We examine gender bias in LLMs-generated outputs for different languages. We use three measurements: 1) gender bias in selecting descriptive words given the gender-related context. 2) gender bias in selecting gender-related pronouns (she/he) given the descriptive words. 3) gender bias in the topics of LLM-generated dialogues. We investigate the outputs of the GPT series of LLMs in various languages using our three measurement methods. Our findings revealed significant gender biases across all the languages we examined.

[Arxiv](https://arxiv.org/abs/2403.00277)