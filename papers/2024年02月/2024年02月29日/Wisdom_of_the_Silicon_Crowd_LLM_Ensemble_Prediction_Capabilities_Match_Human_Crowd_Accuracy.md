# [在“硅智慧群体”中，大型语言模型（LLM）集成预测的准确性已达到与人类群体相当的水平，展示了强大的智能集成预测能力。](https://arxiv.org/abs/2402.19379)

发布时间：2024年02月29日

`LLM应用`

> Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy

> 实践中，人类预测准确性仰仗“众人智慧”效应，即汇聚众多个体预测者的观点能有效提升对未来事件的预见力。前人研究显示，即便最先进的LLMs作为独立预测者，其预测表现也逊色于多人预测比赛结果的平均水平。在第一项研究中，我们采用了包含12个LLMs的集合策略来深化这一领域研究，比较了该集合在31个二元问题上的预测结果与一项历时三个月、拥有925名人类预测者的比赛中的集体预测结果。结果显示，LLM集合不仅超越了基本无信息参照标准，而且在统计意义上与人类预测群体不相上下，同时还揭示出一种遵从性效应，即使正面和负面结果近乎均衡分布，模型平均预测值仍明显超过50%。在第二项研究中，我们探究了将人类的认知输出融入LLM（如GPT-4和Claude 2）预测过程中，能否提升其预测准确性。研究发现，当这两种模型接触到中位数人类预测作为辅助信息时，预测准确度都有所提升，提高幅度在17%至28%之间，尽管这种提升并未超越单纯将人类和机器预测结果平均的效果。最终，我们的研究成果表明，仅通过实施简便且实用的预测集合策略，LLMs便有望达到与人类群体预测比赛比肩的预测精确度，从而成功复制了“众人智慧”在LLMs领域的效应，并为此类技术在社会各个应用场景中的推广使用铺平了道路。

> Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even split of positive and negative resolutions. Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output. We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17% and 28%: though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation. This replicates the 'wisdom of the crowd' effect for LLMs, and opens up their use for a variety applications throughout society.