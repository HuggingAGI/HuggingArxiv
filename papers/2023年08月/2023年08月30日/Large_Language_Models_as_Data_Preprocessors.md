# 语言巨擘，数据预处理的新利器

发布时间：2023年08月30日

`LLM应用` `人工智能` `数据预处理`

> Large Language Models as Data Preprocessors

# 摘要

> 以 OpenAI 的 GPT 系列和 Meta 的 LLaMA 变种为代表的大型语言模型（LLMs）在人工智能领域取得了显著突破。这些模型经过海量文本数据的训练，能够理解并生成多样化话题的类人文本。本研究进一步探讨了 LLMs 在数据预处理——数据挖掘和分析应用的关键环节——中的应用潜力。我们深入分析了 GPT-3.5、GPT-4 和 Vicuna-13B 等尖端 LLMs 在错误检测、数据填充、模式匹配和实体匹配等任务中的适用性。研究不仅展示了 LLMs 的内在能力，也指出了它们在计算成本和效率上的局限。我们提出了一个融合尖端提示工程技术与传统方法，如情境化和特征选择的 LLM 基础数据预处理框架，旨在提升模型的性能与效率。通过 12 个数据集的广泛实验，我们评估了 LLMs 在数据预处理方面的效能，其中 GPT-4 在 4 个数据集上实现了 100% 的准确率或 F1 分数，彰显了 LLMs 在这些任务上的卓越潜力。尽管存在一些限制，但本研究凸显了 LLMs 在此领域的希望，并展望了未来的发展将如何克服现有挑战。

> Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's LLaMA variants, have marked a significant advancement in artificial intelligence. Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. We delve into the applicability of state-of-the-art LLMs such as GPT-3.5, GPT-4, and Vicuna-13B for error detection, data imputation, schema matching, and entity matching tasks. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the performance and efficiency of these models. The effectiveness of LLMs in data preprocessing is evaluated through an experimental study spanning 12 datasets. GPT-4 emerged as a standout, achieving 100\% accuracy or F1 score on 4 datasets, suggesting LLMs' immense potential in these tasks. Despite certain limitations, our study underscores the promise of LLMs in this domain and anticipates future developments to overcome current hurdles.

[Arxiv](https://arxiv.org/abs/2308.16361)