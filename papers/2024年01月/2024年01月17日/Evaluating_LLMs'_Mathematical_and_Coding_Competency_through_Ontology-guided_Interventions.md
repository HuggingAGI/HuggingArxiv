# 借助本体引导的干预，我们评估了 LLM 在数学和编码方面的能力。

发布时间：2024年01月17日

`LLM应用` `人工智能` `软件开发`

> Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions

# 摘要

> 摘要：近期，大型语言模型 (LLM) 在逻辑推理基准测试中表现出色，甚至超越了人类。然而，其在推理任务中的真正实力和鲁棒性仍待探究。本文聚焦于算术推理和代码生成两大任务，引入了数学和编码问题的扰动本体、半自动扰动方法，以及 MORE 和 CORE 两个数据集，用以测试 LLM 在数值推理和编码任务中的极限。评估显示，所有模型在面对扰动问题时性能大幅下滑，揭示了当前 LLM 在多领域内缺乏鲁棒的问题解决和结构化推理能力。我们已在指定网址开源数据集和代码。

> 
Abstract:Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust problem solving skills and structured reasoning abilities in many areas, as defined by our ontology. We open source the datasets and source codes at: this https URL.
    

[Arxiv](https://arxiv.org/pdf/2401.09395)