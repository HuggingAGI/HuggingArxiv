# 探索系统模型至类别模型的转变：在上下文中学习的新模式

发布时间：2023年12月20日

`LLM理论` `动态系统` `人工智能`

> From system models to class models: An in-context learning paradigm

# 摘要

> 能否通过观察同类系统中其他系统的行为，而不仅仅是输入/输出模式，来洞悉一个动态系统的奥妙？本论文的研究正是基于这一核心问题。为解答此问，我们提出了一种创新的系统识别新范式，旨在解决一步预测和多步模拟两大核心任务。与传统方法不同，我们不直接对特定系统建模，而是构建一个能够代表一类动态系统的元模型。该元模型通过从概率分布中随机抽取设置的模拟器生成的无限合成数据流进行训练。当输入新系统的上下文，例如输入/输出序列时，元模型能够隐式识别其动态特性，实现对其行为的预测。本方法借助了变换器的“上下文学习”能力，一步预测采用了类似 GPT 的仅解码器架构，而多步模拟则使用了编码器-解码器结构。初步的实验结果积极地回应了我们的基本问题，为系统识别研究开辟了新的路径。

> Is it possible to understand the intricacies of a dynamical system not solely from its input/output pattern, but also by observing the behavior of other systems within the same class? This central question drives the study presented in this paper.
  In response to this query, we introduce a novel paradigm for system identification, addressing two primary tasks: one-step-ahead prediction and multi-step simulation. Unlike conventional methods, we do not directly estimate a model for the specific system. Instead, we learn a meta model that represents a class of dynamical systems. This meta model is trained on a potentially infinite stream of synthetic data, generated by simulators whose settings are randomly extracted from a probability distribution. When provided with a context from a new system-specifically, an input/output sequence-the meta model implicitly discerns its dynamics, enabling predictions of its behavior.
  The proposed approach harnesses the power of Transformers, renowned for their \emph{in-context learning} capabilities. For one-step prediction, a GPT-like decoder-only architecture is utilized, whereas the simulation problem employs an encoder-decoder structure. Initial experimental results affirmatively answer our foundational question, opening doors to fresh research avenues in system identification.

[Arxiv](https://arxiv.org/abs/2308.13380)