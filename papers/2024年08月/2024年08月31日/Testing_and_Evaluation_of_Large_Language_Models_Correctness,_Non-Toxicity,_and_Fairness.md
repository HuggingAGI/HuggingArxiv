# 大型语言模型的测试与评估聚焦于正确性、非毒性和公平性。

发布时间：2024年08月31日

`LLM应用` `软件测试` `人工智能`

> Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness

# 摘要

> 近年来，像ChatGPT这样的大型语言模型因其出色的对话能力和智能，迅速融入我们的工作和日常生活。ChatGPT以惊人的速度成为史上用户增长最快的软件，为新一代AI应用奠定了坚实基础。然而，这些模型的输出并非完全可靠，常包含事实错误、偏见和有害内容。面对其广泛的用户基础和应用场景，这些不可靠的输出可能带来严重负面影响。本论文聚焦于LLMs的可靠性，从软件测试和自然语言处理角度，探讨其正确性、无害性和公平性。我们通过FactChecker和LogicAsker两个框架评估模型的知识准确性和逻辑推理能力，通过红队工作提升模型的安全性，并通过BiasAsker和XCulturalBench两个框架检测模型的社会和文化偏见。

> Large language models (LLMs), such as ChatGPT, have rapidly penetrated into people's work and daily lives over the past few years, due to their extraordinary conversational skills and intelligence. ChatGPT has become the fastest-growing software in terms of user numbers in human history and become an important foundational model for the next generation of artificial intelligence applications. However, the generations of LLMs are not entirely reliable, often producing content with factual errors, biases, and toxicity. Given their vast number of users and wide range of application scenarios, these unreliable responses can lead to many serious negative impacts. This thesis introduces the exploratory works in the field of language model reliability during the PhD study, focusing on the correctness, non-toxicity, and fairness of LLMs from both software testing and natural language processing perspectives. First, to measure the correctness of LLMs, we introduce two testing frameworks, FactChecker and LogicAsker, to evaluate factual knowledge and logical reasoning accuracy, respectively. Second, for the non-toxicity of LLMs, we introduce two works for red-teaming LLMs. Third, to evaluate the fairness of LLMs, we introduce two evaluation frameworks, BiasAsker and XCulturalBench, to measure the social bias and cultural bias of LLMs, respectively.

[Arxiv](https://arxiv.org/abs/2409.00551)