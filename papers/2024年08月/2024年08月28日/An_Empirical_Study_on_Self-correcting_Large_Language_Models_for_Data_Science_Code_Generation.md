# 探究数据科学代码生成中大型语言模型的自修正能力

发布时间：2024年08月28日

`LLM应用` `软件工程` `数据科学`

> An Empirical Study on Self-correcting Large Language Models for Data Science Code Generation

# 摘要

> 大型语言模型（LLM）在软件工程领域的应用，尤其是在代码生成方面，近期取得了显著进展。然而，LLM 生成的代码常因不准确和幻觉问题而需外部修正。为此，我们提出了一种创新方法——CoT-SelfEvolve，它通过自我纠正机制，迭代优化代码，这一过程受实际编程问题反馈构建的思维链引导。针对数据科学领域，包括 NumPy 和 Pandas 等库，我们在 DS-1000 数据集上的测试表明，CoT-SelfEvolve 在处理复杂问题时远超现有模型。该方法不仅在初始代码生成阶段表现出色，且在后续迭代中持续提升准确性，有效应对了程序执行中的错误回溯问题。此外，我们还探讨了将 CoT-SelfEvolve 融入持续软件工程环境的可能性，为基于 LLM 的代码生成提供了切实可行的改进方案。

> Large Language Models (LLMs) have recently advanced many applications on software engineering tasks, particularly the potential for code generation. Among contemporary challenges, code generated by LLMs often suffers from inaccuracies and hallucinations, requiring external inputs to correct. One recent strategy to fix these issues is to refine the code generated from LLMs using the input from the model itself (self-augmented). In this work, we proposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and automatically refines code through a self-correcting process, guided by a chain of thought constructed from real-world programming problem feedback. Focusing on data science code, including Python libraries such as NumPy and Pandas, our evaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve significantly outperforms existing models in solving complex problems. The framework shows substantial improvements in both initial code generation and subsequent iterations, with the model's accuracy increasing significantly with each additional iteration. This highlights the effectiveness of using chain-of-thought prompting to address complexities revealed by program executor traceback error messages. We also discuss how CoT-SelfEvolve can be integrated into continuous software engineering environments, providing a practical solution for improving LLM-based code generation.

[Arxiv](https://arxiv.org/abs/2408.15658)