# LLMSecCode：探究大型语言模型在安全编码领域的评估

发布时间：2024年08月28日

`LLM应用` `网络安全` `软件开发`

> LLMSecCode: Evaluating Large Language Models for Secure Coding

# 摘要

> 在快速部署大型语言模型 (LLM) 时，必须慎重考虑其对网络安全的影响。我们的研究聚焦于优化适合安全编码 (SC) 的 LLM 选择流程。为此，我们提出了几个关键研究问题：如何简化 LLM 评估？评估应侧重哪些方面？如何确保评估的公正性？为解答这些问题，我们开发了开源框架 LLMSecCode，旨在客观评估 LLM 的 SC 能力。实验表明，参数和提示的变化分别导致 10% 和 9% 的性能差异。与外部权威结果对比，差异仅为 5%。我们致力于提升框架的易用性，并欢迎外部贡献以推动其发展。借助 LLMSecCode，我们期望推动安全编码领域 LLM 能力的标准化与基准测试。

> The rapid deployment of Large Language Models (LLMs) requires careful consideration of their effect on cybersecurity. Our work aims to improve the selection process of LLMs that are suitable for facilitating Secure Coding (SC). This raises challenging research questions, such as (RQ1) Which functionality can streamline the LLM evaluation? (RQ2) What should the evaluation measure? (RQ3) How to attest that the evaluation process is impartial? To address these questions, we introduce LLMSecCode, an open-source evaluation framework designed to assess LLM SC capabilities objectively.
  We validate the LLMSecCode implementation through experiments. When varying parameters and prompts, we find a 10% and 9% difference in performance, respectively. We also compare some results to reliable external actors, where our results show a 5% difference.
  We strive to ensure the ease of use of our open-source framework and encourage further development by external actors. With LLMSecCode, we hope to encourage the standardization and benchmarking of LLMs' capabilities in security-oriented code and tasks.

[Arxiv](https://arxiv.org/abs/2408.16100)