# LLM 作为标注者时，党派线索如何影响其标注决策，这一偏见值得关注。

发布时间：2024年08月28日

`LLM理论` `人工智能`

> Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models

# 摘要

> 人类编码者常有偏见，而我们在大型语言模型（LLM）中也发现了类似倾向。通过重现 Ennser-Jedenastik 和 Meyer 的实验，我们证实 LLM 会利用政治信息，尤其是党派线索，来评估政治声明。这些模型不仅依据党派线索判断声明的正负性，还继承了训练数据的偏见。有趣的是，与人类不同，LLM 即便在处理中左或中右党派的声明时，也会显露出偏见。这些发现的意义将在文末深入探讨。

> Human coders are biased. We test similar biases in Large Language Models (LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and Meyer (2018), we find evidence that LLMs use political information, and specifically party cues, to judge political statements. Not only do LLMs use relevant information to contextualize whether a statement is positive, negative, or neutral based on the party cue, they also reflect the biases of the human-generated data upon which they have been trained. We also find that unlike humans, who are only biased when faced with statements from extreme parties, LLMs exhibit significant bias even when prompted with statements from center-left and center-right parties. The implications of our findings are discussed in the conclusion.

[Arxiv](https://arxiv.org/abs/2408.15895)