# 本研究旨在利用大规模语言模型，深入评估基于 EEG 的多模态数据在心理健康领域的应用。

发布时间：2024年08月14日

`Agent` `心理健康`

> Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal Data for Mental Health

# 摘要

> 结合脑电图 (EEG) 等生理信号与面试音频等数据，可为心理状态或神经障碍提供多维洞察。随着大型语言模型 (LLM) 的进步，它们有望成为心理健康评估的新“健康代理”。当前研究多聚焦于单一数据模态，而我们的研究则通过整合多模态数据，利用 LLM 进行心理健康评估，特别是通过零-shot 和少-shot 提示技术。我们选取了三个数据集，涵盖 EEG、面部表情和音频（文本），用于抑郁和情绪分类。研究结果显示，多模态信息在心理健康评估中远胜单一模态方法。特别是，EEG 与音频、图像等常见 LLM 模态的结合，展现出令人鼓舞的前景。此外，1-shot 学习相较于零-shot 学习，更能提升评估效果。

> Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. Recent advancements with Large Language Models (LLMs) position them as prospective ``health agents'' for mental health assessment. However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data. Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting. Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text). The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment. Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential. Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.

[Arxiv](https://arxiv.org/abs/2408.07313)