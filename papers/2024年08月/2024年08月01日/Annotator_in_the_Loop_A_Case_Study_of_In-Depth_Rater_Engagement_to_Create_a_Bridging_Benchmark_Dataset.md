# 标注者在循环中：通过深入评分者参与，创建桥梁基准数据集的案例研究

发布时间：2024年08月01日

`LLM应用` `数据标注` `社会科学`

> Annotator in the Loop: A Case Study of In-Depth Rater Engagement to Create a Bridging Benchmark Dataset

# 摘要

> 随着大型语言模型的普及，使用众包评判者池来为机器学习标注数据集变得越来越常见。然而，这些评判者通常作为个体众包工作者独立工作。在这项工作中，我们将标注视为一种细致的解释性努力，以辨别文本中所说的含义，而不仅仅是一种廉价、可扩展的劳动力。我们提出了一种新颖的、协作的、迭代式的标注者-在-环方法论，用于标注，从而产生了一个“桥梁基准数据集”，该数据集包含了与弥合分歧相关的评论，从Civil Comments数据集的11,973个文本帖子中标注而来。这种方法与流行的匿名众评判标注过程不同，因为它使用了与七名美国评判者的深入、迭代式参与，以协作地细化待标注概念的定义，并迭代地标注复杂的社会概念，并进行检查会议和讨论。这种方法解决了当前匿名众包标注工作的一些缺点，并且我们以评判者间可靠性形式提供了我们标注过程性能的实证证据。我们的发现表明，与仅依赖远程进行的孤立工作相比，与标注者进行协作式参与可以增强标注方法。我们提供了输入文本、属性、标注过程的概述，以及实证结果和按以下属性分类的基准数据集：疏离感、同情、推理、好奇、道德愤怒和尊重。

> With the growing prevalence of large language models, it is increasingly common to annotate datasets for machine learning using pools of crowd raters. However, these raters often work in isolation as individual crowdworkers. In this work, we regard annotation not merely as inexpensive, scalable labor, but rather as a nuanced interpretative effort to discern the meaning of what is being said in a text. We describe a novel, collaborative, and iterative annotator-in-the-loop methodology for annotation, resulting in a 'Bridging Benchmark Dataset' of comments relevant to bridging divides, annotated from 11,973 textual posts in the Civil Comments dataset. The methodology differs from popular anonymous crowd-rating annotation processes due to its use of an in-depth, iterative engagement with seven US-based raters to (1) collaboratively refine the definitions of the to-be-annotated concepts and then (2) iteratively annotate complex social concepts, with check-in meetings and discussions. This approach addresses some shortcomings of current anonymous crowd-based annotation work, and we present empirical evidence of the performance of our annotation process in the form of inter-rater reliability. Our findings indicate that collaborative engagement with annotators can enhance annotation methods, as opposed to relying solely on isolated work conducted remotely. We provide an overview of the input texts, attributes, and annotation process, along with the empirical results and the resulting benchmark dataset, categorized according to the following attributes: Alienation, Compassion, Reasoning, Curiosity, Moral Outrage, and Respect.

[Arxiv](https://arxiv.org/abs/2408.00880)