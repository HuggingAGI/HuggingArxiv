# 三小时内探讨大型语言模型的公平性

发布时间：2024年08月01日

`LLM理论` `人工智能` `社会科学`

> Fairness in Large Language Models in Three Hour

# 摘要

> 尽管大型语言模型 (LLM) 在多领域取得了显著成就，但其公平性考量常被忽视，可能对边缘群体产生歧视。LLM 的公平性有别于传统机器学习，涉及独特背景、分类及实现技术。本教程系统梳理了公平 LLM 的最新研究，从实际案例引入 LLM，深入剖析偏见根源。随后，我们探讨了 LLM 公平性的内涵，概述了偏见评估策略及公平性算法。同时，我们汇总了评估工具与数据集，并探讨了该领域的研究难题与未解之谜。相关资源已整理至 \url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}。

> Large Language Models (LLMs) have demonstrated remarkable success across various domains but often lack fairness considerations, potentially leading to discriminatory outcomes against marginalized populations. Unlike fairness in traditional machine learning, fairness in LLMs involves unique backgrounds, taxonomies, and fulfillment techniques. This tutorial provides a systematic overview of recent advances in the literature concerning fair LLMs, beginning with real-world case studies to introduce LLMs, followed by an analysis of bias causes therein. The concept of fairness in LLMs is then explored, summarizing the strategies for evaluating bias and the algorithms designed to promote fairness. Additionally, resources for assessing bias in LLMs, including toolkits and datasets, are compiled, and current research challenges and open questions in the field are discussed. The repository is available at \url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.

[Arxiv](https://arxiv.org/abs/2408.00992)