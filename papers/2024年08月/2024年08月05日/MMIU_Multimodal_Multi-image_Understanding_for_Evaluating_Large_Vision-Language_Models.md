# MMIU：探索多模态多图像理解，旨在评估大型视觉-语言模型的性能。

发布时间：2024年08月05日

`LLM应用` `计算机视觉` `人工智能`

> MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models

# 摘要

> 为了深入理解场景，大型视觉-语言模型（LVLMs）需具备处理多张图像的能力。尽管多图像LVLMs已开始应对这一挑战，但其评估体系却未同步发展。为此，我们推出了多模态多图像理解（MMIU）基准，一个全面评估LVLMs在多图像任务中表现的工具。MMIU涵盖7种图像关系、52项任务、77K张图片及11K精心设计的选择题，是同类中最全面的基准。我们评估了24个LVLMs，发现多图像理解，尤其是空间理解任务，仍面临重大挑战。即便是最先进的GPT-4o，在MMIU上的准确率也仅达55.7%。通过深入分析，我们揭示了性能瓶颈，为未来模型与数据优化提供了方向。MMIU旨在推动LVLM研究与开发，助力实现复杂的多模态多图像交互。

> The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their evaluation has not kept pace with their development. To fill this gap, we introduce the Multimodal Multi-image Understanding (MMIU) benchmark, a comprehensive evaluation suite designed to assess LVLMs across a wide range of multi-image tasks. MMIU encompasses 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions, making it the most extensive benchmark of its kind. Our evaluation of 24 popular LVLMs, including both open-source and proprietary models, reveals significant challenges in multi-image comprehension, particularly in tasks involving spatial understanding. Even the most advanced models, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Through multi-faceted analytical experiments, we identify key performance gaps and limitations, providing valuable insights for future model and data improvements. We aim for MMIU to advance the frontier of LVLM research and development, moving us toward achieving sophisticated multimodal multi-image user interactions.

[Arxiv](https://arxiv.org/abs/2408.02718)