# 大型语言模型驱动的对话AI在证人采访中加剧了虚假记忆的出现。

发布时间：2024年08月08日

`LLM应用` `人机交互`

> Conversational AI Powered by Large Language Models Amplifies False Memories in Witness Interviews

# 摘要

> 本研究揭示了AI在人机交互中通过暗示性提问诱导虚假记忆的现象，特别是在模拟犯罪目击者访谈中。研究发现，使用大型语言模型的生成式聊天机器人显著增加了虚假记忆的形成，其诱导的即时虚假记忆数量是对照组的三倍多，是问卷方法的1.7倍。一周后，这些虚假记忆的数量保持不变，但用户对其的信心仍高于对照组。研究还发现，对AI技术较熟悉但对聊天机器人不太熟悉的用户，以及对犯罪调查更感兴趣的用户，更容易受到虚假记忆的影响。这些发现提醒我们在敏感情境中使用AI时需谨慎，并强调了伦理考虑的重要性。

> This study examines the impact of AI on human false memories -- recollections of events that did not occur or deviate from actual occurrences. It explores false memory induction through suggestive questioning in Human-AI interactions, simulating crime witness interviews. Four conditions were tested: control, survey-based, pre-scripted chatbot, and generative chatbot using a large language model (LLM). Participants (N=200) watched a crime video, then interacted with their assigned AI interviewer or survey, answering questions including five misleading ones. False memories were assessed immediately and after one week. Results show the generative chatbot condition significantly increased false memory formation, inducing over 3 times more immediate false memories than the control and 1.7 times more than the survey method. 36.4% of users' responses to the generative chatbot were misled through the interaction. After one week, the number of false memories induced by generative chatbots remained constant. However, confidence in these false memories remained higher than the control after one week. Moderating factors were explored: users who were less familiar with chatbots but more familiar with AI technology, and more interested in crime investigations, were more susceptible to false memories. These findings highlight the potential risks of using advanced AI in sensitive contexts, like police interviews, emphasizing the need for ethical considerations.

[Arxiv](https://arxiv.org/abs/2408.04681)