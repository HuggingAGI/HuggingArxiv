# 大型语言模型在开放域生成中，隐式格式控制的运用

发布时间：2024年08月08日

`LLM应用` `人工智能`

> Open-domain Implicit Format Control for Large Language Model Generation

# 摘要

> 在多种应用中，控制大型语言模型（LLM）的输出格式至关重要。现有方法如基于规则的自动机约束解码或手工格式指令微调，均难以满足开放领域的格式需求。为此，我们创新性地利用用户提供的单次问答对，开发了一个新的控制生成框架。研究发现，LLM 在遵循开放领域单次约束并复制示例格式方面面临挑战。此外，我们设计了一种数据集收集方法，通过监督微调提升 LLM 的开放领域格式控制能力，同时确保输出质量，并设立了一个基准来评估 LLM 输出的实用性和格式准确性。相关数据集 OIFC-SFT 及其代码将在 GitHub 上公开，网址为 https://github.com/cofe-ai/OIFC。

> Controlling the format of outputs generated by large language models (LLMs) is a critical functionality in various applications. Current methods typically employ constrained decoding with rule-based automata or fine-tuning with manually crafted format instructions, both of which struggle with open-domain format requirements. To address this limitation, we introduce a novel framework for controlled generation in LLMs, leveraging user-provided, one-shot QA pairs. This study investigates LLMs' capabilities to follow open-domain, one-shot constraints and replicate the format of the example answers. We observe that this is a non-trivial problem for current LLMs. We also develop a dataset collection methodology for supervised fine-tuning that enhances the open-domain format control of LLMs without degrading output quality, as well as a benchmark on which we evaluate both the helpfulness and format correctness of LLM outputs. The resulting datasets, named OIFC-SFT, along with the related code, will be made publicly available at https://github.com/cofe-ai/OIFC.

[Arxiv](https://arxiv.org/abs/2408.04392)