# LLMs 能否在辩论中胜过人类？探索竞技辩论的动态多代理框架

发布时间：2024年08月08日

`Agent` `人工智能`

> Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate

# 摘要

> 竞争性辩论是一项高度复杂的任务，大型语言模型在此领域常显不足。为此，我们推出了 Agent4Debate，一个基于 LLM 的动态多代理系统，旨在提升其在辩论中的表现。借鉴人类辩论的策略，Agent4Debate 设计了四个角色（搜索、分析、写作、审阅），它们在辩论各阶段协同工作。我们构建了中文辩论竞技场，并邀请资深辩手参与，通过 200 场辩论记录，结合 Debatrix 评分系统和专业评审，验证了 Agent4Debate 与人类辩手能力相当。消融研究进一步证实了系统各部分的有效性。

> Competitive debate is a comprehensive and complex computational argumentation task. Large Language Models (LLMs) encounter hallucinations and lack competitiveness in this task. To address these challenges, we introduce Agent for Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs designed to enhance their capabilities in competitive debate. Drawing inspiration from human behavior in debate preparation and execution, Agent4Debate employs a collaborative architecture where four specialized agents (Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate. These agents work throughout the debate process, covering multiple stages from initial research and argument formulation to rebuttal and summary. To comprehensively evaluate framework performance, we construct the Chinese Debate Arena, comprising 66 carefully selected Chinese debate motions. We recruite ten experienced human debaters and collect records of 200 debates involving Agent4Debate, baseline models, and humans. The evaluation employs the Debatrix automatic scoring system and professional human reviewers based on the established Debatrix-Elo and Human-Elo ranking. Experimental results indicate that the state-of-the-art Agent4Debate exhibits capabilities comparable to those of humans. Furthermore, ablation studies demonstrate the effectiveness of each component in the agent structure.

[Arxiv](https://arxiv.org/abs/2408.04472)