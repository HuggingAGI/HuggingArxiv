# 疑问多于答案？探索将可解释 AI 融入网络-AI 工具的实践经验

发布时间：2024年08月08日

`LLM应用` `网络安全` `人工智能`

> More Questions than Answers? Lessons from Integrating Explainable AI into a Cyber-AI Tool

# 摘要

> 我们探讨了在网络安全领域实施XAI的挑战，特别是在源代码分类方面，准确性和时效性至关重要。我们发现，尽管SHAP和LIME等先进解释技术被标榜为非技术用户友好，但缺乏AI知识的人难以理解其输出。此外，这些流行的XAI技术在实时人机协作中提供的见解有限，因为它们的解释往往是事后的且过于局部化。我们强调，网络安全分析师需要更简洁、高层次的解释，以尽量减少对工作流程的干扰。我们还指出了XAI在实际应用中的不足，并探讨了大型语言模型等新兴技术如何帮助克服这些障碍。

> We share observations and challenges from an ongoing effort to implement Explainable AI (XAI) in a domain-specific workflow for cybersecurity analysts. Specifically, we briefly describe a preliminary case study on the use of XAI for source code classification, where accurate assessment and timeliness are paramount. We find that the outputs of state-of-the-art saliency explanation techniques (e.g., SHAP or LIME) are lost in translation when interpreted by people with little AI expertise, despite these techniques being marketed for non-technical users. Moreover, we find that popular XAI techniques offer fewer insights for real-time human-AI workflows when they are post hoc and too localized in their explanations. Instead, we observe that cyber analysts need higher-level, easy-to-digest explanations that can offer as little disruption as possible to their workflows. We outline unaddressed gaps in practical and effective XAI, then touch on how emerging technologies like Large Language Models (LLMs) could mitigate these existing obstacles.

[Arxiv](https://arxiv.org/abs/2408.04746)