# 工业级基于LLM推荐系统的解码加速框架

发布时间：2024年08月10日

`LLM应用` `推荐系统`

> A Decoding Acceleration Framework for Industrial Deployable LLM-based Recommender Systems

# 摘要

> 近期，基于大型语言模型的推荐系统备受瞩目，但其工业应用尚在探索阶段。多数应用将LLM作为特征增强工具，在离线环节生成知识增补。然而，在用户与项目众多的推荐场景中，即便离线生成也耗时耗力。这一低效源于LLM的自回归特性，而推测解码——一种“草拟-验证”模式，能提升每步解码的令牌产出，成为加速的希望所在。本文首先指出，推荐知识的生成适宜采用基于检索的推测解码。接着，我们发现两大特点：一是推荐系统中项目与用户的广泛性导致检索效率低下，二是系统对LLM生成文本的多样性容忍度高。据此，我们提出DARE框架，通过定制检索池提升检索效率，通过宽松验证提高草拟令牌的接受率。实验证明，DARE能实现3-5倍提速，且兼容多种框架与LLM核心。此外，DARE已成功应用于大规模商业环境的在线广告场景，提速3.45倍，同时保持了推荐效果。

> Recently, increasing attention has been paid to LLM-based recommender systems, but their deployment is still under exploration in the industry. Most deployments utilize LLMs as feature enhancers, generating augmentation knowledge in the offline stage. However, in recommendation scenarios, involving numerous users and items, even offline generation with LLMs consumes considerable time and resources. This generation inefficiency stems from the autoregressive nature of LLMs, and a promising direction for acceleration is speculative decoding, a Draft-then-Verify paradigm that increases the number of generated tokens per decoding step. In this paper, we first identify that recommendation knowledge generation is suitable for retrieval-based speculative decoding. Then, we discern two characteristics: (1) extensive items and users in RSs bring retrieval inefficiency, and (2) RSs exhibit high diversity tolerance for text generated by LLMs. Based on the above insights, we propose a Decoding Acceleration Framework for LLM-based Recommendation (dubbed DARE), with Customized Retrieval Pool to improve retrieval efficiency and Relaxed Verification to increase the acceptance rate of draft tokens, respectively. Extensive experiments demonstrate that DARE achieves a 3-5x speedup and is compatible with various frameworks and backbone LLMs. DARE has also been deployed to online advertising scenarios within a large-scale commercial environment, achieving a 3.45x speedup while maintaining the downstream performance.

[Arxiv](https://arxiv.org/abs/2408.05676)