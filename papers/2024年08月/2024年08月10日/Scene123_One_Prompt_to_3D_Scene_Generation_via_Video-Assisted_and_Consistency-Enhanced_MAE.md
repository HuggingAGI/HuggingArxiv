# Scene123：借助视频辅助与一致性增强的 MAE 技术，仅需一个提示即可生成 3D 场景。

发布时间：2024年08月10日

`LLM应用`

> Scene123: One Prompt to 3D Scene Generation via Video-Assisted and Consistency-Enhanced MAE

# 摘要

> 随着AI生成内容的进步，多种技术已能从单一或混合输入创造文本、图像、视频及3D物体，力求模仿人类的创造力。然而，从单个输入构建宏大且真实的场景仍具挑战，因需确保模型扩展视图间的一致性。借助最新的视频生成技术和隐式神经表示，我们推出Scene123模型，它不仅通过视频框架确保场景的真实与多样，还融合隐式神经场与掩码自编码器（MAE），有效维持跨视图的不可见区域一致性。具体操作中，我们先扭曲输入图像（或文本转图像）模拟邻近视角，并用MAE填补盲区。为解决填补图像的视角一致性问题，我们进一步优化神经辐射场，提升几何一致性。为强化生成场景的细节与纹理真实感，我们引入了基于GAN的损失函数，针对视频模型生成的图像进行优化。实验证明，我们的方法能从简短指令中创造出逼真且连贯的3D场景，表现超越现有顶尖技术。欢迎访问https://yiyingyang12.github.io/Scene123.github.io/，观看我们的视频演示。

> As Artificial Intelligence Generated Content (AIGC) advances, a variety of methods have been developed to generate text, images, videos, and 3D objects from single or multimodal inputs, contributing efforts to emulate human-like cognitive content creation. However, generating realistic large-scale scenes from a single input presents a challenge due to the complexities involved in ensuring consistency across extrapolated views generated by models. Benefiting from recent video generation models and implicit neural representations, we propose Scene123, a 3D scene generation model, that not only ensures realism and diversity through the video generation framework but also uses implicit neural fields combined with Masked Autoencoders (MAE) to effectively ensures the consistency of unseen areas across views. Specifically, we initially warp the input image (or an image generated from text) to simulate adjacent views, filling the invisible areas with the MAE model. However, these filled images usually fail to maintain view consistency, thus we utilize the produced views to optimize a neural radiance field, enhancing geometric consistency.
  Moreover, to further enhance the details and texture fidelity of generated views, we employ a GAN-based Loss against images derived from the input image through the video generation model. Extensive experiments demonstrate that our method can generate realistic and consistent scenes from a single prompt. Both qualitative and quantitative results indicate that our approach surpasses existing state-of-the-art methods. We show encourage video examples at https://yiyingyang12.github.io/Scene123.github.io/.

[Arxiv](https://arxiv.org/abs/2408.05477)