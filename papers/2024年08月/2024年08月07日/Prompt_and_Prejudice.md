# 提示与偏见

发布时间：2024年08月07日

`LLM应用` `人工智能伦理`

> Prompt and Prejudice

# 摘要

> 本研究探讨了名字在 LLM 和 VLM 中对伦理决策任务的影响。我们提出将名字添加到伦理注释的文本场景中，以揭示模型输出的群体偏见。通过测试超过 300 个代表不同性别和种族的名字，我们在数千个道德场景中进行了实验。借鉴社会科学的审计方法，我们对流行的 LLM/VLM 进行了详细分析，强调识别和减轻这些系统中偏见的重要性。此外，我们引入了 Pratical Scenarios Benchmark (PSB)，用于评估日常决策和实际场景中的性别或群体偏见，如批准抵押贷款或保险。该基准有助于全面比较不同群体类别间的模型行为，揭示 LLM 和 VLM 实际应用中的潜在风险和偏见。

> This paper investigates the impact of using first names in Large Language Models (LLMs) and Vision Language Models (VLMs), particularly when prompted with ethical decision-making tasks. We propose an approach that appends first names to ethically annotated text scenarios to reveal demographic biases in model outputs. Our study involves a curated list of more than 300 names representing diverse genders and ethnic backgrounds, tested across thousands of moral scenarios. Following the auditing methodologies from social sciences we propose a detailed analysis involving popular LLMs/VLMs to contribute to the field of responsible AI by emphasizing the importance of recognizing and mitigating biases in these systems. Furthermore, we introduce a novel benchmark, the Pratical Scenarios Benchmark (PSB), designed to assess the presence of biases involving gender or demographic prejudices in everyday decision-making scenarios as well as practical scenarios where an LLM might be used to make sensible decisions (e.g., granting mortgages or insurances). This benchmark allows for a comprehensive comparison of model behaviors across different demographic categories, highlighting the risks and biases that may arise in practical applications of LLMs and VLMs.

[Arxiv](https://arxiv.org/abs/2408.04671)