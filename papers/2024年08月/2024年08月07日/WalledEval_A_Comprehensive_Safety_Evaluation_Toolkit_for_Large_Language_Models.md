# WalledEval：专为大型语言模型设计，提供全面的安全评估工具包

发布时间：2024年08月07日

`LLM应用` `网络安全` `人工智能`

> WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models

# 摘要

> WalledEval 是一款全面的 AI 安全测试工具包，专为评估大型语言模型 (LLM) 而设计。它支持多种模型类型，并提供超过 35 个安全基准，涵盖多语言安全、夸大安全及提示注入等多个领域。该框架不仅支持 LLM 和评判基准测试，还通过自定义变异器针对不同文本风格变异进行安全测试。此外，WalledEval 还推出了 WalledGuard 这一小型高效的内容审核工具，以及 SGXSTest 用于评估文化背景下的夸大安全性。WalledEval 已在 GitHub 上公开发布，访问地址为 https://github.com/walledai/walledevalA。

> WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections. The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing. Additionally, WalledEval introduces WalledGuard, a new, small and performant content moderation tool, and SGXSTest, a benchmark for assessing exaggerated safety in cultural contexts. We make WalledEval publicly available at https://github.com/walledai/walledevalA.

![WalledEval：专为大型语言模型设计，提供全面的安全评估工具包](../../../paper_images/2408.03837/x1.png)

![WalledEval：专为大型语言模型设计，提供全面的安全评估工具包](../../../paper_images/2408.03837/x2.png)

[Arxiv](https://arxiv.org/abs/2408.03837)