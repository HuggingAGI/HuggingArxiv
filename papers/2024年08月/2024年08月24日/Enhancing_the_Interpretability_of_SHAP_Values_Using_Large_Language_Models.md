# 利用大型语言模型提升 SHAP 值的解释力

发布时间：2024年08月24日

`LLM应用` `人工智能` `机器学习`

> Enhancing the Interpretability of SHAP Values Using Large Language Models

# 摘要

> 模型可解释性对信任复杂机器学习模型的决策至关重要。SHAP值虽是解释模型的有效工具，但其技术性常令非技术用户难以理解。为此，我们利用大型语言模型（LLM）将SHAP值转化为通俗易懂的解释，既保持了准确性，又提升了清晰度和可用性。实验表明，这种LLM增强的SHAP解释能更直观地展现模型预测，增强模型的可解释性。未来，我们将进一步探索定制化、多模态解释及用户反馈，以优化和拓展这一方法。

> Model interpretability is crucial for understanding and trusting the decisions made by complex machine learning models, such as those built with XGBoost. SHAP (SHapley Additive exPlanations) values have become a popular tool for interpreting these models by attributing the output to individual features. However, the technical nature of SHAP explanations often limits their utility to researchers, leaving non-technical end-users struggling to understand the model's behavior. To address this challenge, we explore the use of Large Language Models (LLMs) to translate SHAP value outputs into plain language explanations that are more accessible to non-technical audiences. By applying a pre-trained LLM, we generate explanations that maintain the accuracy of SHAP values while significantly improving their clarity and usability for end users. Our results demonstrate that LLM-enhanced SHAP explanations provide a more intuitive understanding of model predictions, thereby enhancing the overall interpretability of machine learning models. Future work will explore further customization, multimodal explanations, and user feedback mechanisms to refine and expand the approach.

[Arxiv](https://arxiv.org/abs/2409.00079)