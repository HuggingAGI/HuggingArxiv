# IQA-EVAL：人机交互问答的自动评估系统

发布时间：2024年08月24日

`LLM应用` `人工智能` `问答系统`

> IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering

# 摘要

> 在评估大型语言模型（LLM）的问答能力时，传统方法往往忽视了人机对话的动态性。为了解决这一问题，我们推出了自动评估框架IQA-EVAL，专门用于交互式问答评估。该框架的核心是基于LLM的评估代理（LEA），它不仅能模拟人类行为与模型互动，还能自动评估这些互动。我们还创新性地为LEA赋予角色，以更真实地模拟人类评估者。实验表明，以GPT-4为骨干的评估框架在问答任务上与人类评估高度一致，而赋予角色进一步提升了这种一致性。此外，我们用这一自动指标评估了五种代表性LLM，处理了超过1000个复杂问题，若由人类评估，成本将高达5000美元。

> To evaluate Large Language Models (LLMs) for question answering (QA), traditional methods typically focus on directly assessing the immediate responses generated by the models based on the given question and context. In the common use case of humans seeking AI assistant's help in finding information, these non-interactive evaluations do not account for the dynamic nature of human-model conversations, and interaction-aware evaluations have shown that accurate QA models are preferred by humans (Lee et al., 2023). Recent works in human-computer interaction (HCI) have employed human evaluators to conduct interactions and evaluations, but they are often prohibitively expensive and time-consuming to scale. In this work, we introduce an automatic evaluation framework IQA-EVAL to Interactive Question Answering Evaluation. More specifically, we introduce LLM-based Evaluation Agent (LEA) that can: (1) simulate human behaviors to generate interactions with IQA models; (2) automatically evaluate the generated interactions. Moreover, we propose assigning personas to LEAs to better simulate groups of real human evaluators. We show that: (1) our evaluation framework with GPT-4 (or Claude) as the backbone model achieves a high correlation with human evaluations on the IQA task; (2) assigning personas to LEA to better represent the crowd further significantly improves correlations. Finally, we use our automatic metric to evaluate five recent representative LLMs with over 1000 questions from complex and ambiguous question answering tasks, which comes with a substantial cost of $5k if evaluated by humans.

[Arxiv](https://arxiv.org/abs/2408.13545)