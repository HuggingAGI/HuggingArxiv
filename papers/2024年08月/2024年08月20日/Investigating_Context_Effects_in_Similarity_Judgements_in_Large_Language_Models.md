# 探究大规模语言模型中相似性判断的上下文影响

发布时间：2024年08月20日

`LLM理论` `人工智能` `行为科学`

> Investigating Context Effects in Similarity Judgements in Large Language Models

# 摘要

> 大型语言模型（LLM）的崛起，极大地提升了 AI 理解和生成自然语言的能力。这些模型正被广泛应用于现实世界的决策系统中，它们依据上下文做出判断和行动。为了确保这些决策符合人类价值观和期望，研究者、政策制定者和企业正共同努力。然而，人类价值观和决策往往复杂且易受认知偏见影响。行为科学领域对此类偏见有深入研究。本研究聚焦于 LLM 与人类判断的一致性，特别是受顺序偏见影响的情况。我们复现了一项显示相似性判断中存在顺序效应的经典研究，并使用多种 LLM 进行实验。研究发现，在特定条件下，LLM 也会表现出类似人类的顺序偏见。这些发现对于优化基于 LLM 的应用设计具有重要意义。

> Large Language Models (LLMs) have revolutionised the capability of AI models in comprehending and generating natural language text. They are increasingly being used to empower and deploy agents in real-world scenarios, which make decisions and take actions based on their understanding of the context. Therefore researchers, policy makers and enterprises alike are working towards ensuring that the decisions made by these agents align with human values and user expectations. That being said, human values and decisions are not always straightforward to measure and are subject to different cognitive biases. There is a vast section of literature in Behavioural Science which studies biases in human judgements. In this work we report an ongoing investigation on alignment of LLMs with human judgements affected by order bias. Specifically, we focus on a famous human study which showed evidence of order effects in similarity judgements, and replicate it with various popular LLMs. We report the different settings where LLMs exhibit human-like order effect bias and discuss the implications of these findings to inform the design and development of LLM based applications.

[Arxiv](https://arxiv.org/abs/2408.10711)