# 在医疗问答领域，我们正努力提升可靠性，专注于减少语言模型中的幻觉现象，这既涉及创新技术，也面临诸多挑战。

发布时间：2024年08月25日

`LLM应用` `生物医学`

> Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models

# 摘要

> 随着大型语言模型的飞速进步，医疗和生物医学等领域也深受其益。但幻觉现象——模型输出与事实或上下文不符——在高风险领域尤为棘手。本文针对医疗领域的特殊需求，探讨了减轻幻觉的现有技术，如基于 RAG 的方法、迭代反馈、监督微调及提示工程。这些技术虽具潜力，但需针对医疗领域的专业性和规范性进行优化。确保 AI 系统的可靠性，对于提升临床决策质量和生物医学研究准确性至关重要。

> The rapid advancement of large language models (LLMs) has significantly impacted various domains, including healthcare and biomedicine. However, the phenomenon of hallucination, where LLMs generate outputs that deviate from factual accuracy or context, poses a critical challenge, especially in high-stakes domains. This paper conducts a scoping study of existing techniques for mitigating hallucinations in knowledge-based task in general and especially for medical domains. Key methods covered in the paper include Retrieval-Augmented Generation (RAG)-based techniques, iterative feedback loops, supervised fine-tuning, and prompt engineering. These techniques, while promising in general contexts, require further adaptation and optimization for the medical domain due to its unique demands for up-to-date, specialized knowledge and strict adherence to medical guidelines. Addressing these challenges is crucial for developing trustworthy AI systems that enhance clinical decision-making and patient safety as well as accuracy of biomedical scientific research.

[Arxiv](https://arxiv.org/abs/2408.13808)