# GLIMMER：融合图与词汇特征，革新无监督多文档摘要技术

发布时间：2024年08月19日

`LLM应用` `文本摘要` `信息处理`

> GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization

# 摘要

> 预训练语言模型虽在多文档摘要任务中广泛应用，但需大规模语料库预训练且领域依赖性强。传统非神经无监督方法多依赖关键句提取，易导致信息遗漏。为此，我们提出GLIMMER：一种轻巧高效的无监督多文档摘要方法，融合图与词汇特征。该方法先构建句子图，再通过挖掘文本低级特征自动形成语义聚类，优化聚类内关联与句子流畅性，最终生成自然摘要。实验显示，GLIMMER在多个数据集上超越现有无监督方法，零-shot设置下ROUGE分数也领先于顶尖预训练模型。人类评估更证实其摘要的高可读性与信息性。代码已公开于https://github.com/Oswald1997/GLIMMER。

> Pre-trained language models are increasingly being used in multi-document summarization tasks. However, these models need large-scale corpora for pre-training and are domain-dependent. Other non-neural unsupervised summarization approaches mostly rely on key sentence extraction, which can lead to information loss. To address these challenges, we propose a lightweight yet effective unsupervised approach called GLIMMER: a Graph and LexIcal features based unsupervised Multi-docuMEnt summaRization approach. It first constructs a sentence graph from the source documents, then automatically identifies semantic clusters by mining low-level features from raw texts, thereby improving intra-cluster correlation and the fluency of generated sentences. Finally, it summarizes clusters into natural sentences. Experiments conducted on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach outperforms existing unsupervised approaches. Furthermore, it surpasses state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally, human evaluations indicate that summaries generated by GLIMMER achieve high readability and informativeness scores. Our code is available at https://github.com/Oswald1997/GLIMMER.

[Arxiv](https://arxiv.org/abs/2408.10115)