# 大型语言模型是否能解读符号图形程序？

发布时间：2024年08月15日

`LLM应用` `图形设计` `人工智能`

> Can Large Language Models Understand Symbolic Graphics Programs?

# 摘要

> 评估大型语言模型（LLM）的能力颇具挑战，原因之一是难以找到它们未曾接触过的任务。为此，我们引入了一个新任务：聚焦于符号图形程序，这是一种流行的图形内容生成方式。LLM在程序合成方面展现出巨大潜力，但它们是否真正理解这些符号图形程序呢？与传统程序不同，符号图形程序能直接转化为视觉内容。我们通过LLM回答与图形内容相关问题的能力来衡量其对符号程序的理解程度。这一任务颇具挑战性，因为仅凭符号程序本身难以回答这些问题，但通过人类实验验证，从相应的图形内容中则能轻松解答。为了深入理解符号程序，LLM可能需要具备想象相应图形内容的能力，而无需直接查看渲染结果。我们为此创建了一个大型基准，用于评估LLM对符号图形程序的语义理解。该基准通过程序与图形的对应关系构建，几乎无需人工干预。我们在基准上对当前LLM进行了评估，初步揭示了它们从程序中推理视觉场景的能力。结果显示，这一任务能有效区分不同LLM，且擅长推理的模型表现更佳。此外，我们提出了符号指令调优（SIT）方法，通过符号程序生成的问题和图像来增强LLM的理解能力，并发现SIT数据还能提升LLM遵循指令的通用能力。

> Assessing the capabilities of large language models (LLMs) is often challenging, in part, because it is hard to find tasks to which they have not been exposed during training. We take one step to address this challenge by turning to a new task: focusing on symbolic graphics programs, which are a popular representation for graphics content that procedurally generates visual data. LLMs have shown exciting promise towards program synthesis, but do they understand symbolic graphics programs? Unlike conventional programs, symbolic graphics programs can be translated to graphics content. Here, we characterize an LLM's understanding of symbolic programs in terms of their ability to answer questions related to the graphics content. This task is challenging as the questions are difficult to answer from the symbolic programs alone -- yet, they would be easy to answer from the corresponding graphics content as we verify through a human experiment. To understand symbolic programs, LLMs may need to possess the ability to imagine how the corresponding graphics content would look without directly accessing the rendered visual content. We use this task to evaluate LLMs by creating a large benchmark for the semantic understanding of symbolic graphics programs. This benchmark is built via program-graphics correspondence, hence requiring minimal human efforts. We evaluate current LLMs on our benchmark to elucidate a preliminary assessment of their ability to reason about visual scenes from programs. We find that this task distinguishes existing LLMs and models considered good at reasoning perform better. Lastly, we introduce Symbolic Instruction Tuning (SIT) to improve this ability. Specifically, we query GPT4-o with questions and images generated by symbolic programs. Such data are then used to finetune an LLM. We also find that SIT data can improve the general instruction following ability of LLMs.

[Arxiv](https://arxiv.org/abs/2408.08313)