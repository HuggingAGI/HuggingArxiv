# VisualAgentBench：引领大型多模态模型成为视觉基础代理的新篇章

发布时间：2024年08月12日

`Agent` `人工智能` `视觉设计`

> VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents

# 摘要

> 大型多模态模型 (LMMs) 引领了人工智能的新时代，融合语言与视觉能力，打造出强大的视觉基础代理。这些代理在众多任务中表现出色，甚至可能接近通用人工智能。然而，现有基准未能充分挖掘 LMMs 在复杂现实环境中的潜力。为此，我们推出了 VisualAgentBench (VAB)，这是一个全面且创新的基准，旨在多样化场景中训练和评估 LMMs，涵盖实体化、图形用户界面和视觉设计等领域，任务设计深入探查 LMMs 的理解与交互能力。通过在多个专有与开放模型上进行严格测试，我们展示了这些模型在代理能力上的显著进步。VAB 还通过混合方法构建轨迹训练集，大幅提升 LMMs 性能。我们的工作不仅为现有模型提供基准，更为未来视觉基础代理的发展奠定坚实基础。相关资源已公开在 \url{https://github.com/THUDM/VisualAgentBench}。

> Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence. However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments. To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities. Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models. Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning. Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents. Code, train \& test data, and part of fine-tuned open LMMs are available at \url{https://github.com/THUDM/VisualAgentBench}.

[Arxiv](https://arxiv.org/abs/2408.06327)