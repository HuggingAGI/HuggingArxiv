# ANVIL：一种无需标记训练数据的基于异常漏洞识别方法

发布时间：2024年08月27日

`LLM应用` `软件开发` `网络安全`

> ANVIL: Anomaly-based Vulnerability Identification without Labelled Training Data

# 摘要

> 监督学习型的软件漏洞检测器常因缺乏充足的标记训练数据而表现不佳。相反，如 GPT-4 这样的大型语言模型（LLMs）虽未依赖标记数据训练，但在漏洞检测上，其准确率仅略胜于随机猜测。本文另辟蹊径，将漏洞检测视为异常检测问题。鉴于绝大多数代码无漏洞，LLMs 训练于海量此类代码，漏洞代码可被视作 LLMs 预测代码分布中的异类，无需标记数据即可识别。我们发现，LLMs 在重建含漏洞与无漏洞代码时，预测准确率差距显著。据此，我们开发了 ANVIL，一款行级别漏洞检测器。实验显示，不同异常评分方法的辨别力及 ANVIL 对上下文大小的敏感性。我们还评估了 ANVIL 在不同 LLM 家族中的效能，并进行了知识截止后漏洞的泄露实验。在 Magma 基准漏洞测试中，ANVIL 超越了依赖标记数据训练的 LineVul 和 LineVD，实现了 $1.62\times$ 至 $2.18\times$ 的 Top-5 准确率提升和 $1.02\times$ 至 $1.29\times$ 的 ROC 分数改进。

> Supervised learning-based software vulnerability detectors often fall short due to the inadequate availability of labelled training data. In contrast, Large Language Models (LLMs) such as GPT-4, are not trained on labelled data, but when prompted to detect vulnerabilities, LLM prediction accuracy is only marginally better than random guessing. In this paper, we explore a different approach by reframing vulnerability detection as one of anomaly detection. Since the vast majority of code does not contain vulnerabilities and LLMs are trained on massive amounts of such code, vulnerable code can be viewed as an anomaly from the LLM's predicted code distribution, freeing the model from the need for labelled data to provide a learnable representation of vulnerable code. Leveraging this perspective, we demonstrate that LLMs trained for code generation exhibit a significant gap in prediction accuracy when prompted to reconstruct vulnerable versus non-vulnerable code.
  Using this insight, we implement ANVIL, a detector that identifies software vulnerabilities at line-level granularity. Our experiments explore the discriminating power of different anomaly scoring methods, as well as the sensitivity of ANVIL to context size. We also study the effectiveness of ANVIL on various LLM families, and conduct leakage experiments on vulnerabilities that were discovered after the knowledge cutoff of our evaluated LLMs. On a collection of vulnerabilities from the Magma benchmark, ANVIL outperforms state-of-the-art line-level vulnerability detectors, LineVul and LineVD, which have been trained with labelled data, despite ANVIL having never been trained with labelled vulnerabilities. Specifically, our approach achieves $1.62\times$ to $2.18\times$ better Top-5 accuracies and $1.02\times$ to $1.29\times$ times better ROC scores on line-level vulnerability detection tasks.

[Arxiv](https://arxiv.org/abs/2408.16028)