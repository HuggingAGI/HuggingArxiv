# 生成验证器：将奖励建模视为下一个标记预测

发布时间：2024年08月27日

`LLM应用` `人工智能`

> Generative Verifiers: Reward Modeling as Next-Token Prediction

# 摘要

> 验证器或奖励模型常用于提升大型语言模型的推理性能。常见的方法是Best-of-N，即由验证器对LLM生成的N个候选解决方案进行排序，并选出最佳方案。然而，基于LLM的验证器通常仅作为判别分类器来评分，未充分利用预训练LLM的文本生成能力。为此，我们提出了一种新方法，通过联合训练下一个标记预测目标，实现验证与解决方案生成的结合。这种生成式验证器（GenRM）不仅与指令调优无缝集成，还支持思维链推理，并能通过多数投票优化推理时计算，从而提升验证效果。实验表明，在算法和小学数学推理任务中，GenRM显著优于传统判别验证器和LLM-as-a-Judge，解决问题率提升16-64%。此外，GenRM在数据集大小、模型容量和推理时计算方面展现出优异的扩展性。

> Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, we instead propose training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional inference-time compute via majority voting for better verification. We demonstrate that when using Gemma-based verifiers on algorithmic and grade-school math reasoning tasks, GenRM outperforms discriminative verifiers and LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems solved with Best-of-N. Furthermore, we show that GenRM scales favorably across dataset size, model capacity, and inference-time compute.

[Arxiv](https://arxiv.org/abs/2408.15240)