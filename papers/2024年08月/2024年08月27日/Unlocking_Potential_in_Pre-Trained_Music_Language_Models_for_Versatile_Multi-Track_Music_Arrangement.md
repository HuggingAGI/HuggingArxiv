# 探索预训练音乐语言模型在多轨音乐编排中的无限可能

发布时间：2024年08月27日

`LLM应用` `人工智能`

> Unlocking Potential in Pre-Trained Music Language Models for Versatile Multi-Track Music Arrangement

# 摘要

> 大型语言模型在符号音乐生成等领域表现出色，但在可控音乐编排任务中，每个任务需不同音乐信息作为控制，仍具挑战。本文提出统一序列到序列框架，使符号音乐模型能微调于多轨编排任务，如乐队编排、钢琴简化等。实验显示，该方法在所有任务中均超越特定基线，音乐质量更高。额外实验表明，预训练赋予模型理解音乐条件的必要知识，这是单纯微调难以实现的。

> Large language models have shown significant capabilities across various domains, including symbolic music generation. However, leveraging these pre-trained models for controllable music arrangement tasks, each requiring different forms of musical information as control, remains a novel challenge. In this paper, we propose a unified sequence-to-sequence framework that enables the fine-tuning of a symbolic music language model for multiple multi-track arrangement tasks, including band arrangement, piano reduction, drum arrangement, and voice separation. Our experiments demonstrate that the proposed approach consistently achieves higher musical quality compared to task-specific baselines across all four tasks. Furthermore, through additional experiments on probing analysis, we show the pre-training phase equips the model with essential knowledge to understand musical conditions, which is hard to acquired solely through task-specific fine-tuning.

[Arxiv](https://arxiv.org/abs/2408.15176)