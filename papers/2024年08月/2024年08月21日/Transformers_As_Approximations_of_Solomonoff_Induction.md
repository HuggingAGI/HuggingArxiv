# Transformer 模型近似于 Solomonoff 归纳法

发布时间：2024年08月21日

`LLM理论` `人工智能` `机器学习`

> Transformers As Approximations of Solomonoff Induction

# 摘要

> Solomonoff 归纳法是一种在极限内最优的序列预测算法，它融合了所有可计算的概率分布，并在预测可计算序列时表现出色。由于其最优性，它可能成为评估其他序列预测方法的理想参照模型。我们提出并深入探讨了一个假设：基于 Transformer 的大型语言模型比其他任何现有方法更接近 Solomonoff 归纳。我们分析了支持与反对该假设的证据，并据此提出了替代假设，同时概述了未来针对 Transformer 和其他 AI 模型进行此类建模的研究方向。

> Solomonoff Induction is an optimal-in-the-limit unbounded algorithm for sequence prediction, representing a Bayesian mixture of every computable probability distribution and performing close to optimally in predicting any computable sequence.
  Being an optimal form of computational sequence prediction, it seems plausible that it may be used as a model against which other methods of sequence prediction might be compared.
  We put forth and explore the hypothesis that Transformer models - the basis of Large Language Models - approximate Solomonoff Induction better than any other extant sequence prediction method. We explore evidence for and against this hypothesis, give alternate hypotheses that take this evidence into account, and outline next steps for modelling Transformers and other kinds of AI in this way.

[Arxiv](https://arxiv.org/abs/2408.12065)