# CogVLM2：专为图像与视频理解设计的视觉语言模型

发布时间：2024年08月29日

`LLM应用` `计算机视觉` `视频处理`

> CogVLM2: Visual Language Models for Image and Video Understanding

# 摘要

> 摘要：我们始于 VisualGLM 和 CogVLM，持续探索 VLMs，旨在提升视觉与语言的融合、构建高效的高分辨率架构，并拓展更多模态与应用。本文介绍 CogVLM2 系列，新一代视觉语言模型，涵盖图像与视频理解，包括 CogVLM2、CogVLM2-Video 及 GLM-4V。CogVLM2 作为图像理解模型，沿用视觉专家架构，优化预训练与后训练流程，最高支持 $1344 \times 1344$ 像素输入。CogVLM2-Video 则整合时间戳多帧输入，并创新自动化时间定位数据构建。该系列在 MMBench、MM-Vet、TextVQA、MVBench 及 VCGBench 等评测中表现卓越。所有模型已开源，助力领域进步。

> 
Abstract:Beginning with VisualGLM and CogVLM, we are continuously exploring VLMs in pursuit of enhanced vision-language fusion, efficient higher-resolution architecture, and broader modalities and applications. Here we propose the CogVLM2 family, a new generation of visual language models for image and video understanding including CogVLM2, CogVLM2-Video and GLM-4V. As an image understanding model, CogVLM2 inherits the visual expert architecture with improved training recipes in both pre-training and post-training stages, supporting input resolution up to $1344 \times 1344$ pixels. As a video understanding model, CogVLM2-Video integrates multi-frame input with timestamps and proposes automated temporal grounding data construction. Notably, CogVLM2 family has achieved state-of-the-art results on benchmarks like MMBench, MM-Vet, TextVQA, MVBench and VCGBench. All models are open-sourced in this https URL and this https URL, contributing to the advancement of the field.
    

[Arxiv](https://arxiv.org/pdf/2408.16500)