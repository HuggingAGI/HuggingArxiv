# 探究大型语言模型所生成代码的质量

发布时间：2024年08月29日

`LLM应用` `软件开发`

> Examination of Code generated by Large Language Models

# 摘要

> ChatGPT 和 Copilot 等大型语言模型（LLM）通过自动化代码生成，不仅加速了原型设计，还支持教育并提升生产效率。因此，这些模型生成的代码必须与人工编写的代码一样正确且高质量。为此，我们通过控制实验，让 ChatGPT 和 Copilot 生成 Java 和 Python 的简单算法及相应单元测试，并评估其正确性与覆盖率。结果显示，LLM 间、编程语言间、算法与测试代码间以及时间维度上均存在显著差异。本文不仅呈现了这些发现，还提供了实验方法，以便未来对更多算法、语言及 LLM 进行可重复且可比的评估。

> Large language models (LLMs), such as ChatGPT and Copilot, are transforming software development by automating code generation and, arguably, enable rapid prototyping, support education, and boost productivity. Therefore, correctness and quality of the generated code should be on par with manually written code. To assess the current state of LLMs in generating correct code of high quality, we conducted controlled experiments with ChatGPT and Copilot: we let the LLMs generate simple algorithms in Java and Python along with the corresponding unit tests and assessed the correctness and the quality (coverage) of the generated (test) codes. We observed significant differences between the LLMs, between the languages, between algorithm and test codes, and over time. The present paper reports these results together with the experimental methods allowing repeated and comparable assessments for more algorithms, languages, and LLMs over time.

[Arxiv](https://arxiv.org/abs/2408.16601)