# 探究大型语言模型在代码生成任务中的评估现状

发布时间：2024年08月29日

`LLM应用` `软件开发` `自动化`

> A Survey on Evaluating Large Language Models in Code Generation Tasks

# 摘要

> 本文深入探讨了评估大型语言模型（LLM）在代码生成方面的现状，涵盖了方法、指标及数据集的全面分析。随着自动化软件开发的兴起，LLM在代码生成领域展现出巨大潜力。文章追溯了LLM的发展历程及其在代码生成中的应用，并详述了评估其能力的多种手段，如代码的正确性、效率、可读性，以及专家评审和用户体验为基础的评价方法。同时，文章指出了现有基准数据集的不足，并提出了改进建议。通过综合运用代码成功率、测试通过率及性能效率等多维度指标，文章全面评估了LLM在代码生成中的实际效能。最后，文章探讨了评估过程中的挑战，强调了方法的全面性与准确性，并探讨了适应软件开发动态变化的必要性。这些深入分析为LLM在代码生成领域的进一步优化与提升提供了重要指导。

> This paper provides a comprehensive review of the current methods and metrics used to evaluate the performance of Large Language Models (LLMs) in code generation tasks. With the rapid growth in demand for automated software development, LLMs have demonstrated significant potential in the field of code generation. The paper begins by reviewing the historical development of LLMs and their applications in code generation. Next, it details various methods and metrics for assessing the code generation capabilities of LLMs, including code correctness, efficiency, readability, and evaluation methods based on expert review and user experience. The paper also evaluates the widely used benchmark datasets, identifying their limitations and proposing directions for future improvements. Specifically, the paper analyzes the performance of code generation models across different tasks by combining multiple evaluation metrics, such as code compilation/interpretation success rates, unit test pass rates, and performance and efficiency metrics, to comprehensively assess the practical application of LLMs in code generation. Finally, the paper discusses the challenges faced in evaluating LLMs in code generation, particularly how to ensure the comprehensiveness and accuracy of evaluation methods and how to adapt to the evolving practices of software development. These analyses and discussions provide valuable insights for further optimizing and improving the application of LLMs in code generation tasks.

[Arxiv](https://arxiv.org/abs/2408.16498)