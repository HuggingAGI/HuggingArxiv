# VLM-KD：利用 VLM 进行知识蒸馏，助力长尾视觉识别

发布时间：2024年08月29日

`LLM应用` `计算机视觉` `机器学习`

> VLM-KD: Knowledge Distillation from VLM for Long-Tail Visual Recognition

# 摘要

> 在视觉识别领域，知识蒸馏常用于将大型教师模型的知识传递给小型学生模型。本文提出了一种新颖方法，利用现成的视觉-语言模型（VLM）进行知识蒸馏，不仅提供传统视觉模型的监督，还引入新的文本监督。我们开发了一个创新框架，生成文本监督并将其融入视觉编码器。实验证明，我们的VLM-KD方法在多个基准数据集上表现优异，超越了现有的长期视觉分类器。这是首次尝试使用VLM生成的文本监督进行知识蒸馏，并应用于随机初始化的视觉编码器。

> For visual recognition, knowledge distillation typically involves transferring knowledge from a large, well-trained teacher model to a smaller student model. In this paper, we introduce an effective method to distill knowledge from an off-the-shelf vision-language model (VLM), demonstrating that it provides novel supervision in addition to those from a conventional vision-only teacher model. Our key technical contribution is the development of a framework that generates novel text supervision and distills free-form text into a vision encoder. We showcase the effectiveness of our approach, termed VLM-KD, across various benchmark datasets, showing that it surpasses several state-of-the-art long-tail visual classifiers. To our knowledge, this work is the first to utilize knowledge distillation with text supervision generated by an off-the-shelf VLM and apply it to vanilla randomly initialized vision encoders.

[Arxiv](https://arxiv.org/abs/2408.16930)