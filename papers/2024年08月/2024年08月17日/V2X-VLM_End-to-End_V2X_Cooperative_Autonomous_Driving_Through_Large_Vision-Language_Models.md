# V2X-VLM：借助大型视觉-语言模型，实现端到端的 V2X 协作自动驾驶

发布时间：2024年08月17日

`Agent` `自动驾驶` `智能交通`

> V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models

# 摘要

> 自动驾驶技术的最新进展聚焦于端到端系统，这些系统全面管理驾驶任务，从环境感知到车辆导航与控制。本文推出的V2X-VLM，是一种创新的车辆-基础设施协同自动驾驶框架，结合了大型视觉-语言模型。V2X-VLM通过融合车载摄像头、基础设施传感器及文本信息，旨在提升情境感知、决策效率和轨迹规划精度。其强大的多模态数据融合能力，确保了在复杂多变的驾驶环境中，实现精确且安全的端到端轨迹规划。实验证明，V2X-VLM在协同自动驾驶领域超越了现有顶尖技术。

> Advancements in autonomous driving have increasingly focused on end-to-end (E2E) systems that manage the full spectrum of driving tasks, from environmental perception to vehicle navigation and control. This paper introduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative autonomous driving (VICAD) framework with large vision-language models (VLMs). V2X-VLM is designed to enhance situational awareness, decision-making, and ultimate trajectory planning by integrating data from vehicle-mounted cameras, infrastructure sensors, and textual information. The strength of the comprehensive multimodel data fusion of the VLM enables precise and safe E2E trajectory planning in complex and dynamic driving scenarios. Validation on the DAIR-V2X dataset demonstrates that V2X-VLM outperforms existing state-of-the-art methods in cooperative autonomous driving.

[Arxiv](https://arxiv.org/abs/2408.09251)