# VITA：开启开源交互式全模态LLM新篇章

发布时间：2024年08月09日

`LLM应用` `人工智能` `多媒体`

> VITA: Towards Open-Source Interactive Omni Multimodal LLM

# 摘要

> GPT-4o 的卓越多模态能力和交互体验凸显了其在实际应用中的重要性，但开源模型在这两方面往往表现平平。本文介绍的 VITA，作为首个开源的多模态大型语言模型，不仅擅长处理视频、图像、文本和音频，还提供了先进的多模态交互体验。我们从 Mixtral 8x7B 出发，扩展其中文词汇并进行双语调优，再通过两阶段多任务学习，赋予模型视觉和音频处理能力。VITA 在多语言、视觉和音频理解方面展现出坚实的基础能力，并在各类基准测试中表现出色。此外，我们在提升多模态人机交互的自然性方面取得了重要进展，首次在 MLLM 中实现了非唤醒交互和音频中断功能。VITA 的诞生标志着开源社区在多模态理解和交互集成探索的起点。尽管 VITA 仍有提升空间，我们期待它能为未来的研究奠定坚实基础。项目详情请访问：https://vita-home.github.io。

> The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. To the best of our knowledge, we are the first to exploit non-awakening interaction and audio interrupt in MLLM. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page: https://vita-home.github.io.

[Arxiv](https://arxiv.org/abs/2408.05211)