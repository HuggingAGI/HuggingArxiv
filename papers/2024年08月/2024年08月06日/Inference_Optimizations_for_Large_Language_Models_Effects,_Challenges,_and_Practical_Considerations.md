# 大型语言模型的推理优化涉及效果、挑战及实际考量。

发布时间：2024年08月06日

`LLM理论` `计算机科学`

> Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations

# 摘要

> 大型语言模型因其无需重新训练即可适应新任务的特性，在自然语言处理领域广泛应用。然而，其庞大的规模和复杂性也带来了挑战与机遇，激发了新的模型训练、优化和部署方法的研究。本综述深入探讨了量化、剪枝、知识蒸馏和架构优化等技术，旨在压缩模型并降低资源需求。通过构建一个优化方法的分类体系，我们不仅概览了优化领域的现状，还助力于更清晰地把握研究发展脉络。

> Large language models are ubiquitous in natural language processing because they can adapt to new tasks without retraining. However, their sheer scale and complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations. The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications. The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better.

[Arxiv](https://arxiv.org/abs/2408.03130)