# 在计算社会科学任务中，我们应选择提示优化还是模型微调？本文探讨了使用大型语言模型的最佳策略。

发布时间：2024年08月02日

`LLM应用` `社会科学` `人工智能`

> Prompt Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social Science Tasks

# 摘要

> 大型语言模型在计算社会科学中是处理文本理解复杂任务的强大工具，但其多样性也使得建立标准化最佳实践变得困难。我们通过分析23个社会知识任务的基准测试，揭示了三种高效策略：优先选择词汇丰富、预训练广泛的模型；采用AI增强提示而非简单零-shot；针对特定任务进行数据微调，并在训练数据充足时考虑多数据集的复杂指令调优。

> Large Language Models are expressive tools that enable complex tasks of text understanding within Computational Social Science. Their versatility, while beneficial, poses a barrier for establishing standardized best practices within the field. To bring clarity on the values of different strategies, we present an overview of the performance of modern LLM-based classification methods on a benchmark of 23 social knowledge tasks. Our results point to three best practices: select models with larger vocabulary and pre-training corpora; avoid simple zero-shot in favor of AI-enhanced prompting; fine-tune on task-specific data, and consider more complex forms instruction-tuning on multiple datasets only when only training data is more abundant.

[Arxiv](https://arxiv.org/abs/2408.01346)