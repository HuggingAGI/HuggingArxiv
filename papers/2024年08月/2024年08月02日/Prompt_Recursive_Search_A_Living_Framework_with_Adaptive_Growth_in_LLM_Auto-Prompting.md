# 提示递归搜索：在 LLM 自动提示领域，这是一个随需应变、不断成长的动态框架。

发布时间：2024年08月02日

`LLM应用` `人工智能`

> Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting

# 摘要

> 大型语言模型 (LLM) 在 NLP 领域内解决多样任务方面表现卓越，多种提示设计策略显著提升了其性能。然而，这些提示虽有益，也各有局限。主要提示设计方法包括：专家设计提示 (EDP)，如思维链 (CoT)，针对特定数据集手动设计，一旦确立便不可更改，其有效性受限于设计者的专业水平。应用于 LLM 时，EDP 的静态性质导致同一数据集中简单和复杂问题采用统一方法，造成直接问题的令牌使用效率低下。另一种方法是 LLM 衍生提示 (LDP)，为特定问题提供定制解决方案，缓解了 EDP 的限制，但在解决复杂问题时可能因错误累积而性能下降。为应对这些挑战，我们设计了提示递归搜索 (PRS) 框架，利用 LLM 生成特定问题的解决方案，节省令牌，并包含问题复杂性评估和可调整结构，降低错误风险。通过在不同领域的一系列数据集上使用不同参数数量的 LLM 进行广泛实验，我们证实了 PRS 框架的有效性。与 CoT 方法相比，PRS 方法在使用 Llama3-7B 模型时，在 BBH 数据集上的准确率提高了 8%，实现了 22% 的改进。

> Large Language Models (LLMs) exhibit remarkable proficiency in addressing a diverse array of tasks within the Natural Language Processing (NLP) domain, with various prompt design strategies significantly augmenting their capabilities. However, these prompts, while beneficial, each possess inherent limitations. The primary prompt design methodologies are twofold: The first, exemplified by the Chain of Thought (CoT), involves manually crafting prompts specific to individual datasets, hence termed Expert-Designed Prompts (EDPs). Once these prompts are established, they are unalterable, and their effectiveness is capped by the expertise of the human designers. When applied to LLMs, the static nature of EDPs results in a uniform approach to both simple and complex problems within the same dataset, leading to the inefficient use of tokens for straightforward issues. The second method involves prompts autonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which provide tailored solutions to specific problems, mitigating the limitations of EDPs. However, LDPs may encounter a decline in performance when tackling complex problems due to the potential for error accumulation during the solution planning process. To address these challenges, we have conceived a novel Prompt Recursive Search (PRS) framework that leverages the LLM to generate solutions specific to the problem, thereby conserving tokens. The framework incorporates an assessment of problem complexity and an adjustable structure, ensuring a reduction in the likelihood of errors. We have substantiated the efficacy of PRS framework through extensive experiments using LLMs with different numbers of parameters across a spectrum of datasets in various domains. Compared to the CoT method, the PRS method has increased the accuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22% improvement.

![提示递归搜索：在 LLM 自动提示领域，这是一个随需应变、不断成长的动态框架。](../../../paper_images/2408.01423/eurai)

[Arxiv](https://arxiv.org/abs/2408.01423)