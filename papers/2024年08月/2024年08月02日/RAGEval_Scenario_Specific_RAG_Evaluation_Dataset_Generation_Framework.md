# RAGEval：专为特定场景设计的 RAG 评估数据集生成框架

发布时间：2024年08月02日

`RAG` `人工智能` `软件开发`

> RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework

# 摘要

> RAG 系统在减轻 LLM 幻觉方面表现出色，但现有基准主要评估一般知识回答的准确性，未能全面评估其在不同垂直领域数据处理的有效性。为此，我们推出了 RAGEval 框架，自动生成评估数据集，以多维度评估 LLM 在各种场景下的知识运用能力。RAGEval 通过种子文档生成多样化文档并构建问答对，并引入 Completeness、Hallucination 和 Irrelevance 三个新指标，精准评估 LLM 响应。该框架在垂直领域对 RAG 模型进行基准测试，有效区分知识来源，无论是参数化记忆还是检索，从而更全面地评估 LLM 的知识使用能力。

> Retrieval-Augmented Generation (RAG) systems have demonstrated their advantages in alleviating the hallucination of Large Language Models (LLMs). Existing RAG benchmarks mainly focus on evaluating whether LLMs can correctly answer the general knowledge. However, they are unable to evaluate the effectiveness of the RAG system in dealing with the data from different vertical domains. This paper introduces RAGEval, a framework for automatically generating evaluation datasets to evaluate the knowledge usage ability of different LLMs in different scenarios. Specifically, RAGEval summarizes a schema from seed documents, applies the configurations to generate diverse documents, and constructs question-answering pairs according to both articles and configurations. We propose three novel metrics, Completeness, Hallucination, and Irrelevance, to carefully evaluate the responses generated by LLMs. By benchmarking RAG models in vertical domains, RAGEval has the ability to better evaluate the knowledge usage ability of LLMs, which avoids the confusion regarding the source of knowledge in answering question in existing QA datasets--whether it comes from parameterized memory or retrieval.

[Arxiv](https://arxiv.org/abs/2408.01262)