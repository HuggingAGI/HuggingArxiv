# Re-TASK：从能力、技能与知识视角重新探索 LLM 任务

发布时间：2024年08月13日

`LLM理论` `人工智能`

> Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives

# 摘要

> 随着 LLM 的不断扩展，它们在特定领域任务中的表现往往不尽如人意。本文提出的 Re-TASK 框架，从能力、技能、知识三重视角重新审视 LLM 任务，借鉴布鲁姆分类法和知识空间理论，为我们提供了一套系统的方法论，以深化对 LLM 在特定领域任务中的理解、评估和提升。该框架揭示了 LLM 能力、知识处理与技能应用之间的复杂互动，并阐明了这些元素如何共同影响任务表现。通过应用 Re-TASK 框架，我们发现许多特定领域任务的失败源于知识匮乏或技能适应不足。为此，我们提出了一系列结构化策略，通过精准的知识注入和技能适应来强化 LLM。具体而言，我们识别任务相关的关键能力项，并运用精心设计的提示策略提升任务表现，从而减少对广泛微调的依赖。此外，我们还通过特定能力的指令对 LLM 进行微调，进一步验证了框架的有效性。实验结果显示，该框架在提升 LLM 性能和适用性方面成效显著。

> As large language models (LLMs) continue to scale, their enhanced performance often proves insufficient for solving domain-specific tasks. Systematically analyzing their failures and effectively enhancing their performance remain significant challenges. This paper introduces the Re-TASK framework, a novel theoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge perspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space Theory. The Re-TASK framework provides a systematic methodology to deepen our understanding, evaluation, and enhancement of LLMs for domain-specific tasks. It explores the interplay among an LLM's capabilities, the knowledge it processes, and the skills it applies, elucidating how these elements are interconnected and impact task performance. Our application of the Re-TASK framework reveals that many failures in domain-specific tasks can be attributed to insufficient knowledge or inadequate skill adaptation. With this insight, we propose structured strategies for enhancing LLMs through targeted knowledge injection and skill adaptation. Specifically, we identify key capability items associated with tasks and employ a deliberately designed prompting strategy to enhance task performance, thereby reducing the need for extensive fine-tuning. Alternatively, we fine-tune the LLM using capability-specific instructions, further validating the efficacy of our framework. Experimental results confirm the framework's effectiveness, demonstrating substantial improvements in both the performance and applicability of LLMs.

[Arxiv](https://arxiv.org/abs/2408.06904)