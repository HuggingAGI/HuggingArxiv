# 如何定义好故事并衡量其质量？这是一场关于故事评估的全面探索。

发布时间：2024年08月26日

`LLM应用`

> What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation

# 摘要

> 随着AI技术的进步，尤其是大型语言模型的成功，自动生成故事的数量与质量大幅提升。这促使我们需要自动评估故事，以衡量计算系统的生成能力，并分析机器与人类创作的故事质量。故事评估比其他生成任务更具挑战性，因为它不仅关注流畅与准确，还需考虑整体连贯性、角色塑造及趣味性等复杂因素。本调查首先概述了现有的讲故事任务，并强调了其评估难点，介绍了衡量故事的人类标准及基准数据集。接着，我们提出了一种分类法，以组织适用于故事评估的指标，并详细描述了这些指标的优缺点。此外，我们还探讨了人机协作在故事评估与生成中的应用。最后，我们展望了未来研究方向，从故事评估延伸至更广泛的评估领域。

> With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories. Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations.

[Arxiv](https://arxiv.org/abs/2408.14622)