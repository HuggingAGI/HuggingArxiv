# MMR：衡量大型多模态模型的阅读技能

发布时间：2024年08月26日

`LLM应用` `人工智能` `计算机视觉`

> MMR: Evaluating Reading Ability of Large Multimodal Models

# 摘要

> 大型多模态模型 (LMMs) 在处理文本丰富的图像方面表现出色，但现有的基准主要集中在简单的问答任务上，导致许多模型轻易取得高分。这表明当前的评估体系未能准确反映模型的真实能力。为此，我们推出了多模态阅读 (MMR) 基准，涵盖 11 项多样化任务，旨在全面评估模型在复杂推理和空间理解方面的能力。MMR 基准首次采用语言模型辅助的人工标注，通过测试包括 GPT-4o 在内的顶尖 LMMs，揭示了现有模型的局限，凸显了我们新基准的重要价值。

> Large multimodal models (LMMs) have demonstrated impressive capabilities in understanding various types of image, including text-rich images. Most existing text-rich image benchmarks are simple extraction-based question answering, and many LMMs now easily achieve high scores. This means that current benchmarks fail to accurately reflect performance of different models, and a natural idea is to build a new benchmark to evaluate their complex reasoning and spatial understanding abilities. In this work, we propose the Multi-Modal Reading (MMR) benchmark in 11 diverse tasks to evaluate LMMs for text-rich image understanding. MMR is the first text-rich image benchmark built on human annotations with the help of language models. By evaluating several state-of-the-art LMMs, including GPT-4o, it reveals the limited capabilities of existing LMMs underscoring the value of our benchmark.

[Arxiv](https://arxiv.org/abs/2408.14594)