# 探究大型语言模型与人类在随机数生成任务中的表现差异

发布时间：2024年08月18日

`LLM应用` `心理学` `人工智能`

> A Comparison of Large Language Model and Human Performance on Random Number Generation Tasks

# 摘要

> 在心理学领域，随机数生成任务 (RNGTs) 用于探究人类如何创造无规律的序列。本研究通过调整现有的人类 RNGT 以适应 LLM 环境，探索了 ChatGPT-3.5 这一基于人类文本训练的 LLM，在生成随机数时是否展现人类般的认知偏差。初步结果显示，ChatGPT-3.5 在避免重复和顺序模式方面优于人类，其重复和相邻数字的出现频率显著降低。未来对不同模型、参数及提示策略的研究将进一步揭示 LLM 如何更精准地模拟人类的随机生成行为，并拓宽其在认知与行为科学研究中的应用范围。

> Random Number Generation Tasks (RNGTs) are used in psychology for examining how humans generate sequences devoid of predictable patterns. By adapting an existing human RNGT for an LLM-compatible environment, this preliminary study tests whether ChatGPT-3.5, a large language model (LLM) trained on human-generated text, exhibits human-like cognitive biases when generating random number sequences. Initial findings indicate that ChatGPT-3.5 more effectively avoids repetitive and sequential patterns compared to humans, with notably lower repeat frequencies and adjacent number frequencies. Continued research into different models, parameters, and prompting methodologies will deepen our understanding of how LLMs can more closely mimic human random generation behaviors, while also broadening their applications in cognitive and behavioral science research.

[Arxiv](https://arxiv.org/abs/2408.09656)