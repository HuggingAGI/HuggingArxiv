# 利用 ChatGPT 评估作文与简短构建性回答

发布时间：2024年08月18日

`LLM应用` `人工智能`

> Using ChatGPT to Score Essays and Short-Form Constructed Responses

# 摘要

> 本研究探讨了 ChatGPT 的大型语言模型是否能媲美 ASAP 竞赛中的人类和机器评分准确度。研究涉及多种预测模型，如线性回归、随机森林等，并通过二次加权kappa（QWK）指标评估 ChatGPT 与人类评分者的表现。结果表明，虽然 ChatGPT 的梯度提升模型在某些数据集上接近人类评分，但整体表现不稳定且常低于人类评分。研究指出，需进一步优化以处理偏见并确保评分公正。尽管面临挑战，ChatGPT 在特定领域微调后显示出评分效率的潜力。研究结论是，ChatGPT 可作为人类评分的补充，但需更多开发以确保高风险评估的可靠性。未来研究应聚焦于提升模型准确性、处理伦理问题，并探索结合 ChatGPT 与实证方法的混合模型。

> This study aimed to determine if ChatGPT's large language models could match the scoring accuracy of human and machine scores from the ASAP competition. The investigation focused on various prediction models, including linear regression, random forest, gradient boost, and boost. ChatGPT's performance was evaluated against human raters using quadratic weighted kappa (QWK) metrics. Results indicated that while ChatGPT's gradient boost model achieved QWKs close to human raters for some data sets, its overall performance was inconsistent and often lower than human scores. The study highlighted the need for further refinement, particularly in handling biases and ensuring scoring fairness. Despite these challenges, ChatGPT demonstrated potential for scoring efficiency, especially with domain-specific fine-tuning. The study concludes that ChatGPT can complement human scoring but requires additional development to be reliable for high-stakes assessments. Future research should improve model accuracy, address ethical considerations, and explore hybrid models combining ChatGPT with empirical methods.

[Arxiv](https://arxiv.org/abs/2408.09540)