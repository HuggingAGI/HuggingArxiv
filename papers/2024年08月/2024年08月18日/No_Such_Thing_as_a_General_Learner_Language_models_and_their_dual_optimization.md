# 不存在通用学习者：探讨语言模型的双重优化之道

发布时间：2024年08月18日

`LLM理论` `人工智能` `认知科学`

> No Such Thing as a General Learner: Language models and their dual optimization

# 摘要

> 大型语言模型（LLM）在理解人类认知，尤其是在语言习得辩论方面，能发挥何种作用？我们首先指出，人类和LLM都不是广义上的通用学习者。我们提出，LLM通过双重优化过程运作：在训练期间优化（常与语言习得相比），并通过类似自然选择的过程被选择。因此，无论LLM的表现与人类是否相似，都不易影响关于人类认知偏见对语言重要性的辩论。

> What role can the otherwise successful Large Language Models (LLMs) play in the understanding of human cognition, and in particular in terms of informing language acquisition debates? To contribute to this question, we first argue that neither humans nor LLMs are general learners, in a variety of senses. We make a novel case for how in particular LLMs follow a dual-optimization process: they are optimized during their training (which is typically compared to language acquisition), and modern LLMs have also been selected, through a process akin to natural selection in a species. From this perspective, we argue that the performance of LLMs, whether similar or dissimilar to that of humans, does not weigh easily on important debates about the importance of human cognitive biases for language.

[Arxiv](https://arxiv.org/abs/2408.09544)