# LLM作为评分者：探讨输出顺序如何影响对话评估

发布时间：2024年06月04日

`LLM应用

这篇论文探讨了如何通过提示设计来优化大型语言模型（LLMs）在对话评估中的应用。研究关注的是如何通过调整提示的结构，如输出指令的顺序和加入解释性理由，来提高LLMs在评分任务中的表现。这种研究直接应用于LLMs的实际使用场景，即对话评估，因此属于LLM应用分类。` `对话评估` `语言模型`

> LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation

# 摘要

> 本研究探讨了提示设计如何影响大型语言模型（LLMs）在对话评估中的应用。尽管LLMs在评分任务中日益普及，但设计出适合对话评估的有效提示仍是一大挑战，这主要是因为模型对评估的主观性和敏感性。我们的实验通过调整输出指令的顺序和加入解释性理由，探索了不同提示结构的效果。结果显示，先给出理由再给出分数的“理由优先”方法，能更全面地影响LLMs的评分，这对于提升基于LLM的评估的准确性和一致性具有重要意义。

> This research investigates the effect of prompt design on dialogue evaluation using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for dialogue evaluation remains challenging due to model sensitivity and subjectivity in dialogue assessments. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a "reason-first" approach yielding more comprehensive evaluations. This insight is crucial for enhancing the accuracy and consistency of LLM-based evaluations.

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x1.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x2.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x3.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x4.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x5.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x6.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x7.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x8.png)

[Arxiv](https://arxiv.org/abs/2406.02863)