# LLM作为评分者：探讨输出顺序如何影响对话评估

发布时间：2024年06月04日

`LLM应用

理由：这篇论文主要探讨了如何通过提示设计来优化大型语言模型（LLMs）在对话评估中的应用。研究内容集中在如何通过调整提示结构和内容来提高LLMs在特定任务（即对话评估）中的性能，这属于LLM在实际应用中的优化和改进，因此归类为LLM应用。` `对话评估` `语言模型`

> LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation

# 摘要

> 本研究深入探讨了提示设计如何影响大型语言模型（LLMs）在对话评估中的应用。尽管LLMs在评分任务中日益普及，但设计出既能应对模型敏感性又能适应对话评估主观性的有效提示仍是一大挑战。我们通过实验不同结构的提示，调整输出指令顺序并加入解释性理由，发现“理由优先”的提示方式能显著提升LLMs的评估质量，为提高基于LLM的评估的准确性和一致性提供了关键见解。

> This research investigates the effect of prompt design on dialogue evaluation using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for dialogue evaluation remains challenging due to model sensitivity and subjectivity in dialogue assessments. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a "reason-first" approach yielding more comprehensive evaluations. This insight is crucial for enhancing the accuracy and consistency of LLM-based evaluations.

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x1.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x2.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x3.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x4.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x5.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x6.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x7.png)

![LLM作为评分者：探讨输出顺序如何影响对话评估](../../../paper_images/2406.02863/x8.png)

[Arxiv](https://arxiv.org/abs/2406.02863)