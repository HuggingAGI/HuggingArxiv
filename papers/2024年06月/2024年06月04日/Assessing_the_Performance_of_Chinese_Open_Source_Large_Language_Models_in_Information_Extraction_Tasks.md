# 探究中国开源大型语言模型在信息提取领域的实力

发布时间：2024年06月04日

`LLM应用

理由：这篇论文主要探讨了大型语言模型（LLMs）在中文信息抽取（IE）任务中的表现，包括零样本和少样本条件下的性能，并与知名语言模型ChatGPT进行了对比分析。这些研究内容直接关联到LLMs在实际NLP任务中的应用，特别是在信息抽取这一具体领域。因此，它属于LLM应用分类。` `信息抽取`

> Assessing the Performance of Chinese Open Source Large Language Models in Information Extraction Tasks

# 摘要

> 信息抽取（IE）是NLP中的关键技术，它从非结构化文本中提炼出结构化信息，助力于与依赖结构化数据的现实应用无缝对接。尽管IE的重要性不言而喻，但近期针对英语IE任务的实验显示，大型语言模型（LLMs）在如命名实体识别（NER）等子任务上达到最佳性能仍面临挑战。本文对主流中文开源LLMs在零样本条件下的IE任务表现进行了全面探究，并展示了少样本实验的结果，以评估这些模型的潜力。同时，我们还将这些开源LLMs与知名语言模型ChatGPT在IE性能上进行了对比分析。通过精心的实验与分析，本文旨在揭示中文开源LLMs在NLP领域信息抽取方面的优势、局限及改进潜力。

> Information Extraction (IE) plays a crucial role in Natural Language Processing (NLP) by extracting structured information from unstructured text, thereby facilitating seamless integration with various real-world applications that rely on structured data. Despite its significance, recent experiments focusing on English IE tasks have shed light on the challenges faced by Large Language Models (LLMs) in achieving optimal performance, particularly in sub-tasks like Named Entity Recognition (NER). In this paper, we delve into a comprehensive investigation of the performance of mainstream Chinese open-source LLMs in tackling IE tasks, specifically under zero-shot conditions where the models are not fine-tuned for specific tasks. Additionally, we present the outcomes of several few-shot experiments to further gauge the capability of these models. Moreover, our study includes a comparative analysis between these open-source LLMs and ChatGPT, a widely recognized language model, on IE performance. Through meticulous experimentation and analysis, we aim to provide insights into the strengths, limitations, and potential enhancements of existing Chinese open-source LLMs in the domain of Information Extraction within the context of NLP.

[Arxiv](https://arxiv.org/abs/2406.02079)