# 多模态AI的公平与偏见：深度探索

发布时间：2024年06月27日

`LLM理论

理由：这篇论文主要探讨了大型多模态模型（LMMs）和大型语言模型（LLMs）中的公平性与偏见问题，并提出了一种新的量化偏见的方法。这些问题和方法论的研究属于对LLM理论层面的探讨，而非具体的应用、Agent设计或RAG技术。因此，将其归类为LLM理论是合适的。` `人工智能` `数据分析`

> Fairness and Bias in Multimodal AI: A Survey

# 摘要

> AI系统中的公平性与偏见问题至关重要，近年来主流媒体对此类事件的报道不绝于耳。本调查填补了大型多模态模型（LMMs）与大型语言模型（LLMs）中公平性与偏见研究的空白，列举了50个数据集与模型实例及其面临的挑战，并新增了一种量化偏见的方法（preuse），与文献中已有的内在与外在偏见类别并列。我们深入探讨了研究者们应对这些挑战的不同策略。通过在Google Scholar上进行的两个略有差异的搜索，我们发现“大型多模态模型中的公平性与偏见”和“大型语言模型中的公平性与偏见”分别关联到33,400和538,000个链接。我们坚信，这项研究不仅填补了知识空白，还为研究人员及利益相关者提供了应对多模态AI领域公平性与偏见挑战的宝贵洞见。

> The importance of addressing fairness and bias in artificial intelligence (AI) systems cannot be over-emphasized. Mainstream media has been awashed with news of incidents around stereotypes and bias in many of these systems in recent years. In this survey, we fill a gap with regards to the minimal study of fairness and bias in Large Multimodal Models (LMMs) compared to Large Language Models (LLMs), providing 50 examples of datasets and models along with the challenges affecting them; we identify a new category of quantifying bias (preuse), in addition to the two well-known ones in the literature: intrinsic and extrinsic; we critically discuss the various ways researchers are addressing these challenges. Our method involved two slightly different search queries on Google Scholar, which revealed that 33,400 and 538,000 links are the results for the terms "Fairness and bias in Large Multimodal Models" and "Fairness and bias in Large Language Models", respectively. We believe this work contributes to filling this gap and providing insight to researchers and other stakeholders on ways to address the challenge of fairness and bias in multimodal A!.

[Arxiv](https://arxiv.org/abs/2406.19097)