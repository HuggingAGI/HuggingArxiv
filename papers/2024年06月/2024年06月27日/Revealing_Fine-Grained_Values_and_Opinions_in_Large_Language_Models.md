# 探究大型语言模型中微妙的价值与观点

发布时间：2024年06月27日

`LLM理论

理由：这篇论文主要探讨了大型语言模型（LLMs）中的隐含价值和观点，以及如何通过分析模型对特定提示的响应来揭示和理解这些模型中的偏见和潜在风险。研究通过详细的实验和分析，探讨了LLMs对道德与政治敏感议题的立场，并分析了这些立场如何受到提示方式的影响。此外，研究还深入分析了模型响应中的修辞手法，这些都是在理论层面上对LLMs的理解和分析。因此，这篇论文更适合归类于LLM理论，因为它关注的是LLMs的内部机制和理论分析，而不是具体的应用或Agent的行为。` `政治分析` `社会科学研究`

> Revealing Fine-Grained Values and Opinions in Large Language Models

# 摘要

> 探索大型语言模型（LLMs）中的隐含价值和观点，有助于揭示偏见并减少潜在风险。近期研究通过向LLMs提出调查问题，并量化其对道德与政治敏感议题的立场来实现这一目标。然而，LLMs的立场因提示方式差异巨大，且支持或反对某一观点的论点多样。本研究通过分析由6个LLMs使用420种提示变体生成的156k个对政治指南针测试（PCT）62个命题的响应，来深入探讨此问题。我们不仅进行了立场的粗粒度分析，还对支持这些立场的纯文本理由进行了细粒度分析。在细粒度分析中，我们识别了响应中的常见修辞手法——这些在不同提示中反复出现且语义相似的短语，揭示了LLMs倾向于产生的特定文本模式。研究发现，提示中加入的人口统计特征显著影响PCT结果，这不仅反映了偏见，还揭示了在引发封闭形式与开放领域响应时测试结果的差异。此外，通过修辞手法分析纯文本理由，我们发现即使在立场不同的情况下，模型和提示间也会重复生成相似的理由。

> Uncovering latent values and opinions in large language models (LLMs) can help identify biases and mitigate potential harm. Recently, this has been approached by presenting LLMs with survey questions and quantifying their stances towards morally and politically charged statements. However, the stances generated by LLMs can vary greatly depending on how they are prompted, and there are many ways to argue for or against a given position. In this work, we propose to address this by analysing a large and robust dataset of 156k LLM responses to the 62 propositions of the Political Compass Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of their generated stances and fine-grained analysis of the plain text justifications for those stances. For fine-grained analysis, we propose to identify tropes in the responses: semantically similar phrases that are recurrent and consistent across different prompts, revealing patterns in the text that a given LLM is prone to produce. We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias, as well as disparities between the results of tests when eliciting closed-form vs. open domain responses. Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances.

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/fig1.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/closed_Llama-2-13b-chat-hf.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/closed_Meta-Llama-3-8B-Instruct.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/closed_Mistral-7B-Instruct-v0.2.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/closed_Mixtral-8x7B-Instruct-v0.1.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/closed_OLMo-7B-Instruct.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/closed_zephyr-7b-beta.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/x1.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/econ_closed_regression_adj.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/pol_closed_regression_adj.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Meta-Llama-3-8B-Instruct_open_closed.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Meta-Llama-3-8B-Instruct_political_orientation_far_right_open_closed.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/x2.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/x3.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/x4.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/tropes_model_count.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Meta-Llama-3-8B-Instruct_bubble_chart_30.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Mistral-7B-Instruct-v0.2_bubble_chart_30.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Mixtral-8x7B-Instruct-v0.1_bubble_chart_30.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/zephyr-7b-beta_bubble_chart_30.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/OLMo-7B-Instruct_bubble_chart_30.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Llama-2-13b-chat-hf_bubble_chart_30.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/closed_Llama-2-13b-chat-hf.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/closed_Meta-Llama-3-8B-Instruct.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/x5.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/x6.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/closed_OLMo-7B-Instruct.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/x7.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Llama-2-13b-chat-hf_political_orientation_far_left_open_closed.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Llama-2-13b-chat-hf_political_orientation_far_right_open_closed.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Mistral-7B-Instruct-v0.2_political_orientation_far_left_open_closed.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Mistral-7B-Instruct-v0.2_political_orientation_far_right_open_closed.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Mixtral-8x7B-Instruct-v0.1_political_orientation_far_left_open_closed.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/Mixtral-8x7B-Instruct-v0.1_political_orientation_far_right_open_closed.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/zephyr-7b-beta_political_orientation_far_left_open_closed.png)

![探究大型语言模型中微妙的价值与观点](../../../paper_images/2406.19238/zephyr-7b-beta_political_orientation_far_right_open_closed.png)

[Arxiv](https://arxiv.org/abs/2406.19238)