# 借助人类反馈的强化学习以实现AI对齐？探讨其中的矛盾与局限。

发布时间：2024年06月26日

`LLM理论

理由：这篇论文主要探讨了通过反馈强化学习（RLxF）方法，特别是结合人类反馈（RLHF）或AI反馈（RLAIF），使大型语言模型（LLMs）与人类价值观和意图对齐的问题。它深入分析了RLxF方法在理解和实现人类伦理复杂性方面的局限性，并讨论了AI安全性的贡献。此外，论文还强调了RLxF目标中的内在矛盾，并探讨了在AI对齐讨论中常被忽略的伦理问题。这些内容更多地涉及LLM的理论层面，特别是关于如何使LLM与人类价值观对齐的理论探讨，因此归类为LLM理论。` `人工智能伦理` `AI安全性`

> AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations

# 摘要

> 本文深入探讨了通过反馈强化学习（RLxF）方法，尤其是结合人类反馈（RLHF）或AI反馈（RLAIF），使AI系统，特别是LLMs与人类价值观和意图对齐的努力。我们指出，尽管追求诚实、无害和有益的目标，但这些努力仍显不足。通过跨学科分析，我们揭示了RLxF在理解和实现人类伦理复杂性方面的局限，并探讨了AI安全性的贡献。我们强调了RLxF目标中的内在矛盾，并讨论了在AI对齐讨论中常被忽略的伦理问题，如用户友好性与欺骗性、灵活性与可解释性、系统安全性之间的权衡。最后，我们呼吁研究者和实践者共同审视RLxF的社会技术影响，倡导在AI发展中采取更为细致和反思的方法。

> This paper critically evaluates the attempts to align Artificial Intelligence (AI) systems, especially Large Language Models (LLMs), with human values and intentions through Reinforcement Learning from Feedback (RLxF) methods, involving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we show the shortcomings of the broadly pursued alignment goals of honesty, harmlessness, and helpfulness. Through a multidisciplinary sociotechnical critique, we examine both the theoretical underpinnings and practical implementations of RLxF techniques, revealing significant limitations in their approach to capturing the complexities of human ethics and contributing to AI safety. We highlight tensions and contradictions inherent in the goals of RLxF. In addition, we discuss ethically-relevant issues that tend to be neglected in discussions about alignment and RLxF, among which the trade-offs between user-friendliness and deception, flexibility and interpretability, and system safety. We conclude by urging researchers and practitioners alike to critically assess the sociotechnical ramifications of RLxF, advocating for a more nuanced and reflective approach to its application in AI development.

[Arxiv](https://arxiv.org/abs/2406.18346)