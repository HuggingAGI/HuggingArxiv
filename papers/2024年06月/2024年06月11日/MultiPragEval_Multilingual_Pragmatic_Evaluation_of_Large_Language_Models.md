# MultiPragEval：大型语言模型的多语言实用评估

发布时间：2024年06月11日

`LLM应用

这篇论文介绍了MultiPragEval，一个专为评估大型语言模型（LLMs）在多语言环境下的语用理解能力的工具。它特别关注了英语、德语、韩语和中文，并基于Grice的合作原则及四项对话准则设计了1200个问题。该工具的目的是评估LLMs在基本知识之外的高级语言理解能力，特别是在上下文感知和隐含意义推断方面。论文中提到的Claude3-Opus、Solar-10.7B和Qwen1.5-14B等模型在测试中的表现被详细讨论，显示了它们在多语言环境下的优越性能。因此，这篇论文属于LLM应用类别，因为它专注于开发和使用工具来评估和改进LLMs在实际应用中的性能。` `语言理解评估` `人工智能`

> MultiPragEval: Multilingual Pragmatic Evaluation of Large Language Models

# 摘要

> 随着LLMs能力的提升，评估它们在基本知识之外的高级语言理解能力变得尤为关键。本研究推出的MultiPragEval，是一个专为英语、德语、韩语和中文LLMs设计的多语言语用评估工具，包含1200个问题，依据Grice的合作原则及四项对话准则分类，深入考察LLMs的上下文感知和隐含意义推断能力。研究显示，Claude3-Opus在各语言测试中均表现卓越，树立了行业新标杆。开源模型中，Solar-10.7B和Qwen1.5-14B亦表现出色。此研究不仅在LLMs的语用推断多语言评估领域开创新局，也为AI系统中复杂语言理解能力的深入理解提供了重要启示。

> As the capabilities of LLMs expand, it becomes increasingly important to evaluate them beyond basic knowledge assessment, focusing on higher-level language understanding. This study introduces MultiPragEval, a robust test suite designed for the multilingual pragmatic evaluation of LLMs across English, German, Korean, and Chinese. Comprising 1200 question units categorized according to Grice's Cooperative Principle and its four conversational maxims, MultiPragEval enables an in-depth assessment of LLMs' contextual awareness and their ability to infer implied meanings. Our findings demonstrate that Claude3-Opus significantly outperforms other models in all tested languages, establishing a state-of-the-art in the field. Among open-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors. This study not only leads the way in the multilingual evaluation of LLMs in pragmatic inference but also provides valuable insights into the nuanced capabilities necessary for advanced language comprehension in AI systems.

![MultiPragEval：大型语言模型的多语言实用评估](../../../paper_images/2406.07736/x1.png)

[Arxiv](https://arxiv.org/abs/2406.07736)