# 大型语言模型的安全与隐私挑战：深度探索

发布时间：2024年06月12日

`LLM应用

这篇论文的摘要主要关注大型语言模型（LLMs）在自然语言处理中的应用，特别是在预训练、微调、RAG系统、部署及基于LLM的代理这五大场景下的隐私与安全问题。它探讨了在这些场景中可能遇到的独特威胁模型和风险，并提出了应对策略。因此，这篇论文更偏向于LLM的应用层面，特别是在处理隐私和安全问题方面的应用。` `隐私安全`

> Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey

# 摘要

> 人工智能的迅猛发展推动了大型语言模型（LLMs）在自然语言处理领域的巨大进步。这些模型通过海量数据训练，展现出卓越的语言理解和生成能力，广泛应用于机器翻译、聊天机器人等多样化场景。然而，LLMs在运行过程中也暴露出诸多隐私与安全问题，引起了学术界和工业界的高度关注。这些风险与传统语言模型所面临的问题截然不同。鉴于现有综述未能针对多样场景明确划分独特的威胁模型，本文聚焦于预训练、微调、RAG系统、部署及基于LLM的代理这五大场景下的独特隐私与安全问题，并针对各类风险特征，探讨了潜在威胁及应对策略。深入研究LLMs的攻防现状，不仅能为相关研究指明方向，还能让更多领域充分享受LLMs带来的益处。

> With the rapid development of artificial intelligence, large language models (LLMs) have made remarkable progress in natural language processing. These models are trained on large amounts of data to demonstrate powerful language understanding and generation capabilities for various applications, from machine translation and chatbots to agents. However, LLMs have exposed a variety of privacy and security issues during their life cycle, which have become the focus of academic and industrial attention. Moreover, these risks LLMs face are pretty different from previous traditional language models. Since current surveys lack a clear taxonomy of unique threat models based on diverse scenarios, we highlight unique privacy and security issues based on five scenarios: pre-training, fine-tuning, RAG system, deploying, and LLM-based agent. Concerning the characteristics of each risk, this survey provides potential threats and countermeasures. The research on attack and defense situations LLMs face can provide feasible research directions, making more areas reap LLMs' benefits.

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x1.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x2.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x3.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x4.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x5.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x6.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x7.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x8.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x9.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x10.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x11.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x12.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x13.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x14.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x15.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x16.png)

![大型语言模型的安全与隐私挑战：深度探索](../../../paper_images/2406.07973/x17.png)

[Arxiv](https://arxiv.org/abs/2406.07973)