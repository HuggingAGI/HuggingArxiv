# UrbanLLM：大型语言模型助力自主城市活动规划与管理

发布时间：2024年06月18日

`LLM应用

解析：这篇论文介绍了一个名为UrbanLLM的大型语言模型，该模型专门设计用于解决城市环境中的复杂问题。UrbanLLM通过分解问题为子任务，并匹配相应的时空AI模型来提供解决方案。这种应用性质的模型开发和优化，以及其在城市规划和管理中的实际应用，符合LLM应用分类的定义。因此，这篇论文应归类于LLM应用。` `城市规划` `城市管理`

> UrbanLLM: Autonomous Urban Activity Planning and Management with Large Language Models

# 摘要

> 基于位置的服务极大地提升了我们的生活质量。尽管在时空背景下，针对基于位置的服务已开发出多种专业AI模型，但它们在自主应对复杂城市规划与管理问题时仍显力不从心。为此，我们推出了UrbanLLM，这一经过精细调校的大型语言模型专为解决城市环境中的多样问题而设计。UrbanLLM通过将城市相关问题分解为易于处理的子任务，为每个子任务匹配最合适的时空AI模型，并提供详尽的解答，从而成为城市问题的解决专家。实验数据显示，UrbanLLM在处理复杂城市规划与管理问题上的表现远超Llama和GPT系列等其他知名LLM。UrbanLLM不仅大幅提升了城市问题解决的效率，还减轻了人类专家的负担，降低了对其的依赖。

> Location-based services play an critical role in improving the quality of our daily lives. Despite the proliferation of numerous specialized AI models within spatio-temporal context of location-based services, these models struggle to autonomously tackle problems regarding complex urban planing and management. To bridge this gap, we introduce UrbanLLM, a fine-tuned large language model (LLM) designed to tackle diverse problems in urban scenarios. UrbanLLM functions as a problem-solver by decomposing urban-related queries into manageable sub-tasks, identifying suitable spatio-temporal AI models for each sub-task, and generating comprehensive responses to the given queries. Our experimental results indicate that UrbanLLM significantly outperforms other established LLMs, such as Llama and the GPT series, in handling problems concerning complex urban activity planning and management. UrbanLLM exhibits considerable potential in enhancing the effectiveness of solving problems in urban scenarios, reducing the workload and reliance for human experts.

![UrbanLLM：大型语言模型助力自主城市活动规划与管理](../../../paper_images/2406.12360/x1.png)

![UrbanLLM：大型语言模型助力自主城市活动规划与管理](../../../paper_images/2406.12360/x2.png)

![UrbanLLM：大型语言模型助力自主城市活动规划与管理](../../../paper_images/2406.12360/x3.png)

![UrbanLLM：大型语言模型助力自主城市活动规划与管理](../../../paper_images/2406.12360/x4.png)

![UrbanLLM：大型语言模型助力自主城市活动规划与管理](../../../paper_images/2406.12360/x5.png)

[Arxiv](https://arxiv.org/abs/2406.12360)