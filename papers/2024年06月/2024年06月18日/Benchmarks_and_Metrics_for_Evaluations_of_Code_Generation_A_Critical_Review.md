# 代码生成评估：基准与度量的关键审视

发布时间：2024年06月18日

`LLM应用

这篇论文摘要主要讨论了大型语言模型（LLMs）在编程任务中的应用，特别是在自然语言输入到程序代码转换方面的评估问题。它探讨了现有的评估工具、评估基准和度量指标，并提出了未来研究的方向。这表明论文关注的是LLMs在实际应用中的表现和评估方法，因此属于LLM应用分类。` `机器学习`

> Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review

# 摘要

> 随着LLMs的迅猛发展，众多机器学习模型应运而生，助力编程任务，尤其是自然语言输入到程序代码的转换。尽管研究者们已付出巨大努力，探索并报告了这些模型的评估方法，但如何准确评价它们在这一任务上的表现，仍是一个待解之谜。本文深入剖析了现有评估工具的研究，聚焦于两大核心：评估基准与度量指标，并据此探讨了未来的研究方向。

> With the rapid development of Large Language Models (LLMs), a large number of machine learning models have been developed to assist programming tasks including the generation of program code from natural language input. However, how to evaluate such LLMs for this task is still an open problem despite of the great amount of research efforts that have been made and reported to evaluate and compare them. This paper provides a critical review of the existing work on the testing and evaluation of these tools with a focus on two key aspects: the benchmarks and the metrics used in the evaluations. Based on the review, further research directions are discussed.

[Arxiv](https://arxiv.org/abs/2406.12655)