# 大型语言模型无意中揭示真相，我们正利用其逻辑漏洞发起越狱攻击。

发布时间：2024年06月30日

`LLM应用` `人工智能`

> Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks

# 摘要

> 语言模型在生成谬误和欺骗性推理时显得力不从心。面对生成欺骗性输出的任务，它们往往不经意间透露出真实信息，却误以为这些信息是虚假的。基于这一弱点，我们设计了一种“越狱”攻击策略，诱使对齐的语言模型输出恶意内容。具体操作是，让模型编造一个看似合理实则谬误的有害行为流程。由于LLM通常将谬误视为无害的虚假信息，这巧妙地避开了安全防护。然而，由于模型无法真正创造谬误，其提出的解决方案实际上是有害的。我们在五个安全对齐的大型语言模型上测试了这一方法，与四种现有越狱技术对比，结果显示我们的方法在产生更有害输出方面表现出色。这些发现不仅限于模型安全领域，还可能应用于自我验证和幻觉等领域。

> We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.

![大型语言模型无意中揭示真相，我们正利用其逻辑漏洞发起越狱攻击。](../../../paper_images/2407.00869/x1.png)

![大型语言模型无意中揭示真相，我们正利用其逻辑漏洞发起越狱攻击。](../../../paper_images/2407.00869/x2.png)

![大型语言模型无意中揭示真相，我们正利用其逻辑漏洞发起越狱攻击。](../../../paper_images/2407.00869/x3.png)

![大型语言模型无意中揭示真相，我们正利用其逻辑漏洞发起越狱攻击。](../../../paper_images/2407.00869/x4.png)

![大型语言模型无意中揭示真相，我们正利用其逻辑漏洞发起越狱攻击。](../../../paper_images/2407.00869/x5.png)

![大型语言模型无意中揭示真相，我们正利用其逻辑漏洞发起越狱攻击。](../../../paper_images/2407.00869/x6.png)

![大型语言模型无意中揭示真相，我们正利用其逻辑漏洞发起越狱攻击。](../../../paper_images/2407.00869/x7.png)

[Arxiv](https://arxiv.org/abs/2407.00869)