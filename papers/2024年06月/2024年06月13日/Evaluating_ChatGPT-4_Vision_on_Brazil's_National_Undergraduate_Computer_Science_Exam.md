# 考察 ChatGPT-4 Vision 在巴西本科计算机科学统考中的实际表现

发布时间：2024年06月13日

`LLM应用

理由：这篇论文主要探讨了大型语言模型（LLMs）在视觉能力方面的应用，特别是在教育评估领域中的实际表现。它详细分析了ChatGPT-4 Vision模型在巴西全国本科生考试中的应用情况，包括其处理视觉和文本内容的能力，以及在推理和自我反思方面的表现。此外，论文还讨论了模型在实际应用中遇到的问题和挑战，强调了人类监督的重要性。因此，这篇论文更偏向于LLM的实际应用研究，而不是理论探讨或Agent、RAG相关的研究。` `科技教育` `考试评估`

> Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer Science Exam

# 摘要

> 最近，大型语言模型（LLMs）融入了视觉能力，这一创新有望在科技教育领域大放异彩，因为图表、图表和表格等视觉元素能显著提升学习体验。本研究聚焦于ChatGPT-4 Vision，即OpenAI在研究期间推出的顶尖视觉模型，在巴西2021年全国本科生考试（ENADE）计算机科学部分的实际表现。我们向模型展示了考试中的开放式和多项选择题的原始图像，并允许其根据不同答案键进行自我修正，从而评估了模型在涉及文本和视觉内容的大规模学术评估中的推理和自我反思能力。ChatGPT-4 Vision的表现远超平均考生，跻身前10%的顶尖成绩。尽管它在处理视觉元素问题时游刃有余，但在问题解释、逻辑推理和视觉敏锐度方面仍面临挑战。独立专家小组介入审查模型与答案键之间的争议案例，发现了一些问题设计不佳，含有模糊或含糊的陈述，凸显了未来考试中改进问题设计的紧迫性。我们的研究表明，虽然ChatGPT-4 Vision在多模态学术评估中展现出潜力，但人类的监督仍是确保模型准确性和高风险教育考试公平性的关键。论文的研究材料已公开于https://github.com/nabormendonca/gpt-4v-enade-cs-2021。

> The recent integration of visual capabilities into Large Language Models (LLMs) has the potential to play a pivotal role in science and technology education, where visual elements such as diagrams, charts, and tables are commonly used to improve the learning experience. This study investigates the performance of ChatGPT-4 Vision, OpenAI's most advanced visual model at the time the study was conducted, on the Bachelor in Computer Science section of Brazil's 2021 National Undergraduate Exam (ENADE). By presenting the model with the exam's open and multiple-choice questions in their original image format and allowing for reassessment in response to differing answer keys, we were able to evaluate the model's reasoning and self-reflecting capabilities in a large-scale academic assessment involving textual and visual content. ChatGPT-4 Vision significantly outperformed the average exam participant, positioning itself within the top 10 best score percentile. While it excelled in questions that incorporated visual elements, it also encountered challenges with question interpretation, logical reasoning, and visual acuity. The involvement of an independent expert panel to review cases of disagreement between the model and the answer key revealed some poorly constructed questions containing vague or ambiguous statements, calling attention to the critical need for improved question design in future exams. Our findings suggest that while ChatGPT-4 Vision shows promise in multimodal academic evaluations, human oversight remains crucial for verifying the model's accuracy and ensuring the fairness of high-stakes educational exams. The paper's research materials are publicly available at https://github.com/nabormendonca/gpt-4v-enade-cs-2021.

[Arxiv](https://arxiv.org/abs/2406.09671)