# 大型语言模型（LLMs）具备零-shot上下文感知的同时翻译能力。

发布时间：2024年06月19日

`LLM应用

这篇论文摘要讨论了大型语言模型（LLMs）在机器翻译领域的应用，特别是在零-shot同时机器翻译（SiMT）任务中的表现。它强调了LLMs在无需大量训练或微调的情况下，能够提供高质量的多语言翻译，并且能够通过注入少量背景信息来提升性能，特别是在处理复杂技术主题时。这表明了LLMs在实际应用中的潜力，特别是在构建高效的多语言翻译系统方面。因此，这篇论文属于LLM应用分类。` `机器翻译`

> LLMs Are Zero-Shot Context-Aware Simultaneous Translators

# 摘要

> Transformer 的兴起为机器翻译注入了新活力，而大型语言模型（LLMs）因其广泛的通用性和在众多语言任务中的卓越表现，尤其是翻译领域，正逐渐成为焦点。我们发现，开源 LLMs 在零-shot 同时机器翻译（SiMT）任务中，性能可媲美甚至超越某些顶尖基准。此外，通过 LLM 简单注入少量背景信息，能显著提升性能，尤其在处理复杂技术主题时效果显著。这揭示了 LLMs 在打造下一代无需繁重训练或微调的大规模多语言、上下文敏感且术语精确的 SiMT 系统方面的巨大潜力。

> The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks, including translation. Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks, zero-shot. We also demonstrate that injection of minimal background information, which is easy with an LLM, brings further performance gains, especially on challenging technical subject-matter. This highlights LLMs' potential for building next generation of massively multilingual, context-aware and terminologically accurate SiMT systems that require no resource-intensive training or fine-tuning.

![大型语言模型（LLMs）具备零-shot上下文感知的同时翻译能力。](../../../paper_images/2406.13476/x1.png)

![大型语言模型（LLMs）具备零-shot上下文感知的同时翻译能力。](../../../paper_images/2406.13476/x2.png)

[Arxiv](https://arxiv.org/abs/2406.13476)