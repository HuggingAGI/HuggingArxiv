# 大型语言模型时代下的开放领域问答系统：数据集与评价指标的综合分类法，旨在提升评估的鲁棒性。

发布时间：2024年06月19日

`RAG

理由：这篇论文主要关注开放域问答（ODQA）系统的数据集分类和评估方法，这与RAG（Retrieval-Augmented Generation）模型在问答系统中的应用密切相关。RAG模型是一种结合了检索和生成的方法，用于提高问答系统的性能。论文中提到的数据集分析和评估方法的研究，为理解和发展这类系统提供了理论和实践基础，因此归类为RAG。` `问答系统`

> Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models

# 摘要

> 在自然语言处理领域，开放域问答（ODQA）系统利用庞大的知识库解答事实性问题。得益于大规模数据集、深度学习技术及大型语言模型的发展，ODQA取得了显著进步，这些高质量数据集使模型能在真实场景中训练，并在未知数据上评估。标准化度量帮助比较不同系统，客观追踪技术进步，我们的研究详细分析了52个数据集和20种评估方法，提出了一种结合问题类型与难度的ODQA数据集新分类法，并系统化地探讨了评估度量的内在权衡。我们旨在通过这一框架，为现代问答系统的稳健评估提供支持，并指出了当前挑战及未来研究方向。

> Open Domain Question Answering (ODQA) within natural language processing involves building systems that answer factual questions using large-scale knowledge corpora. Recent advances stem from the confluence of several factors, such as large-scale training datasets, deep learning techniques, and the rise of large language models. High-quality datasets are used to train models on realistic scenarios and enable the evaluation of the system on potentially unseen data. Standardized metrics facilitate comparisons between different ODQA systems, allowing researchers to objectively track advancements in the field. Our study presents a thorough examination of the current landscape of ODQA benchmarking by reviewing 52 datasets and 20 evaluation techniques across textual and multimodal modalities. We introduce a novel taxonomy for ODQA datasets that incorporates both the modality and difficulty of the question types. Additionally, we present a structured organization of ODQA evaluation metrics along with a critical analysis of their inherent trade-offs. Our study aims to empower researchers by providing a framework for the robust evaluation of modern question-answering systems. We conclude by identifying the current challenges and outlining promising avenues for future research and development.

[Arxiv](https://arxiv.org/abs/2406.13232)