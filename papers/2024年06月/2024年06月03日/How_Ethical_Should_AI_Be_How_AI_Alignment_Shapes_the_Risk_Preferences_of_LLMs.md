# AI的道德标准应如何界定？AI对齐如何影响LLMs的风险偏好？

发布时间：2024年06月03日

`LLM应用

理由：这篇论文主要探讨了大型语言模型（LLMs）在经济决策中的应用，特别是在风险偏好和伦理对齐方面的影响。它分析了LLMs如何通过与人类伦理标准的对齐来调整其风险偏好，并讨论了这种对齐在金融决策中的实际应用和潜在影响。因此，这篇论文属于LLM应用类别，因为它关注的是LLMs在特定领域（金融）的实际应用和效果，而不是LLMs的理论研究或Agent的设计与应用。` `人工智能伦理`

> How Ethical Should AI Be? How AI Alignment Shapes the Risk Preferences of LLMs

# 摘要

> 本研究深入探讨了大型语言模型（LLMs）的风险偏好，并分析了将这些模型与人类伦理标准对齐的过程如何塑造它们的经济决策。通过对30个LLMs的分析，我们发现它们的风险特征多样，从极度谨慎到大胆冒险。进一步研究了AI对齐的不同类型——这一过程旨在确保模型遵循人类价值观，强调无害、有益和诚实——如何调整这些基本风险偏好。我们发现，对齐使LLMs更倾向于风险规避，特别是那些全面考虑伦理三维度的模型，其投资行为最为保守。此外，我们重现了一项先前的研究，该研究利用LLMs从公司盈利电话会议记录中预测企业投资，结果显示，虽然一定程度的对齐能提升投资预测的准确性，但过度的对齐会导致预测过于保守。这表明，在金融决策中过度依赖对齐的LLMs可能导致投资不足。因此，我们强调，在金融领域应用LLMs时，需要一种精细的平衡策略，既要考虑伦理对齐的程度，也要满足特定经济领域的需求。

> This study explores the risk preferences of Large Language Models (LLMs) and how the process of aligning them with human ethical standards influences their economic decision-making. By analyzing 30 LLMs, we uncover a broad range of inherent risk profiles ranging from risk-averse to risk-seeking. We then explore how different types of AI alignment, a process that ensures models act according to human values and that focuses on harmlessness, helpfulness, and honesty, alter these base risk preferences. Alignment significantly shifts LLMs towards risk aversion, with models that incorporate all three ethical dimensions exhibiting the most conservative investment behavior. Replicating a prior study that used LLMs to predict corporate investments from company earnings call transcripts, we demonstrate that although some alignment can improve the accuracy of investment forecasts, excessive alignment results in overly cautious predictions. These findings suggest that deploying excessively aligned LLMs in financial decision-making could lead to severe underinvestment. We underline the need for a nuanced approach that carefully balances the degree of ethical alignment with the specific requirements of economic domains when leveraging LLMs within finance.

[Arxiv](https://arxiv.org/abs/2406.01168)