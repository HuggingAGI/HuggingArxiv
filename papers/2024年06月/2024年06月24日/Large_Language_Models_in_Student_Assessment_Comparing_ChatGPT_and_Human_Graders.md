# 学生评估中的巨型语言模型：ChatGPT与人工评分者的较量

发布时间：2024年06月24日

`LLM应用

这篇论文探讨了大型语言模型（LLMs）如GPT-4在评估硕士生论文方面的应用。它通过比较GPT-4模型与大学教师评分的准确性，分析了LLMs在特定评估任务中的表现和局限性。虽然论文涉及了模型的使用和评估方法，但其核心关注点是LLMs在实际应用中的效能和适应性，特别是其在高等教育评估领域的应用，因此属于LLM应用分类。` `高等教育` `评估系统`

> Large Language Models in Student Assessment: Comparing ChatGPT and Human Graders

# 摘要

> 本研究探讨了大型语言模型（LLMs）在评估硕士生论文方面的效用。通过分析60篇政治学论文，研究对比了GPT-4模型与大学教师评分的准确性。研究发现，GPT-4虽在平均分上与人类评分相符，但其评分偏向保守，且与人类评分者的一致性不高。即便调整评分指导（提示工程），AI的表现也未见显著提升，说明GPT-4主要关注语言质量等通用特征，而非适应复杂的评分标准。这些结果揭示了AI在高等教育评估中的潜力与局限，强调了提升其适应性和对特定评估需求敏感性的重要性。

> This study investigates the efficacy of large language models (LLMs) as tools for grading master-level student essays. Utilizing a sample of 60 essays in political science, the study compares the accuracy of grades suggested by the GPT-4 model with those awarded by university teachers. Results indicate that while GPT-4 aligns with human grading standards on mean scores, it exhibits a risk-averse grading pattern and its interrater reliability with human raters is low. Furthermore, modifications in the grading instructions (prompt engineering) do not significantly alter AI performance, suggesting that GPT-4 primarily assesses generic essay characteristics such as language quality rather than adapting to nuanced grading criteria. These findings contribute to the understanding of AI's potential and limitations in higher education, highlighting the need for further development to enhance its adaptability and sensitivity to specific educational assessment requirements.

[Arxiv](https://arxiv.org/abs/2406.16510)