# 利用自我生成的上下文学习与自我校正技术进行日志解析

发布时间：2024年06月05日

`LLM应用

理由：这篇论文主要讨论了如何利用大型语言模型（LLMs）来改进日志解析的过程，并提出了一个名为AdaParser的新框架。这个框架结合了LLMs、自生成上下文学习（SG-ICL）和自我校正机制，以提高日志解析的准确性和适应性。这个研究的重点是如何应用LLMs来解决实际问题，即日志解析，而不是探讨LLMs的理论基础或Agent的行为，也不是关于检索增强生成（RAG）的研究。因此，它属于“LLM应用”类别。` `日志管理` `运维自动化`

> Log Parsing with Self-Generated In-Context Learning and Self-Correction

# 摘要

> 日志解析是将日志消息结构化的关键步骤，但现有方法在处理动态日志数据时表现不佳。大型语言模型（LLMs）的出现为日志解析带来了新希望，因其强大的自然语言和代码理解能力。尽管如此，基于LLM的解析器直接使用模型生成的模板，可能导致解析不准确。此外，它们对历史日志数据的依赖在数据稀缺或变化时成为障碍。为此，我们开发了AdaParser，一种结合LLMs、自生成上下文学习（SG-ICL）和自我校正的灵活日志解析框架。AdaParser通过引入模板校正器，确保解析准确性，并动态调整候选模板集以适应日志数据的变化。实验证明，AdaParser在多种情况下均超越了现有技术，显著提升了LLMs的解析性能。

> Log parsing transforms log messages into structured formats, serving as a crucial step for log analysis. Despite a variety of log parsing methods that have been proposed, their performance on evolving log data remains unsatisfactory due to reliance on human-crafted rules or learning-based models with limited training data. The recent emergence of large language models (LLMs) has demonstrated strong abilities in understanding natural language and code, making it promising to apply LLMs for log parsing. Consequently, several studies have proposed LLM-based log parsers. However, LLMs may produce inaccurate templates, and existing LLM-based log parsers directly use the template generated by the LLM as the parsing result, hindering the accuracy of log parsing. Furthermore, these log parsers depend heavily on historical log data as demonstrations, which poses challenges in maintaining accuracy when dealing with scarce historical log data or evolving log data. To address these challenges, we propose AdaParser, an effective and adaptive log parsing framework using LLMs with self-generated in-context learning (SG-ICL) and self-correction. To facilitate accurate log parsing, AdaParser incorporates a novel component, a template corrector, which utilizes the LLM to correct potential parsing errors in the templates it generates. In addition, AdaParser maintains a dynamic candidate set composed of previously generated templates as demonstrations to adapt evolving log data. Extensive experiments on public large-scale datasets show that AdaParser outperforms state-of-the-art methods across all metrics, even in zero-shot scenarios. Moreover, when integrated with different LLMs, AdaParser consistently enhances the performance of the utilized LLMs by a large margin.

![利用自我生成的上下文学习与自我校正技术进行日志解析](../../../paper_images/2406.03376/x1.png)

![利用自我生成的上下文学习与自我校正技术进行日志解析](../../../paper_images/2406.03376/x2.png)

![利用自我生成的上下文学习与自我校正技术进行日志解析](../../../paper_images/2406.03376/x3.png)

![利用自我生成的上下文学习与自我校正技术进行日志解析](../../../paper_images/2406.03376/x4.png)

![利用自我生成的上下文学习与自我校正技术进行日志解析](../../../paper_images/2406.03376/x5.png)

![利用自我生成的上下文学习与自我校正技术进行日志解析](../../../paper_images/2406.03376/x6.png)

![利用自我生成的上下文学习与自我校正技术进行日志解析](../../../paper_images/2406.03376/robust.png)

![利用自我生成的上下文学习与自我校正技术进行日志解析](../../../paper_images/2406.03376/time.jpg)

[Arxiv](https://arxiv.org/abs/2406.03376)