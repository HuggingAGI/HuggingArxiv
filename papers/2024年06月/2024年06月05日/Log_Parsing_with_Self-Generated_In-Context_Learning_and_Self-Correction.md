# 利用自我生成的上下文学习与自我修正技术解析日志

发布时间：2024年06月05日

`LLM应用

这篇论文介绍了一种名为AdaParser的新型日志解析框架，该框架利用大型语言模型（LLMs）进行自生成上下文学习（SG-ICL）和自我修正，以提高日志解析的准确性和适应性。这种方法特别针对日志数据的变化和数据稀缺问题，通过动态调整模板候选集和引入模板校正器来优化LLM生成的模板。因此，这篇论文属于LLM应用类别，因为它展示了LLMs在特定应用场景（即日志解析）中的实际应用和改进。` `日志管理` `软件开发`

> Log Parsing with Self-Generated In-Context Learning and Self-Correction

# 摘要

> 日志解析是将日志消息结构化的关键步骤，尽管已有多种方法提出，但它们在处理不断演变的日志数据时表现不佳，主要是因为依赖人工规则或训练数据有限的学习模型。大型语言模型（LLMs）的兴起展示了其在理解和处理自然语言及代码方面的潜力，为日志解析带来了新的希望。然而，基于LLM的日志解析器直接使用LLM生成的可能不准确的模板，影响了解析的准确性。此外，这些解析器依赖大量历史日志数据，这在数据稀缺或日志数据不断变化时构成了挑战。为此，我们开发了AdaParser，一种利用LLMs进行自生成上下文学习（SG-ICL）和自我修正的灵活日志解析框架。AdaParser通过引入模板校正器，确保LLM生成的模板准确无误，并维护一个动态模板候选集以适应日志数据的变化。实验证明，AdaParser在各种情况下均超越了现有技术，显著提升了LLMs的解析性能。

> Log parsing transforms log messages into structured formats, serving as a crucial step for log analysis. Despite a variety of log parsing methods that have been proposed, their performance on evolving log data remains unsatisfactory due to reliance on human-crafted rules or learning-based models with limited training data. The recent emergence of large language models (LLMs) has demonstrated strong abilities in understanding natural language and code, making it promising to apply LLMs for log parsing. Consequently, several studies have proposed LLM-based log parsers. However, LLMs may produce inaccurate templates, and existing LLM-based log parsers directly use the template generated by the LLM as the parsing result, hindering the accuracy of log parsing. Furthermore, these log parsers depend heavily on historical log data as demonstrations, which poses challenges in maintaining accuracy when dealing with scarce historical log data or evolving log data. To address these challenges, we propose AdaParser, an effective and adaptive log parsing framework using LLMs with self-generated in-context learning (SG-ICL) and self-correction. To facilitate accurate log parsing, AdaParser incorporates a novel component, a template corrector, which utilizes the LLM to correct potential parsing errors in the templates it generates. In addition, AdaParser maintains a dynamic candidate set composed of previously generated templates as demonstrations to adapt evolving log data. Extensive experiments on public large-scale datasets show that AdaParser outperforms state-of-the-art methods across all metrics, even in zero-shot scenarios. Moreover, when integrated with different LLMs, AdaParser consistently enhances the performance of the utilized LLMs by a large margin.

![利用自我生成的上下文学习与自我修正技术解析日志](../../../paper_images/2406.03376/x1.png)

![利用自我生成的上下文学习与自我修正技术解析日志](../../../paper_images/2406.03376/x2.png)

![利用自我生成的上下文学习与自我修正技术解析日志](../../../paper_images/2406.03376/x3.png)

![利用自我生成的上下文学习与自我修正技术解析日志](../../../paper_images/2406.03376/x4.png)

![利用自我生成的上下文学习与自我修正技术解析日志](../../../paper_images/2406.03376/x5.png)

![利用自我生成的上下文学习与自我修正技术解析日志](../../../paper_images/2406.03376/x6.png)

![利用自我生成的上下文学习与自我修正技术解析日志](../../../paper_images/2406.03376/robust.png)

![利用自我生成的上下文学习与自我修正技术解析日志](../../../paper_images/2406.03376/time.jpg)

[Arxiv](https://arxiv.org/abs/2406.03376)