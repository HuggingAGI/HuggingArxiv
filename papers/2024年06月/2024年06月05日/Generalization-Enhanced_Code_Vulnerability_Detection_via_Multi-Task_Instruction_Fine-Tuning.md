# 利用多任务指令微调提升代码漏洞检测的泛化能力

发布时间：2024年06月05日

`LLM应用

这篇论文介绍了一个名为VulLLM的框架，它结合了多任务学习和大型语言模型（LLMs）来提高代码漏洞检测的深度和泛化能力。该框架通过引入额外的辅助任务，如利用漏洞补丁进行漏洞定位和使用GPT-4解释补丁中的漏洞特征，来增强模型对漏洞本质的理解。实验结果表明，VulLLM在多个数据集上优于其他模型，显示了其在实际应用中的有效性和鲁棒性。因此，这篇论文属于LLM应用分类，因为它专注于将LLM技术应用于具体的实际问题——代码漏洞检测。` `软件安全` `漏洞检测`

> Generalization-Enhanced Code Vulnerability Detection via Multi-Task Instruction Fine-Tuning

# 摘要

> 近年来，基于代码预训练模型（CodePTMs）的漏洞检测取得了令人瞩目的成果。然而，这些模型往往只学会了从代码到标签的表面映射，而非深入理解漏洞的根本原因，导致在实际应用中泛化能力不足。为此，我们推出了VulLLM框架，它巧妙地将多任务学习与大型语言模型（LLMs）结合，以挖掘深层次的漏洞特征。我们额外设计了两个辅助任务：一是利用漏洞补丁进行漏洞定位，二是借助GPT-4对补丁中的漏洞特征进行解释。VulLLM通过生成式LLMs深入解析复杂漏洞模式，不仅提升了漏洞分类的准确性，还促使模型专注于漏洞的本质，而非单一任务的表面特征。实验结果显示，VulLLM在六个大型数据集上的表现优于七个顶尖模型，展现出卓越的有效性、泛化性和鲁棒性。

> Code Pre-trained Models (CodePTMs) based vulnerability detection have achieved promising results over recent years. However, these models struggle to generalize as they typically learn superficial mapping from source code to labels instead of understanding the root causes of code vulnerabilities, resulting in poor performance in real-world scenarios beyond the training instances. To tackle this challenge, we introduce VulLLM, a novel framework that integrates multi-task learning with Large Language Models (LLMs) to effectively mine deep-seated vulnerability features. Specifically, we construct two auxiliary tasks beyond the vulnerability detection task. First, we utilize the vulnerability patches to construct a vulnerability localization task. Second, based on the vulnerability features extracted from patches, we leverage GPT-4 to construct a vulnerability interpretation task. VulLLM innovatively augments vulnerability classification by leveraging generative LLMs to understand complex vulnerability patterns, thus compelling the model to capture the root causes of vulnerabilities rather than overfitting to spurious features of a single task. The experiments conducted on six large datasets demonstrate that VulLLM surpasses seven state-of-the-art models in terms of effectiveness, generalization, and robustness.

[Arxiv](https://arxiv.org/abs/2406.03718)