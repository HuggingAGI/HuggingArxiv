# 多模态 LLM 能否胜任家庭机器人的“大脑”角色？

发布时间：2024年06月28日

`LLM应用` `机器人` `人工智能`

> MMRo: Are Multimodal LLMs Eligible as the Brain for In-Home Robotics?

# 摘要

> 在人类环境中，机器人要成为得力助手，必须克服感知、语言理解、推理和规划等一系列挑战。多模态大型语言模型（MLLMs）的最新进展显示了其在复杂数学问题解决和抽象推理方面的卓越能力，因此被用作机器人系统的大脑，进行高级规划。然而，这些模型是否可靠地担任机器人大脑的角色尚存疑问。为此，我们推出了首个多模态LLM用于机器人的（MMRo）基准，旨在评估MLLMs在机器人应用中的能力。我们确定了四种关键能力：感知、任务规划、视觉推理和安全措施，并为此设计了14项评估指标。实验结果显示，没有单一模型在所有领域表现卓越，表明当前MLLMs尚不足以成为机器人的认知核心。详细数据可访问https://mm-robobench.github.io/。

> It is fundamentally challenging for robots to serve as useful assistants in human environments because this requires addressing a spectrum of sub-problems across robotics, including perception, language understanding, reasoning, and planning. The recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated their exceptional abilities in solving complex mathematical problems, mastering commonsense and abstract reasoning. This has led to the recent utilization of MLLMs as the brain in robotic systems, enabling these models to conduct high-level planning prior to triggering low-level control actions for task execution. However, it remains uncertain whether existing MLLMs are reliable in serving the brain role of robots. In this study, we introduce the first benchmark for evaluating Multimodal LLM for Robotic (MMRo) benchmark, which tests the capability of MLLMs for robot applications. Specifically, we identify four essential capabilities perception, task planning, visual reasoning, and safety measurement that MLLMs must possess to qualify as the robot's central processing unit. We have developed several scenarios for each capability, resulting in a total of 14 metrics for evaluation. We present experimental results for various MLLMs, including both commercial and open-source models, to assess the performance of existing systems. Our findings indicate that no single model excels in all areas, suggesting that current MLLMs are not yet trustworthy enough to serve as the cognitive core for robots. Our data can be found in https://mm-robobench.github.io/.

[Arxiv](https://arxiv.org/abs/2406.19693)