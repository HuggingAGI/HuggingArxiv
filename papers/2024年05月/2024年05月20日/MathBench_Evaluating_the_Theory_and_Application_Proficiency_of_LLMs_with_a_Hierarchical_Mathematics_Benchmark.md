# MathBench：借助层次化数学基准，评估大型语言模型在理论与应用层面的数学能力。

发布时间：2024年05月20日

`LLM应用

理由：这篇论文介绍了一个新的基准测试平台MathBench，专门设计来评估大型语言模型（LLMs）在数学领域的性能。这个平台不仅涵盖了多个数学领域，还从基础算术到高等数学进行了细致的评估，旨在全面考察模型的数学理解和应用能力。这与LLM的应用相关，因为它提供了一个工具来评估和改进LLMs在特定领域（数学）的应用性能。因此，这篇论文属于LLM应用分类。`

> MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark

# 摘要

> 大型语言模型（LLMs）在数学上的最新进展引人注目，但传统基准如GSM8k仅提供片面评估，未能全面揭示LLMs的数学潜能。为此，我们推出了MathBench，这一全新基准旨在深入挖掘LLMs的数学实力。MathBench横跨多个数学领域，细致评估理论知识与实际应用能力。它分为五个阶段，从基础算术到高等数学，旨在多维度考察模型的数学理解与应用。每个阶段均包含理论与应用题，全面检验模型的数学素养及其在现实问题中的应用能力。MathBench致力于在双语环境中，更精细地评估LLMs的数学能力，现已发布于https://github.com/open-compass/MathBench。

> Recent advancements in large language models (LLMs) have showcased significant improvements in mathematics. However, traditional math benchmarks like GSM8k offer a unidimensional perspective, falling short in providing a holistic assessment of the LLMs' math capabilities. To address this gap, we introduce MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large language models. MathBench spans a wide range of mathematical disciplines, offering a detailed evaluation of both theoretical understanding and practical problem-solving skills. The benchmark progresses through five distinct stages, from basic arithmetic to college mathematics, and is structured to evaluate models at various depths of knowledge. Each stage includes theoretical questions and application problems, allowing us to measure a model's mathematical proficiency and its ability to apply concepts in practical scenarios. MathBench aims to enhance the evaluation of LLMs' mathematical abilities, providing a nuanced view of their knowledge understanding levels and problem solving skills in a bilingual context. The project is released at https://github.com/open-compass/MathBench .

[Arxiv](https://arxiv.org/abs/2405.12209)