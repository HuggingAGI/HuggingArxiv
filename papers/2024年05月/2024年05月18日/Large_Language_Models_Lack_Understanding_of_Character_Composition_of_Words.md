# 大型语言模型在理解单词的字符构成方面存在缺陷。

发布时间：2024年05月18日

`LLM理论

这篇论文关注的是大型语言模型（LLMs）在理解单词字符构成方面的表现，这是一个关于LLMs理论性能和局限性的研究。它探讨了LLMs在处理基本字符任务时的能力，并通过对标记级别的性能进行分析，揭示了模型的行为和潜在的研究方向。这与LLM的应用、Agent或RAG分类不符，因为它更多地关注模型的内部机制和理论理解，而不是它们的实际应用或与其他系统的交互。` `机器学习`

> Large Language Models Lack Understanding of Character Composition of Words

# 摘要

> 大型语言模型（LLMs）在众多天然语言任务中表现出色，但它们的成功多限于单词、句子和文档层面，对于构成文本基础的字符理解能力，我们仍心存疑虑。本文深入探讨了LLMs在理解单词字符构成方面的表现，发现它们在处理连人类都能轻松应对的简单任务时，往往力不从心。通过对比标记级别的性能，我们剖析了这些模型的行为，并展望了未来研究的新方向。

> Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to token level performances, and discuss the potential directions for future research.

[Arxiv](https://arxiv.org/abs/2405.11357)