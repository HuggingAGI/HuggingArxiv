# 探索大型语言模型（LLM）支持的应用程序中任务实用性的评估与验证。

发布时间：2024年05月03日

`Agent` `人工智能` `用户体验`

> Assessing and Verifying Task Utility in LLM-Powered Applications

# 摘要

> 大型语言模型（LLMs）的迅猛发展催生了一大批促进多代理协作、助力人类日常任务的应用。然而，对于这些LLM驱动的应用究竟在多大程度上提升了用户体验和任务执行效率，目前尚缺乏充分的评估。这表明，我们需要验证LLM应用的实用性，特别是要确保应用功能与最终用户的实际需求相匹配。为此，我们提出了AgentEval，这是一个创新框架，它通过自动生成一套专为特定应用目的定制的标准，来简化实用性的验证流程。该框架允许我们全面评估并量化应用的实用性。我们还对AgentEval在两个开源数据集——数学问题求解和ALFWorld家庭相关任务——中的有效性和鲁棒性进行了深入分析。为了便于复现研究结果，我们公开了数据、代码和所有日志，链接为 https://bit.ly/3w3yKcS。

> The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at https://bit.ly/3w3yKcS .

[Arxiv](https://arxiv.org/abs/2405.02178)