# M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准

发布时间：2024年05月26日

`LLM应用

理由：这篇论文主要关注的是多模态思维链（MCoT）的发展和评估，特别是在视觉大型语言模型（VLLMs）上的应用。论文提出了一个新的基准M$^3$CoT，用于评估和推动多模态思维链的研究，这直接涉及到LLM的应用层面，特别是在多模态推理和视觉信息处理方面。因此，这篇论文更适合归类于LLM应用，而不是Agent、RAG或LLM理论。` `人工智能` `机器学习`

> M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought

# 摘要

> 多模态思维链（MCoT）要求模型结合文本与视觉信息进行细致推理，正日益受到重视。但现有MCoT基准存在缺陷：缺乏视觉推理、仅限于单步视觉推理，且领域覆盖不足，限制了MCoT的进步。为此，我们推出了创新基准M$^3$CoT，旨在推动多领域、多步骤、多模态的思维链研究。我们对多种MCoT方法在视觉大型语言模型（VLLMs）上进行了深入评估，并指出，尽管VLLMs在旧基准上表现出色，但在M$^3$CoT中推理能力仍显不足，与人类表现差距显著。我们相信，M$^3$CoT的推出是迈向多模态思维链全面发展的重要一步，期待它成为该领域研究的宝贵基石。

> Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) Domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M$^3$CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M$^3$CoT and there remains a large gap between existing VLLMs and human performance in M$^3$CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M$^3$CoT can serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x1.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x2.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x3.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x4.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x5.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x6.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x7.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x8.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x9.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x10.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x11.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x12.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x13.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x14.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x15.png)

![M$^3$CoT：探索多领域、多步骤、多模态思维链的新基准](../../../paper_images/2405.16473/x16.png)

[Arxiv](https://arxiv.org/abs/2405.16473)