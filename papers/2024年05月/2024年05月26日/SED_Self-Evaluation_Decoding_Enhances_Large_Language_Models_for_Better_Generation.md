# SED：通过自我评估解码，大型语言模型的生成能力得到显著提升

发布时间：2024年05月26日

`LLM理论

理由：这篇论文探讨了大型语言模型（LLMs）的解码机制，并提出了一种新的解码方法——自评估解码（SED）。这种新方法旨在改进模型在处理不确定令牌时的决策过程，从而提高文本生成的质量。论文的内容涉及LLMs的理论改进，特别是解码策略的创新，因此属于LLM理论分类。` `人工智能`

> SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation

# 摘要

> 大型语言模型（LLMs）通常采用单向自回归解码来生成文本，以应对多样化的用户查询。然而，这种解码方式在处理不确定令牌（即混沌点）时，往往容易选择次优方案，影响后续文本质量。为此，本文提出了一种名为自评估解码（SED）的新方法，它模仿人类的决策过程，将推测与评估融入解码环节，使模型在关键的混沌点上做出更优的令牌选择。实验证明，SED在不同任务和模型中均显著提升了生成效果。

> Existing Large Language Models (LLMs) generate text through unidirectional autoregressive decoding methods to respond to various user queries. These methods tend to consider token selection in a simple sequential manner, making it easy to fall into suboptimal options when encountering uncertain tokens, referred to as chaotic points in our work. Many chaotic points exist in texts generated by LLMs, and they often significantly affect the quality of subsequently generated tokens, which can interfere with LLMs' generation. This paper proposes Self-Evaluation Decoding, SED, a decoding method for enhancing model generation. Analogous to the human decision-making process, SED integrates speculation and evaluation steps into the decoding process, allowing LLMs to make more careful decisions and thus optimize token selection at chaotic points. Experimental results across various tasks using different LLMs demonstrate SED's effectiveness.

![SED：通过自我评估解码，大型语言模型的生成能力得到显著提升](../../../paper_images/2405.16552/x1.png)

![SED：通过自我评估解码，大型语言模型的生成能力得到显著提升](../../../paper_images/2405.16552/x2.png)

[Arxiv](https://arxiv.org/abs/2405.16552)