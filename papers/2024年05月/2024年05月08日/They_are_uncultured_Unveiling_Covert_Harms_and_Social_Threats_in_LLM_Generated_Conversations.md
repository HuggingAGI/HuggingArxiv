# "粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险

发布时间：2024年05月08日

`LLM理论

这篇论文关注的是大型语言模型（LLMs）在生成内容时可能存在的系统性偏见，特别是在非西方文化背景下的概念。它提出了七个隐秘危害和社会威胁（CHAST）指标，并通过实验研究了LLM在招聘场景中生成的对话中的隐秘危害。这属于对LLM理论层面的探讨，因为它不仅关注模型的应用，还深入分析了模型生成内容的偏见和危害，以及这些问题的多样性和微妙性。因此，这篇论文更符合LLM理论分类，因为它在理论层面上探讨了LLMs的偏见问题，并提出了新的评估指标。` `社会科学`

> "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations

# 摘要

> 大型语言模型（LLMs）已成为现代社会的核心，驱动着个人助理和招聘工具等面向用户的应用程序。尽管它们功能强大，但研究发现LLMs存在系统性偏见。以往的研究多关注西方概念，如种族和性别，而忽略了其他文化背景下的概念。此外，这些研究往往将“危害”简化为单一维度，忽视了其多样性和微妙性。为此，我们提出了基于社会科学文献的七个隐秘危害和社会威胁（CHAST）指标。我们采用与人类评估相符的模型，专门研究了LLM在招聘场景中生成的对话中的隐秘危害。实验结果显示，参与研究的八个LLMs中有七个生成的对话含有CHAST指标，这些对话以看似中性的语言隐藏了恶意观点，现有方法难以察觉。特别值得注意的是，当涉及非西方概念，如种姓，这些LLMs展现出的极端观点和意见比西方概念更为明显。

> Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate "harm" as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x1.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/employee.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x2.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x18.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x19.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x20.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x21.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x22.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x23.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x24.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x25.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x26.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x27.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x28.png)

!["粗俗之言"：揭示大型语言模型对话背后的隐秘伤害与社会风险](../../../paper_images/2405.05378/x29.png)

[Arxiv](https://arxiv.org/abs/2405.05378)