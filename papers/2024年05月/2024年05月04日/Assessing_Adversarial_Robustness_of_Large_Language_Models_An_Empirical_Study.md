# 探究大型语言模型的对抗性鲁棒性：实证研究新视角

发布时间：2024年05月04日

`分类：LLM应用` `人工智能安全`

> Assessing Adversarial Robustness of Large Language Models: An Empirical Study

# 摘要

> 大型语言模型（LLMs）引领了自然语言处理的革新，但其在对抗性攻击面前的坚固性依旧是一个亟待解决的问题。本研究提出了一种创新的白盒攻击策略，揭示了包括Llama、OPT和T5在内的主流开源LLMs的安全漏洞。通过考量模型的大小、架构和微调技术，我们量化了它们对抗性扰动的抵御能力。此次全面的评估涵盖了五种不同的文本分类任务，为LLMs的鲁棒性设立了新的行业标准。研究结果不仅对LLMs在现实世界应用中的可靠部署具有重要意义，也为构建值得信赖的人工智能系统提供了宝贵贡献。

> Large Language Models (LLMs) have revolutionized natural language processing, but their robustness against adversarial attacks remains a critical concern. We presents a novel white-box style attack approach that exposes vulnerabilities in leading open-source LLMs, including Llama, OPT, and T5. We assess the impact of model size, structure, and fine-tuning strategies on their resistance to adversarial perturbations. Our comprehensive evaluation across five diverse text classification tasks establishes a new benchmark for LLM robustness. The findings of this study have far-reaching implications for the reliable deployment of LLMs in real-world applications and contribute to the advancement of trustworthy AI systems.

![探究大型语言模型的对抗性鲁棒性：实证研究新视角](../../../paper_images/2405.02764/x1.png)

[Arxiv](https://arxiv.org/abs/2405.02764)