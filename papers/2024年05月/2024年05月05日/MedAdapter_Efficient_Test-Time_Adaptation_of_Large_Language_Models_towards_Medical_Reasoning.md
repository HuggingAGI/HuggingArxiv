# MedAdapter：为大型语言模型在医学推理任务中实现高效的测试时适应性

发布时间：2024年05月05日

`LLM应用` `生物医学`

> MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning

# 摘要

> 尽管大型语言模型（LLMs）在生成和推理上有所进步，但要将其应用于生物医学领域，仍面临规模庞大和隐私保护的双重挑战。本研究提出了MedAdapter，一种创新的事后适配器，专门用于在测试阶段对LLMs进行生物医学应用的适配。与传统的全面微调不同，MedAdapter通过微调一个小型的BERT尺寸适配器，高效地调整原始模型，以优化LLMs生成的候选解决方案的排序。实验结果证明，MedAdapter在生物医学推理任务中，无论是对白盒还是黑盒LLMs，都能显著提升性能，分别达到了25.48%和11.31%的平均性能增益，且无需额外的计算资源或数据共享。此外，MedAdapter与训练阶段的适配相结合，能够进一步提升性能，展现出其灵活性和对现有方法的有效补充。在考量模型效能、计算资源和数据隐私的平衡问题时，MedAdapter提供了一种既高效又保护隐私、经济且透明的解决方案，以促进LLMs在生物医学领域的应用。

> Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards biomedical applications. Instead of fine-tuning the entire LLM, MedAdapter effectively adapts the original model by fine-tuning only a small BERT-sized adapter to rank candidate solutions generated by LLMs. Experiments demonstrate that MedAdapter effectively adapts both white-box and black-box LLMs in biomedical reasoning, achieving average performance improvements of 25.48% and 11.31%, respectively, without requiring extensive computational resources or sharing data with third parties. MedAdapter also yields superior performance when combined with train-time adaptation, highlighting a flexible and complementary solution to existing adaptation methods. Faced with the challenges of balancing model performance, computational resources, and data privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective, and transparent solution for adapting LLMs to the biomedical domain.

[Arxiv](https://arxiv.org/abs/2405.03000)