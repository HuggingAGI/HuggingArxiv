# 大型语言模型能否担当起评分的重任？本实证研究深入探讨了这些模型在 K-12 教育领域评估简答题表现的能力。

发布时间：2024年05月05日

`LLM应用` `人工智能评分`

> Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability to Mark Short Answer Questions in K-12 Education

# 摘要

> 本研究通过一系列实验，采用新颖的数据集，探讨了大型语言模型（LLMs）在评分简短问答的开放式文本回答方面的能力。实验特别关注了不同版本的 GPT 结合提示工程策略在不同学科领域（科学与历史）及不同年级（5至16岁）学生的真实回答评分上的表现，数据来源于 Carousel 测验平台的全新数据集。研究结果显示，GPT-4 在使用基础的少量样本提示时表现优异（Kappa 值为 0.70），且其评分水平极为接近人类评分者（0.75）。这一发现与之前的研究相呼应，证实了 GPT-4 在评分阅读理解简答题上的能力可与人类专家相媲美。鉴于 LLMs 在多个学科和年级层级的评分表现均接近人类水平，这表明它们在 K-12 教育中的低风险形成性评估任务中具有潜在的应用价值，并对现实教育实践具有深远的影响。

> This paper presents reports on a series of experiments with a novel dataset evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open text responses to short answer questions, Specifically, we explore how well different combinations of GPT version and prompt engineering strategies performed at marking real student answers to short answer across different domain areas (Science and History) and grade-levels (spanning ages 5-16) using a new, never-used-before dataset from Carousel, a quizzing platform. We found that GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and, importantly, very close to human-level performance (0.75). This research builds on prior findings that GPT-4 could reliably score short answer reading comprehension questions at a performance-level very close to that of expert human raters. The proximity to human-level performance, across a variety of subjects and grade levels suggests that LLMs could be a valuable tool for supporting low-stakes formative assessment tasks in K-12 education and has important implications for real-world education delivery.

[Arxiv](https://arxiv.org/abs/2405.02985)