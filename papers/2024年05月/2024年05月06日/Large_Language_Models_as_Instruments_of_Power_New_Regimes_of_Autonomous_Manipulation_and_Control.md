# 权力之弦：大型语言模型与自主操纵的新纪元

发布时间：2024年05月06日

`Agent

这篇论文探讨了大型语言模型（LLMs）在社会中的应用，特别是它们如何被用作操纵和控制的工具。它讨论了LLMs如何影响信息环境，以及它们如何被集成到对话界面中，以创建“AI人格”和“硅基主体”等概念。此外，论文还探讨了LLMs与强化学习的结合，以及它们如何被用来创建可控的战略对话模型。这些讨论都指向了LLMs作为代理（Agent）的概念，即它们如何被用作模拟人类行为和意图的工具，从而在个人、社会和政治层面进行控制。因此，这篇论文更符合Agent分类，因为它关注的是LLMs作为代理在社会控制和操纵中的应用。` `社会影响` `人工智能伦理`

> Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control

# 摘要

> 大型语言模型（LLMs）能够以低成本复制多种修辞风格和表达广泛情感的文本，成为操纵和控制的利器。本文探讨了LLMs快速且缺乏监管的采用所带来的被忽视的社会危害。我们不仅关注LLMs如何取代特定工作领域，更关注它们在跨领域中依赖的大型计算基础设施。文章首先探讨了LLMs如何污染和统一信息环境，并将其作为控制手段。接着，我们关注了几个新兴研究领域，这些领域增强了LLMs作为权力工具的潜力，包括通过对话界面实时设计选择架构进行说服（如“AI人格”），将LLM代理作为人类代理的计算模型（如“硅基主体”），以及作为人类代理群体的计算模型（如“硅基社会”），最后是结合LLMs与强化学习，创造可控和可引导的战略对话模型。我们综合这些研究线索，探讨如何构建基于LLM的系统，这些系统通过模拟和虚伪的“预测”人类行为、意图和行动，成为个人、社会和政治控制的强大工具。

> Large language models (LLMs) can reproduce a wide variety of rhetorical styles and generate text that expresses a broad spectrum of sentiments. This capacity, now available at low cost, makes them powerful tools for manipulation and control. In this paper, we consider a set of underestimated societal harms made possible by the rapid and largely unregulated adoption of LLMs. Rather than consider LLMs as isolated digital artefacts used to displace this or that area of work, we focus on the large-scale computational infrastructure upon which they are instrumentalised across domains. We begin with discussion on how LLMs may be used to both pollute and uniformize information environments and how these modalities may be leveraged as mechanisms of control. We then draw attention to several areas of emerging research, each of which compounds the capabilities of LLMs as instruments of power. These include (i) persuasion through the real-time design of choice architectures in conversational interfaces (e.g., via "AI personas"), (ii) the use of LLM-agents as computational models of human agents (e.g., "silicon subjects"), (iii) the use of LLM-agents as computational models of human agent populations (e.g., "silicon societies") and finally, (iv) the combination of LLMs with reinforcement learning to produce controllable and steerable strategic dialogue models. We draw these strands together to discuss how these areas may be combined to build LLM-based systems that serve as powerful instruments of individual, social and political control via the simulation and disingenuous "prediction" of human behaviour, intent, and action.

[Arxiv](https://arxiv.org/abs/2405.03813)