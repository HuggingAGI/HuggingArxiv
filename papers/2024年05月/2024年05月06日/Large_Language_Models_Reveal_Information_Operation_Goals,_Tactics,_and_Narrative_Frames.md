# 大型语言模型披露了信息战的作战目标、策略运用以及叙述构建的框架。

发布时间：2024年05月06日

`LLM应用` `信息战分析`

> Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames

# 摘要

> 对抗性信息战可能通过破坏公正选举、操纵政策舆论和推广欺诈活动来动摇社会根基。尽管这类事件频发且影响深远，我们对其影响力运作的理解却因手工分析信息和主观行为解读而受限。本文旨在探究大型语言模型（LLMs），尤其是GPT-3.5，是否能够克服这些局限，以更高效地标注和分析协调性的信息战活动。研究首先利用GPT-3.5对过去十年间确认的126起信息战案例进行深入分析，并通过多种指标衡量LLM与实际情况描述的契合度。随后，研究从X（前Twitter）的两大多语言数据库中提取出与2022年法国大选和2023年菲律宾-美国巴利卡兰军事演习相关的协调性信息战活动。对于每项活动，GPT-3.5都被用于分析特定议题相关的帖子，并在关键事件发生前后提取其目标、策略和叙事框架。虽然GPT-3.5的解读有时与主观看法相左，但其归纳和阐释的能力证明了LLMs在从文本中提炼深层次指标方面的巨大潜力，这为我们提供了一种比传统方法更为全面的信息战活动分析视角。

> Adversarial information operations can destabilize societies by undermining fair elections, manipulating public opinions on policies, and promoting scams. Despite their widespread occurrence and potential impacts, our understanding of influence campaigns is limited by manual analysis of messages and subjective interpretation of their observable behavior. In this paper, we explore whether these limitations can be mitigated with large language models (LLMs), using GPT-3.5 as a case-study for coordinated campaign annotation. We first use GPT-3.5 to scrutinize 126 identified information operations spanning over a decade. We utilize a number of metrics to quantify the close (if imperfect) agreement between LLM and ground truth descriptions. We next extract coordinated campaigns from two large multilingual datasets from X (formerly Twitter) that respectively discuss the 2022 French election and 2023 Balikaran Philippine-U.S. military exercise in 2023. For each coordinated campaign, we use GPT-3.5 to analyze posts related to a specific concern and extract goals, tactics, and narrative frames, both before and after critical events (such as the date of an election). While the GPT-3.5 sometimes disagrees with subjective interpretation, its ability to summarize and interpret demonstrates LLMs' potential to extract higher-order indicators from text to provide a more complete picture of the information campaigns compared to previous methods.

[Arxiv](https://arxiv.org/abs/2405.03688)