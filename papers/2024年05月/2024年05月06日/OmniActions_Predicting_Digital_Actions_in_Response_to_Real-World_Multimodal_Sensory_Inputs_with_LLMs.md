# 全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应

发布时间：2024年05月06日

`Agent

这篇论文探讨了如何利用大型语言模型（LLMs）来构建一个智能系统，该系统能够处理多模态输入并预测基于用户情境的数字行动。这种系统可以被视为一个智能代理（Agent），因为它能够理解和响应用户的情境，并提供相应的数字行动建议。论文通过日记研究和用户反馈来评估不同的LLM技术，并开发了一个交互式原型，这表明它更侧重于构建和评估一个实际的智能系统，而不是专注于LLM的理论研究或应用。因此，将其归类为Agent是合适的。` `增强现实` `人机交互`

> OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs

# 摘要

> 向“普遍增强现实”的演进，设想了持续轻松访问多模态信息的可能性。然而，在日常生活中，用户往往因身体、认知或社交活动而分心。这增加了用户对多模态信息采取行动的难度。为了降低这种难度，未来的交互界面应智能地提供基于用户情境的快速数字行动访问。为此，我们进行了一项日记研究，参与者需记录并分享他们打算操作的媒体（如图像或音频），以及他们的行动意图和其他情境信息。基于这些数据，我们构建了一个全面的数字后续行动设计空间，以响应各种多模态感官输入。我们进一步开发了OmniActions，这是一个利用大型语言模型（LLMs）的系统，能够处理多模态输入并预测基于设计空间的针对信息的后续行动。通过日记研究收集的数据，我们评估了三种LLM技术（意图分类、上下文学习和微调），并确定了最适合我们任务的技术。此外，我们创建了一个交互式原型，并收集了用户对行动预测及其错误的初步反馈。

> The progression to "Pervasive Augmented Reality" envisions easy access to multimodal information continuously. However, in many everyday scenarios, users are occupied physically, cognitively or socially. This may increase the friction to act upon the multimodal information that users encounter in the world. To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users' context. To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information. Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to different types of multimodal sensory inputs. We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multimodal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space. Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classification, in-context learning and finetuning) and identified the most effective technique for our task. Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors.

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x1.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x2.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x3.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x4.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x5.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x6.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x7.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x8.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x10.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x11.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x12.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x13.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x14.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x15.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x16.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x17.png)

![全方位行动：借助大型语言模型，精准预测现实世界多模态感官刺激下的数字行动反应](../../../paper_images/2405.03901/x18.png)

[Arxiv](https://arxiv.org/abs/2405.03901)