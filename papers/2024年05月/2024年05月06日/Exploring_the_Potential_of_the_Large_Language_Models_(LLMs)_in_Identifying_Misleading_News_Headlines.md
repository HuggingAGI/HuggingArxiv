# 本文旨在探讨大型语言模型（LLMs）在甄别具有误导性的新闻标题方面的潜在能力。

发布时间：2024年05月06日

`LLM应用` `新闻媒体` `人工智能伦理`

> Exploring the Potential of the Large Language Models (LLMs) in Identifying Misleading News Headlines

# 摘要

> 数字时代，误导性新闻标题泛滥，严重威胁信息的准确性，迫切需要强有力的检测手段。本项研究检验了大型语言模型（LLMs）在辨识误导性与非误导性新闻标题中的效能。研究基于一个60篇文章的数据库，涵盖了健康、科技、商业等多个领域，来源包括知名和有争议的媒体。我们采用了三种语言模型——ChatGPT-3.5、ChatGPT-4和Gemini进行文本分类。研究发现，不同模型在性能上存在显著差异，尤其是ChatGPT-4在标注者对误导性标题看法一致时，准确度更为突出。研究强调了在开发能够识别错误信息复杂性的LLMs时，进行以人为本的评估的重要性，这涉及到将技术专长与人类的细致判断相结合。我们的研究结果为人工智能伦理的讨论增添了新视角，指出我们不仅需要技术上精进的模型，更需要这些模型在伦理上与人类价值观保持一致，并对人类的微妙解读具有敏感度。

> In the digital age, the prevalence of misleading news headlines poses a significant challenge to information integrity, necessitating robust detection mechanisms. This study explores the efficacy of Large Language Models (LLMs) in identifying misleading versus non-misleading news headlines. Utilizing a dataset of 60 articles, sourced from both reputable and questionable outlets across health, science & tech, and business domains, we employ three LLMs- ChatGPT-3.5, ChatGPT-4, and Gemini-for classification. Our analysis reveals significant variance in model performance, with ChatGPT-4 demonstrating superior accuracy, especially in cases with unanimous annotator agreement on misleading headlines. The study emphasizes the importance of human-centered evaluation in developing LLMs that can navigate the complexities of misinformation detection, aligning technical proficiency with nuanced human judgment. Our findings contribute to the discourse on AI ethics, emphasizing the need for models that are not only technically advanced but also ethically aligned and sensitive to the subtleties of human interpretation.

[Arxiv](https://arxiv.org/abs/2405.03153)