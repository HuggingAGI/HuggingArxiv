# V-Zen：借助创新的多模态大型语言模型，实现图形用户界面的高效理解与精准实体定位。

发布时间：2024年05月24日

`LLM应用

理由：这篇论文介绍了一款名为V-Zen的多模态大型语言模型（MLLMs），它专注于GUI的理解和交互，并通过引入双分辨率图像编码器和GUIDE数据集来提升模型的性能。这些改进旨在提高自动化水平，特别是在处理复杂的GUI交互方面。论文的内容涉及模型的应用和实际部署，强调了其在实际场景中的应用潜力，因此属于LLM应用类别。` `自动化` `人工智能`

> V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM

# 摘要

> 在AI领域日新月异的今天，多模态大型语言模型（MLLMs）崭露头角，成为一股革新力量，它们能够巧妙地解读和融合来自文本、图像乃至图形用户界面（GUIs）的多元信息。尽管成就斐然，但GUI的复杂交互与理解仍是挑战重重，制约了现有模型在提升自动化水平上的潜力。为此，本文隆重推出V-Zen，一款精心打造的多模态大型语言模型，旨在颠覆GUI理解与基础领域的现状。V-Zen搭载双分辨率图像编码器，在基础定位与行动预测上树立新标杆，为自主计算机系统的实现奠定基石。与之相辅相成的是GUIDE数据集，一个涵盖真实GUI元素与任务序列的庞大资源库，成为专业微调的加速器。V-Zen与GUIDE的完美融合，预示着多模态AI研究的新篇章，引领我们迈向智能自主的计算新时代。我们诚邀研究界同仁共襄盛举，共同绘制GUI自动化的未来蓝图。秉承开放科学之理念，我们将公开代码、数据及模型，为复杂精准的多模态对话场景开辟道路。

> In the rapidly evolving landscape of AI research and application, Multimodal Large Language Models (MLLMs) have emerged as a transformative force, adept at interpreting and integrating information from diverse modalities such as text, images, and Graphical User Interfaces (GUIs). Despite these advancements, the nuanced interaction and understanding of GUIs pose a significant challenge, limiting the potential of existing models to enhance automation levels. To bridge this gap, this paper presents V-Zen, an innovative Multimodal Large Language Model (MLLM) meticulously crafted to revolutionise the domain of GUI understanding and grounding. Equipped with dual-resolution image encoders, V-Zen establishes new benchmarks in efficient grounding and next-action prediction, thereby laying the groundwork for self-operating computer systems. Complementing V-Zen is the GUIDE dataset, an extensive collection of real-world GUI elements and task-based sequences, serving as a catalyst for specialised fine-tuning. The successful integration of V-Zen and GUIDE marks the dawn of a new era in multimodal AI research, opening the door to intelligent, autonomous computing experiences. This paper extends an invitation to the research community to join this exciting journey, shaping the future of GUI automation. In the spirit of open science, our code, data, and model will be made publicly available, paving the way for multimodal dialogue scenarios with intricate and precise interactions.

![V-Zen：借助创新的多模态大型语言模型，实现图形用户界面的高效理解与精准实体定位。](../../../paper_images/2405.15341/problem_statement.png)

![V-Zen：借助创新的多模态大型语言模型，实现图形用户界面的高效理解与精准实体定位。](../../../paper_images/2405.15341/related_work.png)

![V-Zen：借助创新的多模态大型语言模型，实现图形用户界面的高效理解与精准实体定位。](../../../paper_images/2405.15341/architecture.png)

![V-Zen：借助创新的多模态大型语言模型，实现图形用户界面的高效理解与精准实体定位。](../../../paper_images/2405.15341/guide_sample.png)

![V-Zen：借助创新的多模态大型语言模型，实现图形用户界面的高效理解与精准实体定位。](../../../paper_images/2405.15341/x1.png)

[Arxiv](https://arxiv.org/abs/2405.15341)