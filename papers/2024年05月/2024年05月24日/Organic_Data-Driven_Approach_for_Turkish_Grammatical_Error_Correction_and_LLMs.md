# 采用有机数据驱动策略，本研究致力于土耳其语法错误修正，并探索大型语言模型在此领域的应用。

发布时间：2024年05月24日

`LLM应用

理由：这篇论文主要关注的是在语法错误修正领域中，如何通过创新的方法（清洁插入法）来构建土耳其语的平行数据集，并优化大型语言模型的训练数据。这种方法的应用直接促进了大型语言模型在特定任务（土耳其语法错误修正）上的性能提升。因此，这篇论文更偏向于LLM的应用层面，即如何利用和优化LLM来解决具体的语言处理问题。` `语言处理`

> Organic Data-Driven Approach for Turkish Grammatical Error Correction and LLMs

# 摘要

> 深度学习的进步：语法错误修正领域的新突破。尽管合成数据集的构建弥补了数据需求的缺口，但它们在自然性上仍显不足，有时还需依赖干净数据作为起点。此外，现有研究多聚焦于英语。本研究创新性地提出了一种基于有机数据的方法——清洁插入法，旨在从任意有机数据源构建土耳其语法错误修正的平行数据集，并优化大型语言模型的训练数据。我们在公开的土耳其语法错误修正测试集中取得了领先成果，并验证了该方法在降低语言模型训练损失上的有效性。

> Grammatical Error Correction has seen significant progress with the recent advancements in deep learning. As those methods require huge amounts of data, synthetic datasets are being built to fill this gap. Unfortunately, synthetic datasets are not organic enough in some cases and even require clean data to start with. Furthermore, most of the work that has been done is focused mostly on English. In this work, we introduce a new organic data-driven approach, clean insertions, to build parallel Turkish Grammatical Error Correction datasets from any organic data, and to clean the data used for training Large Language Models. We achieve state-of-the-art results on two Turkish Grammatical Error Correction test sets out of the three publicly available ones. We also show the effectiveness of our method on the training losses of training language models.

![采用有机数据驱动策略，本研究致力于土耳其语法错误修正，并探索大型语言模型在此领域的应用。](../../../paper_images/2405.15320/paper_image_tight.jpg)

![采用有机数据驱动策略，本研究致力于土耳其语法错误修正，并探索大型语言模型在此领域的应用。](../../../paper_images/2405.15320/example_outputs.png)

[Arxiv](https://arxiv.org/abs/2405.15320)