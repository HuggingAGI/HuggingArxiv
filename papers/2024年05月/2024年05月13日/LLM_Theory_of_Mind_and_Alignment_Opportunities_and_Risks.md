# 大型语言模型（LLM）的心智理论与对齐：探索机遇与警惕风险

发布时间：2024年05月13日

`LLM理论

这篇论文探讨了大型语言模型（LLMs）是否具备心智理论（ToM），这是一个关于LLMs理解人类心理和情感能力的理论性问题。它分析了LLM ToM在个人和群体层面的人机交互中的表现，并讨论了与人类价值观对齐的机遇和风险。这属于对LLMs理论层面的研究，特别是关于它们如何模拟人类社会智能和心理理解的理论探讨。因此，它被归类为LLM理论。` `人机交互` `人工智能伦理`

> LLM Theory of Mind and Alignment: Opportunities and Risks

# 摘要

> 大型语言模型（LLMs）以其卓越的自然语言交流和推理能力，正在重塑人机交互和人工智能的认知。人们越来越关注LLMs是否具备心智理论（ToM），即理解他人心理和情感的能力，这是人类社会智能的关键。随着LLMs在我们的生活中扮演越来越重要的角色，并参与具有实际影响的决策，了解它们如何与人类价值观相协调变得至关重要。ToM成为了一个有前景的研究方向。本文基于人类ToM的研究，探讨了LLM ToM在个人和群体层面的人机交互中的表现，以及由此带来的对齐机遇和风险。在个人层面，我们探讨了LLM ToM在目标设定、对话适应、同理心表达和拟人化方面的体现。在群体层面，我们分析了LLM ToM如何促进集体协调、合作或竞争，以及道德判断的形成。本文提出了广泛的潜在影响，并指出了未来研究中最迫切关注的领域。

> Large language models (LLMs) are transforming human-computer interaction and conceptions of artificial intelligence (AI) with their impressive capacities for conversing and reasoning in natural language. There is growing interest in whether LLMs have theory of mind (ToM); the ability to reason about the mental and emotional states of others that is core to human social intelligence. As LLMs are integrated into the fabric of our personal, professional and social lives and given greater agency to make decisions with real-world consequences, there is a critical need to understand how they can be aligned with human values. ToM seems to be a promising direction of inquiry in this regard. Following the literature on the role and impacts of human ToM, this paper identifies key areas in which LLM ToM will show up in human:LLM interactions at individual and group levels, and what opportunities and risks for alignment are raised in each. On the individual level, the paper considers how LLM ToM might manifest in goal specification, conversational adaptation, empathy and anthropomorphism. On the group level, it considers how LLM ToM might facilitate collective alignment, cooperation or competition, and moral judgement-making. The paper lays out a broad spectrum of potential implications and suggests the most pressing areas for future research.

[Arxiv](https://arxiv.org/abs/2405.08154)