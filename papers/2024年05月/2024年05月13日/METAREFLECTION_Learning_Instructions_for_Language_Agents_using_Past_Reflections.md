# METAREFLECTION：借助过往反思，精炼语言代理指令

发布时间：2024年05月13日

`LLM应用

这篇论文摘要讨论了如何通过METAREFLECTION技术来优化大型语言模型（LLMs）在特定任务上的表现，特别是在基础设施即代码（IAC）漏洞检测和问答（QA）领域。这种方法通过模型自我生成的反思性语言反馈来改进提示指令，从而提高任务执行效率。这与LLM的应用层面紧密相关，因为它关注的是如何更有效地利用LLM来解决实际问题，而不是探讨LLM的理论基础或Agent的设计与实现。因此，将其归类为LLM应用是合适的。` `基础设施即代码` `问答系统`

> METAREFLECTION: Learning Instructions for Language Agents using Past Reflections

# 摘要

> 尽管大型语言模型（LLMs）备受青睐，但为其定制特定任务的提示仍是一大挑战。用户往往需与基于LLM的系统多次互动，方能达成目标。新近研究揭示，模型自我生成的反思性语言反馈能作为对话中的强化剂，加速达成预期目标。基于此，我们推出了METAREFLECTION技术，它从训练期间的个体自我反思中提炼出特定领域的通用提示指令。在基础设施即代码（IAC）漏洞检测及使用REACT和COT的问答（QA）领域中，METAREFLECTION表现卓越，分别超越GPT-4 16.82%、31.33%和15.42%，凸显了其提升LLMs效率的巨大潜力。

> Despite the popularity of Large Language Models (LLMs), crafting specific prompts for LLMs to perform particular tasks remains challenging. Users often engage in multiple conversational turns with an LLM-based agent to accomplish their intended task. Recent studies have demonstrated that linguistic feedback, in the form of self-reflections generated by the model, can work as reinforcement during these conversations, thus enabling quicker convergence to the desired outcome. Motivated by these findings, we introduce METAREFLECTION, a novel technique that learns general prompt instructions for a specific domain from individual self-reflections gathered during a training phase. We evaluate our technique in two domains: Infrastructure as Code (IAC) vulnerability detection and question-answering (QA) using REACT and COT. Our results demonstrate a notable improvement, with METARELECTION outperforming GPT-4 by 16.82% (IAC), 31.33% (COT), and 15.42% (REACT), underscoring the potential of METAREFLECTION as a viable method for enhancing the efficiency of LLMs.

[Arxiv](https://arxiv.org/abs/2405.13009)