# 图形卡上的蛋白质合成：探索蛋白质折叠与人工智能研究的关键极限。

发布时间：2024年05月15日

`LLM理论

这篇论文探讨了变压器架构在蛋白质折叠领域的应用，特别是通过DeepMind的AlphaFold项目，并分析了这一应用对大型语言模型（LLM）作为语言理解模型的影响。论文通过对变压器架构的预处理步骤进行深入分析，揭示了其在知识构建方面的潜力，并提出了对传统语言学和人工智能认识论的批判性观点。因此，这篇论文更偏向于对LLM理论层面的探讨，而非Agent、RAG或LLM应用的范畴。` `生物信息学` `人工智能`

> Synthesizing Proteins on the Graphics Card. Protein Folding and the Limits of Critical AI Studies

# 摘要

> 本文深入探讨了变压器架构在蛋白质折叠领域的应用，以DeepMind的AlphaFold项目为例，并分析了其对大型语言模型作为语言理解模型的影响。主流观点常将蛋白质（氨基酸序列编码）与自然语言（离散符号序列编码）直接类比。然而，我们对此类比进行批判性分析，以探究变压器架构在知识构建方面的潜力。首先，我们追溯这一类比的形成及其历史演变，揭示20世纪中叶结构语言学对结构生物学的影响。接着，我们关注变压器架构中常被忽视的三个预处理步骤：子词标记化、词嵌入和位置编码，展示其基于连续高维向量空间的表示方法，这与传统语言学的离散符号体系有所不同。变压器在蛋白质折叠领域的成功应用，揭示了架构内在的非语言学令牌处理方法。我们认为，这种非语言学处理开辟了新的认识论领域，并产生了一种新型知识，与传统领域有所区别。我们的智能机器探索应从智能的形态而非位置出发。因此，批判性人工智能研究领域应借鉴科学史的方法论，探索人工智能在知识创造中的作用，无论是在特定科学领域内还是超越这些领域。

> This paper investigates the application of the transformer architecture in protein folding, as exemplified by DeepMind's AlphaFold project, and its implications for the understanding of large language models as models of language. The prevailing discourse often assumes a ready-made analogy between proteins -- encoded as sequences of amino acids -- and natural language -- encoded as sequences of discrete symbols. Instead of assuming as given the linguistic structure of proteins, we critically evaluate this analogy to assess the kind of knowledge-making afforded by the transformer architecture. We first trace the analogy's emergence and historical development, carving out the influence of structural linguistics on structural biology beginning in the mid-20th century. We then examine three often overlooked pre-processing steps essential to the transformer architecture, including subword tokenization, word embedding, and positional encoding, to demonstrate its regime of representation based on continuous, high-dimensional vector spaces, which departs from the discrete, semantically demarcated symbols of language. The successful deployment of transformers in protein folding, we argue, discloses what we consider a non-linguistic approach to token processing intrinsic to the architecture. We contend that through this non-linguistic processing, the transformer architecture carves out unique epistemological territory and produces a new class of knowledge, distinct from established domains. We contend that our search for intelligent machines has to begin with the shape, rather than the place, of intelligence. Consequently, the emerging field of critical AI studies should take methodological inspiration from the history of science in its quest to conceptualize the contributions of artificial intelligence to knowledge-making, within and beyond the domain-specific sciences.

[Arxiv](https://arxiv.org/abs/2405.09788)