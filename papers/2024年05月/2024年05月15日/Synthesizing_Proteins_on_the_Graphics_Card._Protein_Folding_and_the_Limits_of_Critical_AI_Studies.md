# 图形卡上的蛋白质合成：探索蛋白质折叠与人工智能研究的关键极限。本文将深入探讨利用图形处理单元（GPU）进行蛋白质合成的潜力，以及这一过程如何挑战我们对人工智能在生物科学领域应用的理解。

发布时间：2024年05月15日

`LLM理论

这篇论文探讨了变压器架构在蛋白质折叠领域的应用，特别是DeepMind的AlphaFold项目，并分析了其对大型语言模型（LLM）作为语言理解的模型的影响。论文批判性地分析了将蛋白质与自然语言直接类比的观点，并深入研究了变压器架构如何塑造知识生成的新途径。它强调了变压器架构中预处理步骤的重要性，并提出了变压器在蛋白质折叠领域的成功应用揭示了一种非语言令牌处理方法。这种分析和批判性的探讨属于对LLM理论层面的研究，因此将其分类为LLM理论。` `生物信息学` `人工智能`

> Synthesizing Proteins on the Graphics Card. Protein Folding and the Limits of Critical AI Studies

# 摘要

> 本文深入探讨了变压器架构在蛋白质折叠领域的应用，特别聚焦于DeepMind的AlphaFold项目，并分析了其对大型语言模型作为语言理解的模型的深远影响。主流观点常将蛋白质（编码为氨基酸序列）与自然语言（编码为离散符号序列）直接类比。然而，我们对此类比持批判态度，旨在深入探究变压器架构如何塑造知识生成的新途径。首先，我们追溯了这一类比的渊源及其在20世纪中叶结构语言学影响下结构生物学中的演变。接着，我们剖析了变压器架构中常被忽视的三个关键预处理步骤：子词标记化、词嵌入和位置编码，揭示了其基于连续高维向量空间的表示方法，这与传统语言的离散符号截然不同。我们主张，变压器在蛋白质折叠领域的成功应用，揭示了一种架构固有的非语言令牌处理方法。这种非语言处理不仅开辟了独特的认识论领域，还催生了一类全新的知识，与传统领域迥异。我们的探索智能机器之旅，应始于智能的形态而非其所在。因此，批判性人工智能研究领域应借鉴科学史的方法论，以更好地理解人工智能在知识创造中的贡献，无论是在特定科学领域之内还是之外。

> This paper investigates the application of the transformer architecture in protein folding, as exemplified by DeepMind's AlphaFold project, and its implications for the understanding of large language models as models of language. The prevailing discourse often assumes a ready-made analogy between proteins -- encoded as sequences of amino acids -- and natural language -- encoded as sequences of discrete symbols. Instead of assuming as given the linguistic structure of proteins, we critically evaluate this analogy to assess the kind of knowledge-making afforded by the transformer architecture. We first trace the analogy's emergence and historical development, carving out the influence of structural linguistics on structural biology beginning in the mid-20th century. We then examine three often overlooked pre-processing steps essential to the transformer architecture, including subword tokenization, word embedding, and positional encoding, to demonstrate its regime of representation based on continuous, high-dimensional vector spaces, which departs from the discrete, semantically demarcated symbols of language. The successful deployment of transformers in protein folding, we argue, discloses what we consider a non-linguistic approach to token processing intrinsic to the architecture. We contend that through this non-linguistic processing, the transformer architecture carves out unique epistemological territory and produces a new class of knowledge, distinct from established domains. We contend that our search for intelligent machines has to begin with the shape, rather than the place, of intelligence. Consequently, the emerging field of critical AI studies should take methodological inspiration from the history of science in its quest to conceptualize the contributions of artificial intelligence to knowledge-making, within and beyond the domain-specific sciences.

[Arxiv](https://arxiv.org/abs/2405.09788)