# UQA：乌尔都语问答语料库

发布时间：2024年05月02日

`LLM应用` `多语言处理`

> UQA: Corpus for Urdu Question Answering

# 摘要

> 本研究提出了 UQA 数据集，这是首个针对乌尔都语——一种拥有逾七千万母语使用者的低资源语言——的问答和文本理解的新型数据集。UQA 通过采用 EATS 技术（即包围、锚定、翻译、寻找）翻译斯坦福的 SQuAD2.0 英文问答数据集而成，该技术巧妙地保留了翻译文本中的答案信息。文章详细阐述了在谷歌翻译和 Seamless M4T 两种翻译模型中筛选和评估最佳选项的过程。此外，本研究还对 UQA 数据集上的多个前沿多语言问答模型进行了基准测试，包括 mBERT、XLM-RoBERTa 和 mT5，取得了令人鼓舞的成绩。特别是在 XLM-RoBERTa-XL 模型上，我们分别获得了 85.99 的 F1 分数和 74.56 的精确匹配（EM）分数。UQA 不仅为开发和测试乌尔都语多语言自然语言处理系统提供了宝贵资源，也有助于提升现有模型的跨语言迁移能力。文章还证实了 EATS 技术在创建其他语言和领域高质量数据集方面的潜力。UQA 数据集及相应代码已在 www.github.com/sameearif/UQA 公开发布。

> This paper introduces UQA, a novel dataset for question answering and text comprehension in Urdu, a low-resource language with over 70 million native speakers. UQA is generated by translating the Stanford Question Answering Dataset (SQuAD2.0), a large-scale English QA dataset, using a technique called EATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in the translated context paragraphs. The paper describes the process of selecting and evaluating the best translation model among two candidates: Google Translator and Seamless M4T. The paper also benchmarks several state-of-the-art multilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and reports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and 74.56 EM. UQA is a valuable resource for developing and testing multilingual NLP systems for Urdu and for enhancing the cross-lingual transferability of existing models. Further, the paper demonstrates the effectiveness of EATS for creating high-quality datasets for other languages and domains. The UQA dataset and the code are publicly available at www.github.com/sameearif/UQA.

[Arxiv](https://arxiv.org/abs/2405.01458)