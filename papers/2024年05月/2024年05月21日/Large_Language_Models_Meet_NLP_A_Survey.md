# 大型语言模型与NLP的交汇：一份综述

发布时间：2024年05月21日

`LLM理论

这篇论文的摘要主要探讨了大型语言模型（LLMs）在自然语言处理（NLP）领域的应用现状、挑战和未来展望。它提供了一个全面的概述，并引入了一个包含参数冻结与调整的统一分类法，以统一视角审视LLMs的进展。此外，论文还总结了新的研究前沿及其挑战，旨在激发更多创新。这些内容主要关注LLMs的理论发展和应用分析，而不是具体的Agent行为、RAG（检索增强生成）技术或特定的LLM应用案例。因此，将其归类为LLM理论是合适的。` `人工智能`

> Large Language Models Meet NLP: A Survey

# 摘要

> 尽管ChatGPT等大型语言模型在NLP领域表现出色，但其潜力仍待深入挖掘。本研究旨在探索以下问题：(1) LLMs在NLP任务中的应用现状如何？(2) 传统NLP任务是否已因LLMs而得到解决？(3) LLMs在NLP的未来展望如何？为此，我们首先提供了一个全面的LLMs在NLP中的概述，并引入了一个包含参数冻结与调整的统一分类法，以统一视角审视LLMs的进展。同时，我们总结了新的研究前沿及其挑战，旨在激发更多创新。我们期望这项研究不仅能揭示LLMs在NLP中的潜力与局限，还能为构建高效LLMs提供实用指导。

> While large language models (LLMs) like ChatGPT have shown impressive capabilities in Natural Language Processing (NLP) tasks, a systematic investigation of their potential in this field remains largely unexplored. This study aims to address this gap by exploring the following questions: (1) How are LLMs currently applied to NLP tasks in the literature? (2) Have traditional NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for NLP? To answer these questions, we take the first step to provide a comprehensive overview of LLMs in NLP. Specifically, we first introduce a unified taxonomy including (1) parameter-frozen application and (2) parameter-tuning application to offer a unified perspective for understanding the current progress of LLMs in NLP. Furthermore, we summarize the new frontiers and the associated challenges, aiming to inspire further groundbreaking advancements. We hope this work offers valuable insights into the {potential and limitations} of LLMs in NLP, while also serving as a practical guide for building effective LLMs in NLP.

[Arxiv](https://arxiv.org/abs/2405.12819)