# 检索/生成系统自动评分工作台

发布时间：2024年05月21日

`RAG

理由：这篇论文主要关注的是在自回归大型语言模型（LLM）时代，如何评估信息检索（IR）系统的新挑战。它提出了一种新的评估方法，通过LLM来判断响应的相关性，并分析响应中的关键事实，以及使用响应来解答考试题。这种方法与RAG（Retrieval-Augmented Generation）框架相关，因为它涉及到使用LLM来增强信息检索和评估过程。虽然它也涉及到LLM的应用，但更侧重于评估方法的创新，因此更适合归类为RAG。` `信息检索` `教育评估`

> A Workbench for Autograding Retrieve/Generate Systems

# 摘要

> 在自回归LLM时代，评估IR系统面临新挑战，传统段落级评估方法已不再适用。为此，我们开发了一个工作台，探索新的评估途径：通过LLM判断响应相关性，分析响应中的关键事实，以及用响应解答考试题。此工作台旨在推动可重复使用的测试集开发，研究者可调整关键事实和考题，观察其对系统评估和排行榜的影响。资源地址：https://github.com/TREMA-UNH/autograding-workbench。

> This resource paper addresses the challenge of evaluating Information Retrieval (IR) systems in the era of autoregressive Large Language Models (LLMs). Traditional methods relying on passage-level judgments are no longer effective due to the diversity of responses generated by LLM-based systems. We provide a workbench to explore several alternative evaluation approaches to judge the relevance of a system's response that incorporate LLMs: 1. Asking an LLM whether the response is relevant; 2. Asking the LLM which set of nuggets (i.e., relevant key facts) is covered in the response; 3. Asking the LLM to answer a set of exam questions with the response.
  This workbench aims to facilitate the development of new, reusable test collections. Researchers can manually refine sets of nuggets and exam questions, observing their impact on system evaluation and leaderboard rankings.
  Resource available at https://github.com/TREMA-UNH/autograding-workbench

[Arxiv](https://arxiv.org/abs/2405.13177)