# 人类与大型语言模型均积极运用潜在成分表示，本研究探讨了这一现象及其在语言处理中的应用。

发布时间：2024年05月28日

`LLM理论

这篇论文探讨了人类大脑和大型语言模型（如ChatGPT）在处理句子时的内部表示和潜在机制。通过比较人类和LLMs在一次性学习任务中的表现，研究发现了两者在处理语言时可能存在的共同层次化结构。这种研究深入探讨了LLMs的理论基础，特别是它们如何模拟人类语言处理的机制，因此属于LLM理论分类。` `认知科学`

> Active Use of Latent Constituency Representation in both Humans and Large Language Models

# 摘要

> 探索句子在人脑及大型语言模型（如ChatGPT）中的内部表示，是认知科学的一大挑战。传统语言学理论认为，大脑通过层次化解析句子成分来理解句子。然而，LLMs并未明确解析这些成分，其内部机制尚不明晰。通过一项创新的一次性学习任务，我们发现人类与LLMs在处理句子时，均倾向于删除完整的语言成分而非零散词汇，这表明两者可能构建了相似的潜在层次化语言结构。与此不同，一个仅基于词汇属性和位置的简单模型则无此特性。通过分析这种删除行为，我们能够为人类和LLMs重构句子的潜在成分树结构，揭示了人脑与LLMs中可能存在的共同语言处理机制。

> Understanding how sentences are internally represented in the human brain, as well as in large language models (LLMs) such as ChatGPT, is a major challenge for cognitive science. Classic linguistic theories propose that the brain represents a sentence by parsing it into hierarchically organized constituents. In contrast, LLMs do not explicitly parse linguistic constituents and their latent representations remains poorly explained. Here, we demonstrate that humans and LLMs construct similar latent representations of hierarchical linguistic constituents by analyzing their behaviors during a novel one-shot learning task, in which they infer which words should be deleted from a sentence. Both humans and LLMs tend to delete a constituent, instead of a nonconstituent word string. In contrast, a naive sequence processing model that has access to word properties and ordinal positions does not show this property. Based on the word deletion behaviors, we can reconstruct the latent constituency tree representation of a sentence for both humans and LLMs. These results demonstrate that a latent tree-structured constituency representation can emerge in both the human brain and LLMs.

[Arxiv](https://arxiv.org/abs/2405.18241)