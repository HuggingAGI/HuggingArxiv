# 挖掘大型语言模型的量化潜力

发布时间：2024年05月28日

`LLM理论

理由：这篇论文主要探讨了大型语言模型（LLMs）量化技术在安全角度的潜在风险，并提出了一种三阶段攻击框架来揭示这些风险。这涉及到对LLM量化过程的深入分析和理论探讨，而不是直接应用于实际的Agent或RAG系统，也不是关于LLM的具体应用案例。因此，它更符合LLM理论的分类，专注于模型内部机制和安全性的理论研究。` `人工智能`

> Exploiting LLM Quantization

# 摘要

> 量化技术通过使用低精度权重，有效减少了大型语言模型（LLMs）的内存需求，是实现其在普通硬件上部署的关键。尽管LLM量化的实用性影响已有广泛研究，但本研究首次从安全角度审视其潜在风险。我们发现，常见的量化方法可能被利用，生成看似无害但实则有害的量化LLM，误导用户部署。我们通过一个三阶段攻击框架揭示了这一风险：首先，通过对抗性任务微调得到恶意LLM；其次，量化该模型并计算约束条件，确保所有全精度模型映射到同一量化模型；最后，利用投影梯度下降法，调整全精度模型以消除中毒行为，同时满足约束条件。这一过程产生了一个全精度下表现正常但在量化后执行对抗性行为的LLM。我们在三个不同场景中验证了这种攻击的可行性和严重性：易受攻击的代码生成、内容注入和过度拒绝攻击。实际上，攻击者可能将这种全精度模型托管在如Hugging Face等LLM社区中心，使大量用户面临在其设备上部署恶意量化版本的风险。

> Quantization leverages lower-precision weights to reduce the memory usage of large language models (LLMs) and is a key technique for enabling their deployment on commodity hardware. While LLM quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective. We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model. We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i). We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack. In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.

[Arxiv](https://arxiv.org/abs/2405.18137)