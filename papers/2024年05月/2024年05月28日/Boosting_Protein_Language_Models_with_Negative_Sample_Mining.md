# 利用负样本挖掘优化蛋白质语言模型性能

发布时间：2024年05月28日

`LLM应用

这篇论文摘要描述了一种创新方法，旨在提升大型语言模型在蛋白质表示学习中的表现。这种方法通过训练网络从不同类别蛋白质对构成的负样本中提炼关键信息，减少了对共进化知识的依赖，从而增强了基于变压器模型的性能，并揭示了蛋白质的微妙生物行为。实验结果表明，这种方法在多个任务上超越了现有的大型蛋白质模型，为蛋白质研究和计算生物学的发展开辟了新视野。因此，这篇论文属于LLM应用分类，因为它专注于将大型语言模型应用于蛋白质表示学习这一特定领域，并展示了其在这一领域的应用效果和潜在影响。` `生物信息学` `计算生物学`

> Boosting Protein Language Models with Negative Sample Mining

# 摘要

> 我们提出了一种创新方法，旨在提升大型语言模型在蛋白质表示学习中的表现。核心在于通过训练网络从不同类别蛋白质对构成的负样本中提炼关键信息，减少对共进化知识的依赖。此方法不仅增强了基于变压器模型的性能，还揭示了蛋白质的微妙生物行为，与传统生物机制如蛋白质相互作用相吻合。实验证明，在多个任务上，我们的方法超越了现有的大型蛋白质模型。这一创新方法为蛋白质研究和计算生物学的发展开辟了新视野。

> We introduce a pioneering methodology for boosting large language models in the domain of protein representation learning. Our primary contribution lies in the refinement process for correlating the over-reliance on co-evolution knowledge, in a way that networks are trained to distill invaluable insights from negative samples, constituted by protein pairs sourced from disparate categories. By capitalizing on this novel approach, our technique steers the training of transformer-based models within the attention score space. This advanced strategy not only amplifies performance but also reflects the nuanced biological behaviors exhibited by proteins, offering aligned evidence with traditional biological mechanisms such as protein-protein interaction. We experimentally observed improved performance on various tasks over datasets, on top of several well-established large protein models. This innovative paradigm opens up promising horizons for further progress in the realms of protein research and computational biology.

![利用负样本挖掘优化蛋白质语言模型性能](../../../paper_images/2405.17902/x1.png)

![利用负样本挖掘优化蛋白质语言模型性能](../../../paper_images/2405.17902/x2.png)

![利用负样本挖掘优化蛋白质语言模型性能](../../../paper_images/2405.17902/x3.png)

![利用负样本挖掘优化蛋白质语言模型性能](../../../paper_images/2405.17902/x4.png)

![利用负样本挖掘优化蛋白质语言模型性能](../../../paper_images/2405.17902/x5.png)

[Arxiv](https://arxiv.org/abs/2405.17902)