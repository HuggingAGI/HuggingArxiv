# 大型语言模型在高级心智理论任务中展现出与成人相媲美的表现。

发布时间：2024年05月29日

`LLM理论

这篇论文探讨了大型语言模型（LLMs）在发展高阶心智理论（ToM）方面的进展，这是一个理论性的研究，涉及模型的认知能力和推理机制。通过比较不同LLMs在特定测试集上的表现，论文分析了模型大小和微调策略对ToM能力的影响，并讨论了这些发现对LLM实际应用的潜在影响。因此，这篇论文属于LLM理论分类。` `人工智能` `心理学`

> LLMs achieve adult human performance on higher-order theory of mind tasks

# 摘要

> 本文深入探讨了大型语言模型（LLMs）在发展高阶心智理论（ToM）方面的进展，特别是人类如何递归地推理多重心理和情感状态（如，我认为你相信她知道）。通过引入一个手写测试集——多阶心智理论问答（Multi-Order Theory of Mind Q&A），本文比较了五个LLMs与新成人基准的表现。研究发现，GPT-4和Flan-PaLM在ToM任务上的表现已达到或接近成人水平，GPT-4甚至在第六阶推理中超出了成人表现。这表明，模型大小与微调策略共同促进了ToM能力的实现，且顶尖LLMs已具备通用ToM能力。考虑到高阶ToM在人类合作与竞争行为中的核心作用，这些发现对LLM的实际应用具有深远影响。

> This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in a recursive manner (e.g. I think that you believe that she knows). This paper builds on prior work by introducing a handwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to compare the performance of five LLMs to a newly gathered adult human benchmark. We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences. Our results suggest that there is an interplay between model size and finetuning for the realisation of ToM abilities, and that the best-performing LLMs have developed a generalised capacity for ToM. Given the role that higher-order ToM plays in a wide range of cooperative and competitive human behaviours, these findings have significant implications for user-facing LLM applications.

![大型语言模型在高级心智理论任务中展现出与成人相媲美的表现。](../../../paper_images/2405.18870/LLM_ment_without_labels.png)

[Arxiv](https://arxiv.org/abs/2405.18870)