# 查询与键，是否总是密不可分？本文通过Transformer波函数的案例研究，探讨这一问题。

发布时间：2024年05月29日

`LLM理论

理由：这篇论文探讨了Transformer模型中的点积注意力机制，并将其应用于量子多体自旋哈密顿基态近似问题中，特别关注了二维$J_1$-$J_2$海森堡模型。研究不仅限于NLP领域，还扩展到了物理学领域，但其核心是对Transformer模型理论的深入分析，特别是注意力机制的优化和简化。因此，这篇论文更偏向于对大型语言模型（LLM）理论的研究，而不是具体的应用、Agent设计或RAG（Retrieval-Augmented Generation）技术。` `量子物理`

> Are queries and keys always relevant? A case study on Transformer wave functions

# 摘要

> 点积注意力机制，作为现代Transformer的核心，原为NLP任务量身打造，能精准捕捉词对间的语义纽带。本研究聚焦于Transformer的注意力机制，探索其在量子多体自旋哈密顿基态近似中的应用潜力，特别针对二维$J_1$-$J_2$海森堡模型进行了数值模拟。我们比较了标准注意力机制与一种简化版（仅基于位置，无需查询和键）的性能，发现后者在降低计算负担的同时，依然保持了竞争力。分析标准注意力机制的注意力图显示，优化后，注意力权重实际上与输入无关。通过分析计算，我们揭示了在大系统研究中，理论上应剔除查询和键的原因。这一发现同样适用于NLP领域，尤其是在处理长句时。

> The dot product attention mechanism, originally designed for natural language processing (NLP) tasks, is a cornerstone of modern Transformers. It adeptly captures semantic relationships between word pairs in sentences by computing a similarity overlap between queries and keys. In this work, we explore the suitability of Transformers, focusing on their attention mechanisms, in the specific domain of the parametrization of variational wave functions to approximate ground states of quantum many-body spin Hamiltonians. Specifically, we perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg model, a common benchmark in the field of quantum-many body systems on lattice. By comparing the performance of standard attention mechanisms with a simplified version that excludes queries and keys, relying solely on positions, we achieve competitive results while reducing computational cost and parameter usage. Furthermore, through the analysis of the attention maps generated by standard attention mechanisms, we show that the attention weights become effectively input-independent at the end of the optimization. We support the numerical results with analytical calculations, providing physical insights of why queries and keys should be, in principle, omitted from the attention mechanism when studying large systems. Interestingly, the same arguments can be extended to the NLP domain, in the limit of long input sentences.

![查询与键，是否总是密不可分？本文通过Transformer波函数的案例研究，探讨这一问题。](../../../paper_images/2405.18874/x1.png)

![查询与键，是否总是密不可分？本文通过Transformer波函数的案例研究，探讨这一问题。](../../../paper_images/2405.18874/x2.png)

![查询与键，是否总是密不可分？本文通过Transformer波函数的案例研究，探讨这一问题。](../../../paper_images/2405.18874/x3.png)

![查询与键，是否总是密不可分？本文通过Transformer波函数的案例研究，探讨这一问题。](../../../paper_images/2405.18874/x4.png)

[Arxiv](https://arxiv.org/abs/2405.18874)