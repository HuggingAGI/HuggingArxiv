# DocReLM：驾驭语言模型，精通文档检索之术

发布时间：2024年05月19日

`RAG

理由：这篇论文主要探讨了如何利用大型语言模型（LLM）来改进文档检索系统的性能，特别是在理解学术论文的语义和领域知识方面。通过使用LLM生成的领域数据训练检索与重排机制，并结合论文参考文献的选择，研究提高了检索精度。这种方法属于检索增强生成（RAG）的范畴，因为它涉及使用LLM来增强信息检索过程，而不是直接应用于Agent或LLM理论的研究。此外，虽然它涉及LLM的应用，但更侧重于检索增强的技术细节和性能提升，因此更适合归类为RAG。` `学术研究` `信息检索`

> DocReLM: Mastering Document Retrieval with Language Model

# 摘要

> 面对超过2亿份学术文献和每年新增的数百万文献，学术研究者搜索信息的任务艰巨。现有检索系统在理解学术论文的语义和领域知识方面力不从心。本研究通过大型语言模型，使文档检索系统在语义理解上大放异彩，远超现有技术。我们采用大型语言模型生成的领域数据训练检索与重排机制，并从论文参考文献中精挑细选，进一步提升检索精度。在量子物理学和计算机视觉领域专家标注的测试集上，我们的系统DocReLM在计算机视觉领域的Top 10准确率高达44.12%，远超Google Scholar的15.69%；在量子物理学领域，这一数字提升至36.21%，而Google Scholar仅为12.96%。

> With over 200 million published academic documents and millions of new documents being written each year, academic researchers face the challenge of searching for information within this vast corpus. However, existing retrieval systems struggle to understand the semantics and domain knowledge present in academic papers. In this work, we demonstrate that by utilizing large language models, a document retrieval system can achieve advanced semantic understanding capabilities, significantly outperforming existing systems. Our approach involves training the retriever and reranker using domain-specific data generated by large language models. Additionally, we utilize large language models to identify candidates from the references of retrieved papers to further enhance the performance. We use a test set annotated by academic researchers in the fields of quantum physics and computer vision to evaluate our system's performance. The results show that DocReLM achieves a Top 10 accuracy of 44.12% in computer vision, compared to Google Scholar's 15.69%, and an increase to 36.21% in quantum physics, while that of Google Scholar is 12.96%.

![DocReLM：驾驭语言模型，精通文档检索之术](../../../paper_images/2405.11461/quantum_acc.png)

![DocReLM：驾驭语言模型，精通文档检索之术](../../../paper_images/2405.11461/cv_acc.png)

![DocReLM：驾驭语言模型，精通文档检索之术](../../../paper_images/2405.11461/x1.png)

![DocReLM：驾驭语言模型，精通文档检索之术](../../../paper_images/2405.11461/reference_extraction2.png)

[Arxiv](https://arxiv.org/abs/2405.11461)