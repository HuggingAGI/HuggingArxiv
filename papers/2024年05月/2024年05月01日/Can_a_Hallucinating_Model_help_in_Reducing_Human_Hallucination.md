# 幻觉模型能否助力减轻人类的幻觉现象？

发布时间：2024年05月01日

`LLM应用` `心理学` `信息传播`

> Can a Hallucinating Model help in Reducing Human "Hallucination"?

# 摘要

> 广泛存在的无根据信念，包括伪科学、逻辑谬误和阴谋论，对社会发展构成了重大障碍，并可能导致错误信息的传播。本研究通过心理测量学评估，比较了大型语言模型（LLMs）与普通人在识别常见逻辑陷阱方面的能力。我们进行了深入的哲学探讨，对比了人类与LLMs的理性思维。同时，我们提出了一系列方法，旨在利用LLMs来纠正错误观念，这些方法借鉴了认知失调理论和详述可能性模型等心理学说服理论。通过这项研究，我们展示了LLMs在个性化辟谣方面的潜力。

> The prevalence of unwarranted beliefs, spanning pseudoscience, logical fallacies, and conspiracy theories, presents substantial societal hurdles and the risk of disseminating misinformation. Utilizing established psychometric assessments, this study explores the capabilities of large language models (LLMs) vis-a-vis the average human in detecting prevalent logical pitfalls. We undertake a philosophical inquiry, juxtaposing the rationality of humans against that of LLMs. Furthermore, we propose methodologies for harnessing LLMs to counter misconceptions, drawing upon psychological models of persuasion such as cognitive dissonance theory and elaboration likelihood theory. Through this endeavor, we highlight the potential of LLMs as personalized misinformation debunking agents.

[Arxiv](https://arxiv.org/abs/2405.00843)