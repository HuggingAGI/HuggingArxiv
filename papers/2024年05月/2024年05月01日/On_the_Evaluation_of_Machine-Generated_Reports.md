# 机器生成报告评估研究

发布时间：2024年05月01日

`LLM应用` `信息检索` `自动化报告生成`

> On the Evaluation of Machine-Generated Reports

# 摘要

> 大型语言模型（LLMs）为满足信息需求开辟了新路径。尽管在文档排序和短文本生成等方面已取得显著进展，但它们在撰写全面、精确且可查证的长篇报告方面仍面临挑战。这类报告对于满足用户复杂、细致或多维度的信息需求至关重要。本文集思广益，汇聚了业界和学界的声音，以及多个相关研究领域的见解，旨在展现我们对自动化报告生成的展望，并提出了一个关键的灵活框架，用于评估此类报告。与常见的摘要任务相比，自动化报告生成始于对信息需求的详尽描述，阐明报告所需的背景、需求和范围。生成的报告不仅要全面、准确，还要可查证。这些特性在众多分析报告撰写场景中极为重要，甚至不可或缺，这要求我们重新思考构建和评估具备这些特性的系统的方法。为了激励构建这类系统的新尝试，我们提出了一个评估框架，该框架融合了多种评估理念。框架通过问题和答案形式的信息要点来测试报告的完整性和准确性，这些要点是任何优质生成报告不可或缺的部分。此外，通过评估报告中声明与来源文档相映射的引用，确保了报告的可查证性。

> Large Language Models (LLMs) have enabled new ways to satisfy information needs. Although great strides have been made in applying them to settings like document ranking and short-form text generation, they still struggle to compose complete, accurate, and verifiable long-form reports. Reports with these qualities are necessary to satisfy the complex, nuanced, or multi-faceted information needs of users. In this perspective paper, we draw together opinions from industry and academia, and from a variety of related research areas, to present our vision for automatic report generation, and -- critically -- a flexible framework by which such reports can be evaluated. In contrast with other summarization tasks, automatic report generation starts with a detailed description of an information need, stating the necessary background, requirements, and scope of the report. Further, the generated reports should be complete, accurate, and verifiable. These qualities, which are desirable -- if not required -- in many analytic report-writing settings, require rethinking how to build and evaluate systems that exhibit these qualities. To foster new efforts in building these systems, we present an evaluation framework that draws on ideas found in various evaluations. To test completeness and accuracy, the framework uses nuggets of information, expressed as questions and answers, that need to be part of any high-quality generated report. Additionally, evaluation of citations that map claims made in the report to their source documents ensures verifiability.

[Arxiv](https://arxiv.org/abs/2405.00982)