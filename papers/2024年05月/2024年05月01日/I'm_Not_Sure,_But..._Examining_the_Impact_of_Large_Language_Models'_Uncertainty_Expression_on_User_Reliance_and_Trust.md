# 《“我不确定，但是...”：探究大型语言模型不确定性表述对用户信赖与依赖的影响》

发布时间：2024年05月01日

`LLM应用` `人工智能` `用户研究`

> "I'm Not Sure, But...": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust

# 摘要

> 广泛应用的大型语言模型（LLMs）有时会产生貌似可信却错误的结果，这容易误导那些将其视为绝对正确的用户。为了避免用户过度依赖，人们开始呼吁LLMs能够向用户明确表达其不确定性。尽管如此，关于用户如何理解和响应LLMs不确定性表达的实证研究却寥寥无几。本研究通过一项大规模的、预先注册的、涉及404名参与者的人类实验来探讨这一问题，实验中参与者在有或没有使用一个虚构的LLM支持的搜索引擎回答医疗问题的情况下进行回答。我们综合运用了行为测量和自我报告的方法，来评估不同自然语言中不确定性表达对参与者依赖度、信任感和任务完成表现的影响。研究发现，使用第一人称的不确定性表达（例如，“我不太确定，但是...”）能够减少参与者对系统的信心，降低他们与系统答案一致的倾向，并提高他们的准确性。进一步分析表明，这种准确性的提升源于对错误答案依赖度的减少，尽管这种减少并未完全消除。而从一般角度表达不确定性（例如，“这不太明确，但是...”）虽有相似效果，但影响较小，且未达到统计学意义。研究结果表明，采用自然语言的不确定性表达是减少用户对LLMs过度依赖的有效途径，但所用的具体语言至关重要。这进一步强调了在大规模推广LLMs前进行用户测试的必要性。

> Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance. We find that first-person expressions (e.g., "I'm not sure, but...") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., "It's not clear, but..."), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.

[Arxiv](https://arxiv.org/abs/2405.00623)