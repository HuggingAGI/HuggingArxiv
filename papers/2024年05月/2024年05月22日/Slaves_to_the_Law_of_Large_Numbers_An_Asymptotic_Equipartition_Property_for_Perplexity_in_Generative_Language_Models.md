# 大数定律下的奴隶：生成语言模型中困惑度的渐近等分特性

发布时间：2024年05月22日

`LLM理论

理由：这篇论文主要探讨了语言模型生成文本的困惑度及其渐近等分性质，提供了理论支持，并通过实验验证了相关理论。这些内容主要关注语言模型的理论性质和性能评估，因此属于LLM理论分类。` `人工智能` `语言模型`

> Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models

# 摘要

> 我们提出了一种关于语言模型生成文本困惑度的新渐近等分性质，并提供了理论支持。困惑度作为逆似然函数，是衡量语言模型训练效果的常用指标。我们的研究发现，语言模型生成的大段文本的日志困惑度将渐近地趋向于令牌分布的平均熵，表明模型输出局限于“典型集”，这是一个在所有语法正确输出中几乎可以忽略不计的子集。我们通过开源语言模型的实验初步验证了这一理论。此研究不仅有助于优化“AI检测”工具，还对生成模型的特性、预测能力和创造性潜力提供了理论洞见。

> We propose a new asymptotic equipartition property for the perplexity of a large piece of text generated by a language model and present theoretical arguments for this property. Perplexity, defined as a inverse likelihood function, is widely used as a performance metric for training language models. Our main result states that the logarithmic perplexity of any large text produced by a language model must asymptotically converge to the average entropy of its token distributions. This means that language models are constrained to only produce outputs from a ``typical set", which we show, is a vanishingly small subset of all possible grammatically correct outputs. We present preliminary experimental results from an open-source language model to support our theoretical claims. This work has possible practical applications for understanding and improving ``AI detection" tools and theoretical implications for the uniqueness, predictability and creative potential of generative models.

[Arxiv](https://arxiv.org/abs/2405.13798)