# Xwin-LM：大型语言模型的强大且可扩展的对齐实践

发布时间：2024年05月30日

`LLM应用

这篇论文介绍了一套针对大型语言模型（LLMs）的全面对齐方法，包括监督微调、奖励建模、拒绝采样微调及直接偏好优化等技术。这些技术旨在提高LLMs的性能和可扩展性，并通过实际的测试（如AlpacaEval和MT-bench）展示了其效果。因此，这项工作属于LLM应用的范畴，因为它专注于实际应用中的技术和方法，以改进和优化现有的LLMs。` `机器学习`

> Xwin-LM: Strong and Scalable Alignment Practice for LLMs

# 摘要

> 本研究推出了Xwin-LM，一套针对LLMs的全面对齐方法，涵盖了监督微调、奖励建模、拒绝采样微调及直接偏好优化等关键技术。具体包括：使用优质指令数据微调的Xwin-LM-SFT模型、由GPT-4精心标注的大规模多轮偏好数据集Xwin-Pair、在不同规模上训练的奖励模型Xwin-RM、与64个独特响应关联并由Xwin-RM评分的多重偏好数据集Xwin-Set、基于最高分响应微调的Xwin-LM-RS模型，以及应用DPO算法进一步优化的Xwin-LM-DPO模型。在AlpacaEval和MT-bench的测试中，Xwin-LM展现了显著的性能提升和良好的可扩展性。项目仓库将持续更新，以支持社区研究。

> In this work, we present Xwin-LM, a comprehensive suite of alignment methodologies for large language models (LLMs). This suite encompasses several key techniques, including supervised finetuning (SFT), reward modeling (RM), rejection sampling finetuning (RS), and direct preference optimization (DPO). The key components are as follows: (1) Xwin-LM-SFT, models initially finetuned with high-quality instruction data; (2) Xwin-Pair, a large-scale, multi-turn preference dataset meticulously annotated using GPT-4; (3) Xwin-RM, reward models trained on Xwin-Pair, developed at scales of 7B, 13B, and 70B parameters; (4) Xwin-Set, a multiwise preference dataset in which each prompt is linked to 64 unique responses generated by Xwin-LM-SFT and scored by Xwin-RM; (5) Xwin-LM-RS, models finetuned with the highest-scoring responses from Xwin-Set; (6) Xwin-LM-DPO, models further optimized on Xwin-Set using the DPO algorithm. Our evaluations on AlpacaEval and MT-bench demonstrate consistent and significant improvements across the pipeline, demonstrating the strength and scalability of Xwin-LM. The repository https://github.com/Xwin-LM/Xwin-LM will be continually updated to foster community research.

[Arxiv](https://arxiv.org/abs/2405.20335)