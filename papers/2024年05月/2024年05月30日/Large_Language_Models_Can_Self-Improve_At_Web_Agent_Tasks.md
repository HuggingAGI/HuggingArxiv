# 大型语言模型具备在网络代理任务中自我提升的能力。

发布时间：2024年05月30日

`Agent

这篇论文主要探讨了大型语言模型（LLMs）在复杂环境（如WebArena）中作为代理的能力，特别是在零-shot或少量-shot模式下，通过自然语言指令导航和执行任务的能力。论文还研究了LLMs通过自我微调来提升其代理能力的方法，并引入了新的评估指标来更细致地衡量微调后模型的性能。因此，这篇论文更符合Agent分类，因为它专注于LLMs作为代理在特定环境中的应用和性能提升。` `网页导航` `任务自动化`

> Large Language Models Can Self-Improve At Web Agent Tasks

# 摘要

> 训练模型以在复杂环境中如网页浏览器般自如导航和执行任务，常因数据匮乏而颇具挑战。大型语言模型（LLMs）近期崭露头角，能在零-shot或少量-shot模式下，仅凭自然语言指令导航未知领域。研究亦揭示，LLMs能通过自我微调——即基于自身生成的数据——超越原有性能。本研究深入探讨了LLMs在WebArena这一复杂环境中，面对长期任务时，自我提升其代理能力的极限。在WebArena的挑战中，代理需自主穿梭于网页间，执行任务以达成目标。我们通过微调三种合成训练数据混合，实现了在WebArena基准上任务完成率较基础模型提升31%的自我改进。此外，我们还引入了创新的评估指标，更细致地衡量了微调后代理模型的性能、稳健性、能力及轨迹质量，超越了当前仅依赖综合基准分数的自我改进评估方式。

> Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.

[Arxiv](https://arxiv.org/abs/2405.20309)