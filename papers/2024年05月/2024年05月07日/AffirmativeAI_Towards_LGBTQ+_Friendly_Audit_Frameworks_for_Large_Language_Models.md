# AffirmativeAI：迈向大型语言模型的LGBTQ+友好审计框架。

发布时间：2024年05月07日

`LLM应用

这篇论文关注的是大型语言模型（LLM）在特定应用场景——即LGBTQ+社区心理健康支持中的应用。它提出了一种基于肯定疗法的评估框架，旨在确保聊天机器人能够提供同理心、准确和肯定的回应，从而支持LGBTQ+个体的经历。这与LLM的理论研究不同，因为它不是探讨LLM的内部机制或理论基础，而是关注LLM在实际应用中的效果和影响。同时，它也不属于Agent或RAG的分类，因为Agent通常指的是具有自主决策能力的智能体，而RAG（Retrieval-Augmented Generation）通常指的是一种结合了检索和生成的模型架构，这篇论文并没有特别强调这些方面。因此，最合适的分类是LLM应用。` `心理健康` `LGBTQ+社区支持`

> AffirmativeAI: Towards LGBTQ+ Friendly Audit Frameworks for Large Language Models

# 摘要

> LGBTQ+社区的心理健康问题尤为严峻，他们中的许多人转向了如ChatGPT这样的大型语言模型聊天机器人寻求帮助。虽然这些机器人提供了即时的支持和匿名性，但它们是否能提供同理心、准确和肯定的回应仍令人担忧。为此，我们提出了一种基于肯定疗法的评估框架，旨在确保这些聊天机器人能够真正支持并验证LGBTQ+的经历。我们希望通过定性和定量分析，为“肯定性AI”设定标准，确保它们能为LGBTQ+个体提供安全、有效的心理健康支持。我们的目标不是将LLM视为解决LGBTQ+心理健康问题的万能钥匙，而是在考虑社区内复杂歧视的同时，评估这些技术在心理健康支持方面的潜力和风险。

> LGBTQ+ community face disproportionate mental health challenges, including higher rates of depression, anxiety, and suicidal ideation. Research has shown that LGBTQ+ people have been using large language model-based chatbots, such as ChatGPT, for their mental health needs. Despite the potential for immediate support and anonymity these chatbots offer, concerns regarding their capacity to provide empathetic, accurate, and affirming responses remain. In response to these challenges, we propose a framework for evaluating the affirmativeness of LLMs based on principles of affirmative therapy, emphasizing the need for attitudes, knowledge, and actions that support and validate LGBTQ+ experiences. We propose a combination of qualitative and quantitative analyses, hoping to establish benchmarks for "Affirmative AI," ensuring that LLM-based chatbots can provide safe, supportive, and effective mental health support to LGBTQ+ individuals. We benchmark LLM affirmativeness not as a mental health solution for LGBTQ+ individuals or to claim it resolves their mental health issues, as we highlight the need to consider complex discrimination in the LGBTQ+ community when designing technological aids. Our goal is to evaluate LLMs for LGBTQ+ mental health support since many in the community already use them, aiming to identify potential harms of using general-purpose LLMs in this context.

[Arxiv](https://arxiv.org/abs/2405.04652)