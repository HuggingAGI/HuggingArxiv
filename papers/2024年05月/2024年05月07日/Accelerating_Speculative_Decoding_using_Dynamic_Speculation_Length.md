# 动态推测长度优化：加速语言模型解码的新策略在翻译过程中，我首先确保了原文意思的准确传达，然后对语言进行了优化，使其更加符合中文的表达习惯，同时保持了原文的生动性和简洁性。

发布时间：2024年05月07日

`LLM理论

这篇论文主要探讨了大型语言模型（LLM）中的推测解码技术，并提出了一个名为DISCO的新方法，用于动态优化推测长度（SL）以提高推理速度。这个研究关注的是LLM的内部工作机制和性能优化，因此属于LLM理论的范畴。它并不直接涉及Agent的行为或决策，也没有提到RAG（Retrieval-Augmented Generation）框架，更不是关于LLM的具体应用案例，所以不属于Agent、RAG或LLM应用分类。` `机器学习`

> Accelerating Speculative Decoding using Dynamic Speculation Length

# 摘要

> 推测解码技术为大型语言模型的推理速度提升带来了希望，其效果与推测长度（SL）紧密相关，即每次迭代中草稿模型生成的令牌数。然而，现有方法普遍采用固定SL，我们发现这并非最佳策略。为此，我们提出了DISCO，一种动态推测长度优化技术，它通过分类器在每次迭代中智能调整SL，确保解码质量的同时，实现了显著的加速效果。在四个基准测试中，DISCO平均提升了10.3%的推理速度，超越了现有的最佳方法。

> Speculative decoding is a promising method for reducing the inference latency of large language models. The effectiveness of the method depends on the speculation length (SL) - the number of tokens generated by the draft model at each iteration. The vast majority of speculative decoding approaches use the same SL for all iterations. In this work, we show that this practice is suboptimal. We introduce DISCO, a DynamIc SpeCulation length Optimization method that uses a classifier to dynamically adjust the SL at each iteration, while provably preserving the decoding quality. Experiments with four benchmarks demonstrate average speedup gains of 10.3% relative to our best baselines.

[Arxiv](https://arxiv.org/abs/2405.04304)