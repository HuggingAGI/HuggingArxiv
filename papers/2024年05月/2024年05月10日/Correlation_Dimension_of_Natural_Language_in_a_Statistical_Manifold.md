# 自然语言在统计空间中的相关维度探索

发布时间：2024年05月10日

`LLM理论

这篇论文探讨了大型语言模型（LLM）生成的序列的多重分形特性，以及这些特性如何揭示自然语言的全局自相似性和通用维度。研究使用了Grassberger-Procaccia算法来分析这些高维序列，并发现了长记忆机制在这些自相似性中的作用。这种方法的应用不仅限于语言，还可以扩展到其他离散序列的概率模型，如音乐数据分析。因此，这篇论文的内容更偏向于LLM的理论研究，特别是关于自然语言处理中的复杂性和自相似性的理论探讨。` `音乐分析`

> Correlation Dimension of Natural Language in a Statistical Manifold

# 摘要

> 通过Grassberger-Procaccia算法在大规模语言模型生成的高维序列上应用，我们揭示了自然语言的多重分形特性，其全局自相似性和约6.5的通用维度，介于简单随机序列与Barabási-Albert过程之间。长记忆机制是这种自相似性的核心。我们的方法不仅限于语言，也适用于任何离散序列的概率模型，例如，我们已将其成功应用于音乐数据的分析。

> The correlation dimension of natural language is measured by applying the Grassberger-Procaccia algorithm to high-dimensional sequences produced by a large-scale language model. This method, previously studied only in a Euclidean space, is reformulated in a statistical manifold via the Fisher-Rao distance. Language exhibits a multifractal, with global self-similarity and a universal dimension around 6.5, which is smaller than those of simple discrete random sequences and larger than that of a Barabási-Albert process. Long memory is the key to producing self-similarity. Our method is applicable to any probabilistic model of real-world discrete sequences, and we show an application to music data.

[Arxiv](https://arxiv.org/abs/2405.06321)