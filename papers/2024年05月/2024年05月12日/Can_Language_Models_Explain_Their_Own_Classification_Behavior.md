# 语言模型是否具备自我解释其分类决策的能力？这一问题引发了对于模型透明度和可解释性的深入探讨。

发布时间：2024年05月12日

`LLM理论

这篇论文探讨了大型语言模型（LLMs）的内部运作解释问题，并创建了一个数据集来评估模型是否能提供与其分类行为相符的自然语言解释。这属于对LLMs理论层面的研究，因为它关注的是模型的解释能力和内部机制，而不是它们的应用或特定的Agent或RAG框架。因此，它被归类为LLM理论。` `机器学习模型解释性`

> Can Language Models Explain Their Own Classification Behavior?

# 摘要

> 大型语言模型（LLMs）在众多任务中表现出色，但其内部运作的解释却是一大难题。本文探讨了LLMs是否能提供对其内部过程的忠实解释。为此，我们创建了ArticulateRules数据集，包含由简单规则生成的少样本文本分类任务，每个规则都附有自然语言解释。我们测试了这些模型是否能提供与其分类行为相符的自然语言解释。我们的数据集适用于上下文学习和微调评估。我们评估了多种LLMs，发现解释准确性在模型间差异显著，尤其是GPT-3到GPT-4的提升。我们还探索了提升GPT-3解释准确性的方法。尽管进行了额外微调，GPT-3仍未能解释10个规则中的7个。我们公开了ArticulateRules数据集，用于测试LLMs在上下文学习或微调训练中的自我解释能力。

> Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.

[Arxiv](https://arxiv.org/abs/2405.07436)