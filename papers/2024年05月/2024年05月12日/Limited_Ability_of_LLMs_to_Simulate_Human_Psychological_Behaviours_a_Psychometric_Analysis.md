# 大型语言模型在模拟人类心理行为方面显露局限：心理测量视角下的深度剖析

发布时间：2024年05月12日

`LLM理论

这篇论文探讨了大型语言模型（LLMs）在模拟人类心理特质方面的能力，特别是在回应标准化问卷时的心理测量特性。研究通过心理测量学的方法，分析了GPT-3.5和GPT-4模型在不同角色描述下的回应，并评估了它们反映潜在心理特质的能力。这项研究对LLMs的理论能力提出了质疑，特别是在模拟个体行为方面，因此属于LLM理论分类。` `社会科学` `心理测量学`

> Limited Ability of LLMs to Simulate Human Psychological Behaviours: a Psychometric Analysis

# 摘要

> 大型语言模型（LLMs）的逼真回应引发了社会科学家对其在模拟实验、民意调查和调查中人类参与者角色的探索。研究重点在于通过标准化问卷揭示LLMs的心理特征。然而，从LLMs的问卷回应中解读潜在特质颇具挑战，因此研究结果的矛盾性不足为奇。本研究采用心理测量学，即心理测量的科学，来探索这一问题。我们让OpenAI的GPT-3.5和GPT-4模型扮演不同角色，并回应一系列人格测量。角色描述分为通用和具体两种，后者基于真实人类的人口统计数据。GPT-4在通用角色描述下的回应显示出接近人类标准的心理测量特性，但当角色描述具体化时，两个模型的表现均不佳。这表明，目前LLMs在模拟硅基角色时，其回应难以准确反映潜在心理特质。我们的研究对LLMs在模拟个体行为方面的能力提出了质疑。

> The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys. Of central interest in this line of research has been mapping out the psychological profiles of LLMs by prompting them to respond to standardized questionnaires. The conflicting findings of this research are unsurprising given that mapping out underlying, or latent, traits from LLMs' text responses to questionnaires is no easy task. To address this, we use psychometrics, the science of psychological measurement. In this study, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume different personas and respond to a range of standardized measures of personality constructs. We used two kinds of persona descriptions: either generic (four or five random person descriptions) or specific (mostly demographics of actual humans from a large-scale human dataset). We found that the responses from GPT-4, but not GPT-3.5, using generic persona descriptions show promising, albeit not perfect, psychometric properties, similar to human norms, but the data from both LLMs when using specific demographic profiles, show poor psychometrics properties. We conclude that, currently, when LLMs are asked to simulate silicon personas, their responses are poor signals of potentially underlying latent traits. Thus, our work casts doubt on LLMs' ability to simulate individual-level human behaviour across multiple-choice question answering tasks.

[Arxiv](https://arxiv.org/abs/2405.07248)