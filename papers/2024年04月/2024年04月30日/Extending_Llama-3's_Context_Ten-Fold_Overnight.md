# 一夜之间，Llama-3 的上下文理解能力提升了十倍。

发布时间：2024年04月30日

`LLM应用` `机器学习`

> Extending Llama-3's Context Ten-Fold Overnight

# 摘要

> 我们通过 QLoRA 微调技术，成功将 Llama-3-8B-Instruct 模型的上下文长度从 8,000 个标记扩展至 80,000 个，整个过程在单个 8xA800 (80G) GPU 机器上仅需 8 小时即可完成。新模型在包括 NIHS、主题检索和长上下文语言理解在内的多项评估任务上均展现出卓越的性能，同时对短上下文的处理能力也得到了保持。这一显著的扩展能力主要得益于 GPT-4 生成的 3,500 个合成训练样本，凸显了大型语言模型在扩展上下文长度方面的潜在能力，这一能力之前被大大低估。实际上，若有更多计算资源，上下文长度的扩展潜力远超 80K。为此，我们团队将公开全部资源，包括数据、模型、数据生成流程和训练代码，以推动社区的进一步研究，详细信息可访问 \url{https://github.com/FlagOpen/FlagEmbedding}。

> We extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA fine-tuning. The entire training cycle is super efficient, which takes 8 hours on one 8xA800 (80G) GPU machine. The resulted model exhibits superior performances across a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-context language understanding; meanwhile, it also well preserves the original capability over short contexts. The dramatic context extension is mainly attributed to merely 3.5K synthetic training samples generated by GPT-4 , which indicates the LLMs' inherent (yet largely underestimated) potential to extend its original context length. In fact, the context length could be extended far beyond 80K with more computation resources. Therefore, the team will publicly release the entire resources (including data, model, data generation pipeline, training code) so as to facilitate the future research from the community: \url{https://github.com/FlagOpen/FlagEmbedding}.

[Arxiv](https://arxiv.org/abs/2404.19553)