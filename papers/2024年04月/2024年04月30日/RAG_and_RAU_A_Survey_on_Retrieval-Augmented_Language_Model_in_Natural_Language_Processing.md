# RAG 与 RAU：探索自然语言处理领域中的检索增强语言模型

发布时间：2024年04月30日

`分类：RAG` `信息检索`

> RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing

# 摘要

> 大型语言模型（LLMs）极大地推动了自然语言处理（NLP）的发展，但同时也面临着幻觉现象和对特定领域知识的依赖等挑战。为了应对这些挑战，最新的研究方法开始将外部资源检索到的信息与LLMs相结合，显著提升了模型在各类NLP任务中的表现。本篇综述文章填补了对检索增强语言模型（RALMs）——包括检索增强生成（RAG）和检索增强理解（RAU）——全面概述的空白，深入探讨了它们的模式、发展、分类法和应用场景。文中详细阐述了RALMs的核心组件，包括检索器、语言模型和增强手段，以及这些组件如何相互作用，形成多样化的模型架构和应用。RALMs在多种任务中都显示出其实用性，包括翻译、对话系统和知识密集型应用。文章还包含了对RALMs的多种评估方法，特别强调了评估过程中对鲁棒性、准确性和相关性的关注。同时，也指出了RALMs的局限性，尤其是在检索质量和计算效率方面，并对未来的研究方向提出了建议。总结而言，本综述旨在提供对RALMs的结构化理解，探讨其潜力及其在NLP领域未来发展的可能路径。此外，文章还提供了一个包含相关研究和资源的Github仓库，供进一步研究使用：https://github.com/2471023025/RALM_Survey。

> Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.

[Arxiv](https://arxiv.org/abs/2404.19543)