# CVPR2024 NICE 图像标题挑战的应对之道

发布时间：2024年04月19日

`分类：LLM应用

这篇论文的摘要描述了一种针对零样本图像字幕生成任务的解决方案，使用了大规模视觉-语言预训练模型（OFA）和字幕级别的策略来生成高质量的字幕。这表明了该研究在实际应用中使用了大型语言模型（LLM），因此可以归类为LLM应用。` `计算机视觉`

> The Solution for the CVPR2024 NICE Image Captioning Challenge

# 摘要

> 本报告提出了一种针对2024年NICE挑战赛第1主题——零样本图像字幕生成的创新解决方案。与2023年的数据集相比，新挑战的人类注释在字幕的风格和内容上有了显著变化。我们通过改进的检索增强技术和字幕评分方法，有效提升了字幕的质量。在数据处理上，我们采用了由图像字幕模型生成的优质字幕作为训练素材，以弥补文本风格的差异。模型方面，我们部署了OFA——一种基于定制模板的大规模视觉-语言预训练模型，来执行字幕生成任务。此外，我们还提出了一种基于字幕级别的策略，将这些优质数据与检索增强技术相结合，推动模型产出更精准、语义更丰富的高质量字幕。这一方法在排行榜上勇夺第一，CIDEr得分高达234.11分，并在其他所有评价指标上均领跑。

> This report introduces a solution to the Topic 1 Zero-shot Image Captioning of 2024 NICE : New frontiers for zero-shot Image Captioning Evaluation. In contrast to NICE 2023 datasets, this challenge involves new annotations by humans with significant differences in caption style and content. Therefore, we enhance image captions effectively through retrieval augmentation and caption grading methods. At the data level, we utilize high-quality captions generated by image caption models as training data to address the gap in text styles. At the model level, we employ OFA (a large-scale visual-language pre-training model based on handcrafted templates) to perform the image captioning task. Subsequently, we propose caption-level strategy for the high-quality caption data generated by the image caption models and integrate them with retrieval augmentation strategy into the template to compel the model to generate higher quality, more matching, and semantically enriched captions based on the retrieval augmentation prompts. Our approach ranks first on the leaderboard, achieving a CIDEr score of 234.11 and 1st in all other metrics.

![CVPR2024 NICE 图像标题挑战的应对之道](../../../paper_images/2404.12739/x1.png)

![CVPR2024 NICE 图像标题挑战的应对之道](../../../paper_images/2404.12739/x2.png)

![CVPR2024 NICE 图像标题挑战的应对之道](../../../paper_images/2404.12739/x3.png)

![CVPR2024 NICE 图像标题挑战的应对之道](../../../paper_images/2404.12739/x4.png)

![CVPR2024 NICE 图像标题挑战的应对之道](../../../paper_images/2404.12739/x5.png)

[Arxiv](https://arxiv.org/abs/2404.12739)