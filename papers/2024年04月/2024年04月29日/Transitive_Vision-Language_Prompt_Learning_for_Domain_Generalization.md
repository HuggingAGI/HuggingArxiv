# 在领域泛化领域，传递式的视觉-语言提示学习方法展现出了其独特的优势。

发布时间：2024年04月29日

`分类：RAG` `计算机视觉`

> Transitive Vision-Language Prompt Learning for Domain Generalization

# 摘要

> 视觉-语言预训练推动了深度模型在新领域泛化上的重大进展。新近的学习方法，基于这种预训练模型，为领域泛化提供了有效工具，显著提升了问题解决效率。尽管如此，领域不变性与类别可分性之间的平衡仍是技术进步面临的挑战，这对于当前的领域泛化（DG）问题尤为关键。本文提出了一种创新的提示学习策略，通过深度视觉提示强化领域不变性，语言提示保证类别可分性，并采用自适应权重机制来协调这两者。大量实验证明，这种深度视觉提示能够高效抽取领域不变特征，显著增强了深度模型的泛化性能，并在三个基准数据集上达到了最佳表现。

> The vision-language pre-training has enabled deep models to make a huge step forward in generalizing across unseen domains. The recent learning method based on the vision-language pre-training model is a great tool for domain generalization and can solve this problem to a large extent. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. In this paper, we introduce a novel prompt learning strategy that leverages deep vision prompts to address domain invariance while utilizing language prompts to ensure class separability, coupled with adaptive weighting mechanisms to balance domain invariance and class separability. Extensive experiments demonstrate that deep vision prompts effectively extract domain-invariant features, significantly improving the generalization ability of deep models and achieving state-of-the-art performance on three datasets.

[Arxiv](https://arxiv.org/abs/2404.18758)