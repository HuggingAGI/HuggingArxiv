# TableVQA-Bench：多表格领域的视觉问答基准测试

发布时间：2024年04月29日

`LLM应用` `视觉问答` `数据科学`

> TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains

# 摘要

> 本文提出了一个名为 TableVQA-Bench 的表格视觉问答基准测试，该基准基于现有的表格问答和结构识别数据集。现有数据集尚未整合图像或问答对，而这两者对于表格视觉问答至关重要。本文旨在补充这些关键元素。图像来源可以是应用样式表或采用我们提出的表格渲染系统。问答对则是通过大型语言模型（LLM）生成，输入为文本格式的表格。构建完成的 TableVQA-Bench 包含了 1,500 组问答对。我们还对多种多模态大型语言模型（MLLMs）在该基准上的表现进行了全面比较，发现 GPT-4V 在商业和开源 MLLMs 中准确度最高。此外，视觉查询的数量对性能有显著影响。为了深入分析 MLLMs 与其基础 LLMs 的能力差异，我们分别向 MLLMs 和 LLMs 展示了图像格式和文本格式的表格。研究结果表明，处理视觉输入相较于文本输入更具挑战性，尽管 MLLMs 通常需要更高的计算资源，但其性能却低于 LLMs。我们提供的 TableVQA-Bench 基准测试和评估代码可在 GitHub 上找到。

> In this paper, we establish a benchmark for table visual question answering, referred to as the TableVQA-Bench, derived from pre-existing table question-answering (QA) and table structure recognition datasets. It is important to note that existing datasets have not incorporated images or QA pairs, which are two crucial components of TableVQA. As such, the primary objective of this paper is to obtain these necessary components. Specifically, images are sourced either through the application of a \textit{stylesheet} or by employing the proposed table rendering system. QA pairs are generated by exploiting the large language model (LLM) where the input is a text-formatted table. Ultimately, the completed TableVQA-Bench comprises 1,500 QA pairs. We comprehensively compare the performance of various multi-modal large language models (MLLMs) on TableVQA-Bench. GPT-4V achieves the highest accuracy among commercial and open-sourced MLLMs from our experiments. Moreover, we discover that the number of vision queries plays a significant role in TableVQA performance. To further analyze the capabilities of MLLMs in comparison to their LLM backbones, we investigate by presenting image-formatted tables to MLLMs and text-formatted tables to LLMs, respectively. Our findings suggest that processing visual inputs is more challenging than text inputs, as evidenced by the lower performance of MLLMs, despite generally requiring higher computational costs than LLMs. The proposed TableVQA-Bench and evaluation codes are available at \href{https://github.com/naver-ai/tablevqabench}{https://github.com/naver-ai/tablevqabench}.

[Arxiv](https://arxiv.org/abs/2404.19205)