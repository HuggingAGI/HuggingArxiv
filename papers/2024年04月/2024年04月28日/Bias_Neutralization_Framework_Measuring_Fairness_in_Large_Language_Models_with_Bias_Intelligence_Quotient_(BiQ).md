# 偏见中和框架：通过偏见智能商数（BiQ）评估大型语言模型的公平性。

发布时间：2024年04月28日

`分类：LLM应用` `AI偏见检测` `语言模型`

> Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)

# 摘要

> 大型语言模型（LLMs）对公共讨论和决策的影响力不断上升，这凸显了解决AI系统中内在偏见的紧迫性。在AI技术广泛应用于各行各业的今天，消除LLMs中的种族偏见尤为紧要。本文提出了一个创新的全面偏见中和框架（CBNF），旨在量化和降低LLMs中的偏见。该框架融合了大型语言模型偏见指数（LLMBI）和无人口统计信息的偏见移除（BLIND）技术，创新性地推出了一个新的度量工具——偏见智能商数（BiQ），它能够在不依赖人口统计信息的情况下，检测、衡量并减少LLMs中的种族偏见。CBNF通过引入BiQ这一新度量标准，不仅增强了LLMBI的公平性指标，还为偏见评估提供了一个多角度的视角，强调了在AI领域采取细致公平观念的必要性。本文还深入分析了Latimer AI（一个基于黑人历史和文化逐步训练的语言模型）与ChatGPT 3.5的性能对比，展示了Latimer AI在通过专门训练和精细的偏见缓解策略，有效识别种族、文化和性别偏见方面的优势。

> The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems. In the wake of AI's expansive integration across sectors, addressing racial bias in LLMs has never been more critical. This paper introduces a novel framework called Comprehensive Bias Neutralization Framework (CBNF) which embodies an innovative approach to quantifying and mitigating biases within LLMs. Our framework combines the Large Language Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)] and Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)] methodologies to create a new metric called Bias Intelligence Quotient (BiQ)which detects, measures, and mitigates racial bias in LLMs without reliance on demographic annotations.
  By introducing a new metric called BiQ that enhances LLMBI with additional fairness metrics, CBNF offers a multi-dimensional metric for bias assessment, underscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et al., 2021]. This paper presents a detailed analysis of Latimer AI (a language model incrementally trained on black history and culture) in comparison to ChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural, and gender biases through targeted training and refined bias mitigation strategies [Latimer & Bender, 2023].

[Arxiv](https://arxiv.org/abs/2404.18276)