# Elicitron：一款基于大型语言模型（LLM）的代理仿真框架，专为设计需求的挖掘而设计。

发布时间：2024年04月04日

`LLM应用` `产品开发` `人工智能`

> Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation

# 摘要

> 需求挖掘是产品开发中至关重要却费时费力的一环，往往难以全面捕捉用户的多元需求，从而造成产品不尽人意。本篇论文提出了一个创新框架，该框架借助大型语言模型（LLMs）自动化并优化需求挖掘流程。通过LLMs，我们能够创建出众多模拟用户（即LLM代理），这些代理能够探索更宽广的用户需求范围和预测之外的使用情境。在产品体验场景中，这些代理通过阐述自己的行为、观察和所遇挑战，深入参与互动。随后，通过代理访谈和深入分析，我们能够挖掘出包括潜在需求在内的宝贵用户需求。我们的框架通过三项实验得到验证：首先，我们探索了生成多样化代理的不同方法，并比较了它们的优劣；其次，我们展示了如何利用框架有效模拟富有同理心的领先用户访谈，从而识别出比传统人工访谈更多的潜在需求；最后，我们证明了LLMs不仅能够分析访谈内容、捕捉需求，还能将需求分类为显性或隐性。本研究凸显了运用LLM代理在产品开发的早期阶段加速进程、降低成本并提升创新的可能性。

> Requirements elicitation, a critical, yet time-consuming and challenging step in product development, often fails to capture the full spectrum of user needs. This may lead to products that fall short of expectations. This paper introduces a novel framework that leverages Large Language Models (LLMs) to automate and enhance the requirements elicitation process. LLMs are used to generate a vast array of simulated users (LLM agents), enabling the exploration of a much broader range of user needs and unforeseen use cases. These agents engage in product experience scenarios, through explaining their actions, observations, and challenges. Subsequent agent interviews and analysis uncover valuable user needs, including latent ones. We validate our framework with three experiments. First, we explore different methodologies for diverse agent generation, discussing their advantages and shortcomings. We measure the diversity of identified user needs and demonstrate that context-aware agent generation leads to greater diversity. Second, we show how our framework effectively mimics empathic lead user interviews, identifying a greater number of latent needs than conventional human interviews. Third, we showcase that LLMs can be used to analyze interviews, capture needs, and classify them as latent or not. Our work highlights the potential of using LLM agents to accelerate early-stage product development, reduce costs, and increase innovation.

[Arxiv](https://arxiv.org/abs/2404.16045)