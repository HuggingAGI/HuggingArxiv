# 本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。

发布时间：2024年04月24日

`分类：LLM应用` `漫画处理`

> Zero-Shot Character Identification and Speaker Prediction in Comics via Iterative Multimodal Fusion

# 摘要

> 漫画处理任务，如声音生成或翻译，对角色识别和对话中说话者的预测极为关键。然而，由于角色随漫画标题的不同而变化，传统的监督学习方法，如训练需要特定标注的角色分类器，变得不切实际。为此，我们提出了一种创新的零样本方法，使机器能够仅通过未标注的漫画图像来识别角色和预测说话者的名字。尽管这些任务在现实世界的应用中极为重要，但由于故事理解的挑战和多模态整合的复杂性，它们至今未被充分研究。最新的大型语言模型（LLMs）在文本理解和推理方面展现了强大的能力，但其在多模态内容分析上的应用仍然是一个待解决的问题。为应对这一挑战，我们设计了一个迭代的多模态框架，首次将多模态信息应用于角色识别和说话者预测。我们的实验显示，该框架不仅有效，而且为这些任务设定了一个坚实的基准。更重要的是，由于该方法无需训练数据或标注，它可以无缝应用于任何漫画系列。

> Recognizing characters and predicting speakers of dialogue are critical for comic processing tasks, such as voice generation or translation. However, because characters vary by comic title, supervised learning approaches like training character classifiers which require specific annotations for each comic title are infeasible. This motivates us to propose a novel zero-shot approach, allowing machines to identify characters and predict speaker names based solely on unannotated comic images. In spite of their importance in real-world applications, these task have largely remained unexplored due to challenges in story comprehension and multimodal integration. Recent large language models (LLMs) have shown great capability for text understanding and reasoning, while their application to multimodal content analysis is still an open problem. To address this problem, we propose an iterative multimodal framework, the first to employ multimodal information for both character identification and speaker prediction tasks. Our experiments demonstrate the effectiveness of the proposed framework, establishing a robust baseline for these tasks. Furthermore, since our method requires no training data or annotations, it can be used as-is on any comic series.

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x1.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x2.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x3.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x4.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x5.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x6.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x7.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x8.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x9.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x10.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x11.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x12.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x13.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x14.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/x15.png)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/zeroshot_1.jpg)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/zeroshot_2.jpg)

![本研究采用迭代多模态融合技术，实现了在漫画领域无需先验样本即可进行角色识别和说话者预测的创新方法。](../../../paper_images/2404.13993/zeroshot_3.jpg)

[Arxiv](https://arxiv.org/abs/2404.13993)