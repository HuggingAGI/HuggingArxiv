# 大型语言模型漏洞检测能力全面探究

发布时间：2024年03月25日

`LLM应用

这篇论文主要关注大型语言模型（LLMs）在代码生成领域的应用，特别是在漏洞检测任务上的表现和挑战。论文通过评估多个顶尖的代码生成LLMs，并提出新的提示方法来尝试提高漏洞检测的准确性。尽管研究结果显示LLMs在某些方面有所改进，但它们在区分程序错误与修复版本、错误定位和类型识别上仍存在显著问题。这表明LLMs在理解关键代码结构和安全概念方面可能存在局限。因此，这篇论文属于LLM应用分类，因为它探讨了LLMs在特定应用场景（即漏洞检测）中的实际应用和效果。` `软件安全` `漏洞检测`

> A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection

# 摘要

> 大型语言模型（LLMs）在代码生成等领域展现了巨大潜力，但在漏洞检测这一关键任务上，它们的推理能力仍面临挑战。尽管已有研究利用LLMs进行漏洞检测，但其真正的能力及错误类型尚不明朗。本研究评估了十一个顶尖的代码生成LLMs，并创新性地提出了三种提示方法，以期提升漏洞检测的准确性。结果显示，尽管有所改进，LLMs在区分程序错误与修复版本上仍有76%的失败率，且在错误定位和类型识别上频繁出错。这表明，尽管LLMs在其他领域表现出色，但在理解关键代码结构和安全概念上可能存在局限。详细数据和代码已公开分享。

> Large Language Models (LLMs) have demonstrated great potential for code generation and other software engineering tasks. Vulnerability detection is of crucial importance to maintaining the security, integrity, and trustworthiness of software systems. Precise vulnerability detection requires reasoning about the code, making it a good case study for exploring the limits of LLMs' reasoning capabilities. Although recent work has applied LLMs to vulnerability detection using generic prompting techniques, their full capabilities for this task and the types of errors they make when explaining identified vulnerabilities remain unclear.
  In this paper, we surveyed eleven LLMs that are state-of-the-art in code generation and commonly used as coding assistants, and evaluated their capabilities for vulnerability detection. We systematically searched for the best-performing prompts, incorporating techniques such as in-context learning and chain-of-thought, and proposed three of our own prompting methods. Our results show that while our prompting methods improved the models' performance, LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that 57% of LLM responses contained errors, and the models frequently predicted incorrect locations of buggy code and misidentified bug types. LLMs only correctly localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted correctly by 70-100% of human participants. These findings suggest that despite their potential for other tasks, LLMs may fail to properly comprehend critical code structures and security-related concepts. Our data and code are available at https://figshare.com/s/78fe02e56e09ec49300b.

![大型语言模型漏洞检测能力全面探究](../../../paper_images/2403.17218/x1.png)

![大型语言模型漏洞检测能力全面探究](../../../paper_images/2403.17218/x2.png)

![大型语言模型漏洞检测能力全面探究](../../../paper_images/2403.17218/x3.png)

![大型语言模型漏洞检测能力全面探究](../../../paper_images/2403.17218/x4.png)

![大型语言模型漏洞检测能力全面探究](../../../paper_images/2403.17218/x5.png)

![大型语言模型漏洞检测能力全面探究](../../../paper_images/2403.17218/x6.png)

![大型语言模型漏洞检测能力全面探究](../../../paper_images/2403.17218/x7.png)

![大型语言模型漏洞检测能力全面探究](../../../paper_images/2403.17218/x8.png)

[Arxiv](https://arxiv.org/abs/2403.17218)