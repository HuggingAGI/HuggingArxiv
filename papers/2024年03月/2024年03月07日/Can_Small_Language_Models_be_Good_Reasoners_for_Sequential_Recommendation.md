# 小型语言模型能否在序列推荐任务上展现出色的推理能力呢？

发布时间：2024年03月07日

`LLM应用`

> Can Small Language Models be Good Reasoners for Sequential Recommendation?

> 得益于LLMs强大的语言理解与生成技术，序列推荐领域得以突破性发展。然而，要成功应用LLMs优化序列推荐，还需应对两大难题：一是用户行为模式复杂多变，单靠LLMs一步到位的推理可能产生不准确甚至无关的回应；二是像ChatGPT-175B这类巨型模型对资源的需求过于庞大，无法实际应用于现实中的序列推荐系统。为此，本文提出了创新的SLIM框架——一种逐步知识蒸馏推荐方案，让序列推荐在保持“苗条身材”（即资源高效利用）的同时，也能汲取LLMs非凡的推理能力。我们运用基于用户行为序列的CoT提示方式，引导更大规模的教师模型产出推理依据，再将这些依据作为标签来训练小型学生模型（如LLaMA2-7B），使学生模型逐渐掌握推荐任务中的步步推理技巧。此外，我们将学生模型生成的推理依据压缩成密集向量，助力于无论是否依赖用户标识的推荐场景。大量的实验证明了SLIM相比现有最优基准更胜一筹，深入分析还揭示了其在合理成本范围内生成具有实际意义的推荐推理的潜力。

> Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks. We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs.

[Arxiv](https://arxiv.org/abs/2403.04260)