# [跨语言界限探索，对越南大型语言模型进行细致调整与综合评测](https://arxiv.org/abs/2403.02715)

发布时间：2024年03月05日

`LLM应用`

> Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models

> 随着LLMs的最新突破，其在AI进化中的地位日益凸显。然而，即使大规模预训练于多语言数据集，现存开源LLMs在处理越南语时表现力不足，且缺乏针对性的系统基准与评价标准，问题更为突出。因此，我们专为越南语微调了LLMs，并构建了一套包含10大任务、31项指标在内的综合评测框架。实验证明，微调后的LLMs显著提升了越南语的理解与生成能力。进一步分析揭示，模型参数越多，可能带来的偏见和输出不准确问题越严重；而决定LLM性能的核心要素在于训练或微调数据集的质量。这深刻提示我们在提升LLM性能的过程中，精细化微调配合高质量数据集至关重要。

> Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights underscore the significance of meticulous fine-tuning with high-quality datasets in enhancing LLM performance.

[Arxiv](https://arxiv.org/abs/2403.02715)