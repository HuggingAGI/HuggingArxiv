# 为了更准确、直观地评估文本多样性，本研究提出了一种标准化测量工具，并对不同方法所得分数进行了深入对比分析。

发布时间：2024年03月01日

`LLM应用`

> Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores

> 大型语言模型产出多样性的差异深深塑造了人们对模型质量和价值的认知。尽管人们可以轻易察觉到提示泄露、模式化答案框架及跨互动的预设回复等问题，但至今仍缺乏衡量此类模型表现维度的标准评分方法。本研究通过实证手段对英文文本的多样性得分进行了深入探讨，发现在捕捉类似慢速计算的$n$-gram重叠均匀度信息方面，高效的压缩算法具备相当的能力。进一步地，结合压缩比率、长$n$-gram自我重复次数以及Self-BLEU和BERTScore等多元度量方式，即可全面反映多样性，并且彼此间的相关性较低。这些多样性得分不仅适用于生成模型的分析，在指令调优数据集和人为创作文本等方面也有广泛应用潜力。为了推动相关领域的研究进展并促使各类报告保持一致性，我们发布了多样性得分工具包。

> The diversity across outputs generated by large language models shapes the perception of their quality and utility. Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. We release a diversity score package to facilitate research and invite consistency across reports.

[Arxiv](https://arxiv.org/abs/2403.00553)