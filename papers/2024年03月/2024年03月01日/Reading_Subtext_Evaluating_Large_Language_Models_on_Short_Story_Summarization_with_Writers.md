# [在“解读言外之意”的研究中，我们通过让大型语言模型处理由作家编写的短篇小说摘要，来评估其在该任务上的表现能力。](https://arxiv.org/abs/2403.01061)

发布时间：2024年03月01日

`LLM应用`

> Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers

> 面对总结复杂而细腻的短篇小说这一艰巨任务，我们考察了几款最新的大型语言模型（如GPT-4、Claude-2.1和LLama-2-70B）。为了确保模型从未接触过这些故事，我们直接与作者携手，同时利用作者的第一手评价作为衡量摘要质量的标准。通过结合叙事理论的深入量化和质化分析，我们揭示了这些模型在超过半数情况下会出现偏离原文的失误，并在解读曲折潜台词方面存在困难。不过，当它们发挥最佳水平时，却能对故事展开富有洞察力的主题解析。另外，实验表明LLMs对摘要质量的判断并不能准确反映作者的真实反馈。

> We evaluate recent Large language Models (LLMs) on the challenging task of summarizing short stories, which can be lengthy, and include nuanced subtext or scrambled timelines. Importantly, we work directly with authors to ensure that the stories have not been shared online (and therefore are unseen by the models), and to obtain informed evaluations of summary quality using judgments from the authors themselves. Through quantitative and qualitative analysis grounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We find that all three models make faithfulness mistakes in over 50% of summaries and struggle to interpret difficult subtext. However, at their best, the models can provide thoughtful thematic analysis of stories. We additionally demonstrate that LLM judgments of summary quality do not match the feedback from the writers.