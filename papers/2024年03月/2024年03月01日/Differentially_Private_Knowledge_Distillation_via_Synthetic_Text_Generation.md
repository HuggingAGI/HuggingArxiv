# 我们提出了一种新颖的方法，利用合成文本生成技术来实现差异隐私保护下的知识蒸馏，这种方法能够在保护数据隐私的同时提取和传递模型知识。

发布时间：2024年03月01日

`LLM应用`

> Differentially Private Knowledge Distillation via Synthetic Text Generation

> 现今，LLMs在多个下游任务中屡创佳绩，但鉴于对数据隐私保护需求的不断提升，它们需借助DP技术在私有数据上进行训练，并且为适应现实应用中的资源有限设备或低延迟场景，还需要进行模型压缩。然而，兼顾差分隐私与模型压缩往往意味着不得不在功能性和效率之间做出权衡，若二者同时实施，则可能造成更大程度的功能性损失。针对这一问题，我们创新性地设计了一种基于差分隐私的合成数据知识蒸馏算法。该算法巧妙利用经过DP处理的LLM生成的合成数据，以双轨方式进行知识迁移：一方面从合成数据及其硬标签入手；另一方面则借鉴教师模型在合成数据上的输出分布——软标签。更有甚者，在教师模型与学生模型架构相似时，还能进一步通过挖掘隐藏表征强化知识蒸馏效果。实验结果表明，相较于现有的强隐私设置（ε = 2）基准方法，我们的方案显著提升了实用价值，有力验证了在保障训练数据隐私的同时，能够有效压缩自回归LLMs的可行性。

> Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy requires LLMs to train with Differential Privacy (DP) on private data. Concurrently it is also necessary to compress LLMs for real-life deployments on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss. To this end, we propose a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private LLM. The knowledge of a teacher model is transferred onto the student in two ways: one way from the synthetic data itself, the hard labels, and the other way by the output distribution of the teacher model evaluated on the synthetic data, the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further distill knowledge by exploiting hidden representations. Our results show that our framework substantially improves the utility over existing baselines with strong privacy parameters, ε = 2, validating that we can successfully compress autoregressive LLMs while preserving the privacy of training data.

[Arxiv](https://arxiv.org/abs/2403.00932)