# [Chatbot Arena——一个通过人类偏好评判LLMs表现的公开平台，旨在为各类大型语言模型提供公正、直观的人性化评估环境。]

发布时间：2024年03月06日

`LLM应用`

> Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference

> LLMs虽然开启了新能力与应用场景的大门，但在衡量其与人类偏好的契合度上尚存显著挑战。为此，我们推出了Chatbot Arena——一个以人类偏好为核心评价依据的LLMs开放式评测平台。我们运用成对比较法，并通过众包汇集了多样化用户群体的意见。历经数月运营，此平台已累计获得超过24万次投票。本文详述了平台详情、剖析了目前积累的数据，并阐述了我们所采用的有效精准评估和排序模型的成熟统计手段。实证显示，由众包产生的问题既丰富多样又具备辨别力，且普通用户的投票结果与专业评审员的意见保持良好一致性。这一系列深入分析共同铸就了Chatbot Arena强大而可靠的信誉基石。因其独特价值与开放性，Chatbot Arena如今已成为备受推崇的LLM排行榜翘楚，被各大领先LLM开发商与企业广泛引用。公众可随时访问我们的演示版本：\url{https://chat.lmsys.org}。

> Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \url{https://chat.lmsys.org}.

[Arxiv](https://arxiv.org/abs/2403.04132)