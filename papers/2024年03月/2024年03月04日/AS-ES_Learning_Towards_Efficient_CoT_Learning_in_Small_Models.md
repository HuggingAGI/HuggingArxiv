# [AS-ES 学习致力于在小型模型中实现高效的概念到文本（CoT）学习，旨在提升模型理解和应用复杂概念的能力。](https://arxiv.org/abs/2403.01969)

发布时间：2024年03月04日

`LLM理论`

> AS-ES Learning: Towards Efficient CoT Learning in Small Models

> CoT作为LLMs中一项关键的新能力，在处理逻辑推理问题时尤为突出。为了在小模型中也实现这一能力，研究者尝试通过提炼大型语言模型生成的CoT数据进行知识迁移。但目前的方法大多侧重于生成和整合更多的LLMs数据，而忽视了高效利用现有CoT数据的价值。为此，我们创新性地提出了AS-ES学习新范式，它能够充分利用CoT内部信息以迭代方式进行生成训练。实验证明，在MWP问题解决和PET摘要这类高度依赖CoT的任务上，我们的方法即使不借助数据增强或修改模型结构，也能显著超越传统的seq2seq直接训练效果。同时，我们深入探究了小模型在学习CoT过程中效率低下的根源，并阐明了AS-ES学习为何奏效，从而为我们揭示了CoT背后的工作机制提供了一定的理论依据。

> Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs, especially when it comes to logical reasoning. Attempts have been made to induce such ability in small models as well by distilling from the data with CoT generated by Large Language Models (LLMs). However, existing methods often simply generate and incorporate more data from LLMs and fail to note the importance of efficiently utilizing existing CoT data. We here propose a new training paradigm AS-ES (Abstractive Segments - Extractive Segments) learning, which exploits the inherent information in CoT for iterative generation. Experiments show that our methods surpass the direct seq2seq training on CoT-extensive tasks like MWP and PET summarization, without data augmentation or altering the model itself. Furthermore, we explore the reason behind the inefficiency of small models in learning CoT and provide an explanation of why AS-ES learning works, giving insights into the underlying mechanism of CoT.

[Arxiv](https://arxiv.org/abs/2403.01969)