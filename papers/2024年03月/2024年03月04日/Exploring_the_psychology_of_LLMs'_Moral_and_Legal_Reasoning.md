# 本研究旨在深入探讨大型语言模型在道德和法律推理方面的心理学机制。

发布时间：2024年03月04日

`LLM应用` `人工智能伦理` `心理学`

> Exploring the psychology of LLMs' Moral and Legal Reasoning

# 摘要

> 大型语言模型（LLMs）在众多领域的任务上展现了专家级的能力。鉴于LLMs可能引发的伦理争议，以及未来版本需与人类价值观保持一致，探究这些尖端模型是如何处理道德和法律议题的变得尤为关键。本文借鉴实验心理学的手法，深入探讨了这一课题。我们复现了八项来自实验心理学文献的研究，涉及谷歌的Gemini Pro、Anthropic的Claude 2.1、OpenAI的GPT-4以及Meta的Llama 2 Chat 70b等模型。研究发现，模型与人类回应的契合度在不同实验间波动，且各模型在整体契合度上也互有差异，其中GPT-4在所有测试模型中表现最为突出。然而，即便LLM生成的回应与人类高度吻合，仍存在一些系统性差异，模型有时会放大人类反应中的效果，部分原因是通过降低变异性实现的。这提醒我们，在心理学研究中用当前的LLMs替代人类参与者需谨慎行事，并凸显了对机器心理学特有方面进行更深入研究的必要。

> Large language models (LLMs) exhibit expert-level performance in tasks across a wide range of different domains. Ethical issues raised by LLMs and the need to align future versions makes it important to know how state of the art models reason about moral and legal issues. In this paper, we employ the methods of experimental psychology to probe into this question. We replicate eight studies from the experimental literature with instances of Google's Gemini Pro, Anthropic's Claude 2.1, OpenAI's GPT-4, and Meta's Llama 2 Chat 70b. We find that alignment with human responses shifts from one experiment to another, and that models differ amongst themselves as to their overall alignment, with GPT-4 taking a clear lead over all other models we tested. Nonetheless, even when LLM-generated responses are highly correlated to human responses, there are still systematic differences, with a tendency for models to exaggerate effects that are present among humans, in part by reducing variance. This recommends caution with regards to proposals of replacing human participants with current state-of-the-art LLMs in psychological research and highlights the need for further research about the distinctive aspects of machine psychology.

[Arxiv](https://arxiv.org/abs/2308.01264)