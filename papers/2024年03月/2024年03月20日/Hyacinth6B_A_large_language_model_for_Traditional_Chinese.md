# Hyacinth6B，一款专为繁体中文打造的大型语言模型

发布时间：2024年03月20日

`LLM应用` `模型压缩`

> Hyacinth6B: A large language model for Traditional Chinese

> 本研究着力应对大型语言模型所面临的高硬件和计算需求难题，旨在寻求一种既能保持模型轻巧又能保证优秀性能的解决方案。我们致力于在相对轻量级的模型上实现性能最大化，并在此背景下开发了Hyacinth6B。其设计初衷是充分发掘LLM的核心效能，同时避免产生过高的资源消耗，以创新性的训练方式——运用LoRA方法进行参数高效微调，从而有力地拓展了小型模型的表现力边界。

> This research's primary motivation of this study is to address the high hardware and computational demands typically associated with LLMs.Therefore,our goal is to find a balance between model lightness and performance,striving to maximize performance while using a comparatively lightweight model. Hyacinth6B was developed with this objective in mind,aiming to fully leverage the core capabilities of LLMs without incurring substantial resource costs, effectively pushing the boundaries of smaller model's performance. The training approach involves parameter efficient finetuning using the LoRA method.

[Arxiv](https://arxiv.org/abs/2403.13334)