# Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击

发布时间：2024年07月22日

`LLM应用` `网络安全` `人工智能`

> Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models

# 摘要

> 随着 ChatGPT 等大型语言模型的兴起，其广泛应用与潜在风险并存。开发者虽已采取多项安全措施，但对抗性输入仍构成威胁。本研究揭示了一种利用人类对话技巧的攻击手段，从 LLM 中提取有害信息。我们提出了三种核心策略：将恶意问题拆解为无害子问题；将显性恶意问题改写为隐性、听起来无害的问题；通过请求示例增强回答的有害性。与传统攻击不同，我们深入分析了回答中的信息本质。实验表明，我们的方法在 GPT-3.5-turbo、GPT-4 和 Llama2 上均优于传统攻击。这项研究不仅引入了一种新型攻击手段，还提出了一个关键问题：如何识别对话中的恶意意图？

> With the development of large language models (LLMs) like ChatGPT, both their vast applications and potential vulnerabilities have come to the forefront. While developers have integrated multiple safety mechanisms to mitigate their misuse, a risk remains, particularly when models encounter adversarial inputs. This study unveils an attack mechanism that capitalizes on human conversation strategies to extract harmful information from LLMs. We delineate three pivotal strategies: (i) decomposing malicious questions into seemingly innocent sub-questions; (ii) rewriting overtly malicious questions into more covert, benign-sounding ones; (iii) enhancing the harmfulness of responses by prompting models for illustrative examples. Unlike conventional methods that target explicit malicious responses, our approach delves deeper into the nature of the information provided in responses. Through our experiments conducted on GPT-3.5-turbo, GPT-4, and Llama2, our method has demonstrated a marked efficacy compared to conventional attack methods. In summary, this work introduces a novel attack method that outperforms previous approaches, raising an important question: How to discern whether the ultimate intent in a dialogue is malicious?

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/x1.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/x2.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/x3.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/x4.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/x5.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/x6.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/x7.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/1.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/2.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/3.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/4.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/5.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/6.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/7.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/8.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/9.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/10.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/11.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/12.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/13.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/14.png)

![Imposter.AI：针对对齐大型语言模型的隐藏意图的对抗攻击](../../../paper_images/2407.15399/15.png)

[Arxiv](https://arxiv.org/abs/2407.15399)