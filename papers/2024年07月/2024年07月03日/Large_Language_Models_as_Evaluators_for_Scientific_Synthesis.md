# 大型语言模型：科学综合的评判者

发布时间：2024年07月03日

`LLM应用` `科学研究` `人工智能`

> Large Language Models as Evaluators for Scientific Synthesis

# 摘要

> 本研究深入探讨了GPT-4和Mistral等尖端大型语言模型在评估科学综合质量方面的表现，并将其与人类专家的评价进行对比。我们采用了一个包含100个研究问题及其综合的数据集，这些综合由GPT-4从五篇相关论文的摘要中生成，并与人类质量评级进行核对。初步数据显示，尽管LLMs能提供逻辑上合理的解释，这些解释在一定程度上与人类评级相符，但进一步的统计分析揭示了LLM与人类评级之间的弱相关性，这凸显了LLMs在科学综合评估中的潜力与当前局限。

> Our study explores how well the state-of-the-art Large Language Models (LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries or, more fittingly, scientific syntheses, comparing their evaluations to those of human annotators. We used a dataset of 100 research questions and their syntheses made by GPT-4 from abstracts of five related papers, checked against human quality ratings. The study evaluates both the closed-source GPT-4 and the open-source Mistral model's ability to rate these summaries and provide reasons for their judgments. Preliminary results show that LLMs can offer logical explanations that somewhat match the quality ratings, yet a deeper statistical analysis shows a weak correlation between LLM and human ratings, suggesting the potential and current limitations of LLMs in scientific synthesis evaluation.

[Arxiv](https://arxiv.org/abs/2407.02977)