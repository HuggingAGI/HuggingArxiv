# 借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。

发布时间：2024年07月03日

`LLM应用` `人工智能` `对话系统`

> Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment

# 摘要

> 随着大型语言模型 (LLM) 的飞速进步，它们已进化为能够捕捉语境细节并生成恰当语句的对话机器人，通过指令调优和基于人类反馈的强化学习 (RLHF) 等技术，它们越来越贴近人类价值观。但 LLM 所需的计算效率，尤其是通过后训练量化 (PTQ) 等技术实现的效率，带来了令牌翻转等挑战，可能影响聊天机器人的性能。为此，我们创新性地提出了量化感知直接偏好优化 (QDPO)，这一方法有效对齐了量化 LLM 与全精度模型，显著提升了对话能力。在多语言环境下对两种指令调优的 LLM 进行评估，QDPO 在提升对话能力方面超越了传统的 PTQ 和知识蒸馏微调技术，为高效且强大的对话 LLM 的发展开辟了新篇章。

> The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced techniques such as instruction tuning and reinforcement learning from human feedback (RLHF). However, the computational efficiency required for LLMs, achieved through techniques like post-training quantization (PTQ), presents challenges such as token-flipping that can impair chatbot performance. In response, we propose a novel preference alignment approach, quantization-aware direct preference optimization (QDPO), that aligns quantized LLMs with their full-precision counterparts, improving conversational abilities. Evaluated on two instruction-tuned LLMs in various languages, QDPO demonstrated superior performance in improving conversational abilities compared to established PTQ and knowledge-distillation fine-tuning techniques, marking a significant step forward in the development of efficient and effective conversational LLMs.

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x1.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x2.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x3.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x4.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x5.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x6.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x7.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x8.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x9.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x10.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x11.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x12.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x13.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x14.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x15.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x16.png)

![借助直接偏好对齐，我们致力于提升量化大型语言模型在对话中的表现。](../../../paper_images/2407.03051/x17.png)

[Arxiv](https://arxiv.org/abs/2407.03051)