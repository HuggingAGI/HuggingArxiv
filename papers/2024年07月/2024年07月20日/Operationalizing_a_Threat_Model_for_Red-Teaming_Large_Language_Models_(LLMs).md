# 为大型语言模型 (LLM) 实施红队威胁模型的操作

发布时间：2024年07月20日

`LLM应用` `网络安全` `人工智能`

> Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)

# 摘要

> 利用 LLM 构建安全且有韧性的应用，需预见并应对未知威胁。红队测试作为识别 LLM 实际应用中漏洞的关键手段，本文详述了威胁模型，并系统梳理了针对 LLM 的红队攻击知识。我们依据 LLM 的开发与部署阶段，构建了攻击分类，并汲取了过往研究的洞见。同时，我们整理了防御策略与实践者的红队测试方法。通过揭示主要攻击模式与入口点，本文为强化基于 LLM 系统的安全与韧性提供了框架。

> Creating secure and resilient applications with large language models (LLM) requires anticipating, adjusting to, and countering unforeseen threats. Red-teaming has emerged as a critical technique for identifying vulnerabilities in real-world LLM implementations. This paper presents a detailed threat model and provides a systematization of knowledge (SoK) of red-teaming attacks on LLMs. We develop a taxonomy of attacks based on the stages of the LLM development and deployment process and extract various insights from previous research. In addition, we compile methods for defense and practical red-teaming strategies for practitioners. By delineating prominent attack motifs and shedding light on various entry points, this paper provides a framework for improving the security and robustness of LLM-based systems.

[Arxiv](https://arxiv.org/abs/2407.14937)