# 为大型语言模型 (LLM) 构建红队威胁模型并实施

发布时间：2024年07月20日

`LLM应用` `网络安全` `人工智能`

> Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)

# 摘要

> 利用大型语言模型构建安全且有韧性的应用，需预见、适应并反击未知威胁。红队测试技术应运而生，成为识别 LLM 实际应用中漏洞的关键手段。本文详述威胁模型，系统梳理针对 LLM 的红队攻击知识体系。我们依据 LLM 的开发与部署流程，构建攻击分类，并汲取过往研究精华。同时，我们整理防御策略与实践红队测试方法，供从业者参考。通过揭示主要攻击模式与潜在入口，本文为强化基于 LLM 系统的安全与稳健性提供指导框架。

> Creating secure and resilient applications with large language models (LLM) requires anticipating, adjusting to, and countering unforeseen threats. Red-teaming has emerged as a critical technique for identifying vulnerabilities in real-world LLM implementations. This paper presents a detailed threat model and provides a systematization of knowledge (SoK) of red-teaming attacks on LLMs. We develop a taxonomy of attacks based on the stages of the LLM development and deployment process and extract various insights from previous research. In addition, we compile methods for defense and practical red-teaming strategies for practitioners. By delineating prominent attack motifs and shedding light on various entry points, this paper provides a framework for improving the security and robustness of LLM-based systems.

[Arxiv](https://arxiv.org/abs/2407.14937)