# 多语言大型语言模型能否缓解刻板印象偏见？

发布时间：2024年07月08日

`LLM理论` `人工智能` `语言处理`

> Do Multilingual Large Language Models Mitigate Stereotype Bias?

# 摘要

> 初步研究表明，多语言大型语言模型（LLMs）的偏差较单语言模型少。然而，关于多语言训练如何有效减少偏差，我们知之甚少。本研究通过训练六个相同规模（2.6B参数）和架构的LLMs，包括五个单语言模型（英语、德语、法语、意大利语和西班牙语）和一个涵盖这些语言的多语言模型，来深入探讨这一问题。为确保评估的准确性，我们将标准偏差测试自动翻译成五种语言，并由专家验证其翻译质量和偏差保留情况。研究结果显示，多语言训练不仅能有效减少偏差，还能提升预测准确性，优于同等条件下的单语言模型。

> While preliminary findings indicate that multilingual LLMs exhibit reduced bias compared to monolingual ones, a comprehensive understanding of the effect of multilingual training on bias mitigation, is lacking. This study addresses this gap by systematically training six LLMs of identical size (2.6B parameters) and architecture: five monolingual models (English, German, French, Italian, and Spanish) and one multilingual model trained on an equal distribution of data across these languages, all using publicly available data. To ensure robust evaluation, standard bias benchmarks were automatically translated into the five target languages and verified for both translation quality and bias preservation by human annotators. Our results consistently demonstrate that multilingual training effectively mitigates bias. Moreover, we observe that multilingual models achieve not only lower bias but also superior prediction accuracy when compared to monolingual models with the same amount of training data, model architecture, and size.

[Arxiv](https://arxiv.org/abs/2407.05740)