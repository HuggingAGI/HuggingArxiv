# 语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。

发布时间：2024年07月29日

`LLM应用` `人力资源` `人工智能`

> Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval

# 摘要

> AI招聘工具已革新了简历筛选，LLMs同样潜力巨大。但LLMs内含偏见，能否在不歧视特定群体的前提下应用，尚不明朗。本研究通过模拟求职者选拔的文档检索框架，探讨LLMs在简历筛选中的应用。我们进行简历审计，评估大规模文本嵌入（MTE）模型是否偏颇。针对九种职业，我们分析了500份简历与职位描述。结果显示，MTE模型偏爱白人名字的情况高达85.1%，女性名字仅11.1%，少数无显著差异。深入分析揭示，黑人男性在所有案例中均受歧视，反映了现实就业偏见，并证实了交叉性理论。此外，文档长度和名字频率也影响筛选。这些发现对AI就业工具的普及、公平政策和技术发展具有深远意义。

> Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1\% of cases and female-associated names in only 11.1\% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100\% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have implications for widely used AI tools that are automating employment, fairness, and tech policy.

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/mistral_flow.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/doc_retrieval.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x1.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x2.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x3.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x4.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x5.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x6.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x7.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x8.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x9.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x10.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x11.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x12.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x13.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x14.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x15.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x16.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x17.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x18.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x19.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x20.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x21.png)

![语言模型检索在简历筛选中揭示了性别、种族及交叉偏见的问题。](../../../paper_images/2407.20371/x22.png)

[Arxiv](https://arxiv.org/abs/2407.20371)