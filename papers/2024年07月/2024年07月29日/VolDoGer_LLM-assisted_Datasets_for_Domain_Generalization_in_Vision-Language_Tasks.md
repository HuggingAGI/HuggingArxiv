# VolDoGer：一款借助 LLM 辅助的视觉-语言任务领域泛化数据集

发布时间：2024年07月29日

`LLM应用` `人工智能` `计算机视觉`

> VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks

# 摘要

> 领域泛化性对深度学习模型至关重要，它影响模型在未知领域数据上的表现。尽管如此，针对视觉-语言任务的领域泛化研究仍显不足，主要瓶颈在于缺乏合适的数据集。为此，我们创新性地推出了VolDoGer——一个专为领域泛化设计的视觉-语言数据集，涵盖图像描述、视觉问答和视觉蕴涵三大任务。通过扩展LLM数据标注技术至视觉-语言领域，我们有效减轻了人工标注的负担，并成功构建了VolDoGer。借助这一数据集，我们全面评估了从微调模型到前沿多模态大型语言模型的领域泛化能力。

> Domain generalizability is a crucial aspect of a deep learning model since it determines the capability of the model to perform well on data from unseen domains. However, research on the domain generalizability of deep learning models for vision-language tasks remains limited, primarily because of the lack of required datasets. To address these challenges, we propose VolDoGer: Vision-Language Dataset for Domain Generalization, a dedicated dataset designed for domain generalization that addresses three vision-language tasks: image captioning, visual question answering, and visual entailment. We constructed VolDoGer by extending LLM-based data annotation techniques to vision-language tasks, thereby alleviating the burden of recruiting human annotators. We evaluated the domain generalizability of various models, ranging from fine-tuned models to a recent multimodal large language model, through VolDoGer.

[Arxiv](https://arxiv.org/abs/2407.19795)