# 探究大型语言模型在不同 NLP 任务中的提示工程方法

发布时间：2024年07月17日

`LLM应用` `人工智能`

> A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks

# 摘要

> 大型语言模型 (LLM) 在众多 NLP 任务中表现出色。提示工程是提升 LLM 性能的关键，它通过设计自然语言提示来系统地激发 LLM 的知识。与传统方法不同，提示工程无需繁琐的参数调整，仅依赖 LLM 的内置知识。这使得即使非专业人士也能通过简单的对话或提示设计来利用 LLM。近两年，提示工程备受瞩目，研究者们开发了多种技术来优化信息提取。本文综述了 44 篇论文，涵盖 39 种提示方法和 29 种 NLP 任务，详细分析了这些方法在不同数据集上的表现，并探讨了未来可能的 SoTA。

> Large language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract LLMs' knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper, we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.

[Arxiv](https://arxiv.org/abs/2407.12994)