# Meltemi：希腊语领域首个开放的大型语言模型

发布时间：2024年07月30日

`LLM应用` `人工智能` `语言处理`

> Meltemi: The first open Large Language Model for Greek

# 摘要

> 我们详细介绍了首个希腊语开源大型语言模型Meltemi 7B的研发及其功能。该模型拥有70亿参数，基于400亿希腊语词汇的庞大数据集进行训练。在开发过程中，我们通过持续的希腊语语料库预训练对Mistral进行了调整。Meltemi 7B的信息更新至2023年9月。同时，我们精心翻译并整理了一个希腊语指令集，用于优化名为Meltemi 7B Instruct的聊天模型，特别关注了内容的安全性和准确性。所有模型均在广泛的评估数据集上得到验证，并提供了使用示例。Meltemi 7B及其指令版模型均可通过Apache 2.0许可证在Hugging Face平台获取。

> We describe the development and capabilities of Meltemi 7B, the first open Large Language Model for the Greek language. Meltemi 7B has 7 billion parameters and is trained on a 40 billion token Greek corpus. For the development of Meltemi 7B, we adapt Mistral, by continuous pretraining on the Greek Corpus. Meltemi 7B contains up-to-date information up to September 2023. Furthermore, we have translated and curated a Greek instruction corpus, which has been used for the instruction-tuning of a chat model, named Meltemi 7B Instruct. Special care has been given to the alignment and the removal of toxic content for the Meltemi 7B Instruct. The developed models are evaluated on a broad set of collected evaluation corpora, and examples of prompts and responses are presented. Both Meltemi 7B and Meltemi 7B Instruct are available at https://huggingface.co/ilsp under the Apache 2.0 license.

[Arxiv](https://arxiv.org/abs/2407.20743)