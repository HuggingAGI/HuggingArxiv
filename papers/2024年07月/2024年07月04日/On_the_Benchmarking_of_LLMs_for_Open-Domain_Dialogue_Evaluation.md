# 大型语言模型在开放域对话评估中的基准测试研究

发布时间：2024年07月04日

`LLM应用` `人工智能`

> On the Benchmarking of LLMs for Open-Domain Dialogue Evaluation

# 摘要

> LLM 在 NLP 任务中表现卓越，尤其在自动开放域对话评估中，它们与人类评估共同构成了评估的核心。但现有基准多依赖过时数据集，仅评估流畅性和相关性，未能全面反映现代聊天机器人的真实能力。本文深入分析了这些基准的不足，并通过 SODA 数据集的实验表明，即使是先进的 LLM 评估器如 GPT-4，也难以准确识别当前聊天机器人对话中的缺陷。

> Large Language Models (LLMs) have showcased remarkable capabilities in various Natural Language Processing tasks. For automatic open-domain dialogue evaluation in particular, LLMs have been seamlessly integrated into evaluation frameworks, and together with human evaluation, compose the backbone of most evaluations. However, existing evaluation benchmarks often rely on outdated datasets and evaluate aspects like Fluency and Relevance, which fail to adequately capture the capabilities and limitations of state-of-the-art chatbot models.
  This paper critically examines current evaluation benchmarks, highlighting that the use of older response generators and quality aspects fail to accurately reflect modern chatbot capabilities. A small annotation experiment on a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as GPT-4 struggle to detect actual deficiencies in dialogues generated by current LLM chatbots.

![大型语言模型在开放域对话评估中的基准测试研究](../../../paper_images/2407.03841/x1.png)

![大型语言模型在开放域对话评估中的基准测试研究](../../../paper_images/2407.03841/x2.png)

![大型语言模型在开放域对话评估中的基准测试研究](../../../paper_images/2407.03841/x3.png)

![大型语言模型在开放域对话评估中的基准测试研究](../../../paper_images/2407.03841/x4.png)

[Arxiv](https://arxiv.org/abs/2407.03841)