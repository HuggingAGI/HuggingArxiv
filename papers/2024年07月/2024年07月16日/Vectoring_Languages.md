# 语言的向量化

发布时间：2024年07月16日

`LLM理论` `科学研究` `语言学`

> Vectoring Languages

# 摘要

> 大型语言模型 (LLM) 的最新突破已引起全球瞩目，研究步伐随之加快。尽管哲学家和心理学家多年来致力于语言结构的研究，却难以从 LLM 的突破中直接获益。本文提出了一种新颖的语言结构，深入揭示了语言模型的内在机制，并证明其在捕捉语言多样性方面超越了传统方法。我们通过线性代数的类比强化了这一观点的理论基础，并探讨了其与现有语言模型设计哲学的差异。最终，我们探讨了这一视角如何指引我们探索可能加速科学进步的新研究方向。

> Recent breakthroughs in large language models (LLM) have stirred up global attention, and the research has been accelerating non-stop since then. Philosophers and psychologists have also been researching the structure of language for decades, but they are having a hard time finding a theory that directly benefits from the breakthroughs of LLMs. In this article, we propose a novel structure of language that reflects well on the mechanisms behind language models and go on to show that this structure is also better at capturing the diverse nature of language compared to previous methods. An analogy of linear algebra is adapted to strengthen the basis of this perspective. We further argue about the difference between this perspective and the design philosophy for current language models. Lastly, we discuss how this perspective can lead us to research directions that may accelerate the improvements of science fastest.

[Arxiv](https://arxiv.org/abs/2407.11766)