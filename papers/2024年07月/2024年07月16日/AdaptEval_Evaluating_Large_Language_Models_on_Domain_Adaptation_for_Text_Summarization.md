# AdaptEval：聚焦于评估大型语言模型在文本摘要领域适应性方面的表现。

发布时间：2024年07月16日

`LLM应用` `人工智能`

> AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization

# 摘要

> 尽管 LLM 在抽象摘要任务上取得了进步，但关于它们适应不同领域的能力的研究尚显不足。我们在微调和上下文学习两种模式下，对多种 LLM 在跨领域摘要任务上的适应性进行了评估。此外，我们推出了首个领域适应评估工具 AdaptEval，它包含一个领域基准和一系列分析指标，助力领域适应性的深入研究。研究结果显示，LLM 在上下文学习模式下，无论规模大小，其性能均表现出色。

> Despite the advances in the abstractive summarization task using Large Language Models (LLM), there is a lack of research that asses their abilities to easily adapt to different domains. We evaluate the domain adaptation abilities of a wide range of LLMs on the summarization task across various domains in both fine-tuning and in-context learning settings. We also present AdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a domain benchmark and a set of metrics to facilitate the analysis of domain adaptation. Our results demonstrate that LLMs exhibit comparable performance in the in-context learning setting, regardless of their parameter scale.

[Arxiv](https://arxiv.org/abs/2407.11591)