# PrefCLM：利用众包大型语言模型提升基于偏好的强化学习效果

发布时间：2024年07月11日

`LLM应用` `机器人` `人机交互`

> PrefCLM: Enhancing Preference-based Reinforcement Learning with Crowdsourced Large Language Models

# 摘要

> 基于偏好的强化学习（PbRL）正成为一种通过人类比较反馈教授机器人的创新方法，避免了复杂的奖励设计。然而，现有方法对大量反馈的依赖往往转向脚本教师生成的合成反馈，这再次需要精细的奖励设计，并难以适应人类-机器人交互（HRI）中用户对任务的独特期望。为此，我们提出了PrefCLM框架，利用众包大型语言模型（LLMs）作为模拟教师，通过Dempster-Shafer理论融合多个LLM代理的偏好，有效整合多样性与集体智慧。同时，我们设计了包含人在回路的流程，以用户交互反馈为基础进行集体优化。实验表明，PrefCLM在通用RL任务中与传统脚本教师相比表现优异，并能促进更自然高效的行为。一项实际用户研究（N=10）进一步验证了其根据用户个性化需求定制机器人行为的能力，显著提升了HRI场景中的用户满意度。

> Preference-based reinforcement learning (PbRL) is emerging as a promising approach to teaching robots through human comparative feedback, sidestepping the need for complex reward engineering. However, the substantial volume of feedback required in existing PbRL methods often lead to reliance on synthetic feedback generated by scripted teachers. This approach necessitates intricate reward engineering again and struggles to adapt to the nuanced preferences particular to human-robot interaction (HRI) scenarios, where users may have unique expectations toward the same task. To address these challenges, we introduce PrefCLM, a novel framework that utilizes crowdsourced large language models (LLMs) as simulated teachers in PbRL. We utilize Dempster-Shafer Theory to fuse individual preferences from multiple LLM agents at the score level, efficiently leveraging their diversity and collective intelligence. We also introduce a human-in-the-loop pipeline that facilitates collective refinements based on user interactive feedback. Experimental results across various general RL tasks show that PrefCLM achieves competitive performance compared to traditional scripted teachers and excels in facilitating more more natural and efficient behaviors. A real-world user study (N=10) further demonstrates its capability to tailor robot behaviors to individual user preferences, significantly enhancing user satisfaction in HRI scenarios.

[Arxiv](https://arxiv.org/abs/2407.08213)