# 大型多模态模型（LMM）为视觉障碍者提供的辅助手段正不断涌现，这些实践为设计领域带来了新的启示。

发布时间：2024年07月11日

`Agent` `辅助技术` `视觉障碍`

> Emerging Practices for Large Multimodal Model (LMM) Assistance for People with Visual Impairments: Implications for Design

# 摘要

> 视觉障碍者借助 AI 辅助工具，通过非视觉方式感知世界，获取视觉信息的文本描述。最新 AI 工具如 Be My AI，能更好地理解自然语言查询，并以可听文本描述场景，但其对视觉障碍用户的实际效用尚待深入研究。本文通过与 14 名视觉障碍用户的合作，揭示了他们如何自然地适应这些工具，不仅简化复杂互动，更扩展了认知边界，仿佛认知与视觉信息融为一体。尽管这些工具目前缺乏目标导向性，用户却能灵活适应，广泛应用其功能。这些发现启发我们设计更具目标导向性、实时性和可靠性的 AI 辅助技术。

> People with visual impairments perceive their environment non-visually and often use AI-powered assistive tools to obtain textual descriptions of visual information. Recent large vision-language model-based AI-powered tools like Be My AI are more capable of understanding users' inquiries in natural language and describing the scene in audible text; however, the extent to which these tools are useful to visually impaired users is currently understudied. This paper aims to fill this gap. Our study with 14 visually impaired users reveals that they are adapting these tools organically -- not only can these tools facilitate complex interactions in household, spatial, and social contexts, but they also act as an extension of users' cognition, as if the cognition were distributed in the visual information. We also found that although the tools are currently not goal-oriented, users accommodate this limitation and embrace the tools' capabilities for broader use. These findings enable us to envision design implications for creating more goal-oriented, real-time processing, and reliable AI-powered assistive technology.

[Arxiv](https://arxiv.org/abs/2407.08882)