# 探讨大型语言模型的信心归属问题

发布时间：2024年07月11日

`LLM理论` `人工智能`

> On the attribution of confidence to large language models

# 摘要

> 在LLM评估的实证研究中，对LLM的信念归属颇为常见，但其理论基础尚不明朗。我们提出三点论断：首先，从语义角度看，LLM信念归属应被理解为科学家对LLM信念事实的真实信念表达；其次，从形而上学角度，LLM信念的存在虽证据不足，但至少是合理的；最后，从认识论角度，实证文献中的LLM信念归属面临实质性的怀疑，因为评估LLM信念的实验方法可能并不准确，导致这些归属可能普遍失真。

> Credences are mental states corresponding to degrees of confidence in propositions. Attribution of credences to Large Language Models (LLMs) is commonplace in the empirical literature on LLM evaluation. Yet the theoretical basis for LLM credence attribution is unclear. We defend three claims. First, our semantic claim is that LLM credence attributions are (at least in general) correctly interpreted literally, as expressing truth-apt beliefs on the part of scientists that purport to describe facts about LLM credences. Second, our metaphysical claim is that the existence of LLM credences is at least plausible, although current evidence is inconclusive. Third, our epistemic claim is that LLM credence attributions made in the empirical literature on LLM evaluation are subject to non-trivial sceptical concerns. It is a distinct possibility that even if LLMs have credences, LLM credence attributions are generally false because the experimental techniques used to assess LLM credences are not truth-tracking.

[Arxiv](https://arxiv.org/abs/2407.08388)