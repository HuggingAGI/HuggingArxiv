# LLASP：为 Answer Set Programming 任务微调大型语言模型

发布时间：2024年07月26日

`LLM应用` `软件开发` `人工智能`

> LLASP: Fine-tuning Large Language Models for Answer Set Programming

# 摘要

> 近期，大型语言模型 (LLM) 在众多自然语言处理任务中展现了其潜力，包括代码生成。尽管在适应 LLM 生成多种命令式编程语言代码方面取得了重大进展，但在声明式编程如 Answer Set Programming (ASP) 的应用上仍显不足。本文探索了 LLM 在 ASP 代码生成的能力。我们评估了数个顶尖 LLM，尽管它们在参数、数据和计算资源上强大，但在生成正确 ASP 程序上表现不佳。为此，我们提出了 LLASP，一个专为编码基本 ASP 模式而微调的轻量级模型。我们创建了一个包含多样基本问题规范的数据集。实验显示，LLASP 生成的 ASP 程序质量卓越，不仅优于未微调模型，也超越了多数 LLM 候选模型，尤其在语义层面。所有实验代码和数据已公开，详见 https://anonymous.4open.science/r/LLASP-D86C/。

> Recently, Large Language Models (LLMs) have showcased their potential in various natural language processing tasks, including code generation. However, while significant progress has been made in adapting LLMs to generate code for several imperative programming languages and tasks, there remains a notable gap in their application to declarative formalisms, such as Answer Set Programming (ASP). In this paper, we move a step towards exploring the capabilities of LLMs for ASP code generation. First, we perform a systematic evaluation of several state-of-the-art LLMs. Despite their power in terms of number of parameters, training data and computational resources, empirical results demonstrate inadequate performances in generating correct ASP programs. Therefore, we propose LLASP, a fine-tuned lightweight model specifically trained to encode fundamental ASP program patterns. To this aim, we create an ad-hoc dataset covering a wide variety of fundamental problem specifications that can be encoded in ASP. Our experiments demonstrate that the quality of ASP programs generated by LLASP is remarkable. This holds true not only when compared to the non-fine-tuned counterpart but also when compared to the majority of eager LLM candidates, particularly from a semantic perspective. All the code and data used to perform the experiments are publicly available at https://anonymous.4open.science/r/LLASP-D86C/.

[Arxiv](https://arxiv.org/abs/2407.18723)