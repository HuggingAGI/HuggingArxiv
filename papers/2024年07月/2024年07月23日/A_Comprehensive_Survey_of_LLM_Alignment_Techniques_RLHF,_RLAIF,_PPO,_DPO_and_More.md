# 全面探索 LLM 对齐技术：涵盖 RLHF、RLAIF、PPO、DPO 等方法

发布时间：2024年07月23日

`LLM理论` `人工智能`

> A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More

# 摘要

> 随着自监督学习的进步和预训练语料库中数万亿标记的可用性，结合指令微调和大型Transformer的发展，LLMs现在能够生成既事实又连贯的响应。然而，训练数据的质量参差不齐，可能导致生成不理想的响应，这是一个重大挑战。过去两年中，从多个角度提出了多种方法来提升LLMs，尤其是在与人类期望对齐方面。尽管如此，目前还缺乏一篇全面梳理这些方法的调查论文。本研究旨在填补这一空白，通过将相关论文分类并详细解析每种对齐方法，帮助读者深入理解该领域的最新进展。

> With advancements in self-supervised learning, the availability of trillions tokens in a pre-training corpus, instruction fine-tuning, and the development of large Transformers with billions of parameters, large language models (LLMs) are now capable of generating factual and coherent responses to human queries. However, the mixed quality of training data can lead to the generation of undesired responses, presenting a significant challenge. Over the past two years, various methods have been proposed from different perspectives to enhance LLMs, particularly in aligning them with human expectation. Despite these efforts, there has not been a comprehensive survey paper that categorizes and details these approaches. In this work, we aim to address this gap by categorizing these papers into distinct topics and providing detailed explanations of each alignment method, thereby helping readers gain a thorough understanding of the current state of the field.

[Arxiv](https://arxiv.org/abs/2407.16216)