# 图解大型语言模型的越狱攻击：基于分析的方法

发布时间：2024年07月23日

`LLM应用` `网络安全` `人工智能`

> Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models

# 摘要

> 随着大型语言模型 (LLM) 的迅猛发展，其在多任务中的生成能力令人瞩目。但这些模型在面对越狱攻击时仍显脆弱。本文深入探讨了越狱攻击的边界，并提出了基于分析的越狱方法 (ABJ)，利用 LLM 的分析推理能力揭示其潜在漏洞。实验表明，ABJ 在 GPT-4-turbo-0409 上达到了 94.8% 的攻击成功率和 1.06 的攻击效率，展现了卓越的攻击效果。此研究凸显了加强 LLM 安全性的紧迫性，以防范潜在风险。

> The rapid development of Large Language Models (LLMs) has brought remarkable generative capabilities across diverse tasks. However, despite the impressive achievements, these models still have numerous security vulnerabilities, particularly when faced with jailbreak attacks. Therefore, by investigating jailbreak attacks, we can uncover hidden weaknesses in LLMs and guide us in developing more robust defense mechanisms to fortify their security. In this paper, we further explore the boundary of jailbreak attacks on LLMs and propose Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes advantage of LLMs' growing analyzing and reasoning capability and reveals their underlying vulnerabilities when facing analysis-based tasks. We conduct a detailed evaluation of ABJ across various open-source and closed-source LLMs, which achieves 94.8% Attack Success Rate (ASR) and 1.06 Attack Efficiency (AE) on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and efficiency. Our research highlights the importance of prioritizing and enhancing the safety of LLMs to mitigate the risks of misuse.

![图解大型语言模型的越狱攻击：基于分析的方法](../../../paper_images/2407.16205/p1.png)

![图解大型语言模型的越狱攻击：基于分析的方法](../../../paper_images/2407.16205/p2.png)

![图解大型语言模型的越狱攻击：基于分析的方法](../../../paper_images/2407.16205/chart.png)

[Arxiv](https://arxiv.org/abs/2407.16205)