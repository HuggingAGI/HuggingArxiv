# Arondight：借助自动生成的多模态越狱提示，对大型视觉语言模型进行红队挑战。

发布时间：2024年07月21日

`LLM应用` `安全和伦理` `人工智能`

> Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts

# 摘要

> 大型视觉语言模型 (VLMs) 不仅扩展了大型语言模型 (LLMs) 的感知能力，还为 LLM 应用开辟了新天地。然而，这些进步也带来了严峻的安全和伦理挑战，尤其是在有害内容生成方面。尽管 LLMs 已通过红队框架进行了深入的安全评估，但 VLMs 在这方面仍显不足。为此，我们推出了 Arondight，一个专为 VLMs 量身打造的标准化红队框架。Arondight 专注于解决从 LLMs 向 VLMs 过渡时遇到的视觉模态缺失和多样性不足问题。我们的框架采用自动化多模态越狱攻击，结合红队 VLM 生成的视觉提示和红队 LLM 在强化学习代理引导下生成的文本提示。此外，我们引入了熵奖励和新颖性奖励，以激励 RL 代理引导红队 LLM 生成更多样化、新颖的测试案例，从而全面提升 VLM 的安全评估。我们对十个前沿 VLMs 的评估揭示了其在生成有毒图像和多模态提示方面的显著安全漏洞。特别是，Arondight 在 OpenAI 定义的十四个禁止场景中，对 GPT-4 的平均攻击成功率高达 84.5%。我们还根据安全级别对现有 VLMs 进行了分类，并提供了针对性的强化建议。我们的多模态提示数据集和红队代码将在获得伦理委员会批准后公开。请注意，本文包含有害模型响应。

> Large Vision Language Models (VLMs) extend and enhance the perceptual abilities of Large Language Models (LLMs). Despite offering new possibilities for LLM applications, these advancements raise significant security and ethical concerns, particularly regarding the generation of harmful content. While LLMs have undergone extensive security evaluations with the aid of red teaming frameworks, VLMs currently lack a well-developed one. To fill this gap, we introduce Arondight, a standardized red team framework tailored specifically for VLMs. Arondight is dedicated to resolving issues related to the absence of visual modality and inadequate diversity encountered when transitioning existing red teaming methodologies from LLMs to VLMs. Our framework features an automated multi-modal jailbreak attack, wherein visual jailbreak prompts are produced by a red team VLM, and textual prompts are generated by a red team LLM guided by a reinforcement learning agent. To enhance the comprehensiveness of VLM security evaluation, we integrate entropy bonuses and novelty reward metrics. These elements incentivize the RL agent to guide the red team LLM in creating a wider array of diverse and previously unseen test cases. Our evaluation of ten cutting-edge VLMs exposes significant security vulnerabilities, particularly in generating toxic images and aligning multi-modal prompts. In particular, our Arondight achieves an average attack success rate of 84.5\% on GPT-4 in all fourteen prohibited scenarios defined by OpenAI in terms of generating toxic text. For a clearer comparison, we also categorize existing VLMs based on their safety levels and provide corresponding reinforcement recommendations. Our multimodal prompt dataset and red team code will be released after ethics committee approval. CONTENT WARNING: THIS PAPER CONTAINS HARMFUL MODEL RESPONSES.

![Arondight：借助自动生成的多模态越狱提示，对大型视觉语言模型进行红队挑战。](../../../paper_images/2407.15050/x1.png)

![Arondight：借助自动生成的多模态越狱提示，对大型视觉语言模型进行红队挑战。](../../../paper_images/2407.15050/x2.png)

![Arondight：借助自动生成的多模态越狱提示，对大型视觉语言模型进行红队挑战。](../../../paper_images/2407.15050/x3.png)

![Arondight：借助自动生成的多模态越狱提示，对大型视觉语言模型进行红队挑战。](../../../paper_images/2407.15050/x4.png)

![Arondight：借助自动生成的多模态越狱提示，对大型视觉语言模型进行红队挑战。](../../../paper_images/2407.15050/x5.png)

![Arondight：借助自动生成的多模态越狱提示，对大型视觉语言模型进行红队挑战。](../../../paper_images/2407.15050/x6.png)

![Arondight：借助自动生成的多模态越狱提示，对大型视觉语言模型进行红队挑战。](../../../paper_images/2407.15050/x7.png)

[Arxiv](https://arxiv.org/abs/2407.15050)