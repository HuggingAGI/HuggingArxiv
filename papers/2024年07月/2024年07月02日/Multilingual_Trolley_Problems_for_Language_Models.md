# 语言模型面临的多语言电车难题

发布时间：2024年07月02日

`LLM应用` `人工智能伦理` `跨文化研究`

> Multilingual Trolley Problems for Language Models

# 摘要

> 随着 LLM 在现实应用中的普及，探究其在道德困境中的决策变得至关重要。借鉴“道德机器实验”这一跨文化大型研究，我们为 LLM 设计了相同的道德选择挑战。通过将 1K 个道德困境情景翻译成 100 多种语言，我们揭示了 LLM 在各语言中的偏好，并对比了其与 4000 万人类道德判断的异同。结果显示，LLM 在英语、韩语、匈牙利语和中国语中与人类偏好更为契合，而在印地语和索马里语中则稍显偏离。进一步分析 LLM 的道德选择解释，我们发现公平性是 GPT-4 的主要决策依据，而 GPT-3 则更倾向于功利主义。此外，我们还识别出道德决策中存在的“语言不平等”现象，即模型在不同语言中的发展水平差异。

> As large language models (LLMs) are deployed in more and more real-world situations, it is crucial to understand their decision-making when faced with moral dilemmas. Inspired by a large-scale cross-cultural study of human moral preferences, "The Moral Machine Experiment", we set up the same set of moral choices for LLMs. We translate 1K vignettes of moral dilemmas, parametrically varied across key axes, into 100+ languages, and reveal the preferences of LLMs in each of these languages. We then compare the responses of LLMs to that of human speakers of those languages, harnessing a dataset of 40 million human moral judgments. We discover that LLMs are more aligned with human preferences in languages such as English, Korean, Hungarian, and Chinese, but less aligned in languages such as Hindi and Somali (in Africa). Moreover, we characterize the explanations LLMs give for their moral choices and find that fairness is the most dominant supporting reason behind GPT-4's decisions and utilitarianism by GPT-3. We also discover "language inequality" (which we define as the model's different development levels in different languages) in a series of meta-properties of moral decision making.

[Arxiv](https://arxiv.org/abs/2407.02273)