# 通过多模态大型语言模型提升视觉叙事效果

发布时间：2024年07月02日

`LLM应用`

> Improving Visual Storytelling with Multimodal Large Language Models

# 摘要

> 视觉叙事，这一新兴领域融合图像与叙事，旨在编织引人入胜、情境丰富的故事。然而，视觉与文本信息的复杂对齐，使得创造连贯且情感共鸣的视觉故事颇具挑战。本文创新性地结合大型语言模型（LLMs）与视觉-语言模型（LVLMs），并引入指令调优，以应对这一难题。我们构建了一个包含多样化视觉故事的详尽数据集，辅以多模态元素。通过监督学习与强化学习的协同微调，我们的模型叙事生成能力得以显著提升。GPT-4的定量评估与人类定性评价双重验证，我们的方法在叙事连贯性、相关性、情感深度及整体质量上均超越现有模型。这一成果不仅彰显了指令调优的效力，更揭示了LLMs/LVLMs在视觉叙事领域的广阔前景。

> Visual storytelling is an emerging field that combines images and narratives to create engaging and contextually rich stories. Despite its potential, generating coherent and emotionally resonant visual stories remains challenging due to the complexity of aligning visual and textual information. This paper presents a novel approach leveraging large language models (LLMs) and large vision-language models (LVLMs) combined with instruction tuning to address these challenges. We introduce a new dataset comprising diverse visual stories, annotated with detailed captions and multimodal elements. Our method employs a combination of supervised and reinforcement learning to fine-tune the model, enhancing its narrative generation capabilities. Quantitative evaluations using GPT-4 and qualitative human assessments demonstrate that our approach significantly outperforms existing models, achieving higher scores in narrative coherence, relevance, emotional depth, and overall quality. The results underscore the effectiveness of instruction tuning and the potential of LLMs/LVLMs in advancing visual storytelling.

[Arxiv](https://arxiv.org/abs/2407.02586)