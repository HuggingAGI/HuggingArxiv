# 探索基础模型对个体、社会及生物圈的影响

发布时间：2024年07月24日

`LLM理论` `社会伦理` `人工智能治理`

> Mapping the individual, social, and biospheric impacts of Foundation Models

# 摘要

> 随着基础模型、大型语言模型和生成式AI的迅速普及与商业化，一系列新兴研究揭示了这些技术在社会中的广泛影响。这些研究涵盖了从歧视性、虚假和有害内容的产生，到隐私侵犯和资源不公剥削等多个方面。然而，在英国的AI安全峰会和G7的广岛进程等全球北方的重要AI治理倡议中，这些影响并未得到充分关注，它们主导了国际AI治理对话。尽管存在大量算法伤害的警示和证据，AI治理讨论仍过分侧重于安全技术和全球性风险等技术议题，忽视了AI粗放工业化带来的紧迫社会伦理挑战。为此，本文提出一个批判性框架，深入探讨基础模型和生成式AI的社会、政治和环境影响。我们梳理了14类风险和伤害，并按其对个人、社会和生物圈的影响进行分类。我们认为，这一创新分类法为应对基础模型及其应用的紧迫负面影响提供了综合视角。最终，我们建议如何运用这一分类法指导技术和规范干预，推动负责任AI的发展。

> Responding to the rapid roll-out and large-scale commercialization of foundation models, large language models, and generative AI, an emerging body of work is shedding light on the myriad impacts these technologies are having across society. Such research is expansive, ranging from the production of discriminatory, fake and toxic outputs, and privacy and copyright violations, to the unjust extraction of labor and natural resources. The same has not been the case in some of the most prominent AI governance initiatives in the global north like the UK's AI Safety Summit and the G7's Hiroshima process, which have influenced much of the international dialogue around AI governance. Despite the wealth of cautionary tales and evidence of algorithmic harm, there has been an ongoing over-emphasis within the AI governance discourse on technical matters of safety and global catastrophic or existential risks. This narrowed focus has tended to draw attention away from very pressing social and ethical challenges posed by the current brute-force industrialization of AI applications. To address such a visibility gap between real-world consequences and speculative risks, this paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI. We identify 14 categories of risks and harms and map them according to their individual, social, and biospheric impacts. We argue that this novel typology offers an integrative perspective to address the most urgent negative impacts of foundation models and their downstream applications. We conclude with recommendations on how this typology could be used to inform technical and normative interventions to advance responsible AI.

[Arxiv](https://arxiv.org/abs/2407.17129)