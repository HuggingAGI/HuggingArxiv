# 语言模型能否评判人类文笔？本研究聚焦于教育领域，探讨其对韩国学生作文的评估能力。

发布时间：2024年07月24日

`LLM应用` `写作评估`

> Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education

# 摘要

> 基于大型语言模型的评估流程已成功应用于机器生成文本的评估，而将其应用于人类写作评估，虽具挑战，却能为教育领域带来直接反馈，助力写作技能提升。本研究聚焦于 LLM 对人类写作的评估能力，收集了32名韩国学生的100篇各类写作样本，利用 GPT-4-Turbo 进行多维度评估。结果显示，LLM 在语法和流畅性评估上表现稳健，但在其他方面仍有提升空间。我们公开了相关数据集及评估反馈，以供进一步研究。

> Large language model (LLM)-based evaluation pipelines have demonstrated their capability to robustly evaluate machine-generated text. Extending this methodology to assess human-written text could significantly benefit educational settings by providing direct feedback to enhance writing skills, although this application is not straightforward. In this paper, we investigate whether LLMs can effectively assess human-written text for educational purposes. We collected 100 texts from 32 Korean students across 15 types of writing and employed GPT-4-Turbo to evaluate them using grammaticality, fluency, coherence, consistency, and relevance as criteria. Our analyses indicate that LLM evaluators can reliably assess grammaticality and fluency, as well as more objective types of writing, though they struggle with other criteria and types of writing. We publicly release our dataset and feedback.

[Arxiv](https://arxiv.org/abs/2407.17022)