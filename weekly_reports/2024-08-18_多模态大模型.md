# 本周多模态大模型论文汇总 20240818
## 本研究提出一种无需训练的视频车牌追踪与识别框架，仅需一次学习即可实现。
发布时间：2024年08月11日

> A Training-Free Framework for Video License Plate Tracking and Recognition with Only One-Shot
>
> 传统车牌识别模型受限于封闭数据集，难以应对各地车牌格式的多样性。而大规模预训练模型则展现出卓越的泛化能力，支持少样本和零样本学习。为此，我们推出了OneShotLP框架，该框架无需训练，利用先进模型实现视频车牌检测与识别。方法从视频首帧的车牌位置出发，通过点跟踪模块在后续帧中持续追踪，形成提示轨迹。这些提示被送入分割模块，利用大型分割模型生成车牌区域的局部掩码。随后，多模态大型语言模型对分割区域进行处理，确保车牌识别的准确性。OneShotLP不仅无需大量训练数据，还能适应多种车牌样式，展现出显著优势。在UFPR-ALPR和SSIG-SegPlate数据集上的实验结果表明，我们的方法在准确性上超越了传统方法，凸显了预训练模型在智能交通系统中多样化应用的潜力。代码已公开，详见https://github.com/Dinghaoxuan/OneShotLP。
>
> https://arxiv.org/abs/2408.05729

![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.05729/track014101.png)
![](https://raw.githubusercontent.com/HuggingAGI/HuggingArxiv/main/paper_images/2408.05729/track014108.png)

## 基于视觉语言模型的多智能体协同规划
发布时间：2024年08月10日

> Multi-agent Planning using Visual Language Models
>
> 大型语言模型 (LLM) 和视觉语言模型 (VLM) 因其跨领域的性能提升和应用而备受瞩目。然而，在需要深入理解问题领域时，这些模型可能产生错误结果，尤其是在同时进行规划和感知时，因难以整合多模态信息而受困。为解决此问题，我们提出了一种无需特定数据结构的多智能体实体任务规划架构，仅使用环境图像并借助常识知识处理自由领域。此外，我们引入了全自动评估程序 PG2S，以更精准地评估计划质量。通过 ALFRED 数据集的验证，我们将 PG2S 与 KAS 指标对比，进一步评估了生成计划的质量。
>
> https://arxiv.org/abs/2408.05478

## mPLUG-Owl3：探索多模态大型语言模型中的长图像序列理解
发布时间：2024年08月13日

> mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models
>
> 多模态大型语言模型 (MLLMs) 在处理单图像任务方面表现出色，但在长图像序列建模上仍面临挑战。为此，我们推出了多功能模型 mPLUG-Owl3，它通过整合检索的图像-文本知识、交错的图像-文本和长视频，提升了长序列理解能力。我们设计了超注意力块，有效融合视觉与语言，优化多图像场景处理。实验显示，mPLUG-Owl3 在同类模型中性能领先。我们还引入了“干扰抵抗”评估，测试模型在复杂环境下的专注力。mPLUG-Owl3 在处理超长视觉序列时表现卓越，有望推动多模态大型语言模型的发展。
>
> https://arxiv.org/abs/2408.04840

<hr />

- 论文原文: [https://arxiv.org/abs/2408.04840](https://arxiv.org/abs/2408.04840)
- 获取更多最新Arxiv论文更新: [https://github.com/HuggingAGI/HuggingArxiv](https://github.com/HuggingAGI/HuggingArxiv)!
- 加入社群，+v: iamxxn886
- 点击公众号菜单加入讨论
![](https://raw.githubusercontent.com/HuggingAGI/wx_assets/main/2024/07/31/1722434818326-94339e92-22f1-4472-9d27-fed232f70b5d.jpeg)